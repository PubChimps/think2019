{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All notebooks as a single notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## PART 1 dataprep\n",
    "reformat dataset for use with TensorFlow and API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygithub\n",
      "  Downloading https://files.pythonhosted.org/packages/6d/13/6cf2d64c1de3a1d4892d69e4bea2039f920f373bc06b8962940ff5cde8bd/PyGithub-1.43.5.tar.gz (2.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.9MB 352kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement not upgraded as not directly required: requests>=2.14.0 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from pygithub)\n",
      "Collecting pyjwt (from pygithub)\n",
      "  Downloading https://files.pythonhosted.org/packages/87/8b/6a9f14b5f781697e51259d81657e6048fd31a113229cf346880bb7545565/PyJWT-1.7.1-py2.py3-none-any.whl\n",
      "Collecting Deprecated (from pygithub)\n",
      "  Downloading https://files.pythonhosted.org/packages/a9/cb/c1a39ee51e3042df8b284e22c9c440ffad1c25f451bddd4bf9a8dc17cd75/Deprecated-1.2.4-py2.py3-none-any.whl\n",
      "Requirement not upgraded as not directly required: chardet<3.1.0,>=3.0.2 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests>=2.14.0->pygithub)\n",
      "Requirement not upgraded as not directly required: idna<2.7,>=2.5 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests>=2.14.0->pygithub)\n",
      "Requirement not upgraded as not directly required: urllib3<1.23,>=1.21.1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests>=2.14.0->pygithub)\n",
      "Requirement not upgraded as not directly required: certifi>=2017.4.17 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from requests>=2.14.0->pygithub)\n",
      "Requirement not upgraded as not directly required: wrapt<2,>=1 in /opt/conda/envs/DSX-Python35/lib/python3.5/site-packages (from Deprecated->pygithub)\n",
      "Building wheels for collected packages: pygithub\n",
      "  Running setup.py bdist_wheel for pygithub ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/dsxuser/.cache/pip/wheels/72/8c/d2/c007a8333850b70db8df4ce8d34e073d75c190708a8a71e227\n",
      "Successfully built pygithub\n",
      "Installing collected packages: pyjwt, Deprecated, pygithub\n",
      "Successfully installed Deprecated-1.2.4 pygithub-1.43.5 pyjwt-1.7.1\n",
      "--2019-02-13 20:35:23--  https://github.com/PubChimps/think2019/blob/master/data/cloudclassifier.npy.zip?raw=true\n",
      "Resolving github.com (github.com)... 192.30.253.113, 192.30.253.112\n",
      "Connecting to github.com (github.com)|192.30.253.113|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github.com/PubChimps/think2019/raw/master/data/cloudclassifier.npy.zip [following]\n",
      "--2019-02-13 20:35:23--  https://github.com/PubChimps/think2019/raw/master/data/cloudclassifier.npy.zip\n",
      "Reusing existing connection to github.com:443.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/PubChimps/think2019/master/data/cloudclassifier.npy.zip [following]\n",
      "--2019-02-13 20:35:23--  https://raw.githubusercontent.com/PubChimps/think2019/master/data/cloudclassifier.npy.zip\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1611458 (1.5M) [application/zip]\n",
      "Saving to: ‘cloudclassifier.npy.zip?raw=true’\n",
      "\n",
      "100%[======================================>] 1,611,458   --.-K/s   in 0.04s   \n",
      "\n",
      "2019-02-13 20:35:24 (37.8 MB/s) - ‘cloudclassifier.npy.zip?raw=true’ saved [1611458/1611458]\n",
      "\n",
      "Archive:  clouddata.npz.zip\n",
      "  inflating: cloudclassifier.npy     \n"
     ]
    }
   ],
   "source": [
    "!pip install pygithub\n",
    "!pip install watson_developer_cloud\n",
    "!wget https://github.com/PubChimps/think2019/blob/master/data/cloudclassifier.npy.zip?raw=true\n",
    "!mv cloud* clouddata.npz.zip\n",
    "!unzip clouddata.npz.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/dsxuser/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import nltk \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.framework import ops\n",
    "from watson_developer_cloud import NaturalLanguageClassifierV1\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 'import subprocess print subprocess.check output . build and push.sh superradiance from scipy import sparse import numpy as np import os import pickle import matplotlib.pyplot as plt import boto3 from sagemaker import get execution role import sagemaker as sage steps 500 Number of simulated time steps N 8 Number of nuclear spins dim 2 N 1 Dimension of Hilbert space of N nuclear and 1 electronic spin Define Nuclear and Electron spin states as density matrices rhoI sparse.csr matrix 1. 0 0 shape int dim 2 int dim 2 All carbon atoms magnetically excited rhoS sparse.csr matrix 1. 1 1 shape 2 2 Electron magnetically excited sparse rho sparse.kron rhoS rhoI Build the system density matrix tempfile tmp tmp.pckl pickle.dump sparse rho open tempfile wb Upload serialized initial state to S3 resource boto3.resource s3 my bucket resource.Bucket sagemaker kessle31 subsitute this for your s3 bucket name. my bucket.upload file tempfile Key superradiance initial state init.pckl Clean up temporary files os.remove tempfile role get execution role sess sage.Session Specify the name of the ECS repo that contains your docker image. By default the image with tag latest in the repo will be utilized. imagename superradiance account sess.boto session.client sts .get caller identity Account region sess.boto session.region name image .dkr.ecr. .amazonaws.com .format account region imagename Define the training job superradiance sage.estimator.Estimator image role 1 ml.c4.2xlarge output path s3 sagemaker kessle31 superradiance output sagemaker session sess Parameters can be passed to the simulation as a dictionary superradiance.hyperparam dict N N steps steps Pass the location of the training data see above and start the job superradiance.fit s3 sagemaker kessle31 superradiance initial state import tarfile get the results from S3 results my bucket.Object superradiance output output model.tar.gz .format superradiance.latest training job.name tempfile tmp model.tar.gz results.download file tmp model.tar.gz unzip the results tar tarfile.open tempfile r gz tar.extractall path tmp load the results into the notebook out pickle.load open tmp out.pckl rb intensity out intensity I ind out I ind clean up temporary files os.remove tempfile Vizualize the results plt.scatter range steps 1 intensity I ind label Simulated System plt.scatter range steps 1 np.exp I ind N np.arange steps 1 label Classical System benchmark plt.xlabel Time a.u. plt.ylabel Intensity of emitted radiation normalized plt.legend plt.show cat container superradiance train'\n",
      " 'amazon']\n"
     ]
    }
   ],
   "source": [
    "data = np.load('./cloudclassifier.npy')\n",
    "\n",
    "#example data\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(data)\n",
    "trainset, testset = np.array(data)[:int(len(data)*.8),:], np.array(data)[int(len(data)*.8):,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build out dataset for neural network. Data needs to be tokenized and dictionary collected for bag of words encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "each data example will be transformed into a feature vector of size\n",
      "13358\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "\n",
    "labeledlines = []\n",
    "ignore_words = ['?', ',']\n",
    "\n",
    "for line in trainset:\n",
    "    text = str(re.split(r'[.,]', line[0])).replace(\"'\",\"\").replace('[','')\n",
    "    text = re.sub(r'\\b\\w{1,1}\\b', '', text)\n",
    "    w = nltk.word_tokenize(text)\n",
    "    w = [ele for ele in w if ele not in ignore_words]\n",
    "    words.extend(w)\n",
    "    labeledlines.append([w, line[1]])\n",
    "    \n",
    "testlines = []\n",
    "\n",
    "for line in testset:\n",
    "    text = str(re.split(r'[.,]', line[0])).replace(\"'\",\"\").replace('[','')\n",
    "    text = re.sub(r'\\b\\w{1,1}\\b', '', text)\n",
    "    w = nltk.word_tokenize(text)\n",
    "    w = [ele for ele in w if ele not in ignore_words]\n",
    "    words.extend(w)\n",
    "    testlines.append([w, line[1]])\n",
    "    \n",
    "\n",
    "words = list(set(words))\n",
    "\n",
    "print('each data example will be transformed into a feature vector of size')\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate feature vector via bag of words encoding and label into one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = []\n",
    "\n",
    "for line in labeledlines:\n",
    "    bag = []\n",
    "    code = line[0]\n",
    "    for w in words:\n",
    "        bag.append(code.count(w)) \n",
    "        \n",
    "    classes = [0,0,0,0]\n",
    "    if line[1] == 'amazon':\n",
    "        classes[0] = 1\n",
    "    elif line[1] == 'ibm':\n",
    "        classes[1] = 1\n",
    "    elif line[1] == 'microsoft':\n",
    "        classes[2] = 1\n",
    "    elif line[1] == 'google':\n",
    "        classes[3] = 1\n",
    "    else:\n",
    "        print(line[1])\n",
    "\n",
    "    traindata.append([bag,classes])\n",
    "    \n",
    "testdata = []\n",
    "for line in testlines:\n",
    "    bag = []\n",
    "    code = line[0]\n",
    "    for w in words:\n",
    "        bag.append(code.count(w)) \n",
    "        \n",
    "    classes = [0,0,0,0]\n",
    "    if line[1] == 'amazon':\n",
    "        classes[0] = 1\n",
    "    elif line[1] == 'ibm':\n",
    "        classes[1] = 1\n",
    "    elif line[1] == 'microsoft':\n",
    "        classes[2] = 1\n",
    "    elif line[1] == 'google':\n",
    "        classes[3] = 1\n",
    "    else:\n",
    "        print(line[1])\n",
    "\n",
    "    testdata.append([bag,classes])\n",
    "    \n",
    "x_train = np.array([row[0] for row in traindata]).T\n",
    "y_train = np.array([row[1] for row in traindata]).T\n",
    "x_test = np.array([row[0] for row in testdata]).T\n",
    "y_test = np.array([row[1] for row in testdata]).T\n",
    "\n",
    "np.save('x_train.npy',x_train)\n",
    "np.save('y_train.npy',y_train)\n",
    "np.save('x_test.npy',x_test)\n",
    "np.save('y_test.npy',y_test)\n",
    "np.save('testset.npy',testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dataset to .csv file for API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "for i in labeledlines:\n",
    "    for j in re.findall('.{1,1024}', str(i[0]).replace(',', ' ').replace(\"'\",'').replace('[','').replace(']','')):\n",
    "        d.append({'text': j, 'cloud': i[1]})\n",
    "        \n",
    "df = pd.DataFrame(d, columns = ['text', 'cloud'])\n",
    "df['text'].replace(' ', np.nan, inplace=True)\n",
    "df = df.dropna()\n",
    "df.to_csv('cloudtrainingdata.csv', header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List files via !ls \n",
    "There should be a total of eight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloudclassifier.npy  cloudtrainingdata.csv  x_test.npy\t y_test.npy\r\n",
      "clouddata.npz.zip    testset.npy\t    x_train.npy  y_train.npy\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 2 NAUTAL LANGUAGE CLASSIFIER API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_language_classifier = NaturalLanguageClassifierV1(\n",
    "    iam_apikey='YOUR API KEY',\n",
    "    url='https://gateway.watsonplatform.net/natural-language-classifier/api')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"created\": \"2019-02-13T20:46:38.552Z\",\n",
      "  \"language\": \"en\",\n",
      "  \"status\": \"Training\",\n",
      "  \"name\": \"pretrained think classifier\",\n",
      "  \"url\": \"https://gateway.watsonplatform.net/natural-language-classifier/api/v1/classifiers/befe0fx502-nlc-641\",\n",
      "  \"classifier_id\": \"befe0fx502-nlc-641\",\n",
      "  \"status_description\": \"The classifier instance is in its training phase, not yet ready to accept classify requests\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "with open('cloudtrainingdata.csv', 'rb') as training_data:\n",
    "    classifier = natural_language_classifier.create_classifier(\n",
    "        training_data=training_data,\n",
    "        metadata='{\"name\": \"pretrained think classifier\",\"language\": \"en\"}'\n",
    "        ).get_result()\n",
    "print(json.dumps(classifier, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSIFIER_ID = 'YOUR CLASSIFIER'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 3 TENSORFLOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing placeholders and parameters allows TensorFlow to build a dataflow graph\n",
    "def placeholders(n_x, n_y):\n",
    "    X = tf.placeholder(tf.float32, shape=(n_x, None), name=None)\n",
    "    Y = tf.placeholder(tf.float32, shape=(n_y, None), name=None)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "def init_parameters():\n",
    "    \n",
    "    W1 = tf.get_variable(\"W1\", [1024,13358], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b1 = tf.get_variable(\"b1\", [1024,1], initializer = tf.zeros_initializer())\n",
    "    W2 = tf.get_variable(\"W2\", [512,1024], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b2 = tf.get_variable(\"b2\", [512,1], initializer = tf.zeros_initializer())\n",
    "    W3 = tf.get_variable(\"W3\", [64,512], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b3 = tf.get_variable(\"b3\", [64,1], initializer = tf.zeros_initializer())\n",
    "    W4 = tf.get_variable(\"W4\", [4,64], initializer = tf.contrib.layers.xavier_initializer(seed = 1))\n",
    "    b4 = tf.get_variable(\"b4\", [4,1], initializer = tf.zeros_initializer())\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3,\n",
    "                  \"W4\": W4,\n",
    "                  \"b4\": b4}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "#data is fed through a neural network via forward propagation in order to form a prediction\n",
    "def forward_prop(X, parameters):\n",
    "    \n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    W4 = parameters['W4']\n",
    "    b4 = parameters['b4']\n",
    "    \n",
    "    Z1 = tf.add(tf.matmul(W1,X), b1)                      \n",
    "    A1 = tf.nn.relu(Z1)                                   \n",
    "    Z2 = tf.add(tf.matmul(W2,A1), b2)                   \n",
    "    A2 = tf.nn.relu(Z2)                                   \n",
    "    Z3 = tf.add(tf.matmul(W3,A2), b3)  \n",
    "    A3 = tf.nn.relu(Z3)\n",
    "    Z4 = tf.add(tf.matmul(W4,A3), b4)\n",
    "    \n",
    "    return Z4\n",
    "\n",
    "#measure probability error in predictions in order to alter and optimize weights\n",
    "def compute_cost(Z4, Y):\n",
    "   \n",
    "    logits = tf.transpose(Z4)\n",
    "    labels = tf.transpose(Y)\n",
    "   \n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,\n",
    "          num_epochs = 200, print_cost = True):\n",
    "    \n",
    "    ops.reset_default_graph()                         \n",
    "    (n_x, m) = X_train.shape                          \n",
    "    n_y = Y_train.shape[0]                            \n",
    "    costs = []                                        \n",
    "    \n",
    "    #put the above functions together, and optimize weights' value and minimize cost via Adam optimizer and save trained parameters\n",
    "    X, Y = placeholders(n_x, n_y)\n",
    "    parameters = init_parameters()\n",
    "    Z4 = forward_prop(X, parameters)\n",
    "    cost = compute_cost(Z4, Y)\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)\n",
    "    init = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver()\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epochs):\n",
    "            _ , epoch_cost = sess.run([optimizer, cost], feed_dict={X: X_train, Y: Y_train})\n",
    "            if print_cost == True and epoch % 20 == 0:\n",
    "                print (\"Cost after epoch %i: %f\" % (epoch, epoch_cost))\n",
    "            if print_cost == True and epoch % 5 == 0:\n",
    "                costs.append(epoch_cost)\n",
    "                \n",
    "        parameters = sess.run(parameters)\n",
    "        print (\"Parameters have been trained!\")\n",
    "\n",
    "        correct_prediction = tf.equal(tf.argmax(Z4), tf.argmax(Y))\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        print (\"Train Accuracy:\", accuracy.eval({X: X_train, Y: Y_train}))\n",
    "        \n",
    "        save_path = saver.save(sess, \"params.ckpt\",global_step=epoch)\n",
    "        \n",
    "        return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 1.743071\n",
      "Cost after epoch 20: 0.161911\n",
      "Cost after epoch 40: 0.066941\n",
      "Cost after epoch 60: 0.042267\n",
      "Cost after epoch 80: 0.030463\n",
      "Cost after epoch 100: 0.022968\n",
      "Cost after epoch 120: 0.017734\n",
      "Cost after epoch 140: 0.013947\n",
      "Cost after epoch 160: 0.011237\n",
      "Cost after epoch 180: 0.009228\n",
      "Parameters have been trained!\n",
      "Train Accuracy: 0.998267\n"
     ]
    }
   ],
   "source": [
    "parameters = model(x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier_id': 'bef9eax500-nlc-99',\n",
       " 'created': '2019-02-06T22:32:00.904Z',\n",
       " 'language': 'en',\n",
       " 'name': 'pretrained think classifier',\n",
       " 'status': 'Available',\n",
       " 'status_description': 'The classifier instance is now available and is ready to take classifier requests.',\n",
       " 'url': 'https://gateway.watsonplatform.net/natural-language-classifier/api/v1/classifiers/bef9eax500-nlc-99'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "natural_language_classifier.get_classifier(CLASSIFIER_ID).get_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART 4 PREDICTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "watsonpred = []\n",
    "for i in testset:\n",
    "    x = natural_language_classifier.classify(CLASSIFIER_ID,re.sub(' +',' ',\" \".join(re.split(r'[^\\w]', re.sub(re.compile(\"/\\*.*?\\*/\",re.DOTALL ) ,\"\" ,i[0]))))[0:2048])\n",
    "    watsonpred.append(x.get_result()['top_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(pred, testdata):\n",
    "    count = 0\n",
    "    for i in range(len(pred)):\n",
    "        if pred[i]==testdata[i][1]:\n",
    "            count += 1\n",
    "    return float(count)/len(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./params.ckpt-199\n",
      "Test Accuracy Watson NLC API: 0.9724137931034482\n",
      "Test Accuracy TensorFlow: 0.855172\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:    \n",
    "    X, Y = placeholders(x_test.shape[0], y_test.shape[0])\n",
    "    saver = tf.train.import_meta_graph('params.ckpt-199.meta')\n",
    "    saver.restore(sess,tf.train.latest_checkpoint('./'))\n",
    "    parameters = {\"W1\": sess.run('W1:0'),\n",
    "                  \"b1\": sess.run('b1:0'),\n",
    "                  \"W2\": sess.run('W2:0'),\n",
    "                  \"b2\": sess.run('b2:0'),\n",
    "                  \"W3\": sess.run('W3:0'),\n",
    "                  \"b3\": sess.run('b3:0'),\n",
    "                  \"W4\": sess.run('W4:0'),\n",
    "                  \"b4\": sess.run('b4:0')}\n",
    "    Z4 = forward_prop(X, parameters)\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(Z4), tf.argmax(Y))\n",
    "    \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(\"Test Accuracy Watson NLC API: \" + str(compute_accuracy(watsonpred,testset)))\n",
    "    print (\"Test Accuracy TensorFlow:\", accuracy.eval({X: x_test, Y: y_test}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

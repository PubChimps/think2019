{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## dataprep\n",
    "reformat dataset for use with TensorFlow and API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pygithub\n",
    "!wget https://github.com/PubChimps/think2019/blob/master/data/cloudclassifier.npy.zip?raw=true\n",
    "!mv cloud* clouddata.npz.zip\n",
    "!unzip clouddata.npz.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('./cloudclassifier.npy')\n",
    "\n",
    "#example data\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(data)\n",
    "trainset, testset = np.array(data)[:int(len(data)*.8),:], np.array(data)[int(len(data)*.8):,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build out dataset for neural network. Data needs to be tokenized and dictionary collected for bag of words encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "\n",
    "labeledlines = []\n",
    "ignore_words = ['?', ',']\n",
    "\n",
    "for line in trainset:\n",
    "    text = str(re.split(r'[.,]', line[0])).replace(\"'\",\"\").replace('[','')\n",
    "    text = re.sub(r'\\b\\w{1,1}\\b', '', text)\n",
    "    w = nltk.word_tokenize(text)\n",
    "    w = [ele for ele in w if ele not in ignore_words]\n",
    "    words.extend(w)\n",
    "    labeledlines.append([w, line[1]])\n",
    "    \n",
    "testlines = []\n",
    "\n",
    "for line in testset:\n",
    "    text = str(re.split(r'[.,]', line[0])).replace(\"'\",\"\").replace('[','')\n",
    "    text = re.sub(r'\\b\\w{1,1}\\b', '', text)\n",
    "    w = nltk.word_tokenize(text)\n",
    "    w = [ele for ele in w if ele not in ignore_words]\n",
    "    words.extend(w)\n",
    "    testlines.append([w, line[1]])\n",
    "    \n",
    "\n",
    "words = list(set(words))\n",
    "\n",
    "print('each data example will be transformed into a feature vector of size')\n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Translate feature vector via bag of words encoding and label into one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = []\n",
    "\n",
    "for line in labeledlines:\n",
    "    bag = []\n",
    "    code = line[0]\n",
    "    for w in words:\n",
    "        bag.append(code.count(w)) \n",
    "        \n",
    "    classes = [0,0,0,0]\n",
    "    if line[1] == 'amazon':\n",
    "        classes[0] = 1\n",
    "    elif line[1] == 'ibm':\n",
    "        classes[1] = 1\n",
    "    elif line[1] == 'microsoft':\n",
    "        classes[2] = 1\n",
    "    elif line[1] == 'google':\n",
    "        classes[3] = 1\n",
    "    else:\n",
    "        print(line[1])\n",
    "\n",
    "    traindata.append([bag,classes])\n",
    "    \n",
    "testdata = []\n",
    "for line in testlines:\n",
    "    bag = []\n",
    "    code = line[0]\n",
    "    for w in words:\n",
    "        bag.append(code.count(w)) \n",
    "        \n",
    "    classes = [0,0,0,0]\n",
    "    if line[1] == 'amazon':\n",
    "        classes[0] = 1\n",
    "    elif line[1] == 'ibm':\n",
    "        classes[1] = 1\n",
    "    elif line[1] == 'microsoft':\n",
    "        classes[2] = 1\n",
    "    elif line[1] == 'google':\n",
    "        classes[3] = 1\n",
    "    else:\n",
    "        print(line[1])\n",
    "\n",
    "    testdata.append([bag,classes])\n",
    "    \n",
    "x_train = np.array([row[0] for row in traindata]).T\n",
    "y_train = np.array([row[1] for row in traindata]).T\n",
    "x_test = np.array([row[0] for row in testdata]).T\n",
    "y_test = np.array([row[1] for row in testdata]).T\n",
    "\n",
    "np.save('x_train.npy',x_train)\n",
    "np.save('y_train.npy',y_train)\n",
    "np.save('x_test.npy',x_test)\n",
    "np.save('y_test.npy',y_test)\n",
    "np.save('testset.npy',testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert dataset to .csv file for API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "for i in labeledlines:\n",
    "    for j in re.findall('.{1,1024}', str(i[0]).replace(',', ' ').replace(\"'\",'').replace('[','').replace(']','')):\n",
    "        d.append({'text': j, 'cloud': i[1]})\n",
    "        \n",
    "df = pd.DataFrame(d, columns = ['text', 'cloud'])\n",
    "df['text'].replace(' ', np.nan, inplace=True)\n",
    "df = df.dropna()\n",
    "df.to_csv('cloudtrainingdata.csv', header=False,index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List files via !ls \n",
    "There should be a total of eight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

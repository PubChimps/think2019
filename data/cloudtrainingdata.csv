import  numpy  as  np  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  import  seaborn  as  sns  matplotlib  inline  sns  set  from  subprocess  import  check  output  print  check  output  ls  al  decode  utf8  df  train  pd  read  csv  input  train  csv  df  test  pd  read  csv  input  test  csv  The  code  was  removed  by  DSX  for  sharing  body  client  ace6f71a1b9946cbb684ca0d9e6c34c0  get  object  Bucket  projectparkhsu02d1be16ce7e164fc694f1c69332955f0b  Key  train  csv  Body  add  missing  iter  method  so  pandas  accepts  body  as  file  like  object  if  not  hasattr  body  iter  body  iter  types  MethodType  iter  body  df  train  pd  read  csv  body  df  train  head  df  train  info  df  test  head  df  test  info  df  test  shape  df  train  shapedf  test  describe  df  train  describe  sns  countplot  Survived  data  df  train  print  df  train  Survived  sum  df  train  Survived  count  100  df  test  Survived  df  test  PassengerId  Survived  to  csv  no  survivors  csv  index ,ibm
 False  print  df  test  head  ls  al  df  train  groupby  Survived  Sex  Survived  count  sns  factorplot  Sex  col  Survived  kind  count  data  df  train  print  of  women  survived  df  train  df  train  Sex  female  Survived  sum  df  train  df  train  Sex  female  Survived  count  print  of  men  survived  df  train  df  train  Sex  male  Survived  sum  df  train  df  train  Sex  male  Survived  count  ax  plt  subplots  figsize  16  df  train  Survived  df  train  Sex  male  value  counts  plot  pie  explode  autopct  1f  ax  ax  shadow  True  df  train  Survived  df  train  Sex  female  value  counts  plot  pie  explode  autopct  1f  ax  ax  shadow  True  ax  set  title  Survived  male  ax  set  title  Survived  female  plt  show  df  test  Survived  df  test  Sex  female  df  test  Survived  df  test  Survived  apply  lambda  int  df  test  PassengerId  Survived  to  csv  women  survive  csv  index  False  pd  crosstab  df  train  Pclass  df  train  Survived  margins  True  style  background  gradien,ibm
t  cmap  autumn  print  of  survivals  in  print  Pclass  df  train  Survived  df  train  Pclass  sum  df  train  df  train  Pclass  Survived  count  print  Pclass  df  train  Survived  df  train  Pclass  sum  df  train  df  train  Pclass  Survived  count  print  Pclass  df  train  Survived  df  train  Pclass  sum  df  train  df  train  Pclass  Survived  count  sns  factorplot  Pclass  Survived  data  df  train  plt  show  pd  crosstab  df  train  Sex  df  train  Survived  df  train  Pclass  margins  True  style  background  gradient  cmap  autumn  sns  factorplot  Pclass  Survived  hue  Sex  data  df  train  plt  show  sns  factorplot  Survived  col  Embarked  kind  count  data  df  train  sns  factorplot  Embarked  Survived  data  df  train  plt  show  sns  factorplot  Embarked  Survived  hue  Sex  data  df  train  plt  show  sns  factorplot  Embarked  Survived  col  Pclass  hue  Sex  data  df  train  plt  show  pd  crosstab  df  train  Survived  df  train  Sex  df  train  Pclass  df  train  Embarked  margi,ibm
ns  True  model  df  test  Survived  all  women  survived  df  test  loc  df  test  Sex  female  Survived  except  for  those  in  Pclass  and  embarked  in  df  test  loc  df  test  Sex  female  df  test  Pclass  df  test  Embarked  Survived  df  test  PassengerId  Survived  to  csv  embarked  pclass  sex  csv  index  False  for  df  in  df  train  df  test  df  Age  bin  np  nan  for  in  range  df  loc  df  Age  10  Age  bin  iprint  df  train  Age  Age  bin  head  10  sns  factorplot  Age  bin  Survived  hue  Sex  data  df  train  plt  show  sns  factorplot  Age  bin  Survived  col  Pclass  row  Sex  data  df  train  plt  show  pd  crosstab  df  train  Sex  df  train  Survived  df  train  Age  bin  df  train  Pclass  margins  True  style  background  gradient  cmap  autumn  in  Pclass  and  all  men  in  Age  bin  survived  df  test  loc  df  test  Sex  male  df  test  Pclass  df  test  Age  bin  Survived  df  test  loc  df  test  Sex  male  df  test  Pclass  df  test  Age  bin  Survived  1sns  factorplot,ibm
  SibSp  Survived  col  Pclass  row  Sex  data  df  train  plt  show  pd  crosstab  df  train  Sex  df  train  Survived  df  train  SibSp  df  train  Pclass  margins  True  style  background  gradient  cmap  autumn  all  females  with  SibSp  died  df  test  loc  df  test  Sex  female  df  test  SibSp  Survived  0sns  factorplot  Parch  Survived  col  Pclass  row  Sex  data  df  train  plt  show  pd  crosstab  df  train  Sex  df  train  Survived  df  train  Parch  df  train  Pclass  margins  True  style  background  gradient  cmap  autumn  survival  rate  is  below  for  females  with  Parch  and  Pclass  df  test  loc  df  test  Sex  female  df  test  Pclass  df  test  Parch  Survived  All  females  with  Parch  and  Pclass  died  df  test  loc  df  test  Sex  female  df  test  Pclass  df  test  Parch  Survived  all  females  with  Parch  died  df  test  loc  df  test  Sex  female  df  test  Parch  Survived  For  males  with  Parch  and  Pclass  survival  rate  is  above  df  test  loc  df  test  Sex  male  ,ibm
df  test  Pclass  df  test  Parch  Survived  df  test  head  20  sns  distplot  df  train  Fare  plt  show  for  df  in  df  train  df  test  df  Fare  bin  np  nan  for  in  range  12  df  loc  df  Fare  50  Fare  bin  isns  factorplot  Fare  bin  Survived  col  Pclass  row  Sex  data  df  train  plt  show  pd  crosstab  df  train  Sex  df  train  Survived  df  train  Fare  bin  df  train  Pclass  margins  True  style  background  gradient  cmap  autumn  males  in  Fare  bin  11  survived  df  test  loc  df  test  Sex  male  df  test  Fare  bin  11  Survived  model  df  test  PassengerId  Survived  to  csv  model  csv  index  False  df  test  drop  Survived  axis  inplace  True  df  train  ml  df  train  copy  df  test  ml  df  test  copy  df  train  ml  pd  get  dummies  df  train  ml  columns  Sex  Embarked  Pclass  drop  first  True  df  train  ml  drop  PassengerId  Name  Ticket  Cabin  Age  bin  Fare  bin  axis  inplace  True  df  train  ml  dropna  inplace  True  passenger  id  df  test  ml  PassengerI,ibm
d  df  test  ml  pd  get  dummies  df  test  ml  columns  Sex  Embarked  Pclass  drop  first  True  df  test  ml  drop  PassengerId  Name  Ticket  Cabin  Age  bin  Fare  bin  axis  inplace  True  df  train  ml  head  10  df  train  ml  info  df  test  ml  info  df  test  ml  drop  Survived  axis  inplace  True  df  test  ml  head  df  test  ml  info  corr  df  train  ml  corr  sns  heatmap  corr  from  sklearn  preprocessing  import  StandardScaler  scaler  StandardScaler  for  df  train  ml  scaler  fit  df  train  ml  drop  Survived  axis  scaled  features  scaler  transform  df  train  ml  drop  Survived  axis  df  train  ml  sc  pd  DataFrame  scaled  features  columns  df  train  ml  columns  for  df  test  ml  df  test  ml  fillna  df  test  ml  mean  inplace  True  scaler  fit  df  test  ml  scaled  features  scaler  transform  df  test  ml  df  test  ml  sc  pd  DataFrame  scaled  features  columns  df  test  ml  columns  from  sklearn  model  selection  import  train  test  split  train  test  train ,ibm
 test  train  test  split  df  train  ml  drop  Survived  axis  df  train  ml  Survived  test  size  30  random  state  101  train  sc  test  sc  train  sc  test  sc  train  test  split  df  train  ml  sc  df  train  ml  Survived  test  size  30  random  state  101  unscaled  train  all  df  train  ml  drop  Survived  axis  train  all  df  train  ml  Survived  test  all  df  test  ml  scaled  train  all  sc  df  train  ml  sc  train  all  sc  df  train  ml  Survived  test  all  sc  df  test  ml  scX  test  all  fillna  test  all  mean  inplace  True  print  from  sklearn  metrics  import  accuracy  score  classification  report  confusion  matrixfrom  sklearn  linear  model  import  LogisticRegression  logreg  LogisticRegression  logreg  fit  train  train  pred  logreg  logreg  predict  test  print  confusion  matrix  test  pred  logreg  print  classification  report  test  pred  logreg  print  accuracy  score  test  pred  logreg  logreg  fit  train  all  train  all  pred  all  logreg  logreg  predict  test  ,ibm
all  sub  logreg  pd  DataFrame  sub  logreg  PassengerId  df  test  PassengerId  sub  logreg  Survived  pred  all  logreg  sub  logmodel  to  csv  logmodel  csv  index  False  from  sklearn  naive  bayes  import  GaussianNB  gnb  GaussianNB  gnb  fit  train  train  pred  gnb  gnb  predict  test  print  confusion  matrix  test  pred  gnb  print  classification  report  test  pred  gnb  print  accuracy  score  test  pred  gnb  from  sklearn  neighbors  import  KNeighborsClassifier  knn  KNeighborsClassifier  neighbors  20  knn  fit  train  sc  train  sc  pred  knn  knn  predict  test  print  confusion  matrix  test  pred  knn  print  classification  report  test  pred  knn  print  accuracy  score  test  pred  knn  knn  fit  train  all  train  all  pred  all  knn  knn  predict  test  all  sub  knn  pd  DataFrame  sub  knn  PassengerId  df  test  PassengerId  sub  knn  Survived  pred  all  knn  sub  knn  to  csv  knn  csv  index  False  from  sklearn  tree  import  DecisionTreeClassifier  dtree  DecisionTreeClas,ibm
sifier  dtree  fit  train  train  pred  dtree  dtree  predict  test  print  classification  report  test  pred  dtree  print  accuracy  score  test  pred  dtree  dtree  DecisionTreeClassifier  max  features  max  depth  min  samples  split  dtree  fit  train  train  pred  dtree  dtree  predict  test  print  classification  report  test  pred  dtree  print  accuracy  score  test  pred  dtree  dtree  fit  train  all  train  all  pred  all  dtree2  dtree  predict  test  all  from  sklearn  ensemble  import  RandomForestClassifier  rfc  RandomForestClassifier  max  depth  max  features  min  samples  leaf  10  rfc  fit  train  train  pred  rfc  rfc  predict  test  print  confusion  matrix  test  pred  rfc  print  classification  report  test  pred  rfc  print  accuracy  score  test  pred  rfc  rfc  fit  train  all  train  all  pred  all  rfc  rfc  predict  test  all  sub  rfc  pd  DataFrame  sub  rfc  PassengerId  df  test  PassengerId  sub  rfc  Survived  pred  all  rfc  sub  rfc  to  csv  randforest  csv  index,ibm
  False  from  sklearn  svm  import  SVC  svc  SVC  gamma  01  100  probability  True  svc  fit  train  sc  train  sc  pred  svc  svc  predict  test  sc  print  confusion  matrix  test  sc  pred  svc  print  classification  report  test  sc  pred  svc  print  accuracy  score  test  sc  pred  svc  svc  fit  train  all  sc  train  all  sc  pred  all  svc  svc  predict  test  all  sc  sub  svc  pd  DataFrame  sub  svc  PassengerId  df  test  PassengerId  sub  svc  Survived  pred  all  svc  sub  svc  to  csv  svc  csv  index  False  from  sklearn  model  selection  import  cross  val  scorescores  svc  cross  val  score  svc  train  all  sc  train  all  sc  cv  10  scoring  accuracy  print  scores  svc  print  scores  svc  mean  scores  rfc  cross  val  score  rfc  train  all  sc  train  all  sc  cv  10  scoring  accuracy  print  scores  rfc  print  scores  rfc  mean  scores  dtree  cross  val  score  dtree  train  all  sc  train  all  sc  cv  10  scoring  accuracy  print  scores  dtree  print  scores  dtree  mea,ibm
n  print  dtree  scores  dtree  mean  print  rfc  scores  rfc  mean  print  svc  scores  svc  mean  ,ibm
import  wget  import  json  import  osfilename  GoSales  Tx  NaiveBayes  csv  if  not  os  path  isfile  filename  link  to  data  https  apsportal  ibm  com  exchange  api  v1  entries  8044492073eb964f46597b4be06ff5ea  data  accessKey  9561295fa407698694b1e254d0099600  filename  wget  download  link  to  data  print  filename  spark  SparkSession  builder  getOrCreate  df  data  spark  read  format  org  apache  spark  sql  execution  datasources  csv  CSVFileFormat  option  header  true  option  inferSchema  true  load  filename  df  data  printSchema  df  data  show  df  data  count  splitted  data  df  data  randomSplit  24  train  data  splitted  data  test  data  splitted  data  print  Number  of  training  records  str  train  data  count  print  Number  of  testing  records  str  test  data  count  from  pyspark  ml  feature  import  OneHotEncoder  StringIndexer  IndexToString  VectorAssembler  from  pyspark  ml  classification  import  RandomForestClassifier  from  pyspark  ml  evaluation  import  M,ibm
ulticlassClassificationEvaluator  from  pyspark  ml  import  Pipeline  ModelstringIndexer  label  StringIndexer  inputCol  PRODUCT  LINE  outputCol  label  fit  df  data  stringIndexer  prof  StringIndexer  inputCol  PROFESSION  outputCol  PROFESSION  IX  stringIndexer  gend  StringIndexer  inputCol  GENDER  outputCol  GENDER  IX  stringIndexer  mar  StringIndexer  inputCol  MARITAL  STATUS  outputCol  MARITAL  STATUS  IX  vectorAssembler  features  VectorAssembler  inputCols  GENDER  IX  AGE  MARITAL  STATUS  IX  PROFESSION  IX  outputCol  features  rf  RandomForestClassifier  labelCol  label  featuresCol  features  labelConverter  IndexToString  inputCol  prediction  outputCol  predictedLabel  labels  stringIndexer  label  labels  pipeline  rf  Pipeline  stages  stringIndexer  label  stringIndexer  prof  stringIndexer  gend  stringIndexer  mar  vectorAssembler  features  rf  labelConverter  model  rf  pipeline  rf  fit  train  data  predictions  model  rf  transform  test  data  evaluatorRF  MulticlassClass,ibm
ificationEvaluator  labelCol  label  predictionCol  prediction  metricName  accuracy  accuracy  evaluatorRF  evaluate  predictions  print  Accuracy  accuracy  pip  install  watson  machine  learning  client  upgradefrom  watson  machine  learning  client  import  WatsonMachineLearningAPIClientwml  credentials  url  https  ibm  watson  ml  mybluemix  net  access  key  username  password  instance  id  client  WatsonMachineLearningAPIClient  wml  credentials  instance  details  client  service  instance  get  details  print  json  dumps  instance  details  indent  model  props  client  repository  ModelMetaNames  AUTHOR  NAME  IBM  client  repository  ModelMetaNames  AUTHOR  EMAIL  ibm  ibm  com  client  repository  ModelMetaNames  NAME  LOCALLY  created  Product  Line  Prediction  model  published  model  client  repository  store  model  model  model  rf  pipeline  pipeline  rf  meta  props  model  props  training  data  train  data  published  model  uid  client  repository  get  model  uid  published  model,ibm
  model  details  client  repository  get  details  published  model  uid  print  json  dumps  model  details  indent  client  repository  list  models  loaded  model  client  repository  load  published  model  uid  test  predictions  loaded  model  transform  test  data  test  predictions  select  probability  predictedLabel  show  truncate  False  created  deployment  client  deployments  create  published  model  uid  name  Product  line  prediction  scoring  endpoint  client  deployments  get  scoring  url  created  deployment  print  scoring  endpoint  client  deployments  list  scoring  payload  fields  GENDER  AGE  MARITAL  STATUS  PROFESSION  values  23  Single  Student  55  Single  Executive  predictions  client  deployments  score  scoring  endpoint  scoring  payload  print  json  dumps  predictions  indent  client  deployments  delete  client  deployments  get  uid  created  deployment  client  deployments  list  client  repository  delete  published  model  uid  client  repository  list  models  ,ibm
bucket  eduthie  sagemaker  prefix  gluon  recommender  import  sagemaker  role  sagemaker  get  execution  role  import  os  import  mxnet  as  mx  from  mxnet  import  gluon  nd  ndarray  from  mxnet  metric  import  MSE  import  pandas  as  pd  import  numpy  as  np  import  sagemaker  from  sagemaker  mxnet  import  MXNet  import  boto3  import  json  import  matplotlib  pyplot  as  plt  aws  s3  cp  s3  eduthie  sagemaker  gluon  recommender  amazon  reviews  us  Digital  Video  Download  v1  00  tsv  gz  df  pd  read  csv  amazon  reviews  us  Digital  Video  Download  v1  00  tsv  gz  delimiter  error  bad  lines  False  df  head  df  df  customer  id  product  id  star  rating  product  title  customers  df  customer  id  value  counts  products  df  product  id  value  counts  quantiles  01  02  03  04  05  25  75  95  96  97  98  99  print  customers  customers  quantile  quantiles  print  products  products  quantile  quantiles  customers  customers  customers  products  products  products  10  red,amazon
uced  df  df  merge  pd  DataFrame  customer  id  customers  index  merge  pd  DataFrame  product  id  products  index  customers  reduced  df  customer  id  value  counts  products  reduced  df  product  id  value  counts  customer  index  pd  DataFrame  customer  id  customers  index  user  np  arange  customers  shape  product  index  pd  DataFrame  product  id  products  index  item  np  arange  products  shape  reduced  df  reduced  df  merge  customer  index  merge  product  index  reduced  df  head  test  df  reduced  df  groupby  customer  id  last  reset  index  train  df  reduced  df  merge  test  df  customer  id  product  id  on  customer  id  product  id  how  outer  indicator  True  train  df  train  df  train  df  merge  left  only  train  df  head  class  SparseMatrixDataset  gluon  data  Dataset  def  init  self  data  label  assert  data  shape  len  label  self  data  data  self  label  label  if  isinstance  label  ndarray  NDArray  and  len  label  shape  self  label  label  asnumpy  else,amazon
  self  label  label  def  getitem  self  idx  return  self  data  idx  self  data  idx  self  label  idx  def  len  self  return  self  data  shape  batch  size  40  train  iter  gluon  data  DataLoader  SparseMatrixDataset  nd  array  train  df  user  item  values  dtype  np  float32  nd  array  train  df  star  rating  values  dtype  np  float32  shuffle  True  batch  size  batch  size  test  iter  gluon  data  DataLoader  SparseMatrixDataset  nd  array  test  df  user  item  values  dtype  np  float32  nd  array  test  df  star  rating  values  dtype  np  float32  shuffle  True  batch  size  batch  size  class  MFBlock  gluon  HybridBlock  def  init  self  max  users  max  items  num  emb  dropout  super  MFBlock  self  init  self  max  users  max  users  self  max  items  max  items  self  dropout  dropout  self  num  emb  num  emb  with  self  name  scope  self  user  embeddings  gluon  nn  Embedding  max  users  num  emb  self  item  embeddings  gluon  nn  Embedding  max  items  num  emb  self  dropout,amazon
  gluon  nn  Dropout  dropout  self  dense  gluon  nn  Dense  num  emb  activation  relu  def  hybrid  forward  self  users  items  self  user  embeddings  users  self  dense  self  item  embeddings  items  self  dense  predictions  self  dropout  self  dropout  predictions  sum  predictions  axis  return  predictionsnum  embeddings  64  net  MFBlock  max  users  customer  index  shape  max  items  product  index  shape  num  emb  num  embeddings  dropout  net  collect  params  Initialize  network  parameters  ctx  mx  gpu  net  collect  params  initialize  mx  init  Xavier  magnitude  24  ctx  ctx  force  reinit  True  net  hybridize  Set  optimization  parameters  opt  sgd  lr  02  momentum  wd  trainer  gluon  Trainer  net  collect  params  opt  learning  rate  lr  wd  wd  momentum  momentum  def  execute  train  iter  test  iter  net  epochs  ctx  loss  function  gluon  loss  L2Loss  for  in  range  epochs  print  epoch  format  for  user  item  label  in  enumerate  train  iter  try  user  user  as  in  ,amazon
context  ctx  reshape  batch  size  item  item  as  in  context  ctx  reshape  batch  size  label  label  as  in  context  ctx  reshape  batch  size  with  mx  autograd  record  output  net  user  item  loss  loss  function  output  label  loss  backward  trainer  step  batch  size  except  pass  print  EPOCH  MSE  ON  TRAINING  and  TEST  format  eval  net  train  iter  net  ctx  loss  function  eval  net  test  iter  net  ctx  loss  function  print  end  of  training  return  netdef  eval  net  data  net  ctx  loss  function  acc  MSE  for  user  item  label  in  enumerate  data  try  user  user  as  in  context  ctx  reshape  batch  size  item  item  as  in  context  ctx  reshape  batch  size  label  label  as  in  context  ctx  reshape  batch  size  predictions  net  user  item  reshape  batch  size  acc  update  preds  predictions  labels  label  except  pass  return  acc  get  time  epochs  trained  net  execute  train  iter  test  iter  net  epochs  ctx  product  index  u6  predictions  trained  net  n,amazon
d  array  product  index  shape  as  in  context  ctx  nd  array  product  index  item  values  as  in  context  ctx  asnumpy  product  index  sort  values  u6  predictions  ascending  False  product  index  u7  predictions  trained  net  nd  array  product  index  shape  as  in  context  ctx  nd  array  product  index  item  values  as  in  context  ctx  asnumpy  product  index  sort  values  u7  predictions  ascending  False  product  index  u6  predictions  u7  predictions  plot  scatter  u6  predictions  u7  predictions  plt  show  cat  recommender  py  time  import  recommender  local  test  net  local  customer  index  local  product  index  recommender  train  train  home  ec2  user  SageMaker  bbc  poc  data  num  embeddings  64  opt  sgd  lr  02  momentum  wd  epochs  local  boto3  client  s3  copy  Bucket  eduthie  sagemaker  Key  gluon  recommender  amazon  reviews  us  Digital  Video  Download  v1  00  tsv  gz  bucket  prefix  train  amazon  reviews  us  Digital  Video  Download  v1  00  tsv  gz  ,amazon
opt  sgd  lr  02  momentum  wd  MXNet  recommender  py  py  version  py3  role  role  train  instance  count  train  instance  type  ml  p2  xlarge  output  path  s3  output  format  bucket  prefix  hyperparameters  num  embeddings  512  opt  opt  lr  lr  momentum  momentum  wd  wd  epochs  10  fit  train  s3  train  format  bucket  prefix  predictor  deploy  initial  instance  count  instance  type  ml  m4  xlarge  predictor  serializer  Nonepredictor  predict  json  dumps  customer  id  customer  index  customer  index  user  customer  id  values  tolist  product  id  B00KH1O9HW  B00M5KODWO  print  Naive  MSE  np  mean  test  df  star  rating  np  mean  train  df  star  rating  test  preds  for  array  in  np  array  split  test  df  customer  id  product  id  values  40  test  preds  predictor  predict  json  dumps  customer  id  array  tolist  product  id  array  tolist  test  preds  np  array  test  preds  print  MSE  np  mean  test  df  star  rating  test  preds  reduced  df  reduced  df  user  sort  va,amazon
lues  star  rating  item  ascending  False  True  predictions  for  array  in  np  array  split  product  index  product  id  values  40  predictions  predictor  predict  json  dumps  customer  id  customer  index  customer  index  user  customer  id  values  tolist  array  shape  product  id  array  tolist  predictions  pd  DataFrame  product  id  product  index  product  id  prediction  predictions  titles  reduced  df  groupby  product  id  product  title  last  reset  index  predictions  titles  predictions  merge  titles  predictions  titles  sort  values  prediction  product  id  ascending  False  True  predictions  user7  for  array  in  np  array  split  product  index  product  id  values  40  predictions  user7  predictor  predict  json  dumps  customer  id  customer  index  customer  index  user  customer  id  values  tolist  array  shape  product  id  array  tolist  plt  scatter  predictions  prediction  np  array  predictions  user7  plt  show  sagemaker  Session  delete  endpoint  predictor  end,amazon
point  ,amazon
conda  install  anaconda  psycopg2import  os  import  boto3  import  pandas  as  pd  import  json  import  psycopg2  import  sqlalchemy  as  sa  region  boto3  Session  region  name  bucket  your  s3  bucket  name  here  put  your  s3  bucket  name  here  and  create  s3  bucket  prefix  sagemaker  DEMO  redshift  customize  to  your  bucket  where  you  have  stored  the  data  credfile  redshift  creds  template  json  nogit  Read  credentials  to  dictionary  with  open  credfile  as  fh  creds  json  loads  fh  read  Sample  query  for  testing  query  select  from  public  irisdata  print  Reading  from  Redshift  def  get  conn  creds  conn  psycopg2  connect  dbname  creds  db  name  user  creds  username  password  creds  password  port  creds  port  num  host  creds  host  name  return  conn  def  get  df  creds  query  with  get  conn  creds  as  conn  with  conn  cursor  as  cur  cur  execute  query  result  set  cur  fetchall  colnames  desc  name  for  desc  in  cur  description  df  pd  DataFram,amazon
e  from  records  result  set  columns  colnames  return  df  df  get  df  creds  query  print  Saving  file  localFile  iris  csv  df  to  csv  localFile  index  False  print  Done  print  Writing  to  S3  fObj  open  localFile  rb  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  localFile  upload  fileobj  fObj  print  Done  print  Reading  from  S3  key  unchanged  for  demo  purposes  change  key  to  read  from  output  data  key  os  path  join  prefix  localFile  s3  boto3  resource  s3  outfile  iris2  csv  s3  Bucket  bucket  download  file  key  outfile  df2  pd  read  csv  outfile  print  Done  print  Writing  to  Redshift  connection  str  postgresql  psycopg2  creds  username  creds  password  creds  host  name  creds  port  num  creds  db  name  df2  to  sql  irisdata  v2  connection  str  schema  public  index  False  print  Done  pd  options  display  max  rows  conn  get  conn  creds  query  select  from  irisdata3  df  pd  read  sql  query  query  conn  df  ,amazon
import  sys  import  types  import  pandas  as  pd  from  botocore  client  import  Config  import  ibm  boto3  def  iter  self  return  hidden  cell  The  following  code  accesses  file  in  your  IBM  Cloud  Object  Storage  It  includes  your  credentials  You  might  want  to  remove  those  credentials  before  you  share  your  notebook  client  70ab70feb7fb4d8f9a47a408afd9f30f  ibm  boto3  client  service  name  s3  ibm  api  key  id  ZATI1oq  TlsWqN  oEz3Wo1IPXWwOkCx0PXV3gj0d5eui  ibm  auth  endpoint  https  iam  ng  bluemix  net  oidc  token  config  Config  signature  version  oauth  endpoint  url  https  s3  api  us  geo  objectstorage  service  networklayer  com  body  client  70ab70feb7fb4d8f9a47a408afd9f30f  get  object  Bucket  watstudworkshop  donotdelete  pr  basx79wonvxlys  Key  GoSales  Tx  csv  Body  add  missing  iter  method  so  pandas  accepts  body  as  file  like  object  if  not  hasattr  body  iter  body  iter  types  MethodType  iter  body  df  data  pd  read  csv  body  df  data,ibm
  head  if  not  df  data  in  globals  keys  print  nERROR  df  data  variable  is  not  defined  please  check  teh  cell  above  else  Created  convenience  shortcut  variable  df  df  data  1df  dtypesimport  numpy  as  np  df  count  df  describe  include  all  access  column  through  generated  attribute  df  IS  TENT  value  counts  access  column  through  indexing  by  column  name  df  GENDER  value  counts  Convert  value  counts  Series  to  DataFrame  for  prettier  display  df  MARITAL  STATUS  value  counts  to  frame  Access  PROFESSION  by  column  index  using  iloc  df  iloc  value  counts  Apply  multiplication  then  division  operators  to  Series  returning  computed  percentage  df  PROFESSION  value  counts  100  df  PROFESSION  count  Use  map  to  apply  an  unnamed  function  that  computes  percentage  df  MARITAL  STATUS  value  counts  map  lambda  100  len  df  GENDER  Use  map  to  apply  function  that  returns  percentage  evaluated  as  string  df  GENDER  value  counts  m,ibm
ap  lambda  0f  format  100  df  MARITAL  STATUS  count  Variant  using  groupby  to  generate  df  groupby  IS  TENT  size  100  len  df  IS  TENT  tent  gender  pd  crosstab  df  IS  TENT  df  GENDER  tent  genderx  tent  prof  pd  crosstab  df  IS  TENT  df  PROFESSION  tent  prof  df  PROFESSION  value  counts  applymap  lambda  1f  format  100  age  tent  pd  crosstab  df  AGE  df  IS  TENT  age  tent  Rename  the  columns  of  the  CrossTab  for  use  by  Brunel  import  brunel  age  tent  columns  AGE  SUM  TENT  brunel  data  age  tent  bar  AGE  SUM  TENT  ,ibm
az  login  az  account  list  output  table  az  account  set  subscription  subscription  id  subscription  name  location  West  Europe  resourceGroup  group1  appserviceplanName  appserviceplan1  appserviceplanSKU  S2  webappName  testwebappm2  whos  az  group  create  name  resourceGroup  location  location  az  appservice  plan  create  name  appserviceplanName  resource  group  resourceGroup  sku  appserviceplanSKU  az  appservice  web  create  name  webappName  resource  group  resourceGroup  plan  appserviceplanName  az  appservice  web  source  control  config  repo  url  https  github  com  prashanthmadi  Azure  nodejs  API  APP  git  name  webappName  resource  group  resourceGroup  ,microsoft
az  login  az  account  list  output  table  az  account  set  subscription  subscription  id  subscription  name  location  West  Europe  resourceGroup  group1  appserviceplanName  appserviceplan1  appserviceplanSKU  S2  webappName  testwebappm3  whos  az  group  create  name  resourceGroup  location  location  az  appservice  plan  create  name  appserviceplanName  resource  group  resourceGroup  sku  appserviceplanSKU  az  appservice  web  create  name  webappName  resource  group  resourceGroup  plan  appserviceplanName  az  appservice  web  source  control  config  repo  url  https  github  com  prashanthmadi  azure  flask  httpplatformhandler  no  site  ext  git  name  webappName  resource  group  resourceGroup  ,microsoft
import  pip  REQUIRED  MINIMUM  PANDAS  VERSION  17  try  import  pandas  as  pd  assert  pd  version  REQUIRED  MINIMUM  PANDAS  VERSION  except  raise  Exception  Version  or  above  of  Pandas  is  required  to  run  this  notebook  REQUIRED  MINIMUM  PANDAS  VERSION  import  sys  try  import  docplex  mp  except  if  hasattr  sys  real  prefix  we  are  in  virtual  env  pip  install  docplex  else  pip  install  user  docplex  url  https  api  oaas  docloud  ibmcloud  com  job  manager  rest  v1  key  api  xxxxxxxxxxxxxxxxxxxxxxxxxxxxxx  This  notebook  requires  pandas  to  work  from  io  import  StringIO  import  json  import  pandas  as  pd  from  pandas  import  DataFrame  Make  sure  that  xlrd  package  which  is  pandas  optional  dependency  is  installed  This  package  is  required  for  Excel  try  import  xlrd  except  if  hasattr  sys  real  prefix  we  are  in  virtual  env  pip  install  xlrd  else  pip  install  user  xlrd  Use  pandas  to  read  the  file  one  tab  for  each  table  nu,ibm
rse  xls  file  pd  ExcelFile  https  apsportal  ibm  com  exchange  api  v1  entries  2505b070a22403aac9f84884d315219e  data  accessKey  38990e0e17b15be6df05d384006a1d06  df  skills  nurse  xls  file  parse  Skills  df  depts  nurse  xls  file  parse  Departments  df  shifts  nurse  xls  file  parse  Shifts  Rename  df  shifts  index  df  shifts  index  name  shiftId  Index  is  column  name  df  nurses  nurse  xls  file  parse  Nurses  header  index  col  df  nurse  skilles  nurse  xls  file  parse  NurseSkills  df  vacations  nurse  xls  file  parse  NurseVacations  df  associations  nurse  xls  file  parse  NurseAssociations  df  incompatibilities  nurse  xls  file  parse  NurseIncompatibilities  Display  the  nurses  dataframe  print  nurses  format  len  df  nurses  print  shifts  format  len  df  shifts  print  vacations  format  len  df  vacations  maximum  work  time  in  hours  max  work  time  40  maximum  number  of  shifts  worked  in  week  max  nb  shifts  5df  shiftsdays  monday  tuesday  wedn,ibm
esday  thursday  friday  saturday  sunday  day  of  weeks  dict  zip  days  range  utility  to  convert  day  string  Monday  to  an  integer  in  def  day  to  day  of  week  day  return  day  of  weeks  day  strip  lower  for  each  day  name  we  normalize  it  by  stripping  whitespace  and  converting  it  to  lowercase  Monday  monday  df  shifts  dow  df  shifts  day  apply  day  to  day  of  week  df  shiftsdf  shifts  wstart  df  shifts  start  time  24  df  shifts  dow  an  auxiliary  function  to  calculate  absolute  end  time  of  shift  def  calculate  absolute  endtime  start  end  dow  return  24  dow  end  24  if  start  end  else  store  the  results  in  new  column  df  shifts  wend  df  shifts  apply  lambda  row  calculate  absolute  endtime  row  start  time  row  end  time  row  dow  axis  raw  True  df  shifts  duration  df  shifts  wend  df  shifts  wstart  also  compute  minimum  demand  in  nurse  hours  df  shifts  min  demand  df  shifts  min  req  df  shifts  duration  finally  ,ibm
check  the  modified  shifts  dataframe  df  shiftsfrom  docplex  mp  environment  import  Environment  env  Environment  env  print  information  from  docplex  mp  model  import  Model  mdl  Model  name  nurses  first  global  collections  to  iterate  upon  all  nurses  df  nurses  index  values  all  shifts  df  shifts  index  values  the  assignment  variables  assigned  mdl  binary  var  matrix  keys1  all  nurses  keys2  all  shifts  name  assign  Organize  decision  variables  in  DataFrame  df  assigned  DataFrame  assigned  assigned  df  assigned  index  names  all  nurses  all  shifts  Create  pivot  using  nurses  and  shifts  index  as  dimensions  df  assigned  pivot  df  assigned  reset  index  pivot  index  all  nurses  columns  all  shifts  values  assigned  Display  first  rows  of  the  pivot  table  df  assigned  pivot  head  Create  Data  Frame  representing  list  of  shifts  sorted  by  wstart  and  duration  One  keeps  only  the  three  relevant  columns  shiftId  wstart  and  wend  i,ibm
n  the  resulting  Data  Frame  df  sorted  shifts  df  shifts  sort  values  wstart  duration  reset  index  shiftId  wstart  wend  Display  the  first  rows  of  the  newly  created  Data  Frame  df  sorted  shifts  head  number  of  incompatible  shift  constraints  for  shift  in  df  sorted  shifts  itertuples  Iterate  over  following  shifts  shift  contains  the  index  of  the  current  shift  in  the  df  sorted  shifts  Data  Frame  for  shift  in  df  sorted  shifts  iloc  shift  itertuples  if  shift  wstart  shift  wend  Iterate  over  all  nurses  to  force  incompatible  assignment  for  the  current  pair  of  overlapping  shifts  for  nurse  assignments  in  df  assigned  pivot  shift  shiftId  shift  shiftId  itertuples  this  is  actually  logical  OR  mdl  add  constraint  nurse  assignments  nurse  assignments  number  of  incompatible  shift  constraints  else  No  need  to  test  overlap  with  following  shifts  break  print  incompatible  shift  constraints  format  number  of  incom,ibm
patible  shift  constraints  Add  day  of  week  column  to  vacations  Data  Frame  df  vacations  dow  df  vacations  day  apply  day  to  day  of  week  Join  df  vacations  df  shifts  and  df  assigned  Data  Frames  to  create  the  list  of  forbidden  assigments  The  reset  index  function  is  invoked  to  move  shiftId  index  as  column  in  df  shifts  Data  Frame  and  to  move  the  index  pair  all  nurses  all  shifts  as  columns  in  df  assigned  Data  Frame  reset  index  is  invoked  so  that  join  can  be  performed  between  Data  Frame  based  on  column  names  df  assigned  reindexed  df  assigned  reset  index  df  vacation  forbidden  assignments  df  vacations  merge  df  shifts  reset  index  dow  shiftId  merge  df  assigned  reindexed  left  on  nurse  shiftId  right  on  all  nurses  all  shifts  Here  are  the  first  few  rows  of  the  resulting  Data  Frames  joins  df  vacation  forbidden  assignments  head  for  forbidden  assignment  in  df  vacation  forbidden  assig,ibm
nments  itertuples  to  forbid  an  assignment  just  set  the  variable  to  zero  mdl  add  constraint  forbidden  assignment  assigned  print  vacation  forbids  assignments  format  len  df  vacation  forbidden  assignments  Join  df  assignment  Data  Frame  twice  based  on  associations  to  get  corresponding  decision  variables  pairs  for  all  shifts  The  suffixes  parameter  in  the  second  merge  indicates  our  preference  for  updating  the  name  of  columns  that  occur  both  in  the  first  and  second  argument  Data  Frames  in  our  case  these  columns  are  all  nurses  and  assigned  df  preferred  assign  df  associations  merge  df  assigned  reindexed  left  on  nurse1  right  on  all  nurses  merge  df  assigned  reindexed  left  on  nurse2  all  shifts  right  on  all  nurses  all  shifts  suffixes  Here  are  the  first  few  rows  of  the  resulting  Data  Frames  joins  df  preferred  assign  head  for  preferred  assign  in  df  preferred  assign  itertuples  mdl  add  con,ibm
straint  preferred  assign  assigned  preferred  assign  assigned  Join  assignment  Data  Frame  twice  based  on  incompatibilities  Data  Frame  to  get  corresponding  decision  variables  pairs  for  all  shifts  df  incompatible  assign  df  incompatibilities  merge  df  assigned  reindexed  left  on  nurse1  right  on  all  nurses  merge  df  assigned  reindexed  left  on  nurse2  all  shifts  right  on  all  nurses  all  shifts  suffixes  Here  are  the  first  few  rows  of  the  resulting  Data  Frames  joins  df  incompatible  assign  head  for  incompatible  assign  in  df  incompatible  assign  itertuples  mdl  add  constraint  incompatible  assign  assigned  incompatible  assign  assigned  auxiliary  function  to  create  worktime  variable  from  row  def  make  var  row  varname  fmt  return  mdl  continuous  var  name  varname  fmt  row  name  lb  apply  the  function  over  nurse  rows  and  store  result  in  new  column  df  nurses  worktime  df  nurses  apply  lambda  make  var  worktime ,ibm
 axis  display  nurse  dataframe  df  nurses  Use  pandas  groupby  operation  to  enforce  constraint  calculating  worktime  for  each  nurse  as  the  sum  of  all  assigned  shifts  times  the  duration  of  each  shift  for  nurse  nurse  assignments  in  df  assigned  groupby  level  all  nurses  mdl  add  constraint  df  nurses  worktime  nurse  mdl  dot  nurse  assignments  assigned  df  shifts  duration  print  model  information  and  check  we  now  have  32  extra  continuous  variables  mdl  print  information  we  use  pandas  apply  method  to  set  an  upper  bound  on  all  worktime  variables  def  set  max  work  time  ub  max  work  time  Optionally  return  string  for  fancy  display  of  the  constraint  in  the  Output  cell  return  str  str  ub  df  nurses  worktime  apply  convert  dtype  False  func  set  max  work  time  Use  pandas  groupby  operation  to  enforce  minimum  requirement  constraint  for  each  shift  for  shift  shift  nurses  in  df  assigned  groupby  level  all,ibm
  shifts  mdl  add  constraint  mdl  sum  shift  nurses  assigned  df  shifts  min  req  shift  again  leverage  pandas  to  create  series  of  expressions  costs  of  each  nurse  total  salary  series  df  nurses  worktime  df  nurses  pay  rate  compute  global  salary  cost  using  pandas  sum  Note  that  the  result  is  DOcplex  expression  DOcplex  if  fully  compatible  with  pandas  total  salary  cost  total  salary  series  sum  mdl  add  kpi  total  salary  cost  Total  salary  cost  mdl  minimize  total  salary  cost  mdl  print  information  Set  Cplex  mipgap  to  1e  to  enforce  precision  to  be  of  the  order  of  unit  objective  value  magnitude  is  1e  mdl  parameters  mip  tolerances  mipgap  1e  mdl  solve  url  url  key  key  log  output  True  assert  solve  failed  mdl  report  Create  pandas  Series  containing  actual  shift  assignment  decision  variables  value  assigned  df  assigned  assigned  apply  lambda  solution  value  Create  pivot  table  by  nurses  shifts  using,ibm
  pandas  unstack  method  to  transform  the  all  shifts  row  index  into  columns  df  res  assigned  unstack  level  all  shifts  Display  the  first  few  rows  of  the  resulting  pivot  table  df  res  head  demand  df  shifts  min  req  df  shifts  duration  total  demand  demand  sum  avg  worktime  total  demand  float  len  all  nurses  print  theoretical  average  work  time  is  format  avg  worktime  pandas  series  of  worktimes  solution  values  worktime  df  nurses  worktime  apply  lambda  solution  value  returns  new  series  computed  as  deviation  from  average  to  mean  worktime  avg  worktime  take  the  absolute  value  abs  to  mean  to  mean  apply  abs  total  to  mean  abs  to  mean  sum  print  the  sum  of  absolute  deviations  from  mean  is  format  total  to  mean  import  matplotlib  pyplot  as  plt  matplotlib  inline  we  can  also  plot  as  histogram  the  distribution  of  worktimes  worktime  plot  hist  color  LightBlue  plt  xlabel  worktime  pandas  series  of ,ibm
 shifts  worked  df  worked  df  res  all  shifts  sum  axis  df  res  worked  df  worked  df  worked  plot  hist  color  gold  xlim  10  plt  ylabel  shifts  worked  avg  worked  df  shifts  min  req  sum  float  len  all  nurses  print  expected  avg  shifts  worked  is  format  avg  worked  worked  to  avg  df  res  worked  avg  worked  total  to  mean  worked  to  avg  apply  abs  sum  print  total  absolute  deviation  to  mean  shifts  is  format  total  to  mean  add  two  extra  variables  per  nurse  deviations  above  and  below  average  df  nurses  worked  df  nurses  apply  lambda  make  var  worked  axis  df  nurses  overworked  df  nurses  apply  lambda  make  var  overw  axis  df  nurses  underworked  df  nurses  apply  lambda  make  var  underw  axis  Use  the  pandas  groupby  operation  to  enforce  the  constraint  calculating  number  of  worked  shifts  for  each  nurse  for  nurse  nurse  assignments  in  df  assigned  groupby  level  all  nurses  nb  of  worked  shifts  is  sum  of  as,ibm
signed  shifts  mdl  add  constraint  df  nurses  worked  nurse  mdl  sum  nurse  assignments  assigned  for  nurse  in  df  nurses  itertuples  nb  worked  is  average  over  under  mdl  add  constraint  nurse  worked  avg  worked  nurse  overworked  nurse  underworked  finally  define  kpis  for  over  and  under  average  quantities  total  overw  mdl  sum  df  nurses  overworked  mdl  add  kpi  total  overw  Total  over  worked  total  underw  mdl  sum  df  nurses  underworked  mdl  add  kpi  total  underw  Total  under  worked  mdl  minimize  total  salary  cost  total  overw  total  underw  incorporate  over  worked  and  under  worked  in  objectivesol2  mdl  solve  url  url  key  key  log  output  True  solve  again  and  get  new  solution  assert  sol2  Solve  failed  mdl  report  Create  pandas  Series  containing  actual  shift  assignment  decision  variables  value  assigned2  df  assigned  assigned  apply  lambda  solution  value  Create  pivot  table  by  nurses  shifts  using  pandas  unstack,ibm
  method  to  transform  the  all  shifts  row  index  into  columns  df  res2  assigned2  unstack  level  all  shifts  Add  new  column  to  the  pivot  table  containing  the  shifts  worked  by  summing  over  each  row  df  res2  worked  df  res2  all  shifts  sum  axis  total  absolute  deviation  from  average  is  directly  read  on  expressions  new  total  to  mean  total  overw  solution  value  total  underw  solution  value  print  total  absolute  deviation  to  mean  shifts  is  now  down  from  format  new  total  to  mean  total  to  mean  Display  the  first  few  rows  of  the  result  Data  Frame  df  res2  head  df  res2  worked  plot  kind  hist  color  gold  xlim  mdl  minimize  total  overw  total  underw  assert  mdl  solve  url  url  key  key  solve  failed  mdl  report  Create  pandas  Series  containing  actual  shift  assignment  decision  variables  value  assigned  fair  df  assigned  assigned  apply  lambda  solution  value  Create  pivot  table  by  nurses  shifts  using  panda,ibm
s  unstack  method  to  transform  the  all  shifts  row  index  into  columns  df  res  fair  assigned  fair  unstack  level  all  shifts  Add  new  column  to  the  pivot  table  containing  the  shifts  worked  by  summing  over  each  row  df  res  fair  solution  value  fair  df  res  fair  all  shifts  sum  axis  df  res  fair  worked  df  res  fair  all  shifts  sum  axis  df  res  fair  worked  plot  hist  color  plum  xlim  ,ibm
The  code  was  removed  by  Watson  Studio  for  sharing  df  data  drop  columns  city  ascii  pop  iso2  iso3  abbr  country  CLIENT  ID  G0BRLIDJSVXEAYDLPAST5G10XXLNYT5IOHKT4YPFGUV4ZQKG  your  Foursquare  ID  CLIENT  SECRET  UL2WURMQFXYJWNG2AMUJHV5LSZ1D2PJAJBSWNCYVG3K23JDT  your  Foursquare  Secret  VERSION  20180605  Foursquare  API  version  print  Your  credentails  print  CLIENT  ID  CLIENT  ID  print  CLIENT  SECRET  CLIENT  SECRET  def  get  category  type  row  try  categories  list  row  categories  except  categories  list  row  venue  categories  if  len  categories  list  return  None  else  return  categories  list  name  from  pandas  io  json  import  json  normalizefinal  dict  for  one  city  in  zip  df  data  lat  df  data  lng  df  data  city  print  str  one  city  city  Check  str  one  city  lower  neighborhood  latitude  one  city  neighborhood  longitude  one  city  Rad  1500  limit  100  url  https  api  foursquare  com  v2  venues  explore  mode  url  client  id  client  secret  ,ibm
ll  radius  limit  format  CLIENT  ID  CLIENT  SECRET  VERSION  neighborhood  latitude  neighborhood  longitude  Rad  limit  results  requests  get  url  json  print  str  results  venues  results  response  groups  items  nearby  venues  json  normalize  venues  flatten  JSON  filter  columns  filtered  columns  venue  name  venue  categories  venue  location  lat  venue  location  lng  nearby  venues  nearby  venues  loc  filtered  columns  filter  the  category  for  each  row  nearby  venues  venue  categories  nearby  venues  apply  get  category  type  axis  clean  columns  list  to  add  nearby  venues  columns  col  split  for  col  in  nearby  venues  columns  ab  pd  DataFrame  nearby  venues  categories  value  counts  list  to  add  append  value  for  value  in  ab  if  ab  categories  max  ab  categories  final  list  append  city  Check  list  to  add  for  value  in  ab  if  value  categories  max  ab  categories  if  value  in  final  dict  final  dict  value  append  city  Check  else  final,ibm
  dict  value  list  city  Check  print  str  final  dict  restaurant  type  input  lower  if  restaurant  type  in  final  dict  isare  is  if  len  final  dict  restaurant  type  else  are  print  join  final  dict  restaurant  type  isare  the  cities  that  have  interest  in  str  restaurant  type  type  of  restaurants  else  print  This  restaurant  type  isn  there  at  all  in  the  USA  It  good  idea  to  have  introduce  this  restaurant  type  ,ibm
imports  import  boto3  AWS  python  SDK  for  accessing  AWS  services  import  pandas  as  pd  Tabular  data  structure  import  numpy  as  np  Array  libraru  with  probability  and  statistics  capabilities  import  matplotlib  pyplot  as  plt  Plotting  library  import  seaborn  as  sns  plotting  library  import  io  import  sagemaker  amazon  common  as  smac  Amazon  Sagemaker  common  library  that  includes  data  formats  import  sagemaker  sagemaker  python  sdk  import  os  from  sagemaker  predictor  import  csv  serializer  json  deserializer  sagemaker  prediction  sdkbucket  cyrusmv  sagemaker  demos  replace  this  with  your  own  bucket  prefix  visa  kaggle  original  csv  replace  this  with  your  own  file  inside  the  bucket  protocol  s3  datafile  data  original  csv  local  pickel  root  data  processed  visa  kaggle  data  dist  visa  kaggle  data  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  this  is  Sa,amazon
geMaker  role  that  would  be  later  used  for  authorizing  SageMaker  to  access  S3  print  role  sagemaker  session  sagemaker  Session  Downloading  the  file  to  local  folder  client  boto3  client  s3  with  open  datafile  wb  as  client  download  fileobj  bucket  prefix  loading  data  into  pandas  for  inspection  df  pd  read  csv  datafile  print  df  as  matrix  shape  df  head  We  would  like  to  see  what  values  we  have  for  Class  This  snippet  below  shows  that  Class  is  binary  column  print  print  df  groupby  Class  size  num  recs  df  Class  count  num  zeros  df  Class  df  Class  count  num  ones  num  recs  num  zeros  print  of  transactions  are  fraudunat  and  are  legitimate  format  num  ones  num  recs  100  num  zeros  num  recs  100  df  boxplot  figsize  20  10  plt  show  df1  df  drop  Time  Amount  Class  axis  df1  boxplot  figsize  20  10  plt  show  Converting  Data  Into  Numpy  raw  data  df  as  matrix  Shuffling  the  data  and  randomizing  the  d,amazon
istrbution  of  the  data  have  performed  to  shuffles  and  experimented  with  different  seeds  until  the  distribution  of  Class  became  acceptably  smooth  according  to  the  graph  below  np  random  seed  123  np  random  shuffle  raw  data  np  random  seed  499  np  random  shuffle  raw  data  label  raw  data  Taking  last  column  of  the  data  and  creating  lanel  vector  data  raw  data  Taking  the  remains  of  th  da  print  shape  before  split  label  shape  data  shape  format  raw  data  shape  label  shape  data  shape  There  are  very  few  fraudulant  transactions  in  the  dataset  so  am  putting  their  indexes  in  an  array  to  plot  and  ensure  they  are  evenly  distributed  so  when  split  the  dataset  into  test  and  training  don  end  up  with  dispropostionate  distribution  for  in  range  len  label  if  label  append  sns  distplot  kde  True  rug  True  hist  False  plt  show  Splitting  data  into  validation  and  training  and  breaking  dataset  into  d,amazon
ata  and  label  70  30  training  to  validation  train  size  int  data  shape  training  data  and  associated  labels  train  data  data  train  size  val  data  data  train  size  validation  data  and  associated  labels  train  label  label  train  size  val  label  label  train  size  print  training  data  shape  training  label  shape  nValidation  data  shape  validation  label  shape  format  train  data  shape  train  label  shape  val  data  shape  val  label  shape  Saving  arrays  for  later  use  np  save  local  pickel  root  train  train  data  npy  train  data  allow  pickle  True  np  save  local  pickel  root  train  train  label  npy  train  label  allow  pickle  True  np  save  local  pickel  root  test  val  data  npy  val  data  allow  pickle  True  np  save  local  pickel  root  test  val  label  npy  val  label  allow  pickle  True  ls  data  processed  visa  kaggle  data  Uploading  the  data  path  is  the  local  path  on  your  notbooks  instance  bucket  is  your  bucket  name,amazon
  key  prefix  is  your  folder  structure  inside  you  S3  bucket  S3loc  sagemaker  session  upload  data  path  local  pickel  root  bucket  bucket  key  prefix  visa  kaggle  data  print  S3loc  aws  s3  ls  cyrusmv  sagemaker  demos  visa  kaggle  data  recursive  use  the  output  from  your  own  S3loctrain  data  np  load  local  pickel  root  train  train  data  npy  train  label  np  load  local  pickel  root  train  train  label  npy  val  data  np  load  local  pickel  root  test  val  data  npy  val  label  np  load  local  pickel  root  test  val  label  npy  print  training  data  shape  training  label  shape  nValidation  data  shape  validation  label  shape  format  train  data  shape  train  label  shape  val  data  shape  val  label  shape  train  set  train  data  train  label  test  set  val  data  val  label  vectors  np  array  tolist  for  in  train  set  astype  float32  labels  np  array  tolist  for  in  train  set  astype  float32  buf  io  BytesIO  smac  write  numpy  to  dense,amazon
  tensor  buf  vectors  labels  buf  seek  key  recordio  pb  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  dist  key  upload  fileobj  buf  s3  train  data  s3  format  bucket  dist  key  print  uploaded  training  data  location  format  s3  train  data  output  location  s3  output  format  bucket  prefix  print  training  artifacts  will  be  uploaded  to  format  output  location  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  linear  learner  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  linear  learner  latest  sess  sagemaker  Session  linear  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  role  S3  role  so  the  notebook  can  read  the  data  and  upload  the  model  train  instance  count  number  of  instances  for  training  train  in,amazon
stance  type  ml  p2  xlarge  type  of  training  instance  output  path  output  location  s3  location  for  uploading  trained  mdoel  sagemaker  session  sess  linear  set  hyperparameters  feature  dim  30  dataset  has  30  columns  features  predictor  type  binary  classifier  we  predict  binary  value  it  could  have  been  regressor  mini  batch  size  200  linear  fit  train  s3  train  data  linear  predictor  linear  deploy  initial  instance  count  Initial  number  of  instances  Autoscaling  can  increase  the  number  of  instances  instance  type  ml  m4  xlarge  instance  typetype  linear  predictor  linear  predictor  content  type  text  csv  linear  predictor  serializer  csv  serializer  linear  predictor  deserializer  json  deserializer  since  score  is  very  rare  we  want  to  make  sure  we  can  correctly  predict  fradulant  transaction  First  we  print  lost  of  all  labels  where  score  then  then  run  prediction  for  in  range  len  train  label  if  train  label  app,amazon
end  print  print  print  linear  predictor  predict  train  set  515  519  non  zero  np  count  nonzero  test  set  zero  len  test  set  non  zero  print  validation  set  includes  non  zero  and  items  woth  value  zero  format  non  zero  zero  predictions  for  array  in  np  array  split  test  set  100  result  linear  predictor  predict  array  predictions  predicted  label  for  in  result  predictions  predictions  np  array  predictions  import  pandas  as  pd  pd  crosstab  test  set  predictions  rownames  actuals  colnames  predictions  linear  delete  endpoint  ,amazon
import  os  from  azureml  core  import  Workspace  Run  Experiment  ws  Workspace  from  config  print  Workspace  name  ws  name  Azure  region  ws  location  Subscription  id  ws  subscription  id  Resource  group  ws  resource  group  sep  Also  create  Project  and  attach  to  Workspace  scripts  folder  scripts  if  not  os  path  isdir  scripts  folder  os  mkdir  scripts  folder  from  azureml  core  compute  import  BatchAiCompute  ComputeTarget  from  azureml  core  datastore  import  Datastore  from  azureml  data  data  reference  import  DataReference  from  azureml  pipeline  core  import  Pipeline  PipelineData  from  azureml  pipeline  steps  import  PythonScriptStep  from  azureml  core  runconfig  import  CondaDependencies  RunConfiguration  Batch  AI  compute  cluster  name  gpu  cluster  try  cluster  BatchAiCompute  ws  cluster  name  print  found  existing  cluster  except  print  creating  new  cluster  provisioning  config  BatchAiCompute  provisioning  configuration  vm  size  STANDA,microsoft
RD  NC6  autoscale  enabled  True  cluster  min  nodes  cluster  max  nodes  create  the  cluster  cluster  ComputeTarget  create  ws  cluster  name  provisioning  config  cluster  wait  for  completion  show  output  True  writefile  scripts  folder  batchai  score  py  import  os  import  argparse  import  datetime  time  import  tensorflow  as  tf  from  math  import  ceil  import  numpy  as  np  import  shutil  from  tensorflow  contrib  slim  python  slim  nets  import  inception  v3  from  azureml  core  model  import  Model  slim  tf  contrib  slim  parser  argparse  ArgumentParser  description  Start  tensorflow  model  serving  parser  add  argument  model  name  dest  model  name  required  True  parser  add  argument  label  dir  dest  label  dir  required  True  parser  add  argument  dataset  path  dest  dataset  path  required  True  parser  add  argument  output  dir  dest  output  dir  required  True  parser  add  argument  batch  size  dest  batch  size  type  int  required  True  args  parse,microsoft
r  parse  args  image  size  299  num  channel  create  output  directory  if  it  does  not  exist  os  makedirs  args  output  dir  exist  ok  True  def  get  class  label  dict  label  file  label  proto  as  ascii  lines  tf  gfile  GFile  label  file  readlines  for  in  proto  as  ascii  lines  label  append  rstrip  return  label  class  DataIterator  def  init  self  data  dir  self  file  paths  image  list  os  listdir  data  dir  total  size  len  image  list  self  file  paths  data  dir  file  name  rstrip  for  file  name  in  image  list  self  labels  for  file  name  in  self  file  paths  property  def  size  self  return  len  self  labels  def  input  pipeline  self  batch  size  images  tensor  tf  convert  to  tensor  self  file  paths  dtype  tf  string  labels  tensor  tf  convert  to  tensor  self  labels  dtype  tf  int64  input  queue  tf  train  slice  input  producer  images  tensor  labels  tensor  shuffle  False  labels  input  queue  images  content  tf  read  file  input  queu,microsoft
e  image  reader  tf  image  decode  jpeg  images  content  channels  num  channel  name  jpeg  reader  float  caster  tf  cast  image  reader  tf  float32  new  size  tf  constant  image  size  image  size  dtype  tf  int32  images  tf  image  resize  images  float  caster  new  size  images  tf  divide  tf  subtract  images  255  image  batch  label  batch  tf  train  batch  images  labels  batch  size  batch  size  capacity  batch  size  return  image  batch  def  main  start  time  datetime  datetime  now  label  file  name  os  path  join  args  label  dir  labels  txt  label  dict  get  class  label  dict  label  file  name  classes  num  len  label  dict  test  feeder  DataIterator  data  dir  args  dataset  path  total  size  len  test  feeder  labels  count  get  model  from  model  registry  model  path  Model  get  model  path  args  model  name  with  tf  Session  as  sess  test  images  test  feeder  input  pipeline  batch  size  args  batch  size  with  slim  arg  scope  inception  v3  inception,microsoft
  v3  arg  scope  input  images  tf  placeholder  tf  float32  args  batch  size  image  size  image  size  num  channel  logits  inception  v3  inception  v3  input  images  num  classes  classes  num  is  training  False  probabilities  tf  argmax  logits  sess  run  tf  global  variables  initializer  sess  run  tf  local  variables  initializer  coord  tf  train  Coordinator  threads  tf  train  start  queue  runners  sess  sess  coord  coord  saver  tf  train  Saver  saver  restore  sess  model  path  out  filename  os  path  join  args  output  dir  result  labels  txt  with  open  out  filename  as  result  file  while  count  total  size  and  not  coord  should  stop  test  images  batch  sess  run  test  images  file  names  batch  test  feeder  file  paths  args  batch  size  min  test  feeder  size  args  batch  size  results  sess  run  probabilities  feed  dict  input  images  test  images  batch  new  add  min  args  batch  size  total  size  count  count  new  add  for  in  range  new  add  re,microsoft
sult  file  write  os  path  basename  file  names  batch  label  dict  results  result  file  flush  coord  request  stop  coord  join  threads  copy  the  file  to  artifacts  shutil  copy  out  filename  outputs  Move  the  processed  data  out  of  the  blob  so  that  the  next  run  can  process  the  data  if  name  main  tf  app  run  create  directory  for  model  model  dir  models  if  not  os  path  isdir  model  dir  os  mkdir  model  dir  import  tarfile  import  urllib  request  url  http  download  tensorflow  org  models  inception  v3  2016  08  28  tar  gz  response  urllib  request  urlretrieve  url  model  tar  gz  tar  tarfile  open  model  tar  gz  gz  tar  extractall  model  dir  account  name  pipelinedata  sample  data  Datastore  register  azure  blob  container  ws  datastore  name  images  datastore  container  name  sampledata  account  name  account  name  overwrite  True  default  ds  ws  get  default  datastore  from  azureml  core  conda  dependencies  import  CondaDependenci,microsoft
es  from  azureml  data  data  reference  import  DataReference  from  azureml  pipeline  core  import  Pipeline  PipelineData  from  azureml  core  import  Datastore  from  azureml  core  import  Experimentinput  images  DataReference  datastore  sample  data  data  reference  name  input  images  path  on  datastore  batchscoring  images  mode  download  model  dir  DataReference  datastore  sample  data  data  reference  name  input  model  path  on  datastore  batchscoring  models  mode  download  label  dir  DataReference  datastore  sample  data  data  reference  name  input  labels  path  on  datastore  batchscoring  labels  mode  download  output  dir  PipelineData  name  scores  datastore  name  default  ds  name  output  path  on  compute  batchscoring  results  import  shutil  from  azureml  core  model  import  Model  register  downloaded  model  model  Model  register  model  path  models  inception  v3  ckpt  model  name  inception  this  is  the  name  the  model  is  registered  as  tags  pret,microsoft
rained  inception  description  Imagenet  trained  tensorflow  inception  workspace  ws  remove  the  downloaded  dir  after  registration  if  you  wish  shutil  rmtree  models  cd  CondaDependencies  create  pip  packages  tensorflow  gpu  azureml  defaults  Runconfig  batchai  run  config  RunConfiguration  conda  dependencies  cd  batchai  run  config  environment  docker  enabled  True  batchai  run  config  environment  docker  gpu  support  True  batchai  run  config  environment  docker  base  image  microsoft  mmlspark  gpu  12  batchai  run  config  environment  spark  precache  packages  Falsefrom  azureml  pipeline  core  graph  import  PipelineParameter  batch  size  param  PipelineParameter  name  param  batch  size  default  value  20  inception  model  name  inception  v3  ckpt  batch  score  step  PythonScriptStep  name  batch  ai  scoring  script  name  batchai  score  py  arguments  dataset  path  input  images  model  name  inception  label  dir  label  dir  output  dir  output  dir  batch,microsoft
  size  batch  size  param  target  cluster  inputs  input  images  label  dir  outputs  output  dir  runconfig  batchai  run  config  source  directory  scripts  folder  pipeline  Pipeline  workspace  ws  steps  batch  score  step  pipeline  run  Experiment  ws  batch  scoring  submit  pipeline  pipeline  params  param  batch  size  20  from  azureml  train  widgets  import  RunDetails  RunDetails  pipeline  run  show  pipeline  run  wait  for  completion  show  output  True  step  run  list  pipeline  run  get  children  step  run  download  file  outputs  result  labels  txt  import  pandas  as  pd  df  pd  read  csv  result  labels  txt  delimiter  header  None  df  columns  Filename  Prediction  df  head  published  pipeline  pipeline  run  publish  pipeline  name  Inception  v3  scoring  description  Batch  scoring  using  Inception  v3  model  version  published  id  published  pipeline  idfrom  azureml  core  authentication  import  AzureCliAuthentication  import  requests  cli  auth  AzureCliAuthenti,microsoft
cation  aad  token  cli  auth  get  authentication  header  from  azureml  pipeline  core  import  PublishedPipeline  rest  endpoint  PublishedPipeline  get  endpoint  published  id  ws  specify  batch  size  when  running  the  pipeline  response  requests  post  rest  endpoint  headers  aad  token  json  param  batch  size  50  run  id  response  json  Id  from  azureml  pipeline  core  run  import  PipelineRun  published  pipeline  run  PipelineRun  ws  experiments  batch  scoring  run  id  RunDetails  published  pipeline  run  show  ,microsoft
import  numpy  as  np  np  array  np  array  tmp  value  np  sum  tmp  print  value  if  value  print  elif  value  print  np  array  np  array  tmp  value  np  sum  tmp  print  value  if  value  print  elif  value  print  Signle  Perceptron  def  AND  x1  x2  np  array  x1  x2  np  array  tmp  np  sum  if  tmp  return  else  return  Signle  Perceptron  def  NAND  x1  x2  np  array  x1  x2  np  array  tmp  np  sum  if  tmp  return  else  return  Signle  Perceptron  def  OR  x1  x2  np  array  x1  x2  np  array  tmp  np  sum  if  tmp  return  else  return  Multi  Perceptron  def  XOR  x1  x2  s1  NAND  x1  x2  Perceptron  s2  OR  x1  x2  Perceptron  AND  s1  s2  Perceptron  return  yprint  XOR  print  XOR  print  XOR  print  XOR  ,amazon
import  boto3  import  io  import  matplotlib  pyplot  as  plt  import  numpy  as  np  import  os  import  pandas  as  pd  import  sagemaker  import  sagemaker  amazon  common  as  smac  from  sagemaker  import  get  execution  role  from  sagemaker  predictor  import  csv  serializer  json  deserializer  Set  data  locations  bucket  your  s3  bucket  here  replace  this  with  your  own  bucket  prefix  sagemaker  DEMO  linear  learner  loss  weights  replace  this  with  your  own  prefix  s3  train  key  train  recordio  pb  data  format  prefix  s3  train  path  os  path  join  s3  bucket  s3  train  key  local  raw  data  creditcard  csv  zip  role  get  execution  role  Confirm  access  to  s3  bucket  for  obj  in  boto3  resource  s3  Bucket  bucket  objects  all  print  obj  key  Read  the  data  shuffle  and  split  into  train  and  test  sets  separating  the  labels  last  column  from  the  features  raw  data  pd  read  csv  local  raw  data  as  matrix  np  random  seed  np  random  shuffle  ,amazon
raw  data  train  size  int  raw  data  shape  train  features  raw  data  train  size  train  labels  raw  data  train  size  test  features  raw  data  train  size  test  labels  raw  data  train  size  Convert  the  processed  training  data  to  protobuf  and  write  to  S3  for  linear  learner  vectors  np  array  tolist  for  in  train  features  astype  float32  labels  np  array  tolist  for  in  train  labels  astype  float32  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  vectors  labels  buf  seek  boto3  resource  s3  Bucket  bucket  Object  s3  train  key  upload  fileobj  buf  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  def  predictor  from  hyperparams  s3  train  data  hyperparams  output  path  Create  an  Estimator  from  the  given  hyperparams  fit  to  training  data  and  return  deployed  predictor  specify  algorithm  containers  and  instantiate  an  Estimator  with  given  hyperparams  container  get  image  uri  boto3  Session  region  name  ,amazon
linear  learner  linear  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  m4  xlarge  output  path  output  path  sagemaker  session  sagemaker  Session  linear  set  hyperparameters  hyperparams  train  model  linear  fit  train  s3  train  data  deploy  predictor  linear  predictor  linear  deploy  initial  instance  count  instance  type  ml  m4  xlarge  linear  predictor  content  type  text  csv  linear  predictor  serializer  csv  serializer  linear  predictor  deserializer  json  deserializer  return  linear  predictordef  evaluate  linear  predictor  test  features  test  labels  model  name  verbose  True  Evaluate  model  on  test  set  given  the  prediction  endpoint  Return  binary  classification  metrics  split  the  test  data  set  into  100  batches  and  evaluate  using  prediction  endpoint  prediction  batches  linear  predictor  predict  batch  predictions  for  batch  in  np  array  split  test  features  100  parse  raw  predictions  ,amazon
json  to  exctract  predicted  label  test  preds  np  concatenate  np  array  predicted  label  for  in  batch  for  batch  in  prediction  batches  calculate  true  positives  false  positives  true  negatives  false  negatives  tp  np  logical  and  test  labels  test  preds  sum  fp  np  logical  and  test  labels  test  preds  sum  tn  np  logical  and  test  labels  test  preds  sum  fn  np  logical  and  test  labels  test  preds  sum  calculate  binary  classification  metrics  recall  tp  tp  fn  precision  tp  tp  fp  accuracy  tp  tn  tp  fp  tn  fn  f1  precision  recall  precision  recall  if  verbose  print  pd  crosstab  test  labels  test  preds  rownames  actuals  colnames  predictions  print  11  3f  format  Recall  recall  print  11  3f  format  Precision  precision  print  11  3f  format  Accuracy  accuracy  print  11  3f  format  F1  f1  return  TP  tp  FP  fp  FN  fn  TN  tn  Precision  precision  Recall  recall  Accuracy  accuracy  F1  f1  Model  model  name  def  delete  endpoint  pred,amazon
ictor  try  boto3  client  sagemaker  delete  endpoint  EndpointName  predictor  endpoint  print  Deleted  format  predictor  endpoint  except  print  Already  deleted  format  predictor  endpoint  Training  binary  classifier  with  default  settings  logistic  regression  defaults  hyperparams  feature  dim  30  predictor  type  binary  classifier  epochs  40  defaults  output  path  s3  defaults  output  format  bucket  prefix  defaults  predictor  predictor  from  hyperparams  s3  train  path  defaults  hyperparams  defaults  output  path  Training  binary  classifier  with  automated  threshold  tuning  autothresh  hyperparams  feature  dim  30  predictor  type  binary  classifier  binary  classifier  model  selection  criteria  precision  at  target  recall  target  recall  epochs  40  autothresh  output  path  s3  autothresh  output  format  bucket  prefix  autothresh  predictor  predictor  from  hyperparams  s3  train  path  autothresh  hyperparams  autothresh  output  path  Training  binary  classifi,amazon
er  with  class  weights  and  automated  threshold  tuning  class  weights  hyperparams  feature  dim  30  predictor  type  binary  classifier  binary  classifier  model  selection  criteria  precision  at  target  recall  target  recall  positive  example  weight  mult  balanced  epochs  40  class  weights  output  path  s3  class  weights  output  format  bucket  prefix  class  weights  predictor  predictor  from  hyperparams  s3  train  path  class  weights  hyperparams  class  weights  output  path  Training  binary  classifier  with  hinge  loss  and  automated  threshold  tuning  svm  hyperparams  feature  dim  30  predictor  type  binary  classifier  loss  hinge  loss  binary  classifier  model  selection  criteria  precision  at  target  recall  target  recall  epochs  40  svm  output  path  s3  svm  output  format  bucket  prefix  svm  predictor  predictor  from  hyperparams  s3  train  path  svm  hyperparams  svm  output  path  Training  binary  classifier  with  hinge  loss  balanced  class  weigh,amazon
ts  and  automated  threshold  tuning  svm  balanced  hyperparams  feature  dim  30  predictor  type  binary  classifier  loss  hinge  loss  binary  classifier  model  selection  criteria  precision  at  target  recall  target  recall  positive  example  weight  mult  balanced  epochs  40  svm  balanced  output  path  s3  svm  balanced  output  format  bucket  prefix  svm  balanced  predictor  predictor  from  hyperparams  s3  train  path  svm  balanced  hyperparams  svm  balanced  output  path  Evaluate  the  trained  models  predictors  Logistic  defaults  predictor  Logistic  with  auto  threshold  autothresh  predictor  Logistic  with  class  weights  class  weights  predictor  Hinge  with  auto  threshold  svm  predictor  Hinge  with  class  weights  svm  balanced  predictor  metrics  key  evaluate  predictor  test  features  test  labels  key  False  for  key  predictor  in  predictors  items  pd  set  option  display  float  format  lambda  3f  display  pd  DataFrame  list  metrics  values  loc  Model ,amazon
 Recall  Precision  Accuracy  F1  for  predictor  in  defaults  predictor  autothresh  predictor  class  weights  predictor  svm  predictor  svm  balanced  predictor  delete  endpoint  predictor  ,amazon
library  ggplot2  library  randomForest  data  iris  head  iris  qplot  iris  Sepal  Length  geom  histogram  rf  randomForest  Species  data  iris  importance  TRUE  proximity  TRUE  table  iris  Species  rf  predicted  ,amazon
if  require  ndtv  d3  install  packages  ndtv  d3  library  ndtv  d3  install  packages  ndtv  d3  install  packages  ndtv  ,microsoft
import  boto3  import  sagemaker  import  os  region  boto3  Session  region  name  sage  client  boto3  Session  client  sagemaker  tuning  job  name  YOUR  HYPERPARAMETER  TUNING  JOB  NAME  run  this  cell  to  check  current  status  of  hyperparameter  tuning  job  tuning  job  result  sage  client  describe  hyper  parameter  tuning  job  HyperParameterTuningJobName  tuning  job  name  status  tuning  job  result  HyperParameterTuningJobStatus  if  status  Completed  print  Reminder  the  tuning  job  has  not  been  completed  job  count  tuning  job  result  TrainingJobStatusCounters  Completed  print  training  jobs  have  completed  job  count  is  minimize  tuning  job  result  HyperParameterTuningJobConfig  HyperParameterTuningJobObjective  Type  Maximize  objective  name  tuning  job  result  HyperParameterTuningJobConfig  HyperParameterTuningJobObjective  MetricName  from  pprint  import  pprint  if  tuning  job  result  get  BestTrainingJob  None  print  Best  model  found  so  far  pprint  tun,amazon
ing  job  result  BestTrainingJob  else  print  No  training  jobs  have  reported  results  yet  import  pandas  as  pd  tuner  sagemaker  HyperparameterTuningJobAnalytics  tuning  job  name  full  df  tuner  dataframe  if  len  full  df  df  full  df  full  df  FinalObjectiveValue  float  inf  if  len  df  df  df  sort  values  FinalObjectiveValue  ascending  is  minimize  print  Number  of  training  jobs  with  valid  objective  len  df  print  lowest  min  df  FinalObjectiveValue  highest  max  df  FinalObjectiveValue  pd  set  option  display  max  colwidth  Don  truncate  TrainingJobName  else  print  No  training  jobs  have  reported  valid  results  yet  dfimport  bokeh  import  bokeh  io  bokeh  io  output  notebook  from  bokeh  plotting  import  figure  show  from  bokeh  models  import  HoverTool  class  HoverHelper  def  init  self  tuning  analytics  self  tuner  tuning  analytics  def  hovertool  self  tooltips  FinalObjectiveValue  FinalObjectiveValue  TrainingJobName  TrainingJobName  for  ,amazon
in  self  tuner  tuning  ranges  keys  tooltips  append  ht  HoverTool  tooltips  tooltips  return  ht  def  tools  self  standard  tools  pan  crosshair  wheel  zoom  zoom  in  zoom  out  undo  reset  return  self  hovertool  standard  tools  hover  HoverHelper  tuner  figure  plot  width  900  plot  height  400  tools  hover  tools  axis  type  datetime  circle  source  df  TrainingStartTime  FinalObjectiveValue  show  ranges  tuner  tuning  ranges  figures  for  hp  name  hp  range  in  ranges  items  categorical  args  if  hp  range  get  Values  This  is  marked  as  categorical  Check  if  all  options  are  actually  numbers  def  is  num  try  float  return  except  return  vals  hp  range  Values  if  sum  is  num  for  in  vals  len  vals  Bokeh  has  issues  plotting  categorical  range  that  actually  numeric  so  plot  as  numeric  print  Hyperparameter  is  tuned  as  categorical  but  all  values  are  numeric  hp  name  else  Set  up  extra  options  for  plotting  categoricals  bit  tricky  ,amazon
when  they  re  actually  numbers  categorical  args  range  vals  Now  plot  it  figure  plot  width  500  plot  height  500  title  Objective  vs  hp  name  tools  hover  tools  axis  label  hp  name  axis  label  objective  name  categorical  args  circle  source  df  hp  name  FinalObjectiveValue  figures  append  show  bokeh  layouts  Column  figures  ,amazon
time  capture  pip  install  pydub  dev  null  time  import  random  import  base64  import  json  import  tarfile  import  wave  from  contextlib  import  closing  from  os  import  listdir  makedirs  from  os  path  import  isfile  join  from  pickle  import  dump  from  sagemaker  mxnet  import  MXNet  from  shutil  import  rmtree  copy2  from  urllib  request  import  urlretrieve  from  tempfile  import  gettempdir  import  boto3  import  cv2  import  matplotlib  matplotlib  use  agg  import  matplotlib  pyplot  as  plt  import  mxnet  as  mx  import  numpy  as  np  import  pandas  as  pd  import  sagemaker  from  pydub  import  AudioSegmentsagemaker  session  sagemaker  Session  rmtree  data  sentences  True  makedirs  data  sentences  urlretrieve  http  www  cs  cornell  edu  people  pabo  movie  review  data  rotten  imdb  tar  gz  data  sentences  sentences  tar  gz  tar  tarfile  open  data  sentences  sentences  tar  gz  tar  extractall  data  sentences  tar  close  with  open  data  sentences  plot,amazon
  tok  gt9  5000  encoding  ISO  8859  as  first  file  first  sentences  first  file  read  split  5000  with  open  data  sentences  quote  tok  gt9  5000  encoding  ISO  8859  as  second  file  second  sentences  second  file  read  split  5000  rmtree  data  sentences  True  makedirs  data  sentences  with  open  data  sentences  sentences  txt  as  sentences  file  for  sentence  in  first  sentences  second  sentences  sentences  file  write  format  sentence  with  open  data  sentences  sentences  txt  encoding  ISO  8859  as  sentences  file  sentences  sentences  file  read  split  voices  Ivy  Joanna  Joey  Justin  Kendra  Kimberly  Matthew  Salli  time  client  boto3  client  polly  random  seed  42  rmtree  data  mp3  True  makedirs  data  mp3  for  sentence  in  sentences  voice  random  choice  voices  file  mask  data  mp3  sample  05  mp3  format  voice  response  client  synthesize  speech  OutputFormat  mp3  Text  sentence  TextType  text  VoiceId  voice  with  open  file  mask  wb  as  out,amazon
  with  closing  response  AudioStream  as  stream  out  write  stream  read  mp3  files  sorted  for  in  listdir  data  mp3  if  isfile  join  data  mp3  time  rmtree  data  wav  True  makedirs  data  wav  sample  start  random  randint  500  1000  sample  finish  sample  start  2000  for  mp3  in  mp3  files  sound  AudioSegment  from  mp3  data  mp3  format  mp3  sample  start  sample  finish  sound  export  data  wav  wav  format  mp3  format  wav  def  graph  spectrogram  wav  file  out  wav  wave  open  wav  file  frames  wav  readframes  sound  info  np  frombuffer  frames  int16  frame  rate  wav  getframerate  wav  close  fig  plt  figure  fig  set  size  inches  ax  plt  Axes  fig  ax  set  axis  off  fig  add  axes  ax  plt  set  cmap  hot  plt  specgram  sound  info  Fs  frame  rate  plt  savefig  out  format  png  plt  close  fig  wav  files  sorted  for  in  listdir  data  wav  if  isfile  join  data  wav  time  capture  no  stdout  no  display  rmtree  data  spectrograms  True  makedirs  data ,amazon
 spectrograms  for  wav  in  wav  files  graph  spectrogram  data  wav  format  wav  data  spectrograms  png  format  wav  spectrograms  sorted  join  data  spectrograms  for  in  listdir  data  spectrograms  if  isfile  join  data  spectrograms  df  pd  DataFrame  wav  join  data  wav  for  in  wav  files  mp3  join  data  mp3  for  in  mp3  files  spectrogram  spectrograms  df  label  df  spectrogram  str  extract  sample  png  expand  False  apply  lambda  voices  index  df  voice  df  spectrogram  str  extract  sample  png  expand  False  train  df  groupby  voice  apply  lambda  sample  frac  reset  index  drop  True  validation  df  loc  df  index  isin  train  index  groupby  voice  apply  lambda  sample  frac  reset  index  drop  True  test  df  loc  np  logical  not  np  logical  xor  df  index  isin  train  index  df  index  isin  validation  index  def  transform  row  img  cv2  imread  row  spectrogram  img  mx  nd  array  img  img  img  astype  np  float32  img  mx  nd  transpose  img  img  img  ,amazon
255  label  np  float32  row  label  return  img  label  time  train  nd  transform  row  for  row  in  train  iterrows  validation  nd  transform  row  for  row  in  validation  iterrows  def  save  to  disk  data  type  makedirs  pvdwgmas  data  pickles  format  gettempdir  type  with  open  pvdwgmas  data  pickles  data  format  gettempdir  type  wb  as  out  dump  data  out  time  rmtree  pvdwgmas  format  gettempdir  True  save  to  disk  train  nd  train  save  to  disk  validation  nd  validation  time  inputs  sagemaker  session  upload  data  path  pvdwgmas  data  pickles  format  gettempdir  bucket  redacted  key  prefix  cosmin  sagemaker  demo  rmtree  pvdwgmas  format  gettempdir  True  rmtree  data  test  True  makedirs  data  test  for  row  in  test  iterrows  makedirs  data  test  format  row  voice  exist  ok  True  copy2  row  mp3  data  test  format  row  voice  estimator  MXNet  voice  recognition  sagemaker  script  py  role  sagemaker  get  execution  role  train  instance  count  train,amazon
  instance  type  ml  p2  xlarge  hyperparameters  epochs  py  version  py3  framework  version  estimator  fit  inputs  predictor  estimator  deploy  instance  type  ml  m4  xlarge  initial  instance  count  sagemaker  runtime  client  boto3  client  sagemaker  runtime  with  open  Kimberly  recites  some  shameless  self  promotion  ad  mp3  rb  as  audio  file  payload  base64  b64encode  audio  file  read  decode  utf  response  sagemaker  runtime  client  invoke  endpoint  EndpointName  predictor  endpoint  Body  payload  ContentType  audio  mp3  Accept  application  json  Body  read  print  Kimberly  predicted  as  format  json  loads  response  encoding  utf  for  directory  in  listdir  data  test  batch  cnt  total  detected  for  file  in  listdir  data  test  format  directory  with  open  data  test  format  directory  file  rb  as  audio  file  batch  append  base64  b64encode  audio  file  read  decode  utf  cnt  if  cnt  binary  json  json  dumps  batch  encode  utf  response  sagemaker  runtim,amazon
e  client  invoke  endpoint  EndpointName  predictor  endpoint  Body  binary  json  ContentType  application  json  Accept  application  json  Body  read  individual  predictions  json  loads  response  encoding  utf  for  prediction  in  individual  predictions  total  if  prediction  directory  detected  cnt  batch  print  Recordings  with  Total  Detected  Accuracy  2f  format  directory  str  total  str  detected  detected  total  ,amazon
cat  container  Dockerfile  sh  The  name  of  our  algorithm  algorithm  name  decision  trees  sample  cd  container  chmod  decision  trees  train  chmod  decision  trees  serve  account  aws  sts  get  caller  identity  query  Account  output  text  Get  the  region  defined  in  the  current  configuration  default  to  us  west  if  none  defined  region  aws  configure  get  region  region  region  us  west  fullname  account  dkr  ecr  region  amazonaws  com  algorithm  name  latest  If  the  repository  doesn  exist  in  ECR  create  it  aws  ecr  describe  repositories  repository  names  algorithm  name  dev  null  if  ne  then  aws  ecr  create  repository  repository  name  algorithm  name  dev  null  fi  Get  the  login  command  from  ECR  and  execute  it  directly  aws  ecr  get  login  region  region  no  include  email  Build  the  docker  image  locally  with  the  image  name  and  then  push  it  to  ECR  with  the  full  name  docker  build  algorithm  name  docker  tag  algorithm  name,amazon
  fullname  docker  push  fullname  S3  prefix  prefix  DEMO  scikit  byo  iris  Define  IAM  role  import  boto3  import  re  import  os  import  numpy  as  np  import  pandas  as  pd  from  sagemaker  import  get  execution  role  role  get  execution  role  import  sagemaker  as  sage  from  time  import  gmtime  strftime  sess  sage  Session  WORK  DIRECTORY  data  data  location  sess  upload  data  WORK  DIRECTORY  key  prefix  prefix  account  sess  boto  session  client  sts  get  caller  identity  Account  region  sess  boto  session  region  name  image  dkr  ecr  amazonaws  com  decision  trees  sample  latest  format  account  region  tree  sage  estimator  Estimator  image  role  ml  c4  2xlarge  output  path  s3  output  format  sess  default  bucket  sagemaker  session  sess  tree  fit  data  location  from  sagemaker  predictor  import  csv  serializer  predictor  tree  deploy  ml  m4  xlarge  serializer  csv  serializer  shape  pd  read  csv  data  iris  csv  header  None  import  itertools  ,amazon
50  for  in  range  40  for  in  range  10  indices  for  in  itertools  product  test  data  shape  iloc  indices  test  test  data  iloc  test  test  data  iloc  print  predictor  predict  test  values  decode  utf  sess  delete  endpoint  predictor  endpoint  ,amazon
import  boto3def  aws  translate  text  client  boto3  client  service  name  translate  region  name  eu  west  use  ssl  True  result  client  translate  text  Text  text  SourceLanguageCode  auto  TargetLanguageCode  en  return  result  get  TranslatedText  input  text  AWS  result  aws  translate  input  text  Print  translated  text  print  result  ,microsoft
Put  these  at  the  top  of  every  notebook  to  get  automatic  reloading  and  inline  plotting  reload  ext  autoreload  autoreload  matplotlib  inlinebucket  sagemaker  mcclean  eu  west  customize  to  the  name  of  your  S3  bucket  model  file  name  lhr  summit  demo  customize  to  the  name  of  your  model  PATH  data  dogscats  customize  to  the  relative  location  of  your  data  folder  key  models  model  file  name  model  tar  gz  prefix  of  the  S3  bucket  of  the  model  fileimport  boto3  import  re  import  osimport  torch  from  fastai  imports  import  from  fastai  transforms  import  from  fastai  conv  learner  import  from  fastai  model  import  from  fastai  dataset  import  from  fastai  sgdr  import  from  fastai  plots  import  sz  224torch  cuda  is  available  torch  backends  cudnn  enabledarch  resnet34data  ImageClassifierData  from  paths  PATH  tfms  tfms  from  model  arch  sz  learn  ConvLearner  pretrained  arch  data  precompute  True  lrf  learn  lr  find  le,amazon
arn  sched  plot  lr  learn  sched  plot  tfms  tfms  from  model  resnet34  sz  aug  tfms  transforms  side  on  max  zoom  def  get  augs  data  ImageClassifierData  from  paths  PATH  bs  tfms  tfms  num  workers  next  iter  data  aug  dl  return  data  trn  ds  denorm  ims  np  stack  get  augs  for  in  range  plots  ims  rows  data  ImageClassifierData  from  paths  PATH  tfms  tfms  learn  ConvLearner  pretrained  arch  data  precompute  True  learn  fit  1e  learn  precompute  Falselearn  fit  1e  cycle  len  learn  unfreeze  lr  np  array  1e  1e  1e  learn  fit  lr  cycle  len  cycle  mult  learn  sched  plot  lr  learn  save  model  file  name  def  get  relative  path  filename  s1  os  path  split  filename  os  path  split  s1  return  os  path  join  s1  def  create  dummy  data  src  path  dest  root  sub  dir  num  items  if  not  os  path  isdir  dest  root  os  mkdir  dest  root  dst  path  os  path  join  dest  root  sub  dir  classes  os  listdir  src  path  for  in  classes  if  startsw,amazon
ith  continue  if  not  os  path  isdir  dst  path  os  mkdir  dst  path  if  not  os  path  isdir  os  path  join  dst  path  os  mkdir  os  path  join  dst  path  fnames  glob  jpg  format  src  path  for  in  range  num  items  shutil  copyfile  fnames  os  path  join  dst  path  get  relative  path  fnames  create  dummy  data  PATH  train  PATH  models  data  train  create  dummy  data  PATH  valid  PATH  models  data  valid  tar  czvf  data  dogscats  model  tar  gz  exclude  tmp  h5  data  dogscats  models  boto3  client  s3  upload  file  PATH  model  tar  gz  bucket  key  print  Uploaded  model  artefacts  to  s3  bucket  key  ,amazon
import  mxnet  as  mx  mnist  mx  test  utils  get  mnist  batch  size  100  train  iter  mx  io  NDArrayIter  mnist  train  data  mnist  train  label  batch  size  shuffle  True  val  iter  mx  io  NDArrayIter  mnist  test  data  mnist  test  label  batch  size  data  mx  sym  var  data  Flatten  the  data  from  shape  into  batch  size  num  channel  width  height  data  mx  sym  flatten  data  data  The  first  fully  connected  layer  and  the  corresponding  activation  function  fc1  mx  sym  FullyConnected  data  data  num  hidden  128  act1  mx  sym  Activation  data  fc1  act  type  relu  The  second  fully  connected  layer  and  the  corresponding  activation  function  fc2  mx  sym  FullyConnected  data  act1  num  hidden  64  act2  mx  sym  Activation  data  fc2  act  type  relu  MNIST  has  10  classes  fc3  mx  sym  FullyConnected  data  act2  num  hidden  10  Softmax  with  cross  entropy  loss  mlp  mx  sym  SoftmaxOutput  data  fc3  name  softmax  import  logging  logging  getLogger  setLev,amazon
el  logging  DEBUG  logging  to  stdout  create  trainable  module  on  CPU  mlp  model  mx  mod  Module  symbol  mlp  context  mx  cpu  mlp  model  fit  train  iter  train  data  eval  data  val  iter  validation  data  optimizer  sgd  use  SGD  to  train  optimizer  params  learning  rate  use  fixed  learning  rate  eval  metric  acc  report  accuracy  during  training  batch  end  callback  mx  callback  Speedometer  batch  size  100  output  progress  for  each  100  data  batches  num  epoch  10  train  for  at  most  10  dataset  passestest  iter  mx  io  NDArrayIter  mnist  test  data  None  batch  size  prob  mlp  model  predict  test  iter  assert  prob  shape  10000  10  test  iter  mx  io  NDArrayIter  mnist  test  data  mnist  test  label  batch  size  predict  accuracy  of  mlp  acc  mx  metric  Accuracy  mlp  model  score  test  iter  acc  print  acc  assert  acc  get  96data  mx  sym  var  data  first  conv  layer  conv1  mx  sym  Convolution  data  data  kernel  num  filter  20  tanh1  mx  sy,amazon
m  Activation  data  conv1  act  type  tanh  pool1  mx  sym  Pooling  data  tanh1  pool  type  max  kernel  stride  second  conv  layer  conv2  mx  sym  Convolution  data  pool1  kernel  num  filter  50  tanh2  mx  sym  Activation  data  conv2  act  type  tanh  pool2  mx  sym  Pooling  data  tanh2  pool  type  max  kernel  stride  first  fullc  layer  flatten  mx  sym  flatten  data  pool2  fc1  mx  symbol  FullyConnected  data  flatten  num  hidden  500  tanh3  mx  sym  Activation  data  fc1  act  type  tanh  second  fullc  fc2  mx  sym  FullyConnected  data  tanh3  num  hidden  10  softmax  loss  lenet  mx  sym  SoftmaxOutput  data  fc2  name  softmax  create  trainable  module  on  GPU  lenet  model  mx  mod  Module  symbol  lenet  context  mx  cpu  train  with  the  same  lenet  model  fit  train  iter  eval  data  val  iter  optimizer  sgd  optimizer  params  learning  rate  eval  metric  acc  batch  end  callback  mx  callback  Speedometer  batch  size  100  num  epoch  10  test  iter  mx  io  NDArrayIt,amazon
er  mnist  test  data  None  batch  size  prob  lenet  model  predict  test  iter  test  iter  mx  io  NDArrayIter  mnist  test  data  mnist  test  label  batch  size  predict  accuracy  for  lenet  acc  mx  metric  Accuracy  lenet  model  score  test  iter  acc  print  acc  assert  acc  get  98  ,amazon
Check  core  SDK  version  number  import  azureml  core  print  SDK  version  azureml  core  VERSION  from  azureml  core  workspace  import  Workspace  ws  Workspace  from  config  print  Workspace  name  ws  name  Azure  region  ws  location  Resource  group  ws  resource  group  sep  from  azureml  core  compute  import  ComputeTarget  BatchAiCompute  from  azureml  core  compute  target  import  ComputeTargetException  choose  name  for  your  cluster  cluster  name  gpucluster  try  compute  target  ComputeTarget  workspace  ws  name  cluster  name  print  Found  existing  compute  target  except  ComputeTargetException  print  Creating  new  compute  target  compute  config  BatchAiCompute  provisioning  configuration  vm  size  STANDARD  NC6  autoscale  enabled  True  cluster  min  nodes  cluster  max  nodes  create  the  cluster  compute  target  ComputeTarget  create  ws  cluster  name  compute  config  compute  target  wait  for  completion  show  output  True  Use  the  status  property  to  get  ,microsoft
detailed  status  for  the  current  cluster  print  compute  target  status  serialize  import  os  project  folder  pytorch  mnist  os  makedirs  project  folder  exist  ok  True  import  shutil  shutil  copy  mnist  py  project  folder  from  azureml  core  import  Experiment  experiment  name  pytorch1  mnist  experiment  Experiment  ws  name  experiment  name  from  azureml  train  dnn  import  PyTorch  estimator  PyTorch  source  directory  project  folder  script  params  output  dir  outputs  compute  target  compute  target  entry  script  mnist  py  use  gpu  True  upgrade  to  PyTorch  Preview  which  has  better  support  for  ONNX  estimator  conda  dependencies  remove  conda  package  pytorch  estimator  conda  dependencies  add  conda  package  pytorch  nightly  estimator  conda  dependencies  add  channel  pytorch  run  experiment  submit  estimator  print  run  get  details  from  azureml  train  widgets  import  RunDetails  RunDetails  run  show  list  all  the  files  from  the  run  run  ,microsoft
get  file  names  import  os  model  path  os  path  join  outputs  mnist  onnx  run  download  file  model  path  output  file  path  model  path  model  run  register  model  model  name  mnist  model  path  model  path  print  model  name  model  id  model  version  sep  models  ws  models  for  in  models  print  Name  name  tVersion  version  tDescription  description  tags  writefile  score  py  import  json  import  time  import  sys  import  os  from  azureml  core  model  import  Model  import  numpy  as  np  we  re  going  to  use  numpy  to  process  input  and  output  data  import  onnxruntime  to  inference  ONNX  models  we  use  the  ONNX  Runtime  def  init  global  session  model  Model  get  model  path  model  name  mnist  session  onnxruntime  InferenceSession  model  def  preprocess  input  data  json  convert  the  JSON  data  into  the  tensor  input  return  np  array  json  loads  input  data  json  data  astype  float32  def  postprocess  result  We  use  argmax  to  pick  the  high,microsoft
est  confidence  label  return  int  np  argmax  np  array  result  squeeze  axis  def  run  input  data  json  try  start  time  time  start  timer  input  data  preprocess  input  data  json  input  name  session  get  inputs  name  get  the  id  of  the  first  input  of  the  model  result  session  run  input  name  input  data  end  time  time  stop  timer  return  result  postprocess  result  time  end  start  except  Exception  as  result  str  return  error  result  from  azureml  core  conda  dependencies  import  CondaDependencies  myenv  CondaDependencies  create  pip  packages  numpy  onnxruntime  with  open  myenv  yml  as  write  myenv  serialize  to  string  from  azureml  core  image  import  ContainerImage  image  config  ContainerImage  image  configuration  execution  script  score  py  runtime  python  conda  file  myenv  yml  description  MNIST  ONNX  Demo  tags  demo  onnx  image  ContainerImage  create  name  onnxmnistdemo  models  model  image  config  image  config  workspace  ws  im,microsoft
age  wait  for  creation  show  output  True  print  image  image  build  log  uri  from  azureml  core  webservice  import  AciWebservice  aciconfig  AciWebservice  deploy  configuration  cpu  cores  memory  gb  tags  demo  onnx  description  web  service  for  MNIST  ONNX  model  from  azureml  core  webservice  import  Webservice  from  random  import  randint  aci  service  name  onnx  demo  mnist  str  randint  100  print  Service  aci  service  name  aci  service  Webservice  deploy  from  image  deployment  config  aciconfig  image  image  name  aci  service  name  workspace  ws  aci  service  wait  for  deployment  True  print  aci  service  state  if  aci  service  state  Healthy  run  this  command  for  debugging  print  aci  service  get  logs  aci  service  delete  print  aci  service  scoring  uri  aci  service  delete  ,microsoft
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  videogames  xgboost  Define  IAM  role  import  sagemaker  role  sagemaker  get  execution  role  import  numpy  as  np  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  from  IPython  display  import  Image  from  IPython  display  import  display  from  sklearn  datasets  import  dump  svmlight  file  from  time  import  gmtime  strftime  import  sys  import  math  import  json  import  boto3raw  data  filename  Video  Games  Sales  as  at  22  Dec  2016  csv  s3  boto3  resource  s3  s3  Bucket  bucket  download  file  prefix  raw  data  filename  raw  data  csv  data  pd  read  csv  raw  data  csv  pd  set  option  display  max  rows  20  datadata  data  Global  Sales  plt  bar  not  hit  hit  data  value  counts  plt  show  viz  data  filter  User  Score  Critic  Score  Global  Sales  axis  viz  User  Score  pd  Series  viz  User  Score  apply  pd  to  numeric  errors  coerce  viz  User  Score  viz  User  Score  mask  np  isnan  v,amazon
iz  User  Score  viz  Critic  Score  10  viz  plot  kind  scatter  logx  True  logy  True  Critic  Score  Global  Sales  viz  plot  kind  scatter  logx  True  logy  True  User  Score  Global  Sales  plt  show  data  data  drop  Name  Year  of  Release  NA  Sales  EU  Sales  JP  Sales  Other  Sales  Global  Sales  Critic  Count  User  Count  Developer  axis  data  isnull  sum  data  data  dropna  data  User  Score  data  User  Score  apply  pd  to  numeric  errors  coerce  data  User  Score  data  User  Score  mask  np  isnan  data  User  Score  data  Critic  Score  10  data  data  apply  lambda  yes  if  True  else  no  model  data  pd  get  dummies  data  train  data  validation  data  test  data  np  split  model  data  sample  frac  random  state  1729  int  len  model  data  int  len  model  data  dump  svmlight  file  train  data  drop  no  yes  axis  train  data  yes  train  libsvm  dump  svmlight  file  validation  data  drop  no  yes  axis  validation  data  yes  validation  libsvm  dump  svmlight  fi,amazon
le  test  data  drop  no  yes  axis  test  data  yes  test  libsvm  boto3  Session  resource  s3  Bucket  bucket  Object  prefix  train  train  libsvm  upload  file  train  libsvm  boto3  Session  resource  s3  Bucket  bucket  Object  prefix  validation  validation  libsvm  upload  file  validation  libsvm  job  name  DEMO  videogames  xgboost  strftime  gmtime  print  Training  job  job  name  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  boto3  Session  region  name  xgboost  create  training  params  RoleArn  role  TrainingJobName  job  name  AlgorithmSpecification  TrainingImage  container  TrainingInputMode  File  ResourceConfig  InstanceCount  InstanceType  ml  c4  xlarge  VolumeSizeInGB  10  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  S3DataDistributionType  FullyReplicated  ContentType  libsvm  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3,amazon
DataType  S3Prefix  S3Uri  s3  validation  format  bucket  prefix  S3DataDistributionType  FullyReplicated  ContentType  libsvm  CompressionType  None  OutputDataConfig  S3OutputPath  s3  xgboost  video  games  output  format  bucket  prefix  HyperParameters  max  depth  eta  eval  metric  auc  scale  pos  weight  subsample  objective  binary  logistic  num  round  100  StoppingCondition  MaxRuntimeInSeconds  60  60  time  sm  boto3  client  sagemaker  sm  create  training  job  create  training  params  status  sm  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  status  try  sm  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  finally  status  sm  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  Training  job  ended  with  status  status  if  status  Failed  message  sm  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  rais,amazon
e  Exception  Training  job  failed  create  model  response  sm  create  model  ModelName  job  name  ExecutionRoleArn  role  PrimaryContainer  Image  container  ModelDataUrl  sm  describe  training  job  TrainingJobName  job  name  ModelArtifacts  S3ModelArtifacts  print  create  model  response  ModelArn  xgboost  endpoint  config  DEMO  videogames  xgboost  config  strftime  gmtime  print  xgboost  endpoint  config  create  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  xgboost  endpoint  config  ProductionVariants  InstanceType  ml  t2  medium  InitialInstanceCount  ModelName  job  name  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  xgboost  endpoint  DEMO  videogames  xgboost  endpoint  strftime  gmtime  print  xgboost  endpoint  create  endpoint  response  sm  create  endpoint  EndpointName  xgboost  endpoint  EndpointConfigName  xgboost  endpoint  config  print  create  endpoint  response  EndpointArn ,amazon
 resp  sm  describe  endpoint  EndpointName  xgboost  endpoint  status  resp  EndpointStatus  print  Status  status  try  sm  get  waiter  endpoint  in  service  wait  EndpointName  xgboost  endpoint  finally  resp  sm  describe  endpoint  EndpointName  xgboost  endpoint  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Status  status  if  status  InService  message  sm  describe  endpoint  EndpointName  xgboost  endpoint  FailureReason  print  Endpoint  creation  failed  with  the  following  error  format  message  raise  Exception  Endpoint  creation  did  not  succeed  runtime  boto3  client  runtime  sagemaker  def  do  predict  data  endpoint  name  content  type  payload  join  data  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  content  type  Body  payload  result  response  Body  read  result  result  decode  utf  result  result  split  preds  float  num  for  num  in  result  preds  round  num  for  num  in  preds  return  preds  def  batch  predi,amazon
ct  data  batch  size  endpoint  name  content  type  items  len  data  arrs  for  offset  in  range  items  batch  size  if  offset  batch  size  items  results  do  predict  data  offset  offset  batch  size  endpoint  name  content  type  arrs  extend  results  else  arrs  extend  do  predict  data  offset  items  endpoint  name  content  type  sys  stdout  write  return  arrs  time  import  json  with  open  test  libsvm  as  payload  read  strip  labels  int  line  split  for  line  in  payload  split  test  data  line  for  line  in  payload  split  preds  batch  predict  test  data  100  xgboost  endpoint  text  libsvm  print  nerror  rate  sum  for  in  range  len  preds  if  preds  labels  float  len  preds  pd  crosstab  index  np  array  labels  columns  np  array  preds  sm  delete  endpoint  EndpointName  xgboost  endpoint  ,amazon
import  sagemaker  from  sagemaker  mxnet  import  MXNet  sagemaker  session  sagemaker  Session  role  sagemaker  get  execution  role  bin  bash  setup  shfrom  cifar10  utils  import  download  training  data  download  training  data  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  DEMO  gluon  cifar10  print  input  spec  in  this  case  just  an  S3  path  format  inputs  cat  cifar10  py  MXNet  cifar10  py  role  role  train  instance  count  train  instance  type  local  gpu  framework  version  hyperparameters  batch  size  1024  epochs  50  learning  rate  momentum  fit  inputs  predictor  deploy  initial  instance  count  instance  type  local  gpu  load  the  CIFAR  10  samples  and  convert  them  into  format  we  can  use  with  the  prediction  endpoint  from  cifar10  utils  import  read  images  filenames  images  airplane1  png  images  automobile1  png  images  bird1  png  images  cat1  png  images  deer1  png  images  dog1  png  images  frog1  png  images  horse1,amazon
  png  images  ship1  png  images  truck1  png  image  data  read  images  filenames  for  img  in  enumerate  image  data  response  predictor  predict  img  print  image  class  format  int  response  delete  endpoint  ,amazon
hidden  cell  The  project  token  is  an  authorization  token  that  is  used  to  access  project  resources  like  data  sources  connections  and  used  by  platform  APIs  from  project  lib  import  Project  project  Project  project  id  67e68ea0  ec4c  44b9  aefb  1599604634c7  project  access  token  9e00a5da772e2686731bca23576cfb5544a5fb87  pc  project  project  context  import  os  import  numpy  as  np  import  pandas  as  pd  from  datetime  import  datetime  import  matplotlib  pyplot  as  plt  matplotlib  inline  pip  install  upgrade  user  pyflux  import  pyflux  as  pfdata  pd  read  csv  project  get  file  cpu  full  csv  parse  dates  infer  datetime  format  True  data  train  pd  read  csv  project  get  file  cpu  train  csv  parse  dates  infer  datetime  format  True  data  test  pd  read  csv  project  get  file  cpu  test  csv  parse  dates  infer  datetime  format  True  data  pd  read  csv  project  get  file  cpu  full  csv  parse  dates  infer  datetime  format  True  data  tr,ibm
ain  pd  read  csv  project  get  file  cpu  train  csv  parse  dates  infer  datetime  format  True  data  test  pd  read  csv  project  get  file  cpu  test  csv  parse  dates  infer  datetime  format  True  plt  figure  figsize  20  plt  plot  data  train  datetime  data  train  cpu  color  black  plt  ylabel  CPU  plt  title  CPU  Utilization  plt  figure  figsize  20  plt  plot  data  datetime  data  cpu  color  black  plt  ylabel  CPU  plt  title  CPU  Utilization  plt  axvspan  xmin  pd  Timestamp  datetime  2017  28  42  xmax  pd  Timestamp  datetime  2017  28  41  color  d4d4d4  plt  figure  figsize  20  plt  plot  data  train  datetime  data  train  cpu  color  black  plt  ylabel  CPU  plt  title  CPU  Utilization  plt  figure  figsize  20  plt  plot  data  datetime  data  cpu  color  black  plt  ylabel  CPU  plt  title  CPU  Utilization  plt  axvspan  xmin  pd  Timestamp  datetime  2017  28  42  xmax  pd  Timestamp  datetime  2017  28  41  color  d4d4d4  model  pf  ARIMA  data  data  train  ar  11 ,ibm
 ma  11  integ  target  cpu  model  fit  model  plot  fit  figsize  20  model  plot  predict  60  past  values  100  figsize  20  model  plot  predict  is  60  figsize  20  model  pf  ARIMA  data  data  train  ar  11  ma  11  integ  target  cpu  model  fit  model  plot  predict  60  past  values  100  figsize  20  ,ibm
bucket  SageMakerS3Bucket  prefix  sagemaker  DEMO  byo  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  time  import  json  import  os  import  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  sh  The  name  of  our  algorithm  algorithm  name  rmars  set  stop  if  anything  fails  account  aws  sts  get  caller  identity  query  Account  output  text  Get  the  region  defined  in  the  current  configuration  default  to  us  west  if  none  defined  region  aws  configure  get  region  region  region  us  west  fullname  account  dkr  ecr  region  amazonaws  com  algorithm  name  latest  If  the  repository  doesn  exist  in  ECR  create  it  aws  ecr  describe  repositories  repository  names  algorithm  name  dev  null  if  ne  then  aws  ecr  create  repository  repository  name  algorithm  name  dev  null  fi  Get  the  login  command  from  ECR  and  execute  it  directly  aws  ecr  get  login,amazon
  region  region  no  include  email  Build  the  docker  image  locally  with  the  image  name  and  then  push  it  to  ECR  with  the  full  name  docker  build  algorithm  name  docker  tag  algorithm  name  fullname  docker  push  fullname  train  file  iris  csv  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  train  file  upload  file  train  file  region  boto3  Session  region  name  account  boto3  client  sts  get  caller  identity  get  Account  job  DEMO  byo  time  strftime  time  gmtime  print  Training  job  job  training  params  RoleArn  role  TrainingJobName  job  AlgorithmSpecification  TrainingImage  dkr  ecr  amazonaws  com  rmars  latest  format  account  region  TrainingInputMode  File  ResourceConfig  InstanceCount  InstanceType  ml  m4  xlarge  VolumeSizeInGB  10  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  S3DataDistributionType  FullyReplicated  CompressionType  ,amazon
None  RecordWrapperType  None  OutputDataConfig  S3OutputPath  s3  output  format  bucket  prefix  HyperParameters  target  Sepal  Length  degree  StoppingCondition  MaxRuntimeInSeconds  60  60  time  sm  boto3  client  sagemaker  sm  create  training  job  training  params  status  sm  describe  training  job  TrainingJobName  job  TrainingJobStatus  print  status  sm  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  status  sm  describe  training  job  TrainingJobName  job  TrainingJobStatus  print  Training  job  ended  with  status  status  if  status  Failed  message  sm  describe  training  job  TrainingJobName  job  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  hosting  container  Image  dkr  ecr  amazonaws  com  rmars  latest  format  account  region  ModelDataUrl  sm  describe  training  job  TrainingJobName  job  ModelArtifacts  S3ModelArtifacts  create  model  response  sm  create  model ,amazon
 ModelName  job  ExecutionRoleArn  role  PrimaryContainer  hosting  container  print  create  model  response  ModelArn  endpoint  config  DEMO  byo  config  time  strftime  time  gmtime  print  endpoint  config  create  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  endpoint  config  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  job  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  endpoint  DEMO  endpoint  time  strftime  time  gmtime  print  endpoint  create  endpoint  response  sm  create  endpoint  EndpointName  endpoint  EndpointConfigName  endpoint  config  print  create  endpoint  response  EndpointArn  resp  sm  describe  endpoint  EndpointName  endpoint  status  resp  EndpointStatus  print  Status  status  try  sm  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  finally  resp  sm  describe  endpoint  EndpointName  endpoint  status  resp  EndpointSta,amazon
tus  print  Arn  resp  EndpointArn  print  Status  status  if  status  InService  raise  Exception  Endpoint  creation  did  not  succeed  iris  pd  read  csv  iris  csv  runtime  boto3  Session  client  runtime  sagemaker  payload  iris  drop  Sepal  Length  axis  to  csv  index  False  response  runtime  invoke  endpoint  EndpointName  endpoint  ContentType  text  csv  Body  payload  result  json  loads  response  Body  read  decode  result  plt  scatter  iris  Sepal  Length  np  fromstring  result  sep  plt  show  sm  delete  endpoint  EndpointName  endpoint  ,amazon
cat  container  Dockerfile  sh  The  name  of  our  algorithm  algorithm  name  pytorch  extending  our  containers  cifar10  example  cd  container  account  aws  sts  get  caller  identity  query  Account  output  text  Get  the  region  defined  in  the  current  configuration  default  to  us  west  if  none  defined  region  aws  configure  get  region  region  region  us  west  fullname  account  dkr  ecr  region  amazonaws  com  algorithm  name  latest  If  the  repository  doesn  exist  in  ECR  create  it  aws  ecr  describe  repositories  repository  names  algorithm  name  dev  null  if  ne  then  aws  ecr  create  repository  repository  name  algorithm  name  dev  null  fi  Get  the  login  command  from  ECR  and  execute  it  directly  aws  ecr  get  login  region  region  no  include  email  Get  the  login  command  from  ECR  in  order  to  pull  down  the  SageMaker  PyTorch  image  aws  ecr  get  login  registry  ids  520713654638  region  region  no  include  email  Build  the  docker  im,amazon
age  locally  with  the  image  name  and  then  push  it  to  ECR  with  the  full  name  docker  build  algorithm  name  docker  tag  algorithm  name  fullname  docker  push  fullname  from  utils  utils  cifar  import  get  train  data  loader  get  test  data  loader  imshow  classes  trainloader  get  train  data  loader  tmp  pytorch  example  cifar  10  data  testloader  get  test  data  loader  tmp  pytorch  example  cifar  10  data  There  should  be  the  original  tar  file  along  with  the  extracted  data  directory  cifar  10  python  tar  gz  cifar  10  batches  py  ls  tmp  pytorch  example  cifar  10  datafrom  sagemaker  import  get  execution  role  role  get  execution  role  Lets  set  up  our  SageMaker  notebook  instance  for  local  mode  bin  bash  utils  setup  shimport  os  import  subprocess  instance  type  local  if  subprocess  call  nvidia  smi  Set  type  to  GPU  if  one  is  present  instance  type  local  gpu  print  Instance  type  instance  type  from  sagemaker  estima,amazon
tor  import  Estimator  hyperparameters  epochs  estimator  Estimator  role  role  train  instance  count  train  instance  type  instance  type  image  name  pytorch  extending  our  containers  cifar10  example  latest  hyperparameters  hyperparameters  estimator  fit  file  tmp  pytorch  example  cifar  10  data  predictor  estimator  deploy  instance  type  import  torchvision  torch  import  numpy  as  np  from  sagemaker  predictor  import  json  serializer  json  deserializer  get  some  test  images  dataiter  iter  testloader  images  labels  dataiter  next  print  images  imshow  torchvision  utils  make  grid  images  print  GroundTruth  join  4s  classes  labels  for  in  range  predictor  accept  application  json  predictor  content  type  application  json  predictor  serializer  json  serializer  predictor  deserializer  json  deserializer  outputs  predictor  predict  images  numpy  predicted  torch  max  torch  from  numpy  np  array  outputs  print  Predicted  join  4s  classes  predicted  ,amazon
for  in  range  predictor  delete  endpoint  S3  prefix  prefix  DEMO  pytorch  cifar10  import  sagemaker  as  sage  sess  sage  Session  WORK  DIRECTORY  tmp  pytorch  example  cifar  10  data  data  location  sess  upload  data  WORK  DIRECTORY  key  prefix  prefix  import  boto3  client  boto3  client  sts  account  client  get  caller  identity  Account  my  session  boto3  session  Session  region  my  session  region  name  algorithm  name  pytorch  extending  our  containers  cifar10  example  ecr  image  dkr  ecr  amazonaws  com  latest  format  account  region  algorithm  name  print  ecr  image  from  sagemaker  estimator  import  Estimator  hyperparameters  epochs  instance  type  ml  m4  xlarge  estimator  Estimator  role  role  train  instance  count  train  instance  type  instance  type  image  name  ecr  image  hyperparameters  hyperparameters  estimator  fit  data  location  predictor  estimator  deploy  instance  type  get  some  test  images  dataiter  iter  testloader  images  labels  dat,amazon
aiter  next  print  images  imshow  torchvision  utils  make  grid  images  print  GroundTruth  join  4s  classes  labels  for  in  range  predictor  accept  application  json  predictor  content  type  application  json  predictor  serializer  json  serializer  predictor  deserializer  json  deserializer  outputs  predictor  predict  images  numpy  predicted  torch  max  torch  from  numpy  np  array  outputs  print  Predicted  join  4s  classes  predicted  for  in  range  predictor  delete  endpoint  ,amazon
bash  wget  https  archive  ics  uci  edu  ml  machine  learning  databases  covtype  covtype  data  gz  mkdir  tmp  covtype  raw  mv  covtype  data  gz  tmp  covtype  raw  covtype  data  gz  import  numpy  as  np  import  os  data  dir  tmp  covtype  processed  subdir  standardized  raw  data  file  os  path  join  data  dir  raw  covtype  data  gz  train  features  file  os  path  join  data  dir  processed  subdir  train  csv  features  csv  train  labels  file  os  path  join  data  dir  processed  subdir  train  csv  labels  csv  test  features  file  os  path  join  data  dir  processed  subdir  test  csv  features  csv  test  labels  file  os  path  join  data  dir  processed  subdir  test  csv  labels  csv  read  raw  data  print  Reading  raw  data  from  format  raw  data  file  raw  np  loadtxt  raw  data  file  delimiter  split  into  train  test  with  90  10  split  np  random  seed  np  random  shuffle  raw  train  size  int  raw  shape  train  features  raw  train  size  train  labels  raw  tr,amazon
ain  size  test  features  raw  train  size  test  labels  raw  train  size  import  io  import  sagemaker  amazon  common  as  smac  print  train  features  shape  train  features  shape  print  train  labels  shape  train  labels  shape  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  train  features  train  labels  buf  seek  import  boto3  import  os  import  sagemaker  bucket  sagemaker  Session  default  bucket  modify  to  your  bucket  name  prefix  knn  blog  2018  04  17  key  recordio  pb  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  buf  s3  train  data  s3  train  format  bucket  prefix  key  print  uploaded  training  data  location  format  s3  train  data  print  test  features  shape  test  features  shape  print  test  labels  shape  test  labels  shape  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  test  features  test  labels  buf  seek  boto3  resource  s3  Bucket  bucket  Object  os  path  join  pref,amazon
ix  test  key  upload  fileobj  buf  s3  test  data  s3  test  format  bucket  prefix  key  print  uploaded  test  data  location  format  s3  test  data  import  matplotlib  pyplot  as  plt  import  sagemaker  from  sagemaker  import  get  execution  role  from  sagemaker  predictor  import  csv  serializer  json  deserializer  def  trained  estimator  from  hyperparams  s3  train  data  hyperparams  output  path  s3  test  data  None  Create  an  Estimator  from  the  given  hyperparams  fit  to  training  data  and  return  deployed  predictor  specify  algorithm  containers  These  contain  the  code  for  the  training  job  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  knn  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  knn  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  knn  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  knn  ap  northeast  351501993468  dkr  ecr  ap  northeast  amazonaws  com  knn  ap  northeast  835164637446  dkr  ecr  ,amazon
ap  northeast  amazonaws  com  knn  ap  southeast  712309505854  dkr  ecr  ap  southeast  amazonaws  com  knn  set  up  the  estimator  knn  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  get  execution  role  train  instance  count  train  instance  type  ml  m5  2xlarge  output  path  output  path  sagemaker  session  sagemaker  Session  knn  set  hyperparameters  hyperparams  train  model  fit  input  contains  the  locations  of  the  train  and  test  data  fit  input  train  s3  train  data  if  s3  test  data  is  not  None  fit  input  test  s3  test  data  knn  fit  fit  input  return  knnhyperparams  feature  dim  54  10  sample  size  200000  predictor  type  classifier  output  path  s3  bucket  prefix  default  example  output  knn  estimator  trained  estimator  from  hyperparams  s3  train  data  hyperparams  output  path  s3  test  data  s3  test  data  def  predictor  from  estimator  knn  estimator  estimator  name  instance  type  endpoint  name  None  knn  predi,amazon
ctor  knn  estimator  deploy  initial  instance  count  instance  type  instance  type  endpoint  name  endpoint  name  knn  predictor  content  type  text  csv  knn  predictor  serializer  csv  serializer  knn  predictor  deserializer  json  deserializer  return  knn  predictorimport  time  instance  type  ml  m4  xlarge  model  name  knn  instance  type  endpoint  name  knn  ml  m4  xlarge  str  time  time  replace  print  setting  up  the  endpoint  predictor  predictor  from  estimator  knn  estimator  model  name  instance  type  endpoint  name  endpoint  name  batches  np  array  split  test  features  100  print  data  split  into  100  batches  of  size  batches  shape  obtain  an  np  array  with  the  predictions  for  the  entire  test  set  start  time  time  time  predictions  for  batch  in  batches  result  predictor  predict  batch  cur  predictions  np  array  result  predictions  predicted  label  for  in  range  len  result  predictions  predictions  append  cur  predictions  predictions  n,amazon
p  concatenate  predictions  run  time  time  time  start  time  test  size  test  labels  shape  num  correct  sum  predictions  test  labels  accuracy  num  correct  float  test  size  print  time  required  for  predicting  data  point  2f  seconds  test  size  run  time  print  accuracy  of  model  1f  accuracy  100  def  delete  endpoint  predictor  try  boto3  client  sagemaker  delete  endpoint  EndpointName  predictor  endpoint  print  Deleted  format  predictor  endpoint  except  print  Already  deleted  format  predictor  endpoint  delete  endpoint  predictor  import  matplotlib  pyplot  as  plt  import  sagemaker  from  sagemaker  import  get  execution  role  from  sagemaker  predictor  import  csv  serializer  json  deserializer  def  trained  estimator  from  hyperparams  s3  train  data  hyperparams  output  path  s3  test  data  None  Create  an  Estimator  from  the  given  hyperparams  fit  to  training  data  and  return  deployed  predictor  specify  algorithm  containers  These  contain  ,amazon
the  code  for  the  training  job  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  knn  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  knn  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  knn  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  knn  ap  northeast  351501993468  dkr  ecr  ap  northeast  amazonaws  com  knn  ap  northeast  835164637446  dkr  ecr  ap  northeast  amazonaws  com  knn  ap  southeast  712309505854  dkr  ecr  ap  southeast  amazonaws  com  knn  set  up  the  estimator  knn  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  get  execution  role  train  instance  count  train  instance  type  ml  m5  2xlarge  output  path  output  path  sagemaker  session  sagemaker  Session  knn  set  hyperparameters  hyperparams  train  model  fit  input  contains  the  locations  of  the  train  and  test  data  fit  input  train  s3  train  data  if  s3  test  data  is  not  None  fit  input  test  s3  test  data  kn,amazon
n  fit  fit  input  return  knndef  predictor  from  hyperparams  knn  estimator  estimator  name  instance  type  endpoint  name  None  knn  predictor  knn  estimator  deploy  initial  instance  count  instance  type  instance  type  endpoint  name  endpoint  name  knn  predictor  content  type  text  csv  knn  predictor  serializer  csv  serializer  knn  predictor  deserializer  json  deserializer  return  knn  predictorhyperparams  flat  l2  feature  dim  54  100  sample  size  200000  predictor  type  classifier  NOTE  The  default  distance  is  L2  and  index  is  Flat  so  we  don  list  them  here  output  path  flat  l2  s3  bucket  prefix  flat  l2  output  knn  estimator  flat  l2  trained  estimator  from  hyperparams  s3  train  data  hyperparams  flat  l2  output  path  flat  l2  s3  test  data  s3  test  data  hyperparams  flat  l2  large  feature  dim  54  100  sample  size  500000  predictor  type  classifier  output  path  flat  l2  large  s3  bucket  prefix  flat  l2  large  output  knn  es,amazon
timator  flat  l2  large  trained  estimator  from  hyperparams  s3  train  data  hyperparams  flat  l2  large  output  path  flat  l2  large  s3  test  data  s3  test  data  hyperparams  ivfpq  l2  feature  dim  54  100  sample  size  200000  predictor  type  classifier  index  type  faiss  IVFPQ  index  metric  L2  faiss  index  ivf  nlists  500  faiss  index  pq  number  of  distinct  quantizers  In  our  case  dim  54  so  every  quantizer  will  code  up  to  numbers  Each  block  of  up  to  numbers  will  be  coded  into  single  byte  output  path  ivfpq  l2  s3  bucket  prefix  ivfpq  l2  output  knn  estimator  ivfpq  l2  trained  estimator  from  hyperparams  s3  train  data  hyperparams  ivfpq  l2  output  path  ivfpq  l2  s3  test  data  s3  test  data  hyperparams  ivfpq  l2  large  feature  dim  54  100  sample  size  500000  predictor  type  classifier  index  type  faiss  IVFPQ  index  metric  L2  faiss  index  ivf  nlists  700  faiss  index  pq  output  path  ivfpq  l2  large  s3  bucket  pr,amazon
efix  ivfpq  l2  large  output  knn  estimator  ivfpq  l2  large  trained  estimator  from  hyperparams  s3  train  data  hyperparams  ivfpq  l2  large  output  path  ivfpq  l2  large  s3  test  data  s3  test  data  from  scipy  import  stats  import  matplotlib  pyplot  as  plt  from  sklearn  metrics  import  f1  score  accuracy  score  import  time  def  scores  for  ks  test  labels  knn  labels  ks  f1  weight  f1  macro  f1  micro  acc  for  in  ks  pred  stats  mode  knn  labels  axis  reshape  f1  weight  append  f1  score  test  labels  pred  average  weighted  f1  macro  append  f1  score  test  labels  pred  average  macro  f1  micro  append  f1  score  test  labels  pred  average  micro  acc  append  accuracy  score  test  labels  pred  return  f1  weight  f1  weight  f1  macro  f1  macro  f1  micro  f1  micro  accuracy  acc  def  plot  prediction  quality  scores  ks  colors  len  scores  for  color  in  zip  scores  items  colors  plt  plot  ks  color  label  plt  legend  plt  xlabel  plt  ylab,amazon
el  prediction  quality  plt  show  def  evaluate  quality  predictor  test  features  test  labels  model  name  verbose  True  num  batches  100  Evaluate  quality  metrics  of  model  on  test  set  tune  the  predictor  to  provide  the  verbose  response  predictor  accept  application  json  verbose  true  split  the  test  data  set  into  num  batches  batches  and  evaluate  using  prediction  endpoint  print  running  prediction  quality  batches  np  array  split  test  features  num  batches  knn  labels  for  batch  in  batches  pred  result  predictor  predict  batch  cur  knn  labels  np  array  pred  result  predictions  labels  for  in  range  len  pred  result  predictions  knn  labels  append  cur  knn  labels  knn  labels  np  concatenate  knn  labels  print  running  prediction  quality  done  figure  out  different  values  top  knn  labels  shape  ks  range  top  compute  scores  for  the  quality  of  the  model  for  each  value  of  print  computing  scores  for  all  values  of  qua,amazon
lity  scores  scores  for  ks  test  labels  knn  labels  ks  print  computing  scores  for  all  values  of  done  if  verbose  plot  prediction  quality  quality  scores  ks  return  quality  scores  def  evaluate  latency  predictor  test  features  test  labels  model  name  verbose  True  num  batches  100  Evaluate  the  run  time  of  model  on  test  set  tune  the  predictor  to  provide  the  non  verbose  response  predictor  accept  application  json  latency  for  large  batches  split  the  test  data  set  into  num  batches  batches  and  evaluate  the  latencies  of  the  calls  to  endpoint  print  running  prediction  latency  batches  np  array  split  test  features  num  batches  test  preds  latency  sum  for  batch  in  batches  start  time  time  pred  batch  predictor  predict  batch  latency  sum  time  time  start  latency  mean  latency  sum  float  num  batches  avg  batch  size  test  features  shape  num  batches  estimate  the  latency  for  batch  of  size  latencies  attempt,amazon
s  2000  for  in  range  attempts  start  time  time  pred  batch  predictor  predict  test  features  reshape  latencies  append  time  time  start  latencies  sorted  latencies  latency1  mean  sum  latencies  float  attempts  latency1  p90  latencies  int  attempts  latency1  p99  latencies  int  attempts  99  print  running  prediction  latency  done  if  verbose  print  11  3f  format  Latency  ms  batch  size  avg  batch  size  latency  mean  1000  print  11  3f  format  Latency  ms  mean  for  single  item  latency1  mean  1000  print  11  3f  format  Latency  ms  p90  for  single  item  latency1  p90  1000  print  11  3f  format  Latency  ms  p99  for  single  item  latency1  p99  1000  return  Latency  latency  mean  Latency1  mean  latency1  mean  Latency1  p90  latency1  p90  Latency1  p99  latency1  p99  def  evaluate  predictor  test  features  test  labels  model  name  verbose  True  num  batches  100  eval  result  evaluate  quality  pred  test  features  test  labels  model  name  model  name,amazon
  verbose  verbose  num  batches  num  batches  eval  result  evaluate  latency  pred  test  features  test  labels  model  name  model  name  verbose  verbose  num  batches  num  batches  return  dict  list  eval  result  items  list  eval  result  items  import  time  instance  types  ml  c5  xlarge  ml  p2  xlarge  index2estimator  flat  l2  knn  estimator  flat  l2  ivfpq  l2  knn  estimator  ivfpq  l2  flat  l2  large  knn  estimator  flat  l2  large  ivfpq  l2  large  knn  estimator  ivfpq  l2  large  eval  results  for  index  in  index2estimator  estimator  index2estimator  index  eval  results  index  for  instance  type  in  instance  types  model  name  knn  index  instance  type  endpoint  name  knn  latency  index  replace  instance  type  replace  str  time  time  replace  print  nsetting  up  endpoint  for  instance  type  index  type  instance  type  index  pred  predictor  from  hyperparams  estimator  index  instance  type  endpoint  name  endpoint  name  print  eval  result  evaluate  pred ,amazon
 test  features  test  labels  model  name  model  name  verbose  True  eval  result  instance  instance  type  eval  result  index  index  eval  results  index  instance  type  eval  result  delete  endpoint  pred  import  pandas  as  pd  range  range  13  df  index  data  columns  lat  latency1K  latency1  mean  latency1  p90  latency1  p99  columns  acc  acc  for  in  range  columns  columns  lat  columns  acc  for  index  index  res  in  eval  results  items  for  instance  res  in  index  res  items  for  sample  size  df  index  append  index  instance  latencies  np  array  res  Latency  res  Latency1  mean  res  Latency1  p90  res  Latency1  p99  row  np  concatenate  latencies  10  res  accuracy  range  range  row  100  data  append  row  df  pd  DataFrame  index  df  index  data  data  columns  columns  df  acc  df  columns  acc  df  lat  df  columns  lat  def  highlight  apx  max  row  highlight  the  aproximate  best  max  or  min  in  Series  yellow  max  val  row  max  colors  background  color ,amazon
 yellow  if  cur  val  max  val  9975  else  for  cur  val  in  row  return  colors  df  acc  round  decimals  style  apply  highlight  apx  max  axis  def  highlight  far  from  min  row  highlight  the  aproximate  best  max  or  min  in  Series  yellow  med  val  row  median  colors  background  color  red  if  cur  val  med  val  else  for  cur  val  in  row  return  colors  df  lat  round  decimals  style  apply  highlight  far  from  min  axis  ,amazon
pip  install  upgrade  watson  developer  cloud  pip  install  upgrade  beautifulsoup4  pip  install  user  upgrade  pixiedustimport  json  import  sys  import  watson  developer  cloud  from  watson  developer  cloud  import  ToneAnalyzerV3  VisualRecognitionV3  from  watson  developer  cloud  import  NaturalLanguageUnderstandingV1  from  watson  developer  cloud  natural  language  understanding  v1  import  Features  EntitiesOptions  KeywordsOptions  EmotionOptions  SentimentOptions  import  operator  from  functools  import  reduce  from  io  import  StringIO  import  numpy  as  np  from  bs4  import  BeautifulSoup  as  bs  from  operator  import  itemgetter  from  os  path  import  join  dirname  import  pandas  as  pd  import  numpy  as  np  import  requests  import  pixiedust  Suppress  some  pandas  warnings  pd  options  mode  chained  assignment  None  default  warn  hidden  cell  Watson  Visual  Recognition  VISUAL  RECOGNITION  API  KEY  add  vr  api  key  Watson  Natural  Launguage  Understanding,ibm
  NLU  NATURAL  LANGUAGE  UNDERSTANDING  USERNAME  add  nlu  username  NATURAL  LANGUAGE  UNDERSTANDING  PASSWORD  add  nlu  password  Watson  Tone  Analyzer  TONE  ANALYZER  USERNAME  add  tone  analyzer  username  TONE  ANALYZER  PASSWORD  add  tone  analyzer  password  Create  the  Watson  clients  nlu  watson  developer  cloud  NaturalLanguageUnderstandingV1  version  2017  02  27  username  NATURAL  LANGUAGE  UNDERSTANDING  USERNAME  password  NATURAL  LANGUAGE  UNDERSTANDING  PASSWORD  tone  analyzer  ToneAnalyzerV3  version  2016  05  19  username  TONE  ANALYZER  USERNAME  password  TONE  ANALYZER  PASSWORD  visual  recognition  VisualRecognitionV3  2016  05  20  api  key  VISUAL  RECOGNITION  API  KEY  Insert  to  code  Insert  Pandas  DataFrame  Make  sure  this  uses  the  variable  above  The  number  will  vary  in  the  inserted  code  try  df  df  data  except  NameError  as  print  Error  Setup  is  incorrect  or  incomplete  print  Follow  the  instructions  to  insert  the  pandas  DataFrame,ibm
  above  and  edit  to  print  make  the  generated  df  data  variable  match  the  variable  used  here  raise  insert  credentials  for  file  Change  to  credentials  hidden  cell  The  following  code  contains  the  credentials  for  file  in  your  IBM  Cloud  Object  Storage  You  might  want  to  remove  those  credentials  before  you  share  your  notebook  Make  sure  this  uses  the  variable  above  The  number  will  vary  in  the  inserted  code  try  credentials  credentials  except  NameError  as  print  Error  Setup  is  incorrect  or  incomplete  print  Follow  the  instructions  to  insert  the  file  credentials  above  and  edit  to  print  make  the  generated  credentials  variable  match  the  variable  used  here  raisedf  rename  columns  Post  Message  Text  inplace  True  Drop  the  rows  that  have  NaN  for  the  text  df  dropna  subset  Text  inplace  True  df  http  df  Text  str  partition  http  df  www  df  Text  str  partition  www  Combine  delimiters  with  actual  lin,ibm
ks  df  http  Link  df  http  map  str  df  http  df  www  Link1  df  www  map  str  df  www  Include  only  Link  columns  df  http  drop  df  http  columns  axis  inplace  True  df  www  drop  df  www  columns  axis  inplace  True  Merge  http  and  www  DataFrames  dfmerge  pd  concat  df  http  df  www  axis  The  following  steps  will  allow  you  to  merge  data  columns  from  the  left  to  the  right  dfmerge  dfmerge  apply  lambda  str  strip  replace  np  nan  Use  fillna  to  fill  any  blanks  with  the  Link1  column  dfmerge  Link  fillna  dfmerge  Link1  inplace  True  Delete  Link1  www  column  dfmerge  drop  Link1  axis  inplace  True  Combine  Link  data  frame  df  pd  concat  dfmerge  df  axis  Make  sure  text  column  is  string  df  Text  df  Text  astype  str  Strip  links  from  Text  column  df  Text  df  Text  apply  lambda  split  http  df  Text  df  Text  apply  lambda  split  www  Change  links  from  objects  to  strings  for  link  in  df  Link  df  Link  to  string  piclin,ibm
ks  description  for  url  in  df  Link  if  pd  isnull  url  piclinks  append  description  append  continue  try  page3  requests  get  url  if  page3  status  code  requests  codes  ok  piclinks  append  description  append  continue  except  Exception  as  print  Skipping  url  url  piclinks  append  description  append  continue  soup3  bs  page3  text  lxml  pic  soup3  find  meta  property  og  image  if  pic  piclinks  append  pic  content  else  piclinks  append  content  None  desc  soup3  find  attrs  name  Description  if  desc  content  desc  content  if  not  content  or  content  null  Try  again  with  lowercase  description  desc  soup3  find  attrs  name  description  if  desc  content  desc  content  if  not  content  or  content  null  description  append  else  description  append  content  Save  thumbnail  descriptions  to  df  in  column  titled  Thumbnails  df  Thumbnails  description  Save  image  links  to  df  in  column  titled  Image  df  Image  piclinksshortlink  df  Link  extend,ibm
edlink  for  link  in  shortlink  if  isinstance  link  float  Float  is  not  URL  probably  NaN  extendedlink  append  else  try  extended  link  requests  Session  head  link  allow  redirects  True  url  extendedlink  append  extended  link  except  Exception  as  print  Skipping  link  link  extendedlink  append  df  Extended  Links  extendedlink  Define  the  list  of  features  to  get  enrichment  values  for  entities  keywords  emotion  and  sentiment  features  Features  entities  EntitiesOptions  keywords  KeywordsOptions  emotion  EmotionOptions  sentiment  SentimentOptions  overallSentimentScore  overallSentimentType  highestEmotion  highestEmotionScore  kywords  entities  Go  through  every  response  and  enrich  the  text  using  NLU  for  text  in  df  Text  We  are  assuming  English  to  avoid  errors  when  the  language  can  not  be  detected  enriched  json  nlu  analyze  text  text  features  features  language  en  Get  the  SENTIMENT  score  and  type  if  sentiment  in  enriched  j,ibm
son  if  score  in  enriched  json  sentiment  document  overallSentimentScore  append  enriched  json  sentiment  document  score  else  overallSentimentScore  append  if  label  in  enriched  json  sentiment  document  overallSentimentType  append  enriched  json  sentiment  document  label  else  overallSentimentType  append  else  overallSentimentScore  append  overallSentimentType  append  Read  the  EMOTIONS  into  dict  and  get  the  key  emotion  with  maximum  value  if  emotion  in  enriched  json  me  max  enriched  json  emotion  document  emotion  items  key  operator  itemgetter  highestEmotion  append  me  highestEmotionScore  append  enriched  json  emotion  document  emotion  me  else  highestEmotion  append  highestEmotionScore  append  Iterate  and  get  KEYWORDS  with  confidence  of  over  70  if  keywords  in  enriched  json  tmpkw  for  kw  in  enriched  json  keywords  if  float  kw  relevance  tmpkw  append  kw  text  Convert  multiple  keywords  in  list  to  string  and  append  th,ibm
e  string  kywords  append  join  tmpkw  else  kywords  append  Iterate  and  get  Entities  with  confidence  of  over  30  if  entities  in  enriched  json  tmpent  for  ent  in  enriched  json  entities  if  float  ent  relevance  tmpent  append  ent  type  Convert  multiple  entities  in  list  to  string  and  append  the  string  entities  append  join  tmpent  else  entities  append  Create  columns  from  the  list  and  append  to  the  DataFrame  if  highestEmotion  df  TextHighestEmotion  highestEmotion  if  highestEmotionScore  df  TextHighestEmotionScore  highestEmotionScore  if  overallSentimentType  df  TextOverallSentimentType  overallSentimentType  if  overallSentimentScore  df  TextOverallSentimentScore  overallSentimentScore  df  TextKeywords  kywords  df  TextEntities  entities  Choose  first  of  Keywords  Concepts  Entities  df  MaxTextKeywords  df  TextKeywords  apply  lambda  split  df  MaxTextEntity  df  TextEntities  apply  lambda  split  Define  the  list  of  features  to  get  enr,ibm
ichment  values  for  entities  keywords  emotion  and  sentiment  features  Features  entities  EntitiesOptions  keywords  KeywordsOptions  emotion  EmotionOptions  sentiment  SentimentOptions  overallSentimentScore  overallSentimentType  highestEmotion  highestEmotionScore  kywords  entities  Go  through  every  response  and  enrich  the  text  using  NLU  for  text  in  df  Thumbnails  if  not  text  overallSentimentScore  append  overallSentimentType  append  highestEmotion  append  highestEmotionScore  append  kywords  append  entities  append  continue  enriched  json  nlu  analyze  text  text  features  features  language  en  Get  the  SENTIMENT  score  and  type  if  sentiment  in  enriched  json  if  score  in  enriched  json  sentiment  document  overallSentimentScore  append  enriched  json  sentiment  document  score  else  overallSentimentScore  append  if  label  in  enriched  json  sentiment  document  overallSentimentType  append  enriched  json  sentiment  document  label  else  overallSent,ibm
imentType  append  Read  the  EMOTIONS  into  dict  and  get  the  key  emotion  with  maximum  value  if  emotion  in  enriched  json  me  max  enriched  json  emotion  document  emotion  items  key  operator  itemgetter  highestEmotion  append  me  highestEmotionScore  append  enriched  json  emotion  document  emotion  me  else  highestEmotion  append  highestEmotionScore  append  Iterate  and  get  KEYWORDS  with  confidence  of  over  70  if  keywords  in  enriched  json  tmpkw  for  kw  in  enriched  json  keywords  if  float  kw  relevance  tmpkw  append  kw  text  Convert  multiple  keywords  in  list  to  string  and  append  the  string  kywords  append  join  tmpkw  Iterate  and  get  Entities  with  confidence  of  over  30  if  entities  in  enriched  json  tmpent  for  ent  in  enriched  json  entities  if  float  ent  relevance  tmpent  append  ent  type  Convert  multiple  entities  in  list  to  string  and  append  the  string  entities  append  join  tmpent  else  entities  append  Create  ,ibm
columns  from  the  list  and  append  to  the  DataFrame  if  highestEmotion  df  ThumbnailHighestEmotion  highestEmotion  if  highestEmotionScore  df  ThumbnailHighestEmotionScore  highestEmotionScore  if  overallSentimentType  df  ThumbnailOverallSentimentType  overallSentimentType  if  overallSentimentScore  df  ThumbnailOverallSentimentScore  overallSentimentScore  df  ThumbnailKeywords  kywords  df  ThumbnailEntities  entities  Set  Max  to  first  one  from  keywords  and  entities  lists  df  MaxThumbnailKeywords  df  ThumbnailKeywords  apply  lambda  split  df  MaxThumbnailEntity  df  ThumbnailEntities  apply  lambda  split  Define  the  list  of  features  to  get  enrichment  values  for  entities  keywords  emotion  and  sentiment  features  Features  entities  EntitiesOptions  keywords  KeywordsOptions  emotion  EmotionOptions  sentiment  SentimentOptions  overallSentimentScore  overallSentimentType  highestEmotion  highestEmotionScore  kywords  entities  article  text  Go  through  every  respon,ibm
se  and  enrich  the  article  using  NLU  for  url  in  df  Extended  Links  if  not  url  overallSentimentScore  append  overallSentimentType  append  highestEmotion  append  highestEmotionScore  append  kywords  append  entities  append  article  text  append  continue  Run  links  through  NLU  to  get  entities  keywords  emotion  and  sentiment  Use  return  analyzed  text  to  extract  text  for  Tone  Analyzer  to  use  enriched  json  nlu  analyze  url  url  features  features  language  en  return  analyzed  text  True  article  text  append  enriched  json  analyzed  text  Get  the  SENTIMENT  score  and  type  if  sentiment  in  enriched  json  if  score  in  enriched  json  sentiment  document  overallSentimentScore  append  enriched  json  sentiment  document  score  else  overallSentimentScore  append  None  if  label  in  enriched  json  sentiment  document  overallSentimentType  append  enriched  json  sentiment  document  label  else  overallSentimentType  append  Read  the  EMOTIONS  into  ,ibm
dict  and  get  the  key  emotion  with  maximum  value  if  emotion  in  enriched  json  me  max  enriched  json  emotion  document  emotion  items  key  operator  itemgetter  highestEmotion  append  me  highestEmotionScore  append  enriched  json  emotion  document  emotion  me  else  highestEmotion  append  highestEmotionScore  append  Iterate  and  get  KEYWORDS  with  confidence  of  over  70  if  keywords  in  enriched  json  tmpkw  for  kw  in  enriched  json  keywords  if  float  kw  relevance  tmpkw  append  kw  text  Convert  multiple  keywords  in  list  to  string  and  append  the  string  kywords  append  join  tmpkw  else  kywords  append  Iterate  and  get  Entities  with  confidence  of  over  30  if  entities  in  enriched  json  tmpent  for  ent  in  enriched  json  entities  if  float  ent  relevance  tmpent  append  ent  type  Convert  multiple  entities  in  list  to  string  and  append  the  string  entities  append  join  tmpent  else  entities  append  Create  columns  from  the  lis,ibm
t  and  append  to  the  DataFrame  if  highestEmotion  df  LinkHighestEmotion  highestEmotion  if  highestEmotionScore  df  LinkHighestEmotionScore  highestEmotionScore  if  overallSentimentType  df  LinkOverallSentimentType  overallSentimentType  if  overallSentimentScore  df  LinkOverallSentimentScore  overallSentimentScore  df  LinkKeywords  kywords  df  LinkEntities  entities  df  Article  Text  article  text  Set  Max  to  first  one  from  keywords  and  entities  lists  df  MaxLinkKeywords  df  LinkKeywords  apply  lambda  split  df  MaxLinkEntity  df  LinkEntities  apply  lambda  split  highestEmotionTone  emotionToneScore  highestLanguageTone  languageToneScore  highestSocialTone  socialToneScore  for  text  in  df  Text  enriched  json  tone  analyzer  tone  text  content  type  text  plain  me  max  enriched  json  document  tone  tone  categories  tones  key  itemgetter  score  tone  name  highestEmotionTone  append  me  you  max  enriched  json  document  tone  tone  categories  tones  key  item,ibm
getter  score  score  emotionToneScore  append  you  me1  max  enriched  json  document  tone  tone  categories  tones  key  itemgetter  score  tone  name  highestLanguageTone  append  me1  you1  max  enriched  json  document  tone  tone  categories  tones  key  itemgetter  score  score  languageToneScore  append  you1  me2  max  enriched  json  document  tone  tone  categories  tones  key  itemgetter  score  tone  name  highestSocialTone  append  me2  you2  max  enriched  json  document  tone  tone  categories  tones  key  itemgetter  score  score  socialToneScore  append  you2  df  highestEmotionTone  highestEmotionTone  df  emotionToneScore  emotionToneScore  df  languageToneScore  languageToneScore  df  highestLanguageTone  highestLanguageTone  df  highestSocialTone  highestSocialTone  df  socialToneScore  socialToneScorehighestEmotionTone  emotionToneScore  highestLanguageTone  languageToneScore  highestSocialTone  socialToneScore  for  text  in  df  Article  Text  if  not  text  emotionToneScore  append,ibm
  highestEmotionTone  append  languageToneScore  append  highestLanguageTone  append  socialToneScore  append  highestSocialTone  append  continue  enriched  json  tone  analyzer  tone  text  content  type  text  plain  sentences  False  maxTone  max  enriched  json  document  tone  tone  categories  tones  key  itemgetter  score  tone  name  highestEmotionTone  append  maxTone  maxToneScore  max  enriched  json  document  tone  tone  categories  tones  key  itemgetter  score  score  emotionToneScore  append  maxToneScore  maxLanguageTone  max  enriched  json  document  tone  tone  categories  tones  key  itemgetter  score  tone  name  highestLanguageTone  append  maxLanguageTone  maxLanguageScore  max  enriched  json  document  tone  tone  categories  tones  key  itemgetter  score  score  languageToneScore  append  maxLanguageScore  maxSocial  max  enriched  json  document  tone  tone  categories  tones  key  itemgetter  score  tone  name  highestSocialTone  append  maxSocial  maxSocialScore  max  enriched  ,ibm
json  document  tone  tone  categories  tones  key  itemgetter  score  score  socialToneScore  append  maxSocialScore  df  articlehighestEmotionTone  highestEmotionTone  df  articleEmotionToneScore  emotionToneScore  df  articlelanguageToneScore  languageToneScore  df  articlehighestLanguageTone  highestLanguageTone  df  articlehighestSocialTone  highestSocialTone  df  articlesocialToneScore  socialToneScorepiclinks  df  Image  picclass  piccolor  pictype1  pictype2  pictype3  for  pic  in  piclinks  if  not  pic  or  pic  default  img  picclass  append  piccolor  append  pictype1  append  pictype2  append  pictype3  append  continue  classes  enriched  json  try  enriched  json  visual  recognition  classify  parameters  json  dumps  url  pic  except  Exception  as  print  Skipping  url  pic  if  error  in  enriched  json  print  enriched  json  error  if  images  in  enriched  json  and  classifiers  in  enriched  json  images  classes  enriched  json  images  classifiers  classes  color1  None  class1  Non,ibm
e  type  hierarchy1  None  for  iclass  in  classes  Grab  the  first  color  first  class  and  first  type  hierarchy  Note  Usually  you  filter  by  score  too  if  not  type  hierarchy1  and  type  hierarchy  in  iclass  type  hierarchy1  iclass  type  hierarchy  if  not  class1  class1  iclass  class  if  not  color1  and  iclass  class  endswith  color  color1  iclass  class  len  color  if  type  hierarchy1  and  class1  and  color1  We  are  only  using  of  each  per  image  When  we  have  all  break  break  picclass  append  class1  or  piccolor  append  color1  or  type  split  type  hierarchy1  or  split  pictype1  append  type  split  if  len  type  split  else  pictype2  append  type  split  if  len  type  split  else  pictype3  append  type  split  if  len  type  split  else  df  Image  Color  piccolor  df  Image  Class  picclass  df  Image  Type  pictype1  df  Image  Subtype  pictype2  df  Image  Subtype2  pictype3cos  ibm  boto3  client  service  name  s3  ibm  api  key  id  credentials  IB,ibm
M  API  KEY  ID  ibm  service  instance  id  credentials  IAM  SERVICE  ID  ibm  auth  endpoint  credentials  IBM  AUTH  ENDPOINT  config  Config  signature  version  oauth  endpoint  url  credentials  ENDPOINT  Build  the  enriched  file  name  from  the  original  filename  localfilename  enriched  credentials  FILE  Write  CSV  file  from  the  enriched  pandas  DataFrame  df  to  csv  localfilename  index  False  Use  the  above  put  file  method  with  credentials  to  put  the  file  in  Object  Storage  cos  upload  file  localfilename  Bucket  credentials  BUCKET  Key  localfilename  Determine  which  data  points  are  tied  to  metrics  and  put  them  in  list  metrics  Lifetime  Post  Total  Reach  Lifetime  Post  organic  reach  Lifetime  Post  Paid  Reach  Lifetime  Post  Total  Impressions  Lifetime  Post  Organic  Impressions  Lifetime  Post  Paid  Impressions  Lifetime  Engaged  Users  Lifetime  Post  Consumers  Lifetime  Post  Consumptions  Lifetime  Negative  feedback  Lifetime  Negative  ,ibm
Feedback  from  Users  Lifetime  Post  Impressions  by  people  who  have  liked  your  Page  Lifetime  Post  reach  by  people  who  like  your  Page  Lifetime  Post  Paid  Impressions  by  people  who  have  liked  your  Page  Lifetime  Paid  reach  of  post  by  people  who  like  your  Page  Lifetime  People  who  have  liked  your  Page  and  engaged  with  your  post  Lifetime  Talking  About  This  Post  by  action  type  comment  Lifetime  Talking  About  This  Post  by  action  type  like  Lifetime  Talking  About  This  Post  by  action  type  share  Lifetime  Post  Stories  by  action  type  comment  Lifetime  Post  Stories  by  action  type  like  Lifetime  Post  Stories  by  action  type  share  Lifetime  Post  consumers  by  type  link  clicks  Lifetime  Post  consumers  by  type  other  clicks  Lifetime  Post  consumers  by  type  photo  view  Lifetime  Post  Consumptions  by  type  link  clicks  Lifetime  Post  Consumptions  by  type  other  clicks  Lifetime  Post  Consumptions  by  type  phot,ibm
o  view  Lifetime  Negative  feedback  hide  all  clicks  Lifetime  Negative  feedback  hide  clicks  Lifetime  Negative  Feedback  from  Users  by  Type  hide  all  clicks  Lifetime  Negative  Feedback  from  Users  by  Type  hide  clicks  Create  list  with  only  Post  Tone  Values  post  tones  Text  highestEmotionTone  emotionToneScore  languageToneScore  highestLanguageTone  highestSocialTone  socialToneScore  Append  DataFrame  with  these  metrics  post  tones  extend  metrics  Create  new  DataFrame  with  tones  and  metrics  df  post  tones  df  post  tones  Determine  which  tone  values  are  suppose  to  be  numeric  and  ensure  they  are  numeric  post  numeric  values  emotionToneScore  languageToneScore  socialToneScore  for  in  post  numeric  values  df  post  tones  pd  to  numeric  df  post  tones  errors  coerce  Make  all  metrics  numeric  for  in  metrics  df  post  tones  pd  to  numeric  df  post  tones  errors  coerce  Drop  NA  Values  in  Tone  Enrichment  Columns  df  post  ton,ibm
es  dropna  subset  socialToneScore  inplace  True  Add  in  column  to  distinguish  what  portion  the  enrichment  was  happening  df  post  tones  Type  Post  Create  list  with  only  Article  Tone  Values  article  tones  Text  articlehighestEmotionTone  articleEmotionToneScore  articlelanguageToneScore  articlehighestLanguageTone  articlehighestSocialTone  articlesocialToneScore  Append  DataFrame  with  these  metrics  article  tones  extend  metrics  Create  new  DataFrame  with  tones  and  metrics  df  article  tones  df  article  tones  Determine  which  values  are  suppose  to  be  numeric  and  ensure  they  are  numeric  art  numeric  values  articleEmotionToneScore  articlelanguageToneScore  articlesocialToneScore  for  in  art  numeric  values  df  article  tones  pd  to  numeric  df  article  tones  errors  coerce  Make  all  metrics  numeric  for  in  metrics  df  article  tones  pd  to  numeric  df  article  tones  errors  coerce  Drop  NA  Values  in  Tone  Enrichment  Columns  df  artic,ibm
le  tones  dropna  subset  articlesocialToneScore  inplace  True  Add  in  column  to  distinguish  what  portion  the  enrichment  was  happening  df  article  tones  Type  Article  First  make  the  Column  Headers  the  same  df  post  tones  rename  columns  highestEmotionTone  Emotion  Tone  emotionToneScore  Emotion  Tone  Score  languageToneScore  Language  Tone  Score  highestLanguageTone  Language  Tone  highestSocialTone  Social  Tone  socialToneScore  Social  Tone  Score  inplace  True  df  article  tones  rename  columns  articlehighestEmotionTone  Emotion  Tone  articleEmotionToneScore  Emotion  Tone  Score  articlelanguageToneScore  Language  Tone  Score  articlehighestLanguageTone  Language  Tone  articlehighestSocialTone  Social  Tone  articlesocialToneScore  Social  Tone  Score  inplace  True  Combine  into  one  data  frame  df  tones  pd  concat  df  post  tones  df  article  tones  Create  list  with  only  Article  Keywords  article  keywords  Text  MaxLinkKeywords  Append  DataFrame  wit,ibm
h  these  metrics  article  keywords  extend  metrics  Create  new  DataFrame  with  keywords  and  metrics  df  article  keywords  df  article  keywords  Make  all  metrics  numeric  for  in  metrics  df  article  keywords  pd  to  numeric  df  article  keywords  errors  coerce  Drop  NA  Values  in  Keywords  Column  df  article  keywords  MaxLinkKeywords  replace  np  nan  inplace  True  df  article  keywords  dropna  subset  MaxLinkKeywords  inplace  True  Add  in  column  to  distinguish  what  portion  the  enrichment  was  happening  df  article  keywords  Type  Article  Create  list  with  only  Thumbnail  Keywords  thumbnail  keywords  Text  MaxThumbnailKeywords  Append  DataFrame  with  these  metrics  thumbnail  keywords  extend  metrics  Create  new  DataFrame  with  keywords  and  metrics  df  thumbnail  keywords  df  thumbnail  keywords  Make  all  metrics  numeric  for  in  metrics  df  thumbnail  keywords  pd  to  numeric  df  thumbnail  keywords  errors  coerce  Drop  NA  Values  in  Keywords,ibm
  Column  df  thumbnail  keywords  MaxThumbnailKeywords  replace  np  nan  inplace  True  df  thumbnail  keywords  dropna  subset  MaxThumbnailKeywords  inplace  True  Add  in  column  to  distinguish  what  portion  the  enrichment  was  happening  df  thumbnail  keywords  Type  Thumbnails  Create  list  with  only  Thumbnail  Keywords  post  keywords  Text  MaxTextKeywords  Append  DataFrame  with  these  metrics  post  keywords  extend  metrics  Create  new  DataFrame  with  keywords  and  metrics  df  post  keywords  df  post  keywords  Make  all  metrics  numeric  for  in  metrics  df  post  keywords  pd  to  numeric  df  post  keywords  errors  coerce  Drop  NA  Values  in  Keywords  Column  df  post  keywords  MaxTextKeywords  replace  np  nan  inplace  True  df  post  keywords  dropna  subset  MaxTextKeywords  inplace  True  Add  in  column  to  distinguish  what  portion  the  enrichment  was  happening  df  post  keywords  Type  Posts  First  make  the  column  headers  the  same  df  post  keywords,ibm
  rename  columns  MaxTextKeywords  Keywords  inplace  True  df  thumbnail  keywords  rename  columns  MaxThumbnailKeywords  Keywords  inplace  True  df  article  keywords  rename  columns  MaxLinkKeywords  Keywords  inplace  True  Combine  into  one  data  frame  df  keywords  pd  concat  df  post  keywords  df  thumbnail  keywords  df  article  keywords  Discard  keywords  with  lower  consumption  to  make  charting  easier  df  keywords  df  keywords  df  keywords  Lifetime  Post  Consumptions  175  Create  list  with  only  Article  Keywords  article  entities  Text  MaxLinkEntity  Append  DataFrame  with  these  metrics  article  entities  extend  metrics  Create  new  DataFrame  with  keywords  and  metrics  df  article  entities  df  article  entities  Make  all  metrics  numeric  for  in  metrics  df  article  entities  pd  to  numeric  df  article  entities  errors  coerce  Drop  NA  Values  in  Keywords  Column  df  article  entities  MaxLinkEntity  df  MaxLinkEntity  replace  np  nan  regex  True ,ibm
 df  article  entities  dropna  subset  MaxLinkEntity  inplace  True  Add  in  column  to  distinguish  what  portion  the  enrichment  was  happening  df  article  entities  Type  Article  Create  list  with  only  Thumbnail  Keywords  thumbnail  entities  Text  MaxThumbnailEntity  Append  DataFrame  with  these  metrics  thumbnail  entities  extend  metrics  Create  new  DataFrame  with  keywords  and  metrics  df  thumbnail  entities  df  thumbnail  entities  Make  all  metrics  numeric  for  in  metrics  df  thumbnail  entities  pd  to  numeric  df  thumbnail  entities  errors  coerce  Drop  NA  Values  in  Keywords  Column  df  thumbnail  entities  MaxThumbnailEntity  df  thumbnail  entities  MaxThumbnailEntity  replace  np  nan  regex  True  df  thumbnail  entities  dropna  subset  MaxThumbnailEntity  inplace  True  Add  in  column  to  distinguish  what  portion  the  enrichment  was  happening  df  thumbnail  entities  Type  Thumbnails  Create  list  with  only  Thumbnail  Keywords  post  entities  Te,ibm
xt  MaxTextEntity  Append  DataFrame  with  these  metrics  post  entities  extend  metrics  Create  new  DataFrame  with  keywords  and  metrics  df  post  entities  df  post  entities  Make  all  metrics  numeric  for  in  metrics  df  post  entities  pd  to  numeric  df  post  entities  errors  coerce  Drop  NA  Values  in  Keywords  Column  df  post  entities  MaxTextEntity  df  post  entities  MaxTextEntity  replace  np  nan  regex  True  df  post  entities  dropna  subset  MaxTextEntity  inplace  True  Add  in  column  to  distinguish  what  portion  the  enrichment  was  happening  df  post  entities  Type  Posts  First  make  the  column  headers  the  same  df  post  entities  rename  columns  MaxTextEntity  Entities  inplace  True  df  thumbnail  entities  rename  columns  MaxThumbnailEntity  Entities  inplace  True  df  article  entities  rename  columns  MaxLinkEntity  Entities  inplace  True  Combine  into  one  data  frame  df  entities  pd  concat  df  post  entities  df  thumbnail  entities  d,ibm
f  article  entities  df  entities  Entities  df  entities  Entities  replace  np  nan  df  entities  dropna  subset  Entities  inplace  True  Create  list  with  only  Visual  Recognition  columns  pic  keywords  Image  Type  Image  Subtype  Image  Subtype2  Image  Class  Image  Color  Append  DataFrame  with  these  metrics  pic  keywords  extend  metrics  Create  new  DataFrame  with  keywords  and  metrics  df  pic  keywords  df  pic  keywords  Make  all  metrics  numeric  for  in  metrics  df  pic  keywords  pd  to  numeric  df  pic  keywords  errors  coerce  entities  df  entities  tones  df  tones  keywords  df  keywordsdisplay  tones  display  tones  display  entities  display  keywords  display  df  pic  keywords  display  df  pic  keywords  display  df  pic  keywords  Licensed  under  the  Apache  License  Version  the  License  you  may  not  use  this  file  except  in  compliance  with  the  License  You  may  obtain  copy  of  the  License  at  http  www  apache  org  licenses  LICENSE  Unless  ,ibm
required  by  applicable  law  or  agreed  to  in  writing  software  distributed  under  the  License  is  distributed  on  an  AS  IS  BASIS  WITHOUT  WARRANTIES  OR  CONDITIONS  OF  ANY  KIND  either  express  or  implied  See  the  License  for  the  specific  language  governing  permissions  and  limitations  under  the  License  ,ibm
import  tensorflow  as  tf  tf  version  pip  install  tensorflow  Create  constant  op  This  op  is  added  as  node  to  the  default  graph  hello  tf  constant  Hello  TensorFlow  start  TF  session  sess  tf  Session  run  the  op  and  get  result  print  sess  run  hello  rank  tensor  this  is  scalar  with  shape  rank  tensor  this  is  vector  with  shape  rank  tensor  matrix  with  shape  rank  tensor  with  shape  type  node1  tf  constant  tf  float32  node2  tf  constant  also  tf  float32  implicitly  node3  tf  add  node1  node2  print  node1  node1  node2  node2  print  node3  node3  sess  tf  Session  print  sess  run  node1  node2  sess  run  node1  node2  print  sess  run  node3  sess  run  node3  tf  placeholder  tf  float32  tf  placeholder  tf  float32  adder  node  provides  shortcut  for  tf  add  print  sess  run  adder  node  feed  dict  print  sess  run  adder  node  feed  dict  add  and  triple  adder  node  print  sess  run  add  and  triple  feed  dict  ,ibm
from  pyspark  import  SparkContext  SparkConf  from  pyspark  sql  import  SparkSession  import  sagemaker  pyspark  inforegion  your  aws  region  here  trainingData  spark  read  format  libsvm  option  numFeatures  784  option  vectorType  dense  load  s3a  sagemaker  sample  data  spark  mnist  train  format  region  testData  spark  read  format  libsvm  option  numFeatures  784  option  vectorType  dense  load  s3a  sagemaker  sample  data  spark  mnist  test  format  region  trainingData  show  import  random  from  sagemaker  pyspark  import  IAMRole  S3DataPath  from  sagemaker  pyspark  algorithms  import  XGBoostSageMakerEstimator  xgboost  estimator  XGBoostSageMakerEstimator  sagemakerRole  IAMRole  your  sagemaker  iam  role  arn  here  trainingInstanceType  ml  m4  xlarge  trainingInstanceCount  endpointInstanceType  ml  m4  xlarge  endpointInitialInstanceCount  xgboost  estimator  setEta  xgboost  estimator  setGamma  xgboost  estimator  setMinChildWeight  xgboost  estimator  setSilent  xgboo,amazon
st  estimator  setObjective  multi  softmax  xgboost  estimator  setNumClasses  10  xgboost  estimator  setNumRound  10  train  model  xgboost  estimator  fit  trainingData  transformedData  model  transform  trainingData  transformedData  show  Delete  the  endpoint  from  sagemaker  pyspark  import  SageMakerResourceCleanup  resource  cleanup  SageMakerResourceCleanup  model  sagemakerClient  resource  cleanup  deleteResources  model  getCreatedResources  ,amazon
git  clone  https  github  com  tensorflow  modelsfrom  future  import  print  function  from  IPython  import  display  checkpoint  name  mobilenet  v2  224  param  url  https  storage  googleapis  com  mobilenet  v2  checkpoints  checkpoint  name  tgz  print  Downloading  from  url  wget  url  print  Unpacking  tar  xvf  base  name  tgz  checkpoint  base  name  ckpt  display  clear  output  print  Successfully  downloaded  checkpoint  from  url  It  is  available  as  checkpoint  wget  https  upload  wikimedia  org  wikipedia  commons  fe  Giant  Panda  in  Beijing  Zoo  JPG  panda  jpg  setup  path  import  sys  sys  path  append  content  models  research  slim  import  tensorflow  as  tf  from  nets  mobilenet  import  mobilenet  v2  tf  reset  default  graph  For  simplicity  we  just  decode  jpeg  inside  tensorflow  But  one  can  provide  any  input  obviously  file  input  tf  placeholder  tf  string  image  tf  image  decode  jpeg  tf  read  file  file  input  images  tf  expand  dims  image  imag,amazon
es  tf  cast  images  tf  float32  128  images  set  shape  None  None  None  images  tf  image  resize  images  images  224  224  Note  arg  scope  is  optional  for  inference  with  tf  contrib  slim  arg  scope  mobilenet  v2  training  scope  is  training  False  logits  endpoints  mobilenet  v2  mobilenet  images  Restore  using  exponential  moving  average  since  it  produces  higher  accuracy  ema  tf  train  ExponentialMovingAverage  999  vars  ema  variables  to  restore  saver  tf  train  Saver  vars  from  IPython  import  display  import  pylab  from  datasets  import  imagenet  import  PIL  display  display  display  Image  panda  jpg  with  tf  Session  as  sess  saver  restore  sess  checkpoint  endpoints  Predictions  eval  feed  dict  file  input  panda  jpg  label  map  imagenet  create  readable  names  for  imagenet  labels  print  Top  prediction  argmax  label  map  argmax  max  import  numpy  as  np  img  np  array  PIL  Image  open  panda  jpg  resize  224  224  astype  np  float  1,amazon
28  gd  tf  GraphDef  FromString  open  base  name  frozen  pb  rb  read  inp  predictions  tf  import  graph  def  gd  return  elements  input  MobilenetV2  Predictions  Reshape  with  tf  Session  graph  inp  graph  predictions  eval  feed  dict  inp  img  reshape  224  224  label  map  imagenet  create  readable  names  for  imagenet  labels  print  Top  Prediction  argmax  label  map  argmax  max  ,amazon
import  os  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  your  s3  bucket  name  here  enter  your  s3  bucket  where  you  will  copy  data  and  model  artifacts  prefix  sagemaker  DEMO  breast  cancer  prediction  place  to  upload  training  files  within  the  bucketimport  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  import  io  import  time  import  json  import  sagemaker  amazon  common  as  smacdata  pd  read  csv  https  archive  ics  uci  edu  ml  machine  learning  databases  breast  cancer  wisconsin  wdbc  data  header  None  specify  columns  extracted  from  wbdc  names  data  columns  id  diagnosis  radius  mean  texture  mean  perimeter  mean  area  mean  smoothness  mean  compactness  mean  concavity  mean  concave  points  mean  symmetry  mean  fractal  dimension  mean  radius  se  texture  se  perimeter  se  area  se  smoothness  se  compactness  se  concavity  se  concave  points  se  symmet,amazon
ry  se  fractal  dimension  se  radius  worst  texture  worst  perimeter  worst  area  worst  smoothness  worst  compactness  worst  concavity  worst  concave  points  worst  symmetry  worst  fractal  dimension  worst  save  the  data  data  to  csv  data  csv  sep  index  False  print  the  shape  of  the  data  file  print  data  shape  show  the  top  few  rows  display  data  head  describe  the  data  object  display  data  describe  we  will  also  summarize  the  categorical  field  diganosis  display  data  diagnosis  value  counts  rand  split  np  random  rand  len  data  train  list  rand  split  val  list  rand  split  rand  split  test  list  rand  split  data  train  data  train  list  data  val  data  val  list  data  test  data  test  list  train  data  train  iloc  as  matrix  train  data  train  iloc  as  matrix  val  data  val  iloc  as  matrix  val  data  val  iloc  as  matrix  test  data  test  iloc  as  matrix  test  data  test  iloc  as  matrix  train  file  linear  train  data  io  Byt,amazon
esIO  smac  write  numpy  to  dense  tensor  train  astype  float32  train  astype  float32  seek  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  train  file  upload  fileobj  validation  file  linear  validation  data  io  BytesIO  smac  write  numpy  to  dense  tensor  val  astype  float32  val  astype  float32  seek  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  validation  file  upload  fileobj  See  Algorithms  Provided  by  Amazon  SageMaker  Common  Parameters  in  the  SageMaker  documentation  for  an  explanation  of  these  values  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  linear  learner  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  linear  learner  latest  linear  job  DEMO  linear  time  strftime,amazon
  time  gmtime  print  Job  name  is  linear  job  linear  training  params  RoleArn  role  TrainingJobName  linear  job  AlgorithmSpecification  TrainingImage  containers  boto3  Session  region  name  TrainingInputMode  File  ResourceConfig  InstanceCount  InstanceType  ml  c4  2xlarge  VolumeSizeInGB  10  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  S3DataDistributionType  ShardedByS3Key  CompressionType  None  RecordWrapperType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  prefix  S3DataDistributionType  FullyReplicated  CompressionType  None  RecordWrapperType  None  OutputDataConfig  S3OutputPath  s3  format  bucket  prefix  HyperParameters  feature  dim  30  mini  batch  size  100  predictor  type  regressor  epochs  10  num  models  32  loss  absolute  loss  StoppingCondition  MaxRuntimeInSeconds  60  60  time  region  boto3  Session  region  name  ,amazon
sm  boto3  client  sagemaker  sm  create  training  job  linear  training  params  status  sm  describe  training  job  TrainingJobName  linear  job  TrainingJobStatus  print  status  sm  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  linear  job  if  status  Failed  message  sm  describe  training  job  TrainingJobName  linear  job  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  linear  hosting  container  Image  containers  boto3  Session  region  name  ModelDataUrl  sm  describe  training  job  TrainingJobName  linear  job  ModelArtifacts  S3ModelArtifacts  create  model  response  sm  create  model  ModelName  linear  job  ExecutionRoleArn  role  PrimaryContainer  linear  hosting  container  print  create  model  response  ModelArn  linear  endpoint  config  DEMO  linear  endpoint  config  time  strftime  time  gmtime  print  linear  endpoint  config  create  endpoint  config  response  sm  create  ,amazon
endpoint  config  EndpointConfigName  linear  endpoint  config  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  linear  job  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  linear  endpoint  DEMO  linear  endpoint  time  strftime  time  gmtime  print  linear  endpoint  create  endpoint  response  sm  create  endpoint  EndpointName  linear  endpoint  EndpointConfigName  linear  endpoint  config  print  create  endpoint  response  EndpointArn  resp  sm  describe  endpoint  EndpointName  linear  endpoint  status  resp  EndpointStatus  print  Status  status  sm  get  waiter  endpoint  in  service  wait  EndpointName  linear  endpoint  resp  sm  describe  endpoint  EndpointName  linear  endpoint  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Status  status  if  status  InService  raise  Exception  Endpoint  creation  did  not  succeed  def  np2csv  arr  csv  io  BytesIO  np  savetxt  csv  ar,amazon
r  delimiter  fmt  return  csv  getvalue  decode  rstrip  runtime  boto3  client  runtime  sagemaker  payload  np2csv  test  response  runtime  invoke  endpoint  EndpointName  linear  endpoint  ContentType  text  csv  Body  payload  result  json  loads  response  Body  read  decode  test  pred  np  array  score  for  in  result  predictions  test  mae  linear  np  mean  np  abs  test  test  pred  test  mae  baseline  np  mean  np  abs  test  np  median  train  training  median  as  baseline  predictor  print  Test  MAE  Baseline  round  test  mae  baseline  print  Test  MAE  Linear  round  test  mae  linear  test  pred  class  test  pred  test  pred  baseline  np  repeat  np  median  train  len  test  prediction  accuracy  np  mean  test  test  pred  class  100  baseline  accuracy  np  mean  test  test  pred  baseline  100  print  Prediction  Accuracy  round  prediction  accuracy  print  Baseline  Accuracy  round  baseline  accuracy  sm  delete  endpoint  EndpointName  linear  endpoint  ,amazon
Timing  function  from  datetime  import  datetime  Timings  def  timing  tag  fromTag  None  Timings  tag  datetime  now  if  fromTag  print  at  elapsed  since  format  tag  Timings  tag  fromTag  Timings  tag  Timings  fromTag  else  print  at  format  tag  Timings  tag  timing  Start  hidden  cell  The  following  code  contains  the  credentials  for  file  in  your  IBM  Cloud  Object  Storage  You  might  want  to  remove  those  credentials  before  you  share  your  notebook  YourCredentials  IBM  API  KEY  ID  ZATI1oq  TlsWqN  oEz3Wo1IPXWwOkCx0PXV3gj0d5eui  IAM  SERVICE  ID  iam  ServiceId  521e4bd0  5161  49f0  9e11  474c674a5fe7  ENDPOINT  https  s3  api  us  geo  objectstorage  service  networklayer  com  IBM  AUTH  ENDPOINT  https  iam  ng  bluemix  net  oidc  token  BUCKET  watstudworkshop  donotdelete  pr  basx79wonvxlys  FILE  201701  citibike  tripdata  csv  NOTTHISONE  Insert  YOUR  OWN  Credentials  as  YourCredentials  in  the  cell  above  Setup  access  to  COS  import  sys  import  typ,ibm
es  import  pandas  as  pd  from  botocore  client  import  Config  import  ibm  boto3  def  iter  self  return  try  raise  Exception  YourCredentials  NOTTHISONE  except  KeyError  pass  bucket  name  YourCredentials  BUCKET  object  name  2017  csv  cos  ibm  boto3  client  s3  ibm  api  key  id  YourCredentials  IBM  API  KEY  ID  ibm  auth  endpoint  YourCredentials  IBM  AUTH  ENDPOINT  config  Config  signature  version  oauth  endpoint  url  YourCredentials  ENDPOINT  timing  BeforeFTPAndCOS  Get  the  file  via  FTP  download  rm  2017  csv  wget  ftp  ftp  ncdc  noaa  gov  pub  data  ghcn  daily  by  year  2017  csv  gz  ls  2017  Unzip  the  file  gunzip  2017  csv  gz  ls  2017  Send  the  file  to  COS  from  file  system  with  open  object  name  rb  as  cos  upload  fileobj  bucket  name  object  name  print  Done  file  uploaded  to  COS  bucket  format  object  name  bucket  name  timing  AfterFTPAndCOS  BeforeFTPAndCOS  import  ibmos2spark  timing  BeforeDownloadingFromCOS  Create  credenti,ibm
als  in  the  format  required  by  CloudObjectStorage  credentials  endpoint  YourCredentials  ENDPOINT  api  key  YourCredentials  IBM  API  KEY  ID  service  id  YourCredentials  IAM  SERVICE  ID  configuration  name  wheather  data  2017  config  os  See  https  github  com  ibm  watson  data  lab  ibmos2spark  tree  master  python  cos  ibmos2spark  CloudObjectStorage  sc  credentials  configuration  name  configuration  name  cos  type  bluemix  cos  The  sc  object  is  your  SparkContext  object  The  cos  object  will  provide  the  URL  for  SparkContext  to  retrieve  your  data  Get  the  URL  data  url  cos  url  object  name  bucket  name  timing  AfterDownloadingFromCOS  BeforeDownloadingFromCOS  print  COS  URL  for  the  data  asset  format  data  url  timing  BeforeLoadingData  Now  we  load  the  data  Note  that  Spark  uses  lazy  evaluation  so  the  lengthy  operation  will  be  take  weather  sc  textFile  data  url  take  triggers  the  Spark  job  that  loads  the  data  See  the  Sp,ibm
ark  Job  Progress  gauge  weather  take  timing  AfterLoadingData  BeforeLoadingData  print  Total  records  in  the  data  set  format  weather  count  print  The  first  row  in  the  data  set  format  weather  first  Create  RDD  from  Python  array  to  represent  Header  see  https  spark  apache  org  docs  latest  api  python  pyspark  html  pyspark  SparkContext  parallelize  header  sc  parallelize  STATION  DATE  METRIC  VALUE  C5  C6  C7  C8  Append  the  header  and  data  into  single  RDD  weather  header  union  weather  weather  take  timing  BeforeParsing  weatherParse  weather  map  lambda  line  line  split  weatherParse  first  weatherParse  first  weatherParse  first  timing  AfterParsing  BeforeParsing  Create  new  RDD  which  holds  only  rows  which  represent  precipitation  events  weatherPrecp  weatherParse  filter  lambda  PRCP  Display  first  lines  weatherPrecp  take  Map  to  tuples  with  station  ID  as  first  element  then  tuple  made  of  the  precipitation  measure  a,ibm
nd  constant  is  the  station  is  the  precipitation  value  weatherPrecpCountByKey  weatherPrecp  map  lambda  int  weatherPrecpCountByKey  take  timing  AfterFiltering  AfterParsing  weatherPrecpAddByKey  weatherPrecpCountByKey  reduceByKey  lambda  v1  v2  v1  v2  v1  v2  weatherPrecpAddByKey  first  weatherAverages  weatherPrecpAddByKey  map  lambda  float  weatherAverages  first  for  pair  in  weatherAverages  top  10  print  Station  had  average  precipitations  of  2f  format  pair  pair  precTop10  stationsTop10  for  pair  in  weatherAverages  map  lambda  top  10  precTop10  append  pair  stationsTop10  append  pair  print  Station  had  average  precipitations  of  2f  format  pair  pair  timing  AfterAverages  AfterFiltering  matplotlib  inline  import  numpy  as  np  import  matplotlib  pyplot  as  plt  10  index  np  arange  bar  width  plt  bar  index  precTop10  bar  width  color  plt  xlabel  Stations  plt  ylabel  Precipitations  plt  title  10  stations  with  the  highest  average  pre,ibm
cipitation  plt  xticks  index  bar  width  stationsTop10  rotation  90  plt  show  timing  BeginSparkSQL  Filter  where  type  is  snow  weatherSnow  weatherParse  filter  lambda  SNOW  print  There  are  SNOW  events  format  weatherSnow  count  timing  BeginBuildSparkSQLDataFrame  from  datetime  import  datetime  from  pyspark  sql  import  Row  spark  SparkSession  builder  getOrCreate  Convert  each  line  of  snowWeather  RDD  into  Row  object  snowRows  weatherSnow  map  lambda  Row  station  month  datetime  strptime  month  date  datetime  strptime  day  metric  value  int  Apply  Row  schema  to  create  Spark  DataFrame  snowSchema  spark  createDataFrame  snowRows  Register  snow2017  table  with  columns  station  month  date  metric  and  value  snowSchema  registerTempTable  snow2017  timing  EndBuildSparkSQLDataFrame  BeginBuildSparkSQLDataFrame  timing  BeginCountSnowDays  snow  US10chey021  spark  sql  SELECT  month  COUNT  AS  snowdays  FROM  snow2017  WHERE  station  US10chey021  GROUP  ,ibm
BY  month  ORDER  BY  month  collect  timing  EndCountSnowDays  BeginCountSnowDays  snow  US10chey021  Convert  to  python  array  of  12  elements  initialized  to  US10chey021  snowdays  12  fill  in  array  with  snow  days  per  month  notice  the  indexed  array  versus  indexed  months  ranks  for  row  in  snow  US10chey021  US10chey021  snowdays  row  month  row  snowdays  print  Snow  days  per  month  US10chey021  snowdays  timing  BeginCountSnowDays  snow  USW00094985  spark  sql  SELECT  month  COUNT  AS  snowdays  FROM  snow2017  WHERE  station  USW00094985  GROUP  BY  month  ORDER  BY  month  collect  timing  EndCountSnowDays  BeginCountSnowDays  Create  array  of  12  to  start  with  USW00094985  snowdays  12  For  each  row  compute  number  of  snow  days  for  row  in  snow  USW00094985  USW00094985  snowdays  row  month  row  snowdays  print  USW00094985  snowdays  matplotlib  inline  import  matplotlib  import  numpy  as  np  import  matplotlib  pyplot  as  plt  12  ind  np  arange  width,ibm
  35  pUS10chey021  plt  bar  ind  US10chey021  snowdays  width  color  label  US10chey021  pUSW00094985  plt  bar  ind  width  USW00094985  snowdays  width  color  label  USW00094985  plt  ylabel  SNOW  DAYS  plt  xlabel  MONTH  plt  title  Snow  Days  in  2017  at  Stations  US10chey021  vs  USW00094985  plt  xticks  ind  width  Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec  plt  legend  plt  show  timing  BeginCountSnowDays  snowStations  spark  sql  SELECT  station  COUNT  AS  snowdays  FROM  snow2017  WHERE  station  LIKE  US  GROUP  BY  station  ORDER  BY  station  LIMIT  100  snowStations  head  timing  EndCountSnowDays  BeginCountSnowDays  snowStations  registerTempTable  snowdays  2017  snowStations  top5  spark  sql  SELECT  station  snowdays  FROM  snowdays  2017  ORDER  BY  snowdays  DESC  LIMIT  collect  for  row  in  snowStations  top5  print  row  Query  the  station  snowdays  station  snowdays  spark  sql  SELECT  station  snowdays  FROM  snowdays  2017  ORDER  BY  snowdays  Make,ibm
  RDD  with  snowdays  as  first  column  used  as  key  snowday  station  station  snowdays  rdd  map  lambda  snowdays  station  Collapse  by  key  snowdays  and  make  list  of  stations  as  second  column  snowday  stationsList  snowday  station  reduceByKey  lambda  sortByKey  collect  for  snowday  in  snowday  stationsList  print  Snow  days  Stations  format  snowday  snowday  timing  EndSparkSQLQueries  BeginSparkSQL  Save  as  parquet  file  If  you  are  running  this  cell  multiple  times  you  will  need  to  overwrite  the  data  in  the  parquet  file  snowStations  write  mode  overwrite  parquet  bmos  url  CONTAINER  snowStations  parquet  snowStations  url  cos  url  snowStations  parquet  format  int  datetime  now  timestamp  bucket  name  snowStations  write  parquet  snowStations  url  timing  EndWriteToCOS  EndSparkSQLQueries  snowDaysParquetFile  spark  read  parquet  cos  url  snowStations  parquet  bucket  name  snowDaysParquetFile  registerTempTable  snow  from  parquet  timing  ,ibm
ReadFromCOS  EndWriteToCOS  Display  structure  of  the  DataFrame  snowDaysParquetFile  describe  station  snowdays  spark  sql  SELECT  DISTINCT  COUNT  AS  countSnow  FROM  snow  from  parquet  print  There  are  stations  format  station  snowdays  first  countSnow  timing  EndSQLFromCOS  ReadFromCOS  timing  End  Start  ,ibm
import  json  import  math  from  os  import  path  import  pandas  as  pdCPU  DF  PATH  path  join  path  curdir  dftoformat  csv  cpu  use  df  pd  read  csv  CPU  DF  PATH  cpu  use  df  head  cpu  use  timeseries  vm  index  range  cpu  use  df  VM  unique  for  vm  in  vm  index  range  vm  series  cpu  use  df  cpu  use  df  VM  vm  some  data  is  invalid  and  thus  needs  to  be  set  to  something  it  could  be  the  previous  num  for  normalization  or  removed  target  list  for  num  in  vm  series  target  if  math  isnan  num  False  target  list  append  float  num  else  target  list  append  cpu  use  timeseries  append  start  vm  series  Timestamp  tolist  the  starting  timestamp  target  target  list  these  need  to  be  converted  to  floats  for  json  cat  vm  if  we  used  categories  we  could  do  this  with  open  cpu  use  timeseries  json  as  new  file  new  file  write  json  dumps  cpu  use  timeseries  ,amazon
import  pandas  as  pd  import  numpy  as  npimport  sys  import  types  import  pandas  as  pd  from  botocore  client  import  Config  import  ibm  boto3  def  iter  self  return  hidden  cell  The  following  code  accesses  file  in  your  IBM  Cloud  Object  Storage  It  includes  your  credentials  You  might  want  to  remove  those  credentials  before  you  share  your  notebook  client  01fa29576c1949bf8bf19f9e93a1ba4a  ibm  boto3  client  service  name  s3  ibm  api  key  id  sEmL  NZBUKJEpzPUIPnf4zqGziaw9NmQpRqVC81YaQ  ibm  auth  endpoint  https  iam  ng  bluemix  net  oidc  token  config  Config  signature  version  oauth  endpoint  url  https  s3  api  us  geo  objectstorage  service  networklayer  com  body  client  01fa29576c1949bf8bf19f9e93a1ba4a  get  object  Bucket  watsondeploymentexampleminimalwit  donotdelete  pr  ufwoo44tqds7eb  Key  madrid  train  csv  Body  add  missing  iter  method  so  pandas  accepts  body  as  file  like  object  if  not  hasattr  body  iter  body  iter  types  M,ibm
ethodType  iter  body  df  data  pd  read  csv  body  df  data  head  houses  raw  df  data  houses  raw  head  filterd  on  price  houses  raw  houses  raw  price  50000  filtered  on  area  filterd  on  price  filterd  on  price  mts2  10  houses  filtered  on  area  houses  head  import  xgboostfiltered  on  area  columnshouses  feature  engineered  filtered  on  areaX  houses  feature  engineered  mts2  rooms  distance  to  centre  sauna  houses  feature  engineered  price  from  sklearn  cross  validation  import  train  test  split  train  test  train  test  train  test  split  test  size  35  from  xgboost  sklearn  import  XGBClassifier  XGBRegressor  from  sklearn  pipeline  import  Pipeline  from  sklearn  preprocessing  import  StandardScaler  from  sklearn  cross  validation  import  cross  val  score  from  sklearn  metrics  import  accuracy  score  mean  absolute  error  from  xgboost  import  plot  importance  from  matplotlib  import  pyplot  import  pprintpipeline  Pipeline  scaler  StandardS,ibm
caler  regressor  XGBRegressor  estimators  10  learning  rate  pipelinepipeline  fit  train  values  train  values  matplotlib  inline  import  matplotlib  pyplot  as  plt  labels  of  columns  list  columns  xgbooster  of  fit  pipeline  steps  booster  feature  scores  xgbooster  of  fit  get  fscore  labels  feat  importance  zip  labels  of  columns  int  imp  for  imp  in  feature  scores  items  fig  ax  plt  subplots  ticks  np  array  range  len  feat  importance  width  ax  barh  ticks  feat  importance  color  blue  ax  set  yticklabels  labels  ax  set  yticks  ticks  width  plt  show  from  watson  machine  learning  client  import  WatsonMachineLearningAPIClientwml  credentials  url  https  ibm  watson  ml  mybluemix  net  access  key  JTWUrvdQerqXu3Y  Fcf0wBoxs1eoUpp3ivXynFqbNIBZN3amVmF80kroVV8Cpce9HxGxQ3pIogjgEOjN0TGDTcL0h32gVzPkwMbmHXNpi  FQYUqQmv73SQJrb1WXWeZv  username  9fb26c5d  919f  491b  a47e  172479d63b4c  password  3527ddba  55ef  4039  a039  b8135aabc66f  instance  id  3423aa28  09e1,ibm
  4287  985a  1e77f6018a5f  client  WatsonMachineLearningAPIClient  wml  credentials  model  props  client  repository  ModelMetaNames  NAME  XGBoost  for  madrid  house  prices  model  details  client  repository  store  model  pipeline  model  props  Display  list  of  all  the  models  client  repository  list  models  Extract  the  uid  model  uid  client  repository  get  model  uid  model  details  print  model  uid  Create  the  deployment  deployment  details  client  deployments  create  model  uid  XGBoost  for  madrid  house  prices  List  the  deployments  client  deployments  list  Extract  endpoint  url  and  display  it  scoring  url  client  deployments  get  scoring  url  deployment  details  print  scoring  url  Prepare  scoring  payload  payload  scoring  values  100  print  payload  scoring  Perform  prediction  and  display  the  result  response  scoring  client  deployments  score  scoring  url  payload  scoring  print  response  scoring  client  repository  list  models  ,ibm
import  time  import  numpy  as  np  np  random  seed  import  pandas  as  pd  import  json  import  matplotlib  pyplot  as  plt  conda  install  s3fsimport  boto3  import  s3fs  import  sagemaker  from  sagemaker  import  get  execution  rolebucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  deepar  sagemaker  session  sagemaker  Session  role  get  execution  role  s3  data  path  data  format  bucket  prefix  s3  output  path  output  format  bucket  prefix  containers  us  east  522234722520  dkr  ecr  us  east  amazonaws  com  forecasting  deepar  latest  us  east  566113047672  dkr  ecr  us  east  amazonaws  com  forecasting  deepar  latest  us  west  156387875391  dkr  ecr  us  west  amazonaws  com  forecasting  deepar  latest  eu  west  224300973850  dkr  ecr  eu  west  amazonaws  com  forecasting  deepar  latest  image  name  containers  boto3  Session  region  name  freq  prediction  length  48context  length  72t0  2016  01  01  00  00  00  data  length  400  num  ts  200  period  24tim,amazon
e  series  for  in  range  num  ts  level  10  np  random  rand  seas  amplitude  np  random  rand  level  sig  05  level  noise  parameter  constant  in  time  time  ticks  np  array  range  data  length  source  level  seas  amplitude  np  sin  time  ticks  np  pi  period  noise  sig  np  random  randn  data  length  data  source  noise  index  pd  DatetimeIndex  start  t0  freq  freq  periods  data  length  time  series  append  pd  Series  data  data  index  index  time  series  plot  plt  show  time  series  training  for  ts  in  time  series  time  series  training  append  ts  prediction  length  time  series  plot  label  test  time  series  training  plot  label  train  ls  plt  legend  plt  show  def  series  to  obj  ts  cat  None  obj  start  str  ts  index  target  list  ts  if  cat  obj  cat  cat  return  obj  def  series  to  jsonline  ts  cat  None  return  json  dumps  series  to  obj  ts  cat  encoding  utf  s3filesystem  s3fs  S3FileSystem  with  s3filesystem  open  s3  data  path  train  ,amazon
train  json  wb  as  fp  for  ts  in  time  series  training  fp  write  series  to  jsonline  ts  encode  encoding  fp  write  encode  encoding  with  s3filesystem  open  s3  data  path  test  test  json  wb  as  fp  for  ts  in  time  series  fp  write  series  to  jsonline  ts  encode  encoding  fp  write  encode  encoding  estimator  sagemaker  estimator  Estimator  sagemaker  session  sagemaker  session  image  name  image  name  role  role  train  instance  count  train  instance  type  ml  c4  xlarge  base  job  name  DEMO  deepar  output  path  s3  s3  output  path  hyperparameters  time  freq  freq  context  length  str  context  length  prediction  length  str  prediction  length  num  cells  40  num  layers  likelihood  gaussian  epochs  20  mini  batch  size  32  learning  rate  001  dropout  rate  05  early  stopping  patience  10  estimator  set  hyperparameters  hyperparameters  data  channels  train  s3  train  format  s3  data  path  test  s3  test  format  s3  data  path  estimator  fit  inp,amazon
uts  data  channels  job  name  estimator  latest  training  job  name  endpoint  name  sagemaker  session  endpoint  from  job  job  name  job  name  initial  instance  count  instance  type  ml  m4  xlarge  deployment  image  image  name  role  role  class  DeepARPredictor  sagemaker  predictor  RealTimePredictor  def  set  prediction  parameters  self  freq  prediction  length  Set  the  time  frequency  and  prediction  length  parameters  This  method  must  be  called  before  being  able  to  use  predict  Parameters  freq  string  indicating  the  time  frequency  prediction  length  integer  number  of  predicted  time  points  Return  value  none  self  freq  freq  self  prediction  length  prediction  length  def  predict  self  ts  cat  None  encoding  utf  num  samples  100  quantiles  Requests  the  prediction  of  for  the  time  series  listed  in  ts  each  with  the  optional  corresponding  category  listed  in  cat  Parameters  ts  list  of  pandas  Series  objects  the  time  series  to  ,amazon
predict  cat  list  of  integers  default  None  encoding  string  encoding  to  use  for  the  request  default  utf  num  samples  integer  number  of  samples  to  compute  at  prediction  time  default  100  quantiles  list  of  strings  specifying  the  quantiles  to  compute  default  Return  value  list  of  pandas  DataFrame  objects  each  containing  the  predictions  prediction  times  index  for  in  ts  req  self  encode  request  ts  cat  encoding  num  samples  quantiles  res  super  DeepARPredictor  self  predict  req  return  self  decode  response  res  prediction  times  encoding  def  encode  request  self  ts  cat  encoding  num  samples  quantiles  instances  series  to  obj  ts  cat  if  cat  else  None  for  in  range  len  ts  configuration  num  samples  num  samples  output  types  quantiles  quantiles  quantiles  http  request  data  instances  instances  configuration  configuration  return  json  dumps  http  request  data  encode  encoding  def  decode  response  self  response ,amazon
 prediction  times  encoding  response  data  json  loads  response  decode  encoding  list  of  df  for  in  range  len  prediction  times  prediction  index  pd  DatetimeIndex  start  prediction  times  freq  self  freq  periods  self  prediction  length  list  of  df  append  pd  DataFrame  data  response  data  predictions  quantiles  index  prediction  index  return  list  of  dfpredictor  DeepARPredictor  endpoint  endpoint  name  sagemaker  session  sagemaker  session  content  type  application  json  predictor  set  prediction  parameters  freq  prediction  length  list  of  df  predictor  predict  time  series  training  actual  data  time  series  for  in  range  len  list  of  df  plt  figure  figsize  12  actual  data  prediction  length  context  length  plot  label  target  p10  list  of  df  p90  list  of  df  plt  fill  between  p10  index  p10  p90  color  alpha  label  80  confidence  interval  list  of  df  plot  label  prediction  median  plt  legend  plt  show  sagemaker  session  delete,amazon
  endpoint  endpoint  name  ,amazon
import  tensorflow  as  tf  import  numpy  as  np  tf  set  random  seed  777  for  reproducibility  def  MinMaxScaler  data  numerator  data  np  min  data  denominator  np  max  data  np  min  data  noise  term  prevents  the  zero  division  return  numerator  denominator  1e  xy  np  array  828  659973  833  450012  908100  828  349976  831  659973  823  02002  828  070007  1828100  821  655029  828  070007  819  929993  824  400024  1438100  818  97998  824  159973  816  820  958984  1008100  815  48999  819  23999  819  359985  823  1188100  818  469971  818  97998  819  823  1198100  816  820  450012  811  700012  815  25  1098100  809  780029  813  669983  809  51001  816  659973  1398100  804  539978  809  559998  very  important  It  does  not  work  without  it  xy  MinMaxScaler  xy  print  xy  data  xy  data  xy  placeholders  for  tensor  that  will  be  always  fed  tf  placeholder  tf  float32  shape  None  tf  placeholder  tf  float32  shape  None  tf  Variable  tf  random  normal  name  weigh,ibm
t  tf  Variable  tf  random  normal  name  bias  Hypothesis  hypothesis  tf  matmul  Simplified  cost  loss  function  cost  tf  reduce  mean  tf  square  hypothesis  Minimize  optimizer  tf  train  GradientDescentOptimizer  learning  rate  1e  train  optimizer  minimize  cost  Launch  the  graph  in  session  sess  tf  Session  Initializes  global  variables  in  the  graph  sess  run  tf  global  variables  initializer  for  step  in  range  101  cost  val  hy  val  sess  run  cost  hypothesis  train  feed  dict  data  data  if  step  10  print  step  Cost  cost  val  print  Prediction  hy  val  100  Cost  152254  Prediction  63450289  06628087  35014752  67070574  61131608  61466062  23175186  13716528  ,ibm
import  os  import  boto3  from  pyspark  import  SparkContext  SparkConf  from  pyspark  sql  import  SparkSession  import  sagemaker  from  sagemaker  import  get  execution  role  import  sagemaker  pyspark  role  get  execution  role  Configure  Spark  to  use  the  SageMaker  Spark  dependency  jars  jars  sagemaker  pyspark  classpath  jars  classpath  join  sagemaker  pyspark  classpath  jars  See  the  SageMaker  Spark  Github  to  learn  how  to  connect  to  EMR  from  notebook  instance  spark  SparkSession  builder  config  spark  driver  extraClassPath  classpath  master  local  getOrCreate  spark  cat  container  Dockerfile  sh  The  name  of  our  algorithm  algorithm  name  pytorch  mnist  byo  cd  container  account  aws  sts  get  caller  identity  query  Account  output  text  Get  the  region  defined  in  the  current  configuration  default  to  us  west  if  none  defined  region  aws  configure  get  region  region  region  us  west  fullname  account  dkr  ecr  region  amazonaws  com ,amazon
 algorithm  name  latest  If  the  repository  doesn  exist  in  ECR  create  it  aws  ecr  describe  repositories  repository  names  algorithm  name  dev  null  if  ne  then  aws  ecr  create  repository  repository  name  algorithm  name  dev  null  fi  Get  the  login  command  from  ECR  and  execute  it  directly  aws  ecr  get  login  region  region  no  include  email  Get  the  login  command  from  ECR  in  order  to  pull  down  the  SageMaker  PyTorch  image  aws  ecr  get  login  registry  ids  520713654638  region  region  no  include  email  Build  the  docker  image  locally  with  the  image  name  and  then  push  it  to  ECR  with  the  full  name  docker  build  algorithm  name  docker  tag  algorithm  name  fullname  docker  push  fullname  import  boto3  region  boto3  Session  region  name  trainingData  spark  read  format  libsvm  option  numFeatures  784  option  vectorType  dense  load  s3a  sagemaker  sample  data  spark  mnist  train  format  region  testData  spark  read  format ,amazon
 libsvm  option  numFeatures  784  option  vectorType  dense  load  s3a  sagemaker  sample  data  spark  mnist  test  format  region  trainingData  show  pygmentize  container  mnist  mnist  pyaccount  boto3  client  sts  get  caller  identity  Account  region  boto3  Session  region  namefrom  sagemaker  pyspark  import  SageMakerEstimator  from  sagemaker  pyspark  import  IAMRole  from  sagemaker  pyspark  import  RandomNamePolicyFactory  from  sagemaker  pyspark  import  EndpointCreationPolicy  from  sagemaker  pyspark  transformation  serializers  import  LibSVMRequestRowSerializer  from  sagemaker  pyspark  transformation  deserializers  import  XGBoostCSVRowDeserializer  Create  an  Estimator  from  scratch  estimator  SageMakerEstimator  trainingImage  dkr  ecr  amazonaws  com  pytorch  mnist  byo  latest  format  account  region  modelImage  dkr  ecr  amazonaws  com  pytorch  mnist  byo  latest  format  account  region  sagemakerRole  IAMRole  role  trainingInstanceType  ml  m4  xlarge  ml  p3  2xlar,amazon
ge  could  be  better  but  you  might  not  have  the  limits  for  it  trainingInstanceCount  trainingChannelName  training  trainingSparkDataFormat  libsvm  trainingSparkDataFormatOptions  None  trainingCompressionCodec  None  hyperParameters  epochs  backend  gloo  endpointCreationPolicy  EndpointCreationPolicy  DO  NOT  CREATE  endpointInstanceType  None  endpointInitialInstanceCount  requestRowSerializer  None  responseRowDeserializer  None  namePolicyFactory  RandomNamePolicyFactory  sparksm  customModel  estimator  fit  trainingData  job  name  customModel  modelPath  objectPath  split  boto3  client  sagemaker  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  ,amazon
from  keras  models  import  Sequential  from  keras  layers  import  Convolution2D  MaxPooling2D  from  keras  layers  import  Activation  Flatten  Dense  Dropout  import  numpy  as  np  hidden  cell  The  following  code  contains  the  credentials  for  file  in  your  IBM  Cloud  Object  Storage  You  might  want  to  remove  those  credentials  before  you  share  your  notebook  credentials  IBM  API  KEY  ID  fMqiFRVvxP61ocwZod8tafnB63PgtpxFZePWqwpsq4PZ  IAM  SERVICE  ID  iam  ServiceId  45cb39d3  a49f  4b3c  aea0  81ef7b905a3d  ENDPOINT  https  s3  api  us  geo  objectstorage  service  networklayer  com  IBM  AUTH  ENDPOINT  https  iam  ng  bluemix  net  oidc  token  BUCKET  labskmd1e0cb59090d4c5090e233914e9d7ad8  FILE  hotdog  zip  training  data  unzip  hotdog  zip  hidden  cell  The  following  code  contains  the  credentials  for  file  in  your  IBM  Cloud  Object  Storage  You  might  want  to  remove  those  credentials  before  you  share  your  notebook  credentials  IBM  API  KEY  ID  fMqiF,ibm
RVvxP61ocwZod8tafnB63PgtpxFZePWqwpsq4PZ  IAM  SERVICE  ID  iam  ServiceId  45cb39d3  a49f  4b3c  aea0  81ef7b905a3d  ENDPOINT  https  s3  api  us  geo  objectstorage  service  networklayer  com  IBM  AUTH  ENDPOINT  https  iam  ng  bluemix  net  oidc  token  BUCKET  labskmd1e0cb59090d4c5090e233914e9d7ad8  FILE  chesseshotdog  zip  validation  data  unzip  hotdog  zip  img  width  224  img  height  224  model  Sequential  model  add  Convolution2D  32  input  shape  img  width  img  height  model  add  Activation  relu  model  add  MaxPooling2D  pool  size  model  add  Convolution2D  32  model  add  Activation  relu  model  add  MaxPooling2D  pool  size  model  add  Convolution2D  64  model  add  Activation  relu  model  add  MaxPooling2D  pool  size  model  add  Flatten  model  add  Dense  64  model  add  Activation  relu  model  add  Dropout  model  add  Dense  model  add  Activation  sigmoid  model  compile  loss  binary  crossentropy  optimizer  rmsprop  metrics  accuracy  model  fit  generator  training  ,ibm
data  samples  per  epoch  2048  nb  epoch  30  validation  data  validation  data  nb  val  samples  832  model  save  weight  models  hotdogNoHotdog  h5  ,ibm
import  boto3  import  re  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  import  numpy  as  np  import  io  import  sagemaker  amazon  common  as  smac  import  os  import  sagemaker  from  future  import  division  matplotlib  inlinedata  pd  read  csv  credit  data  creditcard  csv  data  head  count  classes  pd  value  counts  data  Class  sort  True  sort  index  count  classes  plot  kind  bar  plt  title  Fraud  class  histogram  plt  xlabel  Class  plt  ylabel  Frequency  print  How  often  are  the  fraudulent  transactions  occuring  ax1  ax2  plt  subplots  sharex  True  figsize  12  bins  50  ax1  hist  data  Time  data  Class  bins  bins  ax1  set  title  Fraud  ax2  hist  data  Time  data  Class  bins  bins  ax2  set  title  Normal  plt  xlabel  Time  in  Seconds  plt  ylabel  Number  of  Transactions  plt  show  print  How  much  money  is  spent  in  fraudulent  transactions  ax1  ax2  plt  subplots  sharex  True  figsize  12  bins  30  ax1  hist  data  Amount  data  Class  bins,amazon
  bins  ax1  set  title  Fraud  ax2  hist  data  Amount  data  Class  bins  bins  ax2  set  title  Normal  plt  xlabel  Amount  plt  ylabel  Number  of  Transactions  plt  yscale  log  plt  show  We  are  removing  the  Time  and  Amount  columns  because  they  have  too  much  variance  and  will  confuse  our  classifier  Standardization  of  dataset  is  common  requirement  for  many  machine  learning  estimators  they  might  behave  badly  if  the  individual  feature  do  not  more  or  less  look  like  standard  normally  distributed  data  Gaussian  with  mean  and  unit  variance  from  sklearn  preprocessing  import  StandardScaler  data  normAmount  StandardScaler  fit  transform  data  Amount  values  reshape  data  data  drop  Time  Amount  axis  data  head  data  iloc  data  columns  Class  data  iloc  data  columns  Class  Number  of  data  points  in  the  minority  class  number  records  fraud  len  data  data  Class  fraud  indices  np  array  data  data  Class  index  Picking  the  ind,amazon
ices  of  the  normal  classes  normal  indices  data  data  Class  index  Out  of  the  indices  we  picked  randomly  select  number  number  records  fraud  random  normal  indices  np  random  choice  normal  indices  number  records  fraud  replace  False  random  normal  indices  np  array  random  normal  indices  Appending  the  indices  under  sample  indices  np  concatenate  fraud  indices  random  normal  indices  Under  sample  dataset  under  sample  data  data  iloc  under  sample  indices  undersample  under  sample  data  iloc  under  sample  data  columns  Class  undersample  under  sample  data  iloc  under  sample  data  columns  Class  Showing  ratio  print  Percentage  of  normal  transactions  str  len  under  sample  data  under  sample  data  Class  print  Percentage  of  fraud  transactions  str  len  under  sample  data  under  sample  data  Class  print  Total  number  of  transactions  in  resampled  data  str  len  under  sample  data  from  sklearn  model  selection  import  tra,amazon
in  test  split  Whole  dataset  train  test  train  test  train  test  split  test  size  random  state  print  Number  transactions  train  dataset  str  len  train  print  Number  transactions  test  dataset  str  len  test  print  Total  number  of  transactions  str  len  train  len  test  Undersampled  dataset  train  undersample  test  undersample  train  undersample  test  undersample  train  test  split  undersample  undersample  test  size  random  state  print  print  Number  transactions  train  dataset  str  len  train  undersample  print  Number  transactions  test  dataset  str  len  test  undersample  print  Total  number  of  transactions  str  len  train  undersample  len  test  undersample  from  sklearn  import  metrics  from  sklearn  cluster  import  KMeans  from  sklearn  datasets  import  load  digits  from  sklearn  decomposition  import  PCA  from  sklearn  preprocessing  import  scale  scaled  scale  undersample  pca  PCA  components  reduced  pca  fit  transform  scaled  ktrain  kt,amazon
est  ktrain  ktest  train  test  split  reduced  undersample  test  size  33  random  state  500  kmeans  KMeans  init  means  clusters  init  10  kmeans  fit  ktrain  Step  size  of  the  mesh  Decrease  to  increase  the  quality  of  the  VQ  01  point  in  the  mesh  min  max  min  max  Plot  the  decision  boundary  For  that  we  will  assign  color  to  each  min  max  reduced  min  reduced  max  min  max  reduced  min  reduced  max  xx  yy  np  meshgrid  np  arange  min  max  np  arange  min  max  Obtain  labels  for  each  point  in  mesh  Use  last  trained  model  kmeans  predict  np  xx  ravel  yy  ravel  Put  the  result  into  color  plot  reshape  xx  shape  plt  figure  plt  clf  plt  imshow  interpolation  nearest  extent  xx  min  xx  max  yy  min  yy  max  cmap  plt  cm  Paired  aspect  auto  origin  lower  plt  plot  reduced  reduced  markersize  Plot  the  centroids  as  white  centroids  kmeans  cluster  centers  plt  scatter  centroids  centroids  marker  169  linewidths  color  zorder ,amazon
 10  plt  title  means  clustering  on  the  credit  card  fraud  dataset  PCA  reduced  data  Centroids  are  marked  with  white  cross  Blue  boundry  is  non  fraud  brown  is  fraud  plt  xlim  min  max  plt  ylim  min  max  plt  xticks  plt  yticks  plt  show  predictions  kmeans  predict  ktest  pred  fraud  np  where  predictions  real  fraud  np  where  ktest  false  pos  len  np  setdiff1d  pred  fraud  real  fraud  pred  good  np  where  predictions  real  good  np  where  ktest  false  neg  len  np  setdiff1d  pred  good  real  good  false  neg  rate  false  neg  false  pos  false  neg  accuracy  len  ktest  false  neg  false  pos  len  ktest  print  Accuracy  accuracy  from  sklearn  manifold  import  TSNE  tsne  TSNE  components  random  state  np  random  randint  100  matrix  2d  tsne  fit  transform  undersample  colors  if  else  for  in  undersample  Class  df  tsne  pd  DataFrame  matrix  2d  df  tsne  Class  if  else  for  in  undersample  Class  df  tsne  color  colors  df  tsne  columns,amazon
  Class  color  cols  Class  color  df  tsne  df  tsne  cols  df  tsne  head  number  of  rows  and  columns  df  tsne  shapefig  ax  plt  subplots  figsize  15  10  ax  scatter  df  tsne  df  tsne  Class  values  df  tsne  df  tsne  Class  values  red  10  alpha  label  Fraud  ax  scatter  df  tsne  df  tsne  Class  values  df  tsne  df  tsne  Class  values  green  10  alpha  label  Legal  ax  tick  params  axis  both  which  major  labelsize  15  ax  legend  plt  show  Configure  SageMaker  for  training  job  bucket  951232522638  sagemaker  us  east  prefix  sagemaker  credit  card  Define  IAM  role  role  sagemaker  get  execution  role  vectors  np  array  np  array  train  undersample  tolist  astype  float32  labels  np  array  np  array  train  undersample  Class  tolist  astype  float32  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  vectors  labels  buf  seek  key  recordio  pb  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  b,amazon
uf  s3  train  data  s3  train  format  bucket  prefix  key  print  uploaded  training  data  location  format  s3  train  data  output  location  s3  output  format  bucket  prefix  print  training  artifacts  will  be  uploaded  to  format  output  location  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  linear  learner  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  linear  learner  latest  ap  northeast  351501993468  dkr  ecr  ap  northeast  amazonaws  com  linear  learner  latest  ap  northeast  835164637446  dkr  ecr  ap  northeast  amazonaws  com  linear  learner  latest  sess  sagemaker  Session  linear  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  output  location  sagemaker  ,amazon
session  sess  linear  set  hyperparameters  feature  dim  29  predictor  type  binary  classifier  mini  batch  size  200  linear  fit  train  s3  train  data  Deploy  the  SageMaker  endpoint  linear  predictor  linear  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  sagemaker  predictor  import  csv  serializer  json  deserializer  linear  predictor  content  type  text  csv  linear  predictor  serializer  csv  serializer  linear  predictor  deserializer  json  deserializerpredictions  for  array  in  np  array  split  test  undersample  100  array  np  array  array  tolist  result  linear  predictor  predict  array  predictions  predicted  label  for  in  result  predictions  predictions  np  array  predictions  pd  crosstab  test  undersample  Class  predictions  rownames  actuals  colnames  predictions  TN  134  139  100  TP  144  157  100  print  Normalized  Data  print  Fraud  Detection  Accuracy  2f  format  TN  print  Non  Fraud  Detection  Accuracy  2f  format  TP  vectors  ,amazon
np  array  np  array  train  tolist  astype  float32  labels  np  array  np  array  train  Class  tolist  astype  float32  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  vectors  labels  buf  seek  key  recordio  pb  data  nonorm  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  buf  s3  train  data  s3  train  format  bucket  prefix  key  print  uploaded  training  data  location  format  s3  train  data  sess  sagemaker  Session  linear  nonorm  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  output  location  sagemaker  session  sess  linear  nonorm  set  hyperparameters  feature  dim  29  predictor  type  binary  classifier  mini  batch  size  200  linear  nonorm  fit  train  s3  train  data  Deploy  the  SageMaker  endpoint  linear  nonorm  predictor  linear  nonorm  deploy  initial  instance  count  instance  type  ml  m4  xlarge  f,amazon
rom  sagemaker  predictor  import  csv  serializer  json  deserializer  linear  nonorm  predictor  content  type  text  csv  linear  nonorm  predictor  serializer  csv  serializer  linear  nonorm  predictor  deserializer  json  deserializerpredictions  nonorm  for  array  in  np  array  split  test  100  array  np  array  array  tolist  result  linear  nonorm  predictor  predict  array  predictions  nonorm  predicted  label  for  in  result  predictions  predictions  nonorm  np  array  predictions  pd  crosstab  test  Class  predictions  nonorm  rownames  actuals  colnames  predictions  TN  nonorm  116  137  100  TP  nonorm  85275  85306  100  print  Non  normalized  Data  print  Fraud  Detection  Accuracy  2f  format  TN  nonorm  print  Non  Fraud  Detection  Accuracy  2f  format  TP  nonorm  ,amazon
bucket  cm  ozawa  sagemaker  prefix  sagemaker  linear  mnist  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  for  in  range  30  40  show  digit  train  set  This  is  format  train  set  import  io  import  numpy  as  np  import  sagemaker  amazon  common  as  smac  vectors  np  array  tolist  for  in  train  set  astype  float32  labels  np  array  tolist  for  in  trai,amazon
n  set  astype  float32  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  vectors  labels  buf  seek  import  boto3  import  os  key  recordio  pb  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  buf  s3  train  data  s3  train  format  bucket  prefix  key  print  uploaded  training  data  location  format  s3  train  data  output  location  s3  output  format  bucket  prefix  print  training  artifacts  will  be  uploaded  to  format  output  location  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  boto3  Session  region  name  linear  learner  import  boto3  import  sagemaker  sess  sagemaker  Session  linear  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  output  location  sagemaker  session  sess  linear  set  hyperparameters  feature  dim  784  num  classes  10  predictor  type  multiclass  classifier  mini  ,amazon
batch  size  200  linear  fit  train  s3  train  data  linear  predictor  linear  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  sagemaker  predictor  import  csv  serializer  json  deserializer  linear  predictor  content  type  text  csv  linear  predictor  serializer  csv  serializer  linear  predictor  deserializer  json  deserializerresult  linear  predictor  predict  train  set  30  40  print  result  for  in  zip  range  30  40  result  predictions  show  digit  train  set  This  is  format  predicted  label  import  numpy  as  np  predictions  for  array  in  np  array  split  test  set  100  result  linear  predictor  predict  array  predictions  predicted  label  for  in  result  predictions  predictions  np  array  predictions  import  pandas  as  pd  pd  crosstab  test  set  predictions  rownames  actuals  colnames  predictions  predict  result  pd  DataFrame  actual  test  set  prediction  predictions  predict  result  head  sum  predict  result  apply  lambda  row  row  ,amazon
actual  row  prediction  axis  len  test  set  import  sagemaker  sagemaker  Session  delete  endpoint  linear  predictor  endpoint  ,amazon
Lab  10  MNIST  and  Dropout  import  tensorflow  as  tf  import  random  import  matplotlib  pyplot  as  plt  from  tensorflow  examples  tutorials  mnist  import  input  data  tf  set  random  seed  777  reproducibility  parameters  learning  rate  001  training  epochs  15  batch  size  100  input  place  holders  tf  placeholder  tf  float32  None  784  tf  placeholder  tf  float32  None  10  dropout  keep  prob  rate  on  training  but  should  be  for  testing  keep  prob  tf  placeholder  tf  float32  weights  bias  for  nn  layers  http  stackoverflow  com  questions  33640581  how  to  do  xavier  initialization  on  tensorflow  W1  tf  get  variable  W1  shape  784  512  initializer  tf  contrib  layers  xavier  initializer  b1  tf  Variable  tf  random  normal  512  L1  tf  nn  relu  tf  matmul  W1  b1  L1  tf  nn  dropout  L1  keep  prob  keep  prob  W2  tf  get  variable  W2  shape  512  512  initializer  tf  contrib  layers  xavier  initializer  b2  tf  Variable  tf  random  normal  512  L2  tf ,ibm
 nn  relu  tf  matmul  L1  W2  b2  L2  tf  nn  dropout  L2  keep  prob  keep  prob  W3  tf  get  variable  W3  shape  512  512  initializer  tf  contrib  layers  xavier  initializer  b3  tf  Variable  tf  random  normal  512  L3  tf  nn  relu  tf  matmul  L2  W3  b3  L3  tf  nn  dropout  L3  keep  prob  keep  prob  W4  tf  get  variable  W4  shape  512  512  initializer  tf  contrib  layers  xavier  initializer  b4  tf  Variable  tf  random  normal  512  L4  tf  nn  relu  tf  matmul  L3  W4  b4  L4  tf  nn  dropout  L4  keep  prob  keep  prob  W5  tf  get  variable  W5  shape  512  10  initializer  tf  contrib  layers  xavier  initializer  b5  tf  Variable  tf  random  normal  10  hypothesis  tf  matmul  L4  W5  b5  define  cost  loss  optimizer  cost  tf  reduce  mean  tf  nn  softmax  cross  entropy  with  logits  logits  hypothesis  labels  optimizer  tf  train  AdamOptimizer  learning  rate  learning  rate  minimize  cost  initialize  sess  tf  Session  sess  run  tf  global  variables  initializer  mnist,ibm
  input  data  read  data  sets  MNIST  data  one  hot  True  Check  out  https  www  tensorflow  org  get  started  mnist  beginners  for  more  information  about  the  mnist  dataset  train  my  model  for  epoch  in  range  training  epochs  avg  cost  total  batch  int  mnist  train  num  examples  batch  size  for  in  range  total  batch  batch  xs  batch  ys  mnist  train  next  batch  batch  size  feed  dict  batch  xs  batch  ys  keep  prob  30  dropout  sess  run  cost  optimizer  feed  dict  feed  dict  avg  cost  total  batch  print  Epoch  04d  epoch  cost  9f  format  avg  cost  print  Learning  Finished  Test  model  and  check  accuracy  correct  prediction  tf  equal  tf  argmax  hypothesis  tf  argmax  accuracy  tf  reduce  mean  tf  cast  correct  prediction  tf  float32  print  Accuracy  sess  run  accuracy  feed  dict  mnist  test  images  mnist  test  labels  keep  prob  Get  one  and  predict  random  randint  mnist  test  num  examples  print  Label  sess  run  tf  argmax  mnist  test,ibm
  labels  print  Prediction  sess  run  tf  argmax  hypothesis  feed  dict  mnist  test  images  keep  prob  testing  100  plt  imshow  mnist  test  images  reshape  28  28  cmap  Greys  interpolation  nearest  plt  show  Epoch  0001  cost  447322626  Epoch  0002  cost  157285590  Epoch  0003  cost  121884535  Epoch  0004  cost  098128681  Epoch  0005  cost  082901778  Epoch  0006  cost  075337573  Epoch  0007  cost  069752543  Epoch  0008  cost  060884363  Epoch  0009  cost  055276413  Epoch  0010  cost  054631256  Epoch  0011  cost  049675195  Epoch  0012  cost  049125314  Epoch  0013  cost  047231930  Epoch  0014  cost  041290121  Epoch  0015  cost  043621063  Learning  Finished  Accuracy  9804  ,ibm
Lab  10  MNIST  and  Deep  learning  import  tensorflow  as  tf  import  random  import  matplotlib  pyplot  as  plt  from  tensorflow  examples  tutorials  mnist  import  input  data  tf  set  random  seed  777  reproducibility  parameters  learning  rate  001  training  epochs  15  batch  size  100  input  place  holders  tf  placeholder  tf  float32  None  784  tf  placeholder  tf  float32  None  10  weights  bias  for  nn  layers  http  stackoverflow  com  questions  33640581  how  to  do  xavier  initialization  on  tensorflow  W1  tf  get  variable  W1  shape  784  512  initializer  tf  contrib  layers  xavier  initializer  b1  tf  Variable  tf  random  normal  512  L1  tf  nn  relu  tf  matmul  W1  b1  W2  tf  get  variable  W2  shape  512  512  initializer  tf  contrib  layers  xavier  initializer  b2  tf  Variable  tf  random  normal  512  L2  tf  nn  relu  tf  matmul  L1  W2  b2  W3  tf  get  variable  W3  shape  512  512  initializer  tf  contrib  layers  xavier  initializer  b3  tf  Variable  tf  ,ibm
random  normal  512  L3  tf  nn  relu  tf  matmul  L2  W3  b3  W4  tf  get  variable  W4  shape  512  512  initializer  tf  contrib  layers  xavier  initializer  b4  tf  Variable  tf  random  normal  512  L4  tf  nn  relu  tf  matmul  L3  W4  b4  W5  tf  get  variable  W5  shape  512  10  initializer  tf  contrib  layers  xavier  initializer  b5  tf  Variable  tf  random  normal  10  hypothesis  tf  matmul  L4  W5  b5  define  cost  loss  optimizer  cost  tf  reduce  mean  tf  nn  softmax  cross  entropy  with  logits  logits  hypothesis  labels  optimizer  tf  train  AdamOptimizer  learning  rate  learning  rate  minimize  cost  initialize  sess  tf  Session  sess  run  tf  global  variables  initializer  mnist  input  data  read  data  sets  MNIST  data  one  hot  True  Check  out  https  www  tensorflow  org  get  started  mnist  beginners  for  more  information  about  the  mnist  dataset  train  my  model  for  epoch  in  range  training  epochs  avg  cost  total  batch  int  mnist  train  num  examples,ibm
  batch  size  for  in  range  total  batch  batch  xs  batch  ys  mnist  train  next  batch  batch  size  feed  dict  batch  xs  batch  ys  sess  run  cost  optimizer  feed  dict  feed  dict  avg  cost  total  batch  print  Epoch  04d  epoch  cost  9f  format  avg  cost  print  Learning  Finished  Test  model  and  check  accuracy  correct  prediction  tf  equal  tf  argmax  hypothesis  tf  argmax  accuracy  tf  reduce  mean  tf  cast  correct  prediction  tf  float32  print  Accuracy  sess  run  accuracy  feed  dict  mnist  test  images  mnist  test  labels  Get  one  and  predict  random  randint  mnist  test  num  examples  print  Label  sess  run  tf  argmax  mnist  test  labels  print  Prediction  sess  run  tf  argmax  hypothesis  feed  dict  mnist  test  images  plt  imshow  mnist  test  images  reshape  28  28  cmap  Greys  interpolation  nearest  plt  show  Epoch  0001  cost  266061549  Epoch  0002  cost  080796588  Epoch  0003  cost  049075800  Epoch  0004  cost  034772298  Epoch  0005  cost  02478,ibm
0529  Epoch  0006  cost  017072763  Epoch  0007  cost  014031383  Epoch  0008  cost  013763446  Epoch  0009  cost  009164047  Epoch  0010  cost  008291388  Epoch  0011  cost  007319742  Epoch  0012  cost  006434021  Epoch  0013  cost  005684378  Epoch  0014  cost  004781207  Epoch  0015  cost  004342310  Learning  Finished  Accuracy  9742  ,ibm
import  subprocess  print  subprocess  check  output  build  and  push  sh  superradiance  from  scipy  import  sparse  import  numpy  as  np  import  os  import  pickle  import  matplotlib  pyplot  as  plt  import  boto3  from  sagemaker  import  get  execution  role  import  sagemaker  as  sage  steps  500  Number  of  simulated  time  steps  Number  of  nuclear  spins  dim  Dimension  of  Hilbert  space  of  nuclear  and  electronic  spin  Define  Nuclear  and  Electron  spin  states  as  density  matrices  rhoI  sparse  csr  matrix  shape  int  dim  int  dim  All  carbon  atoms  magnetically  excited  rhoS  sparse  csr  matrix  shape  Electron  magnetically  excited  sparse  rho  sparse  kron  rhoS  rhoI  Build  the  system  density  matrix  tempfile  tmp  tmp  pckl  pickle  dump  sparse  rho  open  tempfile  wb  Upload  serialized  initial  state  to  S3  resource  boto3  resource  s3  my  bucket  resource  Bucket  sagemaker  kessle31  subsitute  this  for  your  s3  bucket  name  my  bucket  upload  fil,amazon
e  tempfile  Key  superradiance  initial  state  init  pckl  Clean  up  temporary  files  os  remove  tempfile  role  get  execution  role  sess  sage  Session  Specify  the  name  of  the  ECS  repo  that  contains  your  docker  image  By  default  the  image  with  tag  latest  in  the  repo  will  be  utilized  imagename  superradiance  account  sess  boto  session  client  sts  get  caller  identity  Account  region  sess  boto  session  region  name  image  dkr  ecr  amazonaws  com  format  account  region  imagename  Define  the  training  job  superradiance  sage  estimator  Estimator  image  role  ml  c4  2xlarge  output  path  s3  sagemaker  kessle31  superradiance  output  sagemaker  session  sess  Parameters  can  be  passed  to  the  simulation  as  dictionary  superradiance  hyperparam  dict  steps  steps  Pass  the  location  of  the  training  data  see  above  and  start  the  job  superradiance  fit  s3  sagemaker  kessle31  superradiance  initial  state  import  tarfile  get  the  results  ,amazon
from  S3  results  my  bucket  Object  superradiance  output  output  model  tar  gz  format  superradiance  latest  training  job  name  tempfile  tmp  model  tar  gz  results  download  file  tmp  model  tar  gz  unzip  the  results  tar  tarfile  open  tempfile  gz  tar  extractall  path  tmp  load  the  results  into  the  notebook  out  pickle  load  open  tmp  out  pckl  rb  intensity  out  intensity  ind  out  ind  clean  up  temporary  files  os  remove  tempfile  Vizualize  the  results  plt  scatter  range  steps  intensity  ind  label  Simulated  System  plt  scatter  range  steps  np  exp  ind  np  arange  steps  label  Classical  System  benchmark  plt  xlabel  Time  plt  ylabel  Intensity  of  emitted  radiation  normalized  plt  legend  plt  show  cat  container  superradiance  train  ,amazon
pip  install  user  nolearnimport  warnings  warnings  filterwarnings  ignore  from  sklearn  cross  validation  import  train  test  split  from  sklearn  metrics  import  classification  report  from  sklearn  import  datasets  from  nolearn  dbn  import  DBN  import  numpy  as  np  import  matplotlib  pyplot  as  plt  import  matplotlib  image  as  mpimg  matplotlib  inlinefrom  tensorflow  examples  tutorials  mnist  import  input  data  mnist  input  data  read  data  sets  MNIST  data  mnist  images  mnist  train  images  mnist  labels  mnist  train  labelstrX  teX  trY  teY  train  test  split  mnist  images  mnist  labels  astype  int0  test  size  33  print  Number  of  images  for  training  trX  shape  print  Number  of  images  used  for  testing  trY  shape  pix  int  np  sqrt  teX  shape  print  Each  image  is  pix  by  pix  pixels  dbn  DBN  trX  shape  300  10  learn  rates  learn  rate  decays  epochs  10  verbose  set  verbose  to  for  not  printing  output  dbn  fit  trX  trY  preds  dbn ,ibm
 predict  teX  print  classification  report  teY  preds  def  randIm  np  random  choice  np  arange  len  teY  size  pred  dbn  predict  np  atleast  2d  teX  image  teX  255  reshape  28  28  astype  uint8  show  the  prediction  print  Actual  digit  is  teY  predicted  pred  imgplot  plt  imshow  image  imgplot  set  cmap  gray  randIm  randIm  randIm  ,ibm
from  sagemaker  import  get  execution  role  Bucket  location  to  save  your  custom  code  in  tar  gz  format  custom  code  upload  location  s3  bucket  name  customcode  tensorflow  iris  Bucket  location  where  results  of  model  training  are  saved  model  artifacts  location  s3  bucket  name  artifacts  IAM  execution  role  that  gives  SageMaker  access  to  resources  in  your  AWS  account  role  get  execution  role  cat  iris  dnn  classifier  py  def  estimator  model  path  hyperparameters  feature  columns  tf  feature  column  numeric  column  INPUT  TENSOR  NAME  shape  return  tf  estimator  DNNClassifier  feature  columns  feature  columns  hidden  units  10  20  10  classes  model  dir  model  path  def  train  input  fn  training  dir  hyperparameters  training  set  tf  contrib  learn  datasets  base  load  csv  with  header  filename  os  path  join  training  dir  iris  training  csv  target  dtype  np  int  features  dtype  np  float32  return  tf  estimator  inputs  numpy  i,amazon
nput  fn  INPUT  TENSOR  NAME  np  array  training  set  data  np  array  training  set  target  num  epochs  None  shuffle  True  def  serving  input  fn  hyperparameters  feature  spec  INPUT  TENSOR  NAME  tf  FixedLenFeature  dtype  tf  float32  shape  return  tf  estimator  export  build  parsing  serving  input  receiver  fn  feature  spec  from  sagemaker  tensorflow  import  TensorFlow  iris  estimator  TensorFlow  entry  point  iris  dnn  classifier  py  role  role  output  path  model  artifacts  location  code  location  custom  code  upload  location  train  instance  count  train  instance  type  ml  c4  xlarge  training  steps  1000  evaluation  steps  100  time  import  boto3  use  the  region  specific  sample  data  bucket  region  boto3  Session  region  name  train  data  location  s3  sagemaker  sample  data  tensorflow  iris  format  region  iris  estimator  fit  train  data  location  time  iris  predictor  iris  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge,amazon
  iris  predictor  predict  expected  label  to  be  1print  iris  predictor  endpoint  import  sagemaker  sagemaker  Session  delete  endpoint  iris  predictor  endpoint  ,amazon
mkdir  data  wget  data  aclImdb  v1  tar  gz  http  ai  stanford  edu  amaas  data  sentiment  aclImdb  v1  tar  gz  tar  zxf  data  aclImdb  v1  tar  gz  dataimport  os  import  glob  def  read  imdb  data  data  dir  data  aclImdb  data  labels  for  data  type  in  train  test  data  data  type  labels  data  type  for  sentiment  in  pos  neg  data  data  type  sentiment  labels  data  type  sentiment  path  os  path  join  data  dir  data  type  sentiment  txt  files  glob  glob  path  for  in  files  with  open  as  review  data  data  type  sentiment  append  review  read  Here  we  represent  positive  review  by  and  negative  review  by  labels  data  type  sentiment  append  if  sentiment  pos  else  assert  len  data  data  type  sentiment  len  labels  data  type  sentiment  data  size  does  not  match  labels  size  format  data  type  sentiment  return  data  labelsdata  labels  read  imdb  data  print  IMDB  reviews  train  pos  neg  test  pos  neg  format  len  data  train  pos  len  data ,amazon
 train  neg  len  data  test  pos  len  data  test  neg  from  sklearn  utils  import  shuffle  def  prepare  imdb  data  data  labels  Prepare  training  and  test  sets  from  IMDb  movie  reviews  Combine  positive  and  negative  reviews  and  labels  data  train  data  train  pos  data  train  neg  data  test  data  test  pos  data  test  neg  labels  train  labels  train  pos  labels  train  neg  labels  test  labels  test  pos  labels  test  neg  Shuffle  reviews  and  corresponding  labels  within  training  and  test  sets  data  train  labels  train  shuffle  data  train  labels  train  data  test  labels  test  shuffle  data  test  labels  test  Return  unified  training  data  test  data  training  labels  test  labets  return  data  train  data  test  labels  train  labels  testtrain  test  train  test  prepare  imdb  data  data  labels  print  IMDb  reviews  combined  train  test  format  len  train  len  test  print  train  100  print  train  100  import  nltk  from  nltk  corpus  import  stopw,amazon
ords  from  nltk  stem  porter  import  import  re  from  bs4  import  BeautifulSoup  def  review  to  words  review  nltk  download  stopwords  quiet  True  stemmer  PorterStemmer  text  BeautifulSoup  review  html  parser  get  text  Remove  HTML  tags  text  re  sub  zA  Z0  text  lower  Convert  to  lower  case  words  text  split  Split  string  into  words  words  for  in  words  if  not  in  stopwords  words  english  Remove  stopwords  words  PorterStemmer  stem  for  in  words  stem  return  words  TODO  Apply  review  to  words  to  review  train  100  or  any  other  review  import  pickle  cache  dir  os  path  join  cache  sentiment  analysis  where  to  store  cache  files  os  makedirs  cache  dir  exist  ok  True  ensure  cache  directory  exists  def  preprocess  data  data  train  data  test  labels  train  labels  test  cache  dir  cache  dir  cache  file  preprocessed  data  pkl  Convert  each  review  to  words  read  from  cache  if  available  If  cache  file  is  not  None  try  to  re,amazon
ad  from  it  first  cache  data  None  if  cache  file  is  not  None  try  with  open  os  path  join  cache  dir  cache  file  rb  as  cache  data  pickle  load  print  Read  preprocessed  data  from  cache  file  cache  file  except  pass  unable  to  read  from  cache  but  that  okay  If  cache  is  missing  then  do  the  heavy  lifting  if  cache  data  is  None  Preprocess  training  and  test  data  to  obtain  words  for  each  review  words  train  list  map  review  to  words  data  train  words  test  list  map  review  to  words  data  test  words  train  review  to  words  review  for  review  in  data  train  words  test  review  to  words  review  for  review  in  data  test  Write  to  cache  file  for  future  runs  if  cache  file  is  not  None  cache  data  dict  words  train  words  train  words  test  words  test  labels  train  labels  train  labels  test  labels  test  with  open  os  path  join  cache  dir  cache  file  wb  as  pickle  dump  cache  data  print  Wrote  preprocessed ,amazon
 data  to  cache  file  cache  file  else  Unpack  data  loaded  from  cache  file  words  train  words  test  labels  train  labels  test  cache  data  words  train  cache  data  words  test  cache  data  labels  train  cache  data  labels  test  return  words  train  words  test  labels  train  labels  test  Preprocess  data  train  test  train  test  preprocess  data  train  test  train  test  import  numpy  as  np  def  build  dict  data  vocab  size  5000  Construct  and  return  dictionary  mapping  each  of  the  most  frequently  appearing  words  to  unique  integer  TODO  Determine  how  often  each  word  appears  in  data  Note  that  data  is  list  of  sentences  and  that  sentence  is  list  of  words  word  count  dict  storing  the  words  that  appear  in  the  reviews  along  with  how  often  they  occur  TODO  Sort  the  words  found  in  data  so  that  sorted  words  is  the  most  frequently  appearing  word  and  sorted  words  is  the  least  frequently  appearing  words  sorted  wo,amazon
rds  None  word  dict  This  is  what  we  are  building  dictionary  that  translates  words  into  integers  for  idx  word  in  enumerate  sorted  words  vocab  size  The  is  so  that  we  save  room  for  the  no  word  word  dict  word  idx  infrequent  labels  return  word  dictword  dict  build  dict  train  TODO  Use  this  space  to  determine  the  five  most  frequently  appearing  words  in  the  training  set  data  dir  data  pytorch  The  folder  we  will  use  for  storing  data  if  not  os  path  exists  data  dir  Make  sure  that  the  folder  exists  os  makedirs  data  dir  with  open  os  path  join  data  dir  word  dict  pkl  wb  as  pickle  dump  word  dict  def  convert  and  pad  word  dict  sentence  pad  500  NOWORD  We  will  use  to  represent  the  no  word  category  INFREQ  and  we  use  to  represent  the  infrequent  words  words  not  appearing  in  word  dict  working  sentence  NOWORD  pad  for  word  index  word  in  enumerate  sentence  pad  if  word  in  word  dict ,amazon
 working  sentence  word  index  word  dict  word  else  working  sentence  word  index  INFREQ  return  working  sentence  min  len  sentence  pad  def  convert  and  pad  data  word  dict  data  pad  500  result  lengths  for  sentence  in  data  converted  leng  convert  and  pad  word  dict  sentence  pad  result  append  converted  lengths  append  leng  return  np  array  result  np  array  lengths  train  train  len  convert  and  pad  data  word  dict  train  test  test  len  convert  and  pad  data  word  dict  test  Use  this  cell  to  examine  one  of  the  processed  reviews  to  make  sure  everything  is  working  as  intended  import  pandas  as  pd  pd  concat  pd  DataFrame  train  pd  DataFrame  train  len  pd  DataFrame  train  axis  to  csv  os  path  join  data  dir  train  csv  header  False  index  False  import  sagemaker  sagemaker  session  sagemaker  Session  bucket  sagemaker  session  default  bucket  prefix  sagemaker  sentiment  rnn  role  sagemaker  get  execution  role  input,amazon
  data  sagemaker  session  upload  data  path  data  dir  bucket  bucket  key  prefix  prefix  pygmentize  train  model  pyimport  torch  import  torch  utils  data  Read  in  only  the  first  250  rows  train  sample  pd  read  csv  os  path  join  data  dir  train  csv  header  None  names  None  nrows  250  Turn  the  input  pandas  dataframe  into  tensors  train  sample  torch  from  numpy  train  sample  values  float  squeeze  train  sample  torch  from  numpy  train  sample  drop  axis  values  long  Build  the  dataset  train  sample  ds  torch  utils  data  TensorDataset  train  sample  train  sample  Build  the  dataloader  train  sample  dl  torch  utils  data  DataLoader  train  sample  ds  batch  size  50  def  train  model  train  loader  epochs  optimizer  loss  fn  device  for  epoch  in  range  epochs  model  train  total  loss  for  batch  in  train  loader  batch  batch  batch  batch  batch  to  device  batch  batch  to  device  TODO  Complete  this  train  method  to  train  the  model ,amazon
 provided  total  loss  loss  data  item  print  Epoch  BCELoss  format  epoch  total  loss  len  train  loader  import  torch  optim  as  optim  from  train  model  import  LSTMClassifier  device  torch  device  cuda  if  torch  cuda  is  available  else  cpu  model  LSTMClassifier  32  100  5000  to  device  optimizer  optim  Adam  model  parameters  loss  fn  torch  nn  BCELoss  train  model  train  sample  dl  optimizer  loss  fn  device  from  sagemaker  pytorch  import  PyTorch  estimator  PyTorch  entry  point  train  py  source  dir  train  role  role  framework  version  train  instance  count  train  instance  type  ml  p2  xlarge  hyperparameters  epoch  10  hidden  dim  200  estimator  fit  training  input  data  TODO  Deploy  the  trained  model  test  pd  concat  pd  DataFrame  test  len  pd  DataFrame  test  axis  We  split  the  data  into  chunks  and  send  each  chunk  seperately  accumulating  the  results  def  predict  data  rows  512  split  array  np  array  split  data  int  data  sha,amazon
pe  float  rows  predictions  np  array  for  array  in  split  array  predictions  np  append  predictions  predictor  predict  array  return  predictionspredictions  predict  test  values  predictions  round  num  for  num  in  predictions  from  sklearn  metrics  import  accuracy  score  accuracy  score  test  predictions  test  review  The  simplest  pleasures  in  life  are  the  best  and  this  film  is  one  of  them  Combining  rather  basic  storyline  of  love  and  adventure  this  movie  transcends  the  usual  weekend  fair  with  wit  and  unmitigated  charm  TODO  Convert  test  review  into  form  usable  by  the  model  and  save  the  results  in  test  data  test  data  Nonepredictor  predict  test  data  estimator  delete  endpoint  pygmentize  serve  predict  pyfrom  sagemaker  predictor  import  RealTimePredictor  from  sagemaker  pytorch  import  PyTorchModel  class  StringPredictor  RealTimePredictor  def  init  self  endpoint  name  sagemaker  session  super  StringPredictor  self  i,amazon
nit  endpoint  name  sagemaker  session  content  type  text  plain  model  PyTorchModel  model  data  estimator  model  data  role  role  framework  version  entry  point  predict  py  source  dir  serve  predictor  cls  StringPredictor  predictor  model  deploy  initial  instance  count  instance  type  ml  m4  xlarge  import  glob  def  test  reviews  data  dir  data  aclImdb  stop  250  results  ground  We  make  sure  to  test  both  positive  and  negative  reviews  for  sentiment  in  pos  neg  path  os  path  join  data  dir  test  sentiment  txt  files  glob  glob  path  files  read  print  Starting  sentiment  files  Iterate  through  the  files  and  send  them  to  the  predictor  for  in  files  with  open  as  review  First  we  store  the  ground  truth  was  the  review  positive  or  negative  if  sentiment  pos  ground  append  else  ground  append  Read  in  the  review  and  convert  to  utf  for  transmission  via  HTTP  review  input  review  read  encode  utf  Send  the  review  to  the,amazon
  predictor  and  store  the  results  results  append  float  predictor  predict  review  input  Sending  reviews  to  our  endpoint  one  at  time  takes  while  so  we  only  send  small  number  of  reviews  files  read  if  files  read  stop  break  return  ground  resultsground  results  test  reviews  from  sklearn  metrics  import  accuracy  score  accuracy  score  ground  results  predictor  predict  test  review  predictor  endpointpredictor  delete  endpoint  ,amazon
import  tensorflow  as  tf  import  numpy  as  np  tf  set  random  seed  777  for  reproducibility  learning  rate  01  data  data  data  np  array  data  dtype  np  float32  data  np  array  data  dtype  np  float32  tf  placeholder  tf  float32  None  name  input  tf  placeholder  tf  float32  None  name  input  with  tf  name  scope  layer1  as  scope  W1  tf  Variable  tf  random  normal  name  weight1  b1  tf  Variable  tf  random  normal  name  bias1  layer1  tf  sigmoid  tf  matmul  W1  b1  w1  hist  tf  summary  histogram  weights1  W1  b1  hist  tf  summary  histogram  biases1  b1  layer1  hist  tf  summary  histogram  layer1  layer1  with  tf  name  scope  layer2  as  scope  W2  tf  Variable  tf  random  normal  name  weight2  b2  tf  Variable  tf  random  normal  name  bias2  hypothesis  tf  sigmoid  tf  matmul  layer1  W2  b2  w2  hist  tf  summary  histogram  weights2  W2  b2  hist  tf  summary  histogram  biases2  b2  hypothesis  hist  tf  summary  histogram  hypothesis  hypothesis  cost  loss ,ibm
 function  with  tf  name  scope  cost  as  scope  cost  tf  reduce  mean  tf  log  hypothesis  tf  log  hypothesis  cost  summ  tf  summary  scalar  cost  cost  with  tf  name  scope  train  as  scope  train  tf  train  AdamOptimizer  learning  rate  learning  rate  minimize  cost  Accuracy  computation  True  if  hypothesis  else  False  predicted  tf  cast  hypothesis  dtype  tf  float32  accuracy  tf  reduce  mean  tf  cast  tf  equal  predicted  dtype  tf  float32  accuracy  summ  tf  summary  scalar  accuracy  accuracy  Launch  graph  sess  tf  Session  tensorboard  logdir  logs  xor  logs  merged  summary  tf  summary  merge  all  writer  tf  summary  FileWriter  logs  writer  add  graph  sess  graph  Show  the  graph  Initialize  TensorFlow  variables  sess  run  tf  global  variables  initializer  for  step  in  range  10001  summary  sess  run  merged  summary  train  feed  dict  data  data  writer  add  summary  summary  global  step  step  if  step  1000  print  step  sess  run  cost  feed  dict  ,ibm
data  data  print  w1  sess  run  W1  print  w2  sess  run  W2  Accuracy  report  sess  run  hypothesis  predicted  accuracy  feed  dict  data  data  print  nHypothesis  nCorrect  nAccuracy  ls  al  home  dsxuser  work  logs  tensorboard  logdir  home  dsxuser  work  logs  Hypothesis  01338218  98166394  98809403  01135799  Correct  Accuracy  ,ibm
matplotlib  inline  import  os  import  numpy  as  np  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  from  sklearn  datasets  import  load  boston  import  sklearn  model  selectionimport  sagemaker  from  sagemaker  import  get  execution  role  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  from  sagemaker  predictor  import  csv  serializer  This  is  an  object  that  represents  the  SageMaker  session  that  we  are  currently  operating  in  This  object  contains  some  useful  information  that  we  will  need  to  access  later  such  as  our  region  session  sagemaker  Session  This  is  an  object  that  represents  the  IAM  role  that  we  are  currently  assigned  When  we  construct  and  launch  the  training  job  later  we  will  need  to  tell  it  what  IAM  role  it  should  have  Since  our  use  case  is  relatively  simple  we  will  simply  assign  the  training  job  the  role  we  currently  have  role  get  execution  role  boston  load  bosto,amazon
n  First  we  package  up  the  input  data  and  the  target  variable  the  median  value  as  pandas  dataframes  This  will  make  saving  the  data  to  file  little  easier  later  on  bos  pd  pd  DataFrame  boston  data  columns  boston  feature  names  bos  pd  pd  DataFrame  boston  target  We  split  the  dataset  into  training  and  testing  sets  train  test  train  test  sklearn  model  selection  train  test  split  bos  pd  bos  pd  test  size  33  Then  we  split  the  training  set  further  into  training  and  validation  sets  train  val  train  val  sklearn  model  selection  train  test  split  train  train  test  size  33  This  is  our  local  data  directory  We  need  to  make  sure  that  it  exists  data  dir  data  boston  if  not  os  path  exists  data  dir  os  makedirs  data  dir  We  use  pandas  to  save  our  test  train  and  validation  data  to  csv  files  Note  that  we  make  sure  not  to  include  header  information  or  an  index  as  this  is  required  by  the,amazon
  built  in  algorithms  provided  by  Amazon  Also  for  the  train  and  validation  data  it  is  assumed  that  the  first  entry  in  each  row  is  the  target  variable  test  to  csv  os  path  join  data  dir  test  csv  header  False  index  False  pd  concat  val  val  axis  to  csv  os  path  join  data  dir  validation  csv  header  False  index  False  pd  concat  train  train  axis  to  csv  os  path  join  data  dir  train  csv  header  False  index  False  prefix  boston  xgboost  HL  test  location  session  upload  data  os  path  join  data  dir  test  csv  key  prefix  prefix  val  location  session  upload  data  os  path  join  data  dir  validation  csv  key  prefix  prefix  train  location  session  upload  data  os  path  join  data  dir  train  csv  key  prefix  prefix  As  stated  above  we  use  this  utility  method  to  construct  the  image  name  for  the  training  container  container  get  image  uri  session  boto  region  name  xgboost  Now  that  we  know  which  contain,amazon
er  to  use  we  can  construct  the  estimator  object  xgb  sagemaker  estimator  Estimator  container  The  name  of  the  training  container  role  The  IAM  role  to  use  our  current  role  in  this  case  train  instance  count  The  number  of  instances  to  use  for  training  train  instance  type  ml  m4  xlarge  The  type  of  instance  ot  use  for  training  output  path  s3  output  format  session  default  bucket  prefix  Where  to  save  the  output  the  model  artifacts  sagemaker  session  session  The  current  SageMaker  sessionxgb  set  hyperparameters  max  depth  eta  gamma  min  child  weight  subsample  objective  reg  linear  early  stopping  rounds  10  num  round  200  from  sagemaker  tuner  import  IntegerParameter  ContinuousParameter  HyperparameterTuner  xgb  hyperparameter  tuner  HyperparameterTuner  estimator  xgb  The  estimator  object  to  use  as  the  basis  for  the  training  jobs  objective  metric  name  validation  rmse  The  metric  used  to  compare  train,amazon
ed  models  objective  type  Minimize  Whether  we  wish  to  minimize  or  maximize  the  metric  max  jobs  20  The  total  number  of  models  to  train  max  parallel  jobs  The  number  of  models  to  train  in  parallel  hyperparameter  ranges  max  depth  IntegerParameter  12  eta  ContinuousParameter  05  min  child  weight  IntegerParameter  subsample  ContinuousParameter  gamma  ContinuousParameter  10  This  is  wrapper  around  the  location  of  our  train  and  validation  data  to  make  sure  that  SageMaker  knows  our  data  is  in  csv  format  s3  input  train  sagemaker  s3  input  s3  data  train  location  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  val  location  content  type  csv  xgb  hyperparameter  tuner  fit  train  s3  input  train  validation  s3  input  validation  xgb  hyperparameter  tuner  wait  xgb  hyperparameter  tuner  best  training  job  xgb  attached  sagemaker  estimator  Estimator  attach  xgb  hyperparameter  tuner  best  training  ,amazon
job  xgb  transformer  xgb  attached  transformer  instance  count  instance  type  ml  m4  xlarge  xgb  transformer  transform  test  location  content  type  text  csv  split  type  Line  xgb  transformer  wait  aws  s3  cp  recursive  xgb  transformer  output  path  data  dirY  pred  pd  read  csv  os  path  join  data  dir  test  csv  out  header  None  plt  scatter  test  pred  plt  xlabel  Median  Price  plt  ylabel  Predicted  Price  plt  title  Median  Price  vs  Predicted  Price  First  we  will  remove  all  of  the  files  contained  in  the  data  dir  directory  rm  data  dir  And  then  we  delete  the  directory  itself  rmdir  data  dir  ,amazon
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  linear  time  series  forecast  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  import  io  import  os  import  time  import  json  import  sagemaker  amazon  common  as  smac  import  sagemaker  from  sagemaker  predictor  import  csv  serializer  json  deserializer  wget  http  robjhyndman  com  data  gasoline  csvgas  pd  read  csv  gasoline  csv  header  None  names  thousands  barrels  display  gas  head  plt  plot  gas  plt  show  gas  thousands  barrels  lag1  gas  thousands  barrels  shift  gas  thousands  barrels  lag2  gas  thousands  barrels  shift  gas  thousands  barrels  lag3  gas  thousands  barrels  shift  gas  thousands  barrels  lag4  gas  thousands  barrels  shift  gas  trend  np  arange  len  gas  gas  log  trend  np  log1p  np  arange  len  gas  gas  sq  trend  np  arange ,amazon
 len  gas  weeks  pd  get  dummies  np  array  list  range  52  15  len  gas  prefix  week  gas  pd  concat  gas  weeks  axis  gas  gas  iloc  split  train  int  len  gas  split  test  int  len  gas  train  gas  thousands  barrels  split  train  train  gas  drop  thousands  barrels  axis  iloc  split  train  as  matrix  validation  gas  thousands  barrels  split  train  split  test  validation  gas  drop  thousands  barrels  axis  iloc  split  train  split  test  as  matrix  test  gas  thousands  barrels  split  test  test  gas  drop  thousands  barrels  axis  iloc  split  test  as  matrix  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  np  array  train  astype  float32  np  array  train  astype  float32  buf  seek  key  linear  train  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  buf  s3  train  data  s3  train  format  bucket  prefix  key  print  uploaded  training  data  location  format  s3  train  data  buf  io  BytesIO  smac  write,amazon
  numpy  to  dense  tensor  buf  np  array  validation  astype  float32  np  array  validation  astype  float32  buf  seek  key  linear  validation  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  key  upload  fileobj  buf  s3  validation  data  s3  validation  format  bucket  prefix  key  print  uploaded  validation  data  location  format  s3  validation  data  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  linear  learner  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  linear  learner  latest  sess  sagemaker  Session  linear  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  s3  output  format  bucket  prefix  sagemaker  session  sess  linear  set  ,amazon
hyperparameters  feature  dim  59  mini  batch  size  100  predictor  type  regressor  epochs  10  num  models  32  loss  absolute  loss  linear  fit  train  s3  train  data  validation  s3  validation  data  linear  predictor  linear  deploy  initial  instance  count  instance  type  ml  m4  xlarge  gas  thousands  barrels  lag52  gas  thousands  barrels  shift  52  gas  thousands  barrels  lag104  gas  thousands  barrels  shift  104  gas  thousands  barrels  naive  forecast  gas  thousands  barrels  lag52  gas  thousands  barrels  lag104  naive  gas  split  test  thousands  barrels  naive  forecast  as  matrix  print  Naive  MdAPE  np  median  np  abs  test  naive  test  plt  plot  np  array  test  label  actual  plt  plot  naive  label  naive  plt  legend  plt  show  linear  predictor  content  type  text  csv  linear  predictor  serializer  csv  serializer  linear  predictor  deserializer  json  deserializerresult  linear  predictor  predict  test  one  step  np  array  score  for  in  result  predictions,amazon
  print  One  step  ahead  MdAPE  np  median  np  abs  test  one  step  test  plt  plot  np  array  test  label  actual  plt  plot  one  step  label  forecast  plt  legend  plt  show  multi  step  lags  test  for  row  in  test  row  lags  result  linear  predictor  predict  row  prediction  result  predictions  score  multi  step  append  prediction  lags  lags  lags  prediction  multi  step  np  array  multi  step  print  Multi  step  ahead  MdAPE  np  median  np  abs  test  multi  step  test  plt  plot  np  array  test  label  actual  plt  plot  one  step  label  forecast  plt  legend  plt  show  sagemaker  Session  delete  endpoint  linear  predictor  endpoint  ,amazon
import  tensorflow  as  tf  tf  set  random  seed  777  for  reproducibility  tf  Graph  Input  Set  wrong  model  weights  tf  Variable  Linear  model  hypothesis  cost  loss  function  cost  tf  reduce  mean  tf  square  hypothesis  Minimize  Gradient  Descent  Magic  optimizer  tf  train  GradientDescentOptimizer  learning  rate  train  optimizer  minimize  cost  Launch  the  graph  in  session  sess  tf  Session  Initializes  global  variables  in  the  graph  sess  run  tf  global  variables  initializer  for  step  in  range  100  print  step  sess  run  sess  run  train  ,ibm
import  boto3  import  sagemaker  import  os  region  boto3  Session  region  name  sage  client  boto3  Session  client  sagemaker  tuning  job  name  YOUR  HYPERPARAMETER  TUNING  JOB  NAME  run  this  cell  to  check  current  status  of  hyperparameter  tuning  job  tuning  job  result  sage  client  describe  hyper  parameter  tuning  job  HyperParameterTuningJobName  tuning  job  name  status  tuning  job  result  HyperParameterTuningJobStatus  if  status  Completed  print  Reminder  the  tuning  job  has  not  been  completed  job  count  tuning  job  result  TrainingJobStatusCounters  Completed  print  training  jobs  have  completed  job  count  is  minimize  tuning  job  result  HyperParameterTuningJobConfig  HyperParameterTuningJobObjective  Type  Maximize  objective  name  tuning  job  result  HyperParameterTuningJobConfig  HyperParameterTuningJobObjective  MetricName  from  pprint  import  pprint  if  tuning  job  result  get  BestTrainingJob  None  print  Best  model  found  so  far  pprint  tun,amazon
ing  job  result  BestTrainingJob  else  print  No  training  jobs  have  reported  results  yet  import  pandas  as  pd  tuner  sagemaker  HyperparameterTuningJobAnalytics  tuning  job  name  full  df  tuner  dataframe  if  len  full  df  df  full  df  full  df  FinalObjectiveValue  float  inf  if  len  df  df  df  sort  values  FinalObjectiveValue  ascending  is  minimize  print  Number  of  training  jobs  with  valid  objective  len  df  print  lowest  min  df  FinalObjectiveValue  highest  max  df  FinalObjectiveValue  pd  set  option  display  max  colwidth  Don  truncate  TrainingJobName  else  print  No  training  jobs  have  reported  valid  results  yet  dfimport  bokeh  import  bokeh  io  bokeh  io  output  notebook  from  bokeh  plotting  import  figure  show  from  bokeh  models  import  HoverTool  class  HoverHelper  def  init  self  tuning  analytics  self  tuner  tuning  analytics  def  hovertool  self  tooltips  FinalObjectiveValue  FinalObjectiveValue  TrainingJobName  TrainingJobName  for  ,amazon
in  self  tuner  tuning  ranges  keys  tooltips  append  ht  HoverTool  tooltips  tooltips  return  ht  def  tools  self  standard  tools  pan  crosshair  wheel  zoom  zoom  in  zoom  out  undo  reset  return  self  hovertool  standard  tools  hover  HoverHelper  tuner  figure  plot  width  900  plot  height  400  tools  hover  tools  axis  type  datetime  circle  source  df  TrainingStartTime  FinalObjectiveValue  show  ranges  tuner  tuning  ranges  figures  for  hp  name  hp  range  in  ranges  items  categorical  args  if  hp  range  get  Values  This  is  marked  as  categorical  Check  if  all  options  are  actually  numbers  def  is  num  try  float  return  except  return  vals  hp  range  Values  if  sum  is  num  for  in  vals  len  vals  Bokeh  has  issues  plotting  categorical  range  that  actually  numeric  so  plot  as  numeric  print  Hyperparameter  is  tuned  as  categorical  but  all  values  are  numeric  hp  name  else  Set  up  extra  options  for  plotting  categoricals  bit  tricky  ,amazon
when  they  re  actually  numbers  categorical  args  range  vals  Now  plot  it  figure  plot  width  500  plot  height  500  title  Objective  vs  hp  name  tools  hover  tools  axis  label  hp  name  axis  label  objective  name  categorical  args  circle  source  df  hp  name  FinalObjectiveValue  figures  append  show  bokeh  layouts  Column  figures  ,amazon
import  os  os  system  aws  s3  cp  s3  sagemaker  workshop  pdx  mnist  utils  py  utils  py  os  system  aws  s3  cp  s3  sagemaker  workshop  pdx  mnist  mnist  py  mnist  py  import  sagemaker  import  utils  import  numpy  as  np  import  matplotlib  pyplot  as  plt  from  tensorflow  contrib  learn  python  learn  datasets  import  mnist  import  tensorflow  as  tf  import  boto3  role  sagemaker  get  execution  role  sagemaker  session  sagemaker  Session  os  system  aws  s3  cp  recursive  s3  sagemaker  workshop  pdx  mnist  data  data  data  sets  mnist  read  data  sets  mnist  data  dtype  tf  uint8  reshape  False  validation  size  5000  utils  convert  to  data  sets  train  train  mnist  data  utils  convert  to  data  sets  validation  validation  mnist  data  utils  convert  to  data  sets  test  test  mnist  data  cat  utils  pybatch  xs  batch  ys  data  sets  train  next  batch  Change  train  to  test  or  select  different  batch  utils  gen  image  batch  xs  show  utils  gen  image,amazon
  batch  xs  show  utils  gen  image  batch  xs  show  inputs  sagemaker  session  upload  data  path  mnist  data  key  prefix  data  mnist  cat  mnist  py  from  sagemaker  tensorflow  import  TensorFlow  mnist  estimator  TensorFlow  entry  point  mnist  py  role  role  training  steps  1000  evaluation  steps  100  train  instance  count  train  instance  type  ml  c4  8xlarge  mnist  estimator  fit  inputs  mnist  predictor  mnist  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  tensorflow  examples  tutorials  mnist  import  input  data  mnist  input  data  read  data  sets  tmp  data  one  hot  True  for  in  range  10  data  mnist  test  images  tolist  tensor  proto  tf  make  tensor  proto  values  np  asarray  data  shape  len  data  dtype  tf  float32  predict  response  mnist  predictor  predict  tensor  proto  image  mnist  test  images  image  np  array  image  dtype  float  plt  imshow  image  reshape  28  28  plt  show  label  np  argmax  mnist  test  labels,amazon
  print  Label  is  format  label  prediction  predict  response  outputs  classes  int64Val  print  Prediction  is  format  prediction  print  sagemaker  Session  delete  endpoint  mnist  predictor  endpoint  ,amazon
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  kmeans  byom  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  numpy  as  np  import  sklearn  cluster  import  pickle  import  gzip  import  urllib  request  import  json  import  mxnet  as  mx  import  boto3  import  time  import  io  import  osurllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  gzip  open  mnist  pkl  gz  rb  train  set  valid  set  test  set  pickle  load  encoding  latin1  close  kmeans  sklearn  cluster  KMeans  clusters  10  fit  train  set  centroids  mx  ndarray  array  kmeans  cluster  centers  mx  ndarray  save  model  algo  centroids  tar  czvf  model  tar  gz  model  algo  1boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  model  tar  gz  upload  file  model  tar  gz  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  kmeans  model  DEMO  kmeans  ,amazon
byom  time  strftime  time  gmtime  sm  boto3  client  sagemaker  container  get  image  uri  boto3  Session  region  name  kmeans  create  model  response  sm  create  model  ModelName  kmeans  model  ExecutionRoleArn  role  PrimaryContainer  Image  container  ModelDataUrl  s3  model  tar  gz  format  bucket  prefix  print  create  model  response  ModelArn  kmeans  endpoint  config  DEMO  kmeans  byom  endpoint  config  time  strftime  time  gmtime  print  kmeans  endpoint  config  create  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  kmeans  endpoint  config  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  kmeans  model  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  kmeans  endpoint  DEMO  kmeans  byom  endpoint  time  strftime  time  gmtime  print  kmeans  endpoint  create  endpoint  response  sm  create  endpoint  EndpointName  kmeans  endpoint  EndpointConfigName  kmea,amazon
ns  endpoint  config  print  create  endpoint  response  EndpointArn  resp  sm  describe  endpoint  EndpointName  kmeans  endpoint  status  resp  EndpointStatus  print  Status  status  sm  get  waiter  endpoint  in  service  wait  EndpointName  kmeans  endpoint  resp  sm  describe  endpoint  EndpointName  kmeans  endpoint  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Status  status  if  status  InService  raise  Exception  Endpoint  creation  did  not  succeed  def  np2csv  arr  csv  io  BytesIO  np  savetxt  csv  arr  delimiter  fmt  return  csv  getvalue  decode  rstrip  runtime  boto3  Session  client  runtime  sagemaker  payload  np2csv  train  set  100  response  runtime  invoke  endpoint  EndpointName  kmeans  endpoint  ContentType  text  csv  Body  payload  result  json  loads  response  Body  read  decode  scored  labels  np  array  closest  cluster  for  in  result  predictions  scored  labels  kmeans  labels  100  Remove  endpoint  to  avoid  stray  charges  sm  delete  endpoi,amazon
nt  EndpointName  kmeans  endpoint  ,amazon
import  os  import  boto3  import  sagemaker  from  sagemaker  mxnet  import  MXNet  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  bash  mkdir  data  curl  https  raw  githubusercontent  com  saurabh3949  Text  Classification  Datasets  master  stsa  binary  phrases  train  data  train  curl  https  raw  githubusercontent  com  saurabh3949  Text  Classification  Datasets  master  stsa  binary  test  data  test  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  DEMO  sentiment  cat  sentiment  py  MXNet  sentiment  py  role  role  train  instance  count  train  instance  type  ml  c4  2xlarge  hyperparameters  batch  size  epochs  learning  rate  01  embedding  size  50  log  interval  1000  fit  inputs  predictor  deploy  initial  instance  count  instance  type  ml  c4  xlarge  data  this  movie  was  extremely  good  the  plot  was  very  boring  this  film  is  so  slick  superficial  and  trend  hoppy  just  could ,amazon
 not  watch  it  till  the  end  the  movie  was  so  enthralling  response  predictor  predict  data  print  responsesagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
mkdir  data  wget  data  aclImdb  v1  tar  gz  http  ai  stanford  edu  amaas  data  sentiment  aclImdb  v1  tar  gz  tar  zxf  data  aclImdb  v1  tar  gz  dataimport  os  import  glob  def  read  imdb  data  data  dir  data  aclImdb  data  labels  for  data  type  in  train  test  data  data  type  labels  data  type  for  sentiment  in  pos  neg  data  data  type  sentiment  labels  data  type  sentiment  path  os  path  join  data  dir  data  type  sentiment  txt  files  glob  glob  path  for  in  files  with  open  as  review  data  data  type  sentiment  append  review  read  Here  we  represent  positive  review  by  and  negative  review  by  labels  data  type  sentiment  append  if  sentiment  pos  else  assert  len  data  data  type  sentiment  len  labels  data  type  sentiment  data  size  does  not  match  labels  size  format  data  type  sentiment  return  data  labelsdata  labels  read  imdb  data  print  IMDB  reviews  train  pos  neg  test  pos  neg  format  len  data  train  pos  len  data ,amazon
 train  neg  len  data  test  pos  len  data  test  neg  from  sklearn  utils  import  shuffle  def  prepare  imdb  data  data  labels  Prepare  training  and  test  sets  from  IMDb  movie  reviews  Combine  positive  and  negative  reviews  and  labels  data  train  data  train  pos  data  train  neg  data  test  data  test  pos  data  test  neg  labels  train  labels  train  pos  labels  train  neg  labels  test  labels  test  pos  labels  test  neg  Shuffle  reviews  and  corresponding  labels  within  training  and  test  sets  data  train  labels  train  shuffle  data  train  labels  train  data  test  labels  test  shuffle  data  test  labels  test  Return  unified  training  data  test  data  training  labels  test  labets  return  data  train  data  test  labels  train  labels  testtrain  test  train  test  prepare  imdb  data  data  labels  print  IMDb  reviews  combined  train  test  format  len  train  len  test  train  100  import  re  REPLACE  NO  SPACE  re  compile  REPLACE  WITH  SPACE  re  co,amazon
mpile  br  br  def  review  to  words  review  words  REPLACE  NO  SPACE  sub  review  lower  words  REPLACE  WITH  SPACE  sub  words  return  wordsreview  to  words  train  100  import  pickle  cache  dir  os  path  join  cache  sentiment  web  app  where  to  store  cache  files  os  makedirs  cache  dir  exist  ok  True  ensure  cache  directory  exists  def  preprocess  data  data  train  data  test  labels  train  labels  test  cache  dir  cache  dir  cache  file  preprocessed  data  pkl  Convert  each  review  to  words  read  from  cache  if  available  If  cache  file  is  not  None  try  to  read  from  it  first  cache  data  None  if  cache  file  is  not  None  try  with  open  os  path  join  cache  dir  cache  file  rb  as  cache  data  pickle  load  print  Read  preprocessed  data  from  cache  file  cache  file  except  pass  unable  to  read  from  cache  but  that  okay  If  cache  is  missing  then  do  the  heavy  lifting  if  cache  data  is  None  Preprocess  training  and  test  data  t,amazon
o  obtain  words  for  each  review  words  train  list  map  review  to  words  data  train  words  test  list  map  review  to  words  data  test  words  train  review  to  words  review  for  review  in  data  train  words  test  review  to  words  review  for  review  in  data  test  Write  to  cache  file  for  future  runs  if  cache  file  is  not  None  cache  data  dict  words  train  words  train  words  test  words  test  labels  train  labels  train  labels  test  labels  test  with  open  os  path  join  cache  dir  cache  file  wb  as  pickle  dump  cache  data  print  Wrote  preprocessed  data  to  cache  file  cache  file  else  Unpack  data  loaded  from  cache  file  words  train  words  test  labels  train  labels  test  cache  data  words  train  cache  data  words  test  cache  data  labels  train  cache  data  labels  test  return  words  train  words  test  labels  train  labels  test  Preprocess  data  train  test  train  test  preprocess  data  train  test  train  test  import  numpy ,amazon
 as  np  from  sklearn  feature  extraction  text  import  CountVectorizer  from  sklearn  externals  import  joblib  joblib  is  an  enhanced  version  of  pickle  that  is  more  efficient  for  storing  NumPy  arrays  def  extract  BoW  features  words  train  words  test  vocabulary  size  5000  cache  dir  cache  dir  cache  file  bow  features  pkl  Extract  Bag  of  Words  for  given  set  of  documents  already  preprocessed  into  words  If  cache  file  is  not  None  try  to  read  from  it  first  cache  data  None  if  cache  file  is  not  None  try  with  open  os  path  join  cache  dir  cache  file  rb  as  cache  data  joblib  load  print  Read  features  from  cache  file  cache  file  except  pass  unable  to  read  from  cache  but  that  okay  If  cache  is  missing  then  do  the  heavy  lifting  if  cache  data  is  None  Fit  vectorizer  to  training  documents  and  use  it  to  transform  them  NOTE  Training  documents  have  already  been  preprocessed  and  tokenized  into  words,amazon
  pass  in  dummy  functions  to  skip  those  steps  preprocessor  lambda  vectorizer  CountVectorizer  max  features  vocabulary  size  features  train  vectorizer  fit  transform  words  train  toarray  Apply  the  same  vectorizer  to  transform  the  test  documents  ignore  unknown  words  features  test  vectorizer  transform  words  test  toarray  NOTE  Remember  to  convert  the  features  using  toarray  for  compact  representation  Write  to  cache  file  for  future  runs  store  vocabulary  as  well  if  cache  file  is  not  None  vocabulary  vectorizer  vocabulary  cache  data  dict  features  train  features  train  features  test  features  test  vocabulary  vocabulary  with  open  os  path  join  cache  dir  cache  file  wb  as  joblib  dump  cache  data  print  Wrote  features  to  cache  file  cache  file  else  Unpack  data  loaded  from  cache  file  features  train  features  test  vocabulary  cache  data  features  train  cache  data  features  test  cache  data  vocabulary  Return  b,amazon
oth  the  extracted  features  as  well  as  the  vocabulary  return  features  train  features  test  vocabulary  Extract  Bag  of  Words  features  for  both  training  and  test  datasets  train  test  vocabulary  extract  BoW  features  train  test  len  train  100  import  pandas  as  pd  Earlier  we  shuffled  the  training  dataset  so  to  make  things  simple  we  can  just  assign  the  first  10  000  reviews  to  the  validation  set  and  use  the  remaining  reviews  for  training  val  pd  DataFrame  train  10000  train  pd  DataFrame  train  10000  val  pd  DataFrame  train  10000  train  pd  DataFrame  train  10000  First  we  make  sure  that  the  local  directory  in  which  we  like  to  store  the  training  and  validation  csv  files  exists  data  dir  data  sentiment  web  app  if  not  os  path  exists  data  dir  os  makedirs  data  dir  pd  DataFrame  test  to  csv  os  path  join  data  dir  test  csv  header  False  index  False  pd  concat  val  val  axis  to  csv  os  path  jo,amazon
in  data  dir  validation  csv  header  False  index  False  pd  concat  train  train  axis  to  csv  os  path  join  data  dir  train  csv  header  False  index  False  To  save  bit  of  memory  we  can  set  text  train  val  train  and  val  to  None  test  train  val  train  val  Noneimport  sagemaker  session  sagemaker  Session  Store  the  current  SageMaker  session  S3  prefix  which  folder  will  we  use  prefix  sentiment  web  app  test  location  session  upload  data  os  path  join  data  dir  test  csv  key  prefix  prefix  val  location  session  upload  data  os  path  join  data  dir  validation  csv  key  prefix  prefix  train  location  session  upload  data  os  path  join  data  dir  train  csv  key  prefix  prefix  from  sagemaker  import  get  execution  role  Our  current  execution  role  is  require  when  creating  the  model  as  the  training  and  inference  code  will  need  to  access  the  model  artifacts  role  get  execution  role  We  need  to  retrieve  the  location ,amazon
 of  the  container  which  is  provided  by  Amazon  for  using  XGBoost  As  matter  of  convenience  the  training  and  inference  code  both  use  the  same  container  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  session  boto  region  name  xgboost  First  we  create  SageMaker  estimator  object  for  our  model  xgb  sagemaker  estimator  Estimator  container  The  location  of  the  container  we  wish  to  use  role  What  is  our  current  IAM  Role  train  instance  count  How  many  compute  instances  train  instance  type  ml  m4  xlarge  What  kind  of  compute  instances  output  path  s3  output  format  session  default  bucket  prefix  sagemaker  session  session  And  then  set  the  algorithm  specific  parameters  xgb  set  hyperparameters  max  depth  eta  gamma  min  child  weight  subsample  silent  objective  binary  logistic  early  stopping  rounds  10  num  round  500  s3  input  train  sagemaker  s3  input  s3  data  train  lo,amazon
cation  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  val  location  content  type  csv  xgb  fit  train  s3  input  train  validation  s3  input  validation  xgb  transformer  xgb  transformer  instance  count  instance  type  ml  m4  xlarge  xgb  transformer  transform  test  location  content  type  text  csv  split  type  Line  xgb  transformer  wait  aws  s3  cp  recursive  xgb  transformer  output  path  data  dirpredictions  pd  read  csv  os  path  join  data  dir  test  csv  out  header  None  predictions  round  num  for  num  in  predictions  squeeze  values  from  sklearn  metrics  import  accuracy  score  accuracy  score  test  predictions  xgb  predictor  xgb  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  sagemaker  predictor  import  csv  serializer  We  need  to  tell  the  endpoint  what  format  the  data  we  are  sending  is  in  so  that  SageMaker  can  perform  the  serialization  xgb  predictor  content  type  text  csv  xgb  predi,amazon
ctor  serializer  csv  serializer  We  split  the  data  into  chunks  and  send  each  chunk  seperately  accumulating  the  results  def  predict  data  rows  512  split  array  np  array  split  data  int  data  shape  float  rows  predictions  for  array  in  split  array  predictions  join  predictions  xgb  predictor  predict  array  decode  utf  return  np  fromstring  predictions  sep  test  pd  read  csv  os  path  join  data  dir  test  csv  header  None  values  predictions  predict  test  predictions  round  num  for  num  in  predictions  from  sklearn  metrics  import  accuracy  score  accuracy  score  test  predictions  xgb  predictor  delete  endpoint  test  review  Nothing  but  disgusting  materialistic  pageant  of  glistening  abed  remote  control  greed  zombies  totally  devoid  of  any  heart  or  heat  romantic  comedy  that  has  zero  romantic  chemestry  and  zero  laughs  test  words  review  to  words  test  review  print  test  words  def  bow  encoding  words  vocabulary  bow  ,amazon
len  vocabulary  Start  by  setting  the  count  for  each  word  in  the  vocabulary  to  zero  for  word  in  words  split  For  each  word  in  the  string  if  word  in  vocabulary  If  the  word  is  one  that  occurs  in  the  vocabulary  increase  its  count  bow  vocabulary  word  return  bowtest  bow  bow  encoding  test  words  vocabulary  print  test  bow  len  test  bow  xgb  predictor  xgb  deploy  initial  instance  count  instance  type  ml  m4  xlarge  import  boto3  runtime  boto3  Session  client  sagemaker  runtime  xgb  predictor  endpointresponse  runtime  invoke  endpoint  EndpointName  xgb  predictor  endpoint  The  name  of  the  endpoint  we  created  ContentType  text  csv  The  data  format  that  is  expected  Body  test  bow  response  runtime  invoke  endpoint  EndpointName  xgb  predictor  endpoint  The  name  of  the  endpoint  we  created  ContentType  text  csv  The  data  format  that  is  expected  Body  join  str  val  for  val  in  test  bow  encode  utf  print  response ,amazon
 response  response  Body  read  decode  utf  print  response  xgb  predictor  endpointprint  str  vocabulary  xgb  predictor  delete  endpoint  First  we  will  remove  all  of  the  files  contained  in  the  data  dir  directory  rm  data  dir  And  then  we  delete  the  directory  itself  rmdir  data  dir  Similarly  we  remove  the  files  in  the  cache  dir  directory  and  the  directory  itself  rm  cache  dir  rmdir  cache  dir  ,amazon
Load  the  required  libraries  import  warnings  import  zipfile  import  os  import  cv2  import  urllib  request  import  sagemaker  import  numpy  as  np  import  mxnet  as  mx  import  pandas  as  pd  import  seaborn  as  sns  import  matplotlib  pyplot  as  plt  import  matplotlib  image  as  mpimg  from  sklearn  model  selection  import  train  test  split  warnings  simplefilter  ignore  matplotlib  inline  Helper  function  def  download  url  Helper  function  to  download  individual  file  from  given  url  Arguments  url  full  URL  of  the  file  to  download  Returns  filename  downloaded  file  name  filename  url  split  if  not  os  path  exists  filename  urllib  request  urlretrieve  url  filename  return  filename  To  download  and  extract  Sample  Data  enter  the  URL  provided  by  the  intructor  URL  ENTER  THE  URL  PROVIDED  BY  THE  INSTRUCTOR  file  download  URL  Extract  the  file  with  zipfile  ZipFile  file  as  zf  zf  extractall  View  the  log  data  data  df  pd  read,amazon
  csv  data  driving  log  csv  data  df  head  Data  Overview  print  Dataset  Shape  format  data  df  shape  print  data  df  describe  Visualize  the  distribution  of  the  data  using  the  seabourne  library  sns  set  rc  figure  figsize  10  10  fig  sns  distplot  data  df  steering  plt  xlabel  Steering  Angle  plt  title  Distribution  of  Steering  Angles  in  the  Dataset  plt  show  fig  Separate  the  features  data  df  center  left  right  values  data  df  steering  values  Image  Transformation  Crop  def  crop  image  Crops  the  image  by  emoving  the  sky  at  the  top  and  the  car  front  at  the  bottom  Arguments  image  numpy  array  representing  an  RGB  image  of  format  Height  Width  Channel  Returns  Cropped  image  START  OF  YOUR  CODE  line  of  code  image  None  END  OF  YOUR  CODE  return  image  Image  Transformation  Resize  def  resize  image  Resize  the  image  to  the  input  shape  for  the  NVIDIA  model  Note  The  parameters  IMAGE  WIDTH  and  IMAGE  HEIG,amazon
HT  are  declared  as  part  of  pre  processing  pipeline  Arguments  image  numpy  array  representing  image  Returns  Resized  image  image  cv2  resize  image  IMAGE  WIDTH  IMAGE  HEIGHT  cv2  INTER  AREA  return  image  Image  Transformation  Convert  from  RGB  to  YUV  def  rgb2yuv  image  Convert  the  image  from  RGB  to  YUV  color  space  Arguments  image  numpy  array  represnting  the  image  Returns  YUV  image  START  OF  YOUR  CODE  line  of  code  image  None  END  OF  YOUR  CODE  return  image  Image  Augmentation  Random  Flip  def  random  flip  image  steering  angle  Randomly  50  of  the  time  flip  the  image  left  from  left  to  right  and  vice  versa  Additionally  adjust  the  steering  angle  accordingly  Arguments  image  pre  processed  input  image  steering  amngle  pre  processed  steering  angle  Returns  image  flipped  image  steering  angle  adjusted  steering  angle  START  OF  YOUR  CODE  lines  of  code  return  None  END  OF  YOUR  CODE  Image  Augmentation  Ran,amazon
dom  Translate  def  translate  image  steering  angle  range  range  Randomly  shift  translate  the  image  vertically  and  horizontally  Arguments  image  pre  processed  input  image  steering  angle  pre  processed  steering  angle  range  axis  pixels  range  axis  pixels  Returns  image  translated  image  steering  angle  adjusted  steeing  angle  Randomly  adjust  the  and  axis  transform  range  np  random  rand  transform  range  np  random  rand  Adjust  the  steering  angle  steering  angle  transform  002  transform  np  float32  transform  transform  height  width  image  shape  image  cv2  warpAffine  image  transform  width  height  return  image  steering  angle  Image  Augmentation  Random  Distortion  def  distort  image  Add  distortion  to  random  images  and  adjust  the  brightness  Arguments  image  pre  processed  input  image  Returns  new  image  distorted  image  Create  placeholder  numpy  array  for  the  new  image  new  img  image  astype  float  Add  random  brightness  va,amazon
lue  np  random  randint  28  28  if  value  mask  new  img  value  255  if  value  mask  new  img  value  new  img  np  where  mask  value  Add  random  shadow  new  img  shape  mid  np  random  randint  factor  np  random  uniform  if  np  random  rand  new  img  mid  factor  else  new  img  mid  factor  Randomly  shift  the  horizon  new  img  shape  horizon  shift  np  random  randint  pts1  np  float32  horizon  horizon  pts2  np  float32  horizon  shift  horizon  shift  cv2  getPerspectiveTransform  pts1  pts2  new  img  cv2  warpPerspective  new  img  borderMode  cv2  BORDER  REPLICATE  return  new  img  astype  np  uint8  Image  Augmentation  Random  Brightness  def  brightness  image  Randomly  adjust  brightness  of  the  image  Arguments  image  pro  processed  input  image  Returns  HSV  HSB  converted  image  START  OF  YOUR  CODE  lines  of  code  return  None  END  OF  YOUR  CODE  Helper  function  def  load  data  dir  image  file  Load  RGB  images  from  file  return  mpimg  imread  os  path,amazon
  join  data  dir  image  file  strip  def  transform  image  Combine  all  preprocess  functions  into  one  image  crop  image  image  resize  image  image  rgb2yuv  image  return  image  Origional  left  image  IMAGE  HEIGHT  IMAGE  WIDTH  IMAGE  CHANNELS  66  200  INPUT  SHAPE  IMAGE  HEIGHT  IMAGE  WIDTH  IMAGE  CHANNELS  random  image  100  img  load  data  random  image  img  random  image  plt  rcParams  figure  figsize  11  10  plt  imshow  img  Create  Subpluts  for  Augmented  Images  plt  close  all  fig  plt  figure  figsize  18  10  sub1  fig  add  subplot  221  sub1  set  title  Bright  sub1  imshow  brightness  img  sub2  fig  add  subplot  222  sub2  set  title  Horizontal  Flip  sub2  imshow  cv2  flip  img  sub3  fig  add  subplot  223  sub3  set  title  Random  Distortion  sub3  imshow  distort  img  sub4  fig  add  subplot  224  sub4  set  title  Final  Image  sub4  imshow  transform  img  plt  show  Batch  Image  Configurations  HEIGHT  WIDTH  CHANNELS  66  200  INPUT  SHAPE  HEIGHT  WID,amazon
TH  CHANNELS  Aumentation  Pipeline  Functions  def  choose  data  dir  center  left  right  steering  angle  Randomly  choose  an  image  from  the  center  left  or  right  and  adjust  the  steering  angle  choice  np  random  choice  if  choice  return  load  data  dir  left  steering  angle  elif  choice  return  load  data  dir  right  steering  angle  return  load  data  dir  center  steering  angle  def  augment  data  dir  center  left  right  steering  angle  range  100  range  10  Generate  an  augumented  image  and  adjust  steering  angle  The  steering  angle  is  associated  with  the  center  image  image  steering  angle  choose  data  dir  center  left  right  steering  angle  image  steering  angle  random  flip  image  steering  angle  image  steering  angle  translate  image  steering  angle  range  range  image  brightness  image  image  distort  image  return  image  steering  angle  def  aug  pipeline  data  dir  image  paths  steering  angles  is  training  Generate  training  image ,amazon
 given  image  paths  and  associated  steering  angles  Create  numpy  array  to  store  augmented  images  images  np  empty  image  paths  shape  HEIGHT  WIDTH  CHANNELS  steering  np  empty  images  paths  shape  while  True  for  index  in  np  random  permutation  image  paths  shape  center  left  right  image  paths  index  steering  angle  steering  angles  index  Random  augmentation  for  training  data  if  is  training  and  np  random  rand  Augment  all  the  randomly  selected  images  image  steering  angle  augment  data  dir  center  left  right  steering  angle  else  Load  only  the  center  image  image  load  data  dir  center  Transform  and  add  the  image  with  steering  angle  to  the  placeholder  numpy  array  images  transform  image  steering  steering  angle  Return  placeholder  numpy  arrays  return  np  array  images  astype  np  float32  np  array  steering  astype  np  float32  def  aug  pipeline  data  dir  image  paths  steering  angles  batch  size  is  training  Gene,amazon
rate  training  image  give  image  paths  and  associated  steering  angles  images  np  empty  batch  size  HEIGHT  WIDTH  CHANNELS  steering  np  empty  batch  size  while  True  for  index  in  np  random  permutation  image  paths  shape  center  left  right  image  paths  index  steering  angle  steering  angles  index  argumentation  if  is  training  and  np  random  rand  image  steering  angle  augument  data  dir  center  left  right  steering  angle  else  image  load  data  dir  center  add  the  image  and  steering  angle  to  the  batch  images  transform  image  steering  steering  angle  if  batch  size  break  return  np  array  images  astype  np  float32  np  array  steering  astype  np  float32  sample  sample  aug  pipeline  data  True  Plot  New  Distribution  of  training  examples  fig  sns  distplot  sample  plt  xlabel  Steering  Angle  plt  title  New  Distribution  of  Steering  Angles  in  the  Training  Dataset  plt  show  fig  Libraries  and  SageMaker  configuration  sagemake,amazon
r  session  sagemaker  Session  role  sagemaker  get  execution  role  Create  Training  and  Validation  datasets  train  valid  train  valid  train  test  split  test  size  random  state  42  Preprocess  through  the  pipline  train  train  aug  pipeline  data  train  train  True  valid  valid  aug  pipeline  data  valid  valid  False  View  resultant  shape  print  Training  Dataset  Shape  format  train  shape  print  Validation  Dataset  Shape  format  valid  shape  Create  local  repository  for  Numpy  Arrays  if  not  os  path  exists  tmp  data  os  mkdir  tmp  data  Save  the  Dataset  as  Numpy  Arrays  np  save  tmp  data  train  npy  train  np  save  tmp  data  train  npy  train  np  save  tmp  data  valid  npy  valid  np  save  tmp  data  valid  npy  valid  ,amazon
Install  xgboost  in  notebook  instance  Command  to  install  xgboost  conda  install  conda  forge  xgboost  matplotlib  inline  import  sys  import  numpy  as  np  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  import  xgboost  as  xgb  from  sklearn  import  preprocessingcolumn  list  file  iris  train  column  list  txt  train  file  iris  train  csv  validation  file  iris  validation  csv  columns  with  open  column  list  file  as  columns  read  split  columns  Encode  Class  Labels  to  integers  le  preprocessing  LabelEncoder  le  fit  Iris  setosa  Iris  versicolor  Iris  virginica  Specify  the  column  names  as  the  file  does  not  have  column  header  df  train  pd  read  csv  train  file  names  columns  df  validation  pd  read  csv  validation  file  names  columns  df  train  head  df  validation  head  train  df  train  iloc  Features  1st  column  onwards  train  df  train  iloc  ravel  Target  0th  column  validation  df  validation  iloc  validation  df  validation ,amazon
 iloc  ravel  Launch  classifier  XGBoost  Training  Parameter  Reference  https  github  com  dmlc  xgboost  blob  master  doc  parameter  md  classifier  xgb  XGBClassifier  max  depth  objective  multi  softmax  num  class  classifierclassifier  fit  train  train  eval  set  train  train  validation  validation  eval  metric  merror  mlogloss  eval  result  classifier  evals  result  training  rounds  range  len  eval  result  validation  mlogloss  print  training  rounds  plt  scatter  training  rounds  eval  result  validation  mlogloss  label  Training  Error  plt  scatter  training  rounds  eval  result  validation  mlogloss  label  Validation  Error  plt  grid  True  plt  xlabel  Iteration  plt  ylabel  LogLoss  plt  title  Training  Vs  Validation  Error  plt  legend  xgb  plot  importance  classifier  df  pd  read  csv  iris  all  csv  df  head  test  df  iloc  print  test  result  classifier  predict  test  result  df  predicted  class  le  inverse  transform  result  df  head  print  Confusion  ma,amazon
trix  Actual  versus  Predicted  pd  crosstab  df  class  df  predicted  class  import  sklearn  metrics  as  metrics  print  metrics  classification  report  df  class  df  predicted  class  ,amazon
import  numpy  as  np  import  os  import  urllib  import  gzip  import  struct  def  download  data  url  force  download  True  fname  url  split  if  force  download  or  not  os  path  exists  fname  urllib  urlretrieve  url  fname  return  fname  def  read  data  label  url  image  url  with  gzip  open  download  data  label  url  as  flbl  magic  num  struct  unpack  II  flbl  read  label  np  fromstring  flbl  read  dtype  np  int8  with  gzip  open  download  data  image  url  rb  as  fimg  magic  num  rows  cols  struct  unpack  IIII  fimg  read  16  image  np  fromstring  fimg  read  dtype  np  uint8  reshape  len  label  rows  cols  return  label  image  path  http  yann  lecun  com  exdb  mnist  train  lbl  train  img  read  data  path  train  labels  idx1  ubyte  gz  path  train  images  idx3  ubyte  gz  val  lbl  val  img  read  data  path  t10k  labels  idx1  ubyte  gz  path  t10k  images  idx3  ubyte  gz  import  matplotlib  pyplot  as  plt  for  in  range  10  plt  subplot  10  plt  imshow  ,amazon
train  img  cmap  Greys  plt  axis  off  plt  show  display  print  label  train  lbl  10  import  mxnet  as  mx  def  to4d  img  return  img  reshape  img  shape  28  28  astype  np  float32  255  batch  size  100  train  iter  mx  io  NDArrayIter  to4d  train  img  train  lbl  batch  size  shuffle  True  val  iter  mx  io  NDArrayIter  to4d  val  img  val  lbl  batch  size  Create  place  holder  variable  for  the  input  data  data  mx  sym  Variable  data  Flatten  the  data  from  shape  batch  size  num  channel  width  height  into  batch  size  num  channel  width  height  data  mx  sym  Flatten  data  data  The  first  fully  connected  layer  fc1  mx  sym  FullyConnected  data  data  name  fc1  num  hidden  128  Apply  relu  to  the  output  of  the  first  fully  connnected  layer  act1  mx  sym  Activation  data  fc1  name  relu1  act  type  relu  The  second  fully  connected  layer  and  the  according  activation  function  fc2  mx  sym  FullyConnected  data  act1  name  fc2  num  hidden  64  ,amazon
act2  mx  sym  Activation  data  fc2  name  relu2  act  type  relu  The  thrid  fully  connected  layer  note  that  the  hidden  size  should  be  10  which  is  the  number  of  unique  digits  fc3  mx  sym  FullyConnected  data  act2  name  fc3  num  hidden  10  The  softmax  and  loss  layer  mlp  mx  sym  SoftmaxOutput  data  fc3  name  softmax  model  mx  model  FeedForward  symbol  mlp  network  structure  num  epoch  10  number  of  data  passes  for  training  learning  rate  learning  rate  of  SGD  model  fit  train  iter  training  data  eval  data  val  iter  validation  data  batch  end  callback  mx  callback  Speedometer  batch  size  200  output  progress  for  each  200  data  batches  plt  clf  plt  imshow  val  img  cmap  Greys  plt  axis  off  plt  show  display  prob  model  predict  val  img  astype  np  float32  255  print  Classified  as  with  probability  prob  argmax  max  prob  print  Validation  accuracy  model  score  val  iter  100  data  mx  symbol  Variable  data  first  conv,amazon
  layer  conv1  mx  sym  Convolution  data  data  kernel  num  filter  20  tanh1  mx  sym  Activation  data  conv1  act  type  tanh  pool1  mx  sym  Pooling  data  tanh1  pool  type  max  kernel  stride  second  conv  layer  conv2  mx  sym  Convolution  data  pool1  kernel  num  filter  50  tanh2  mx  sym  Activation  data  conv2  act  type  tanh  pool2  mx  sym  Pooling  data  tanh2  pool  type  max  kernel  stride  first  fullc  layer  flatten  mx  sym  Flatten  data  pool2  fc1  mx  symbol  FullyConnected  data  flatten  num  hidden  500  tanh3  mx  sym  Activation  data  fc1  act  type  tanh  second  fullc  fc2  mx  sym  FullyConnected  data  tanh3  num  hidden  10  softmax  loss  lenet  mx  sym  SoftmaxOutput  data  fc2  name  softmax  model  mx  model  FeedForward  ctx  mx  cpu  use  GPU  for  training  others  are  same  as  before  symbol  lenet  num  epoch  10  learning  rate  model  fit  train  iter  eval  data  val  iter  batch  end  callback  mx  callback  Speedometer  batch  size  200  print  Val,amazon
idation  accuracy  model  score  val  iter  100  ,amazon
import  mxnet  as  mx  import  numpy  as  np  from  sagemaker  mxnet  import  MXNet  from  sagemaker  import  get  execution  role  from  io  import  BytesIO  import  boto3role  get  execution  role  mnist  estimator  MXNet  entry  point  part2  sm  mnist  py  py  version  py3  role  role  train  instance  count  train  instance  type  ml  p2  xlarge  mnist  estimator  fit  s3  jakechenawspublic  sample  data  mnist  train  mnist  estimator  fit  inputs  images  s3  jakechenawspublic  sample  data  mnist  train  images  labels  s3  jakechenawspublic  sample  data  mnist  train  labels  predictor  mnist  estimator  deploy  instance  type  ml  m4  xlarge  initial  instance  count  endpoint  name  tutorial  mnist  endpoint  test  np  loadtxt  images  sm  csv  delimiter  reshape  28  28  proba  np  array  predictor  predict  test  proba  shapemx  nd  argmax  mx  nd  array  proba  axis  del  predictor  predictor  predict  test  s3  boto3  client  s3  sm  boto3  client  sagemaker  sm  runtime  boto3  client  sagema,amazon
ker  runtime  Load  the  image  records  from  S3  into  local  memory  resp  s3  get  object  Bucket  jakechenawspublic  Key  sample  data  mnist  test  images  images  sm  csv  test  resp  Body  read  Get  prediction  endpoint  resp  sm  list  endpoints  for  in  resp  Endpoints  print  EndpointName  Send  the  dataset  to  the  prediction  endpoint  resp  sm  runtime  invoke  endpoint  EndpointName  tutorial  mnist  endpoint  Body  test  ContentType  text  csv  These  are  not  required  by  the  docs  but  very  helpful  Accept  text  csv  Not  sure  why  the  values  would  be  different  Load  the  response  body  back  into  an  Numpy  array  for  further  processing  BytesIO  resp  Body  read  array  np  loadtxt  delimiter  array  shapemx  nd  argmax  mx  nd  array  array  axis  resp  sm  delete  endpoint  EndpointName  tutorial  mnist  endpoint  print  resp  ResponseMetadata  HTTPStatusCode  ,amazon
mkdir  data  wget  data  aclImdb  v1  tar  gz  http  ai  stanford  edu  amaas  data  sentiment  aclImdb  v1  tar  gz  tar  zxf  data  aclImdb  v1  tar  gz  dataimport  os  import  glob  def  read  imdb  data  data  dir  data  aclImdb  data  labels  for  data  type  in  train  test  data  data  type  labels  data  type  for  sentiment  in  pos  neg  data  data  type  sentiment  labels  data  type  sentiment  path  os  path  join  data  dir  data  type  sentiment  txt  files  glob  glob  path  for  in  files  with  open  as  review  data  data  type  sentiment  append  review  read  Here  we  represent  positive  review  by  and  negative  review  by  labels  data  type  sentiment  append  if  sentiment  pos  else  assert  len  data  data  type  sentiment  len  labels  data  type  sentiment  data  size  does  not  match  labels  size  format  data  type  sentiment  return  data  labelsdata  labels  read  imdb  data  print  IMDB  reviews  train  pos  neg  test  pos  neg  format  len  data  train  pos  len  data ,amazon
 train  neg  len  data  test  pos  len  data  test  neg  from  sklearn  utils  import  shuffle  def  prepare  imdb  data  data  labels  Prepare  training  and  test  sets  from  IMDb  movie  reviews  Combine  positive  and  negative  reviews  and  labels  data  train  data  train  pos  data  train  neg  data  test  data  test  pos  data  test  neg  labels  train  labels  train  pos  labels  train  neg  labels  test  labels  test  pos  labels  test  neg  Shuffle  reviews  and  corresponding  labels  within  training  and  test  sets  data  train  labels  train  shuffle  data  train  labels  train  data  test  labels  test  shuffle  data  test  labels  test  Return  unified  training  data  test  data  training  labels  test  labets  return  data  train  data  test  labels  train  labels  testtrain  test  train  test  prepare  imdb  data  data  labels  print  IMDb  reviews  combined  train  test  format  len  train  len  test  train  100  import  nltk  nltk  download  stopwords  from  nltk  corpus  import  stop,amazon
words  from  nltk  stem  porter  import  stemmer  PorterStemmer  import  re  from  bs4  import  BeautifulSoup  def  review  to  words  review  text  BeautifulSoup  review  html  parser  get  text  Remove  HTML  tags  text  re  sub  zA  Z0  text  lower  Convert  to  lower  case  words  text  split  Split  string  into  words  words  for  in  words  if  not  in  stopwords  words  english  Remove  stopwords  words  PorterStemmer  stem  for  in  words  stem  return  wordsimport  pickle  cache  dir  os  path  join  cache  sentiment  analysis  where  to  store  cache  files  os  makedirs  cache  dir  exist  ok  True  ensure  cache  directory  exists  def  preprocess  data  data  train  data  test  labels  train  labels  test  cache  dir  cache  dir  cache  file  preprocessed  data  pkl  Convert  each  review  to  words  read  from  cache  if  available  If  cache  file  is  not  None  try  to  read  from  it  first  cache  data  None  if  cache  file  is  not  None  try  with  open  os  path  join  cache  dir  cach,amazon
e  file  rb  as  cache  data  pickle  load  print  Read  preprocessed  data  from  cache  file  cache  file  except  pass  unable  to  read  from  cache  but  that  okay  If  cache  is  missing  then  do  the  heavy  lifting  if  cache  data  is  None  Preprocess  training  and  test  data  to  obtain  words  for  each  review  words  train  list  map  review  to  words  data  train  words  test  list  map  review  to  words  data  test  words  train  review  to  words  review  for  review  in  data  train  words  test  review  to  words  review  for  review  in  data  test  Write  to  cache  file  for  future  runs  if  cache  file  is  not  None  cache  data  dict  words  train  words  train  words  test  words  test  labels  train  labels  train  labels  test  labels  test  with  open  os  path  join  cache  dir  cache  file  wb  as  pickle  dump  cache  data  print  Wrote  preprocessed  data  to  cache  file  cache  file  else  Unpack  data  loaded  from  cache  file  words  train  words  test  labels  tr,amazon
ain  labels  test  cache  data  words  train  cache  data  words  test  cache  data  labels  train  cache  data  labels  test  return  words  train  words  test  labels  train  labels  test  Preprocess  data  train  test  train  test  preprocess  data  train  test  train  test  import  numpy  as  np  from  sklearn  feature  extraction  text  import  CountVectorizer  from  sklearn  externals  import  joblib  joblib  is  an  enhanced  version  of  pickle  that  is  more  efficient  for  storing  NumPy  arrays  def  extract  BoW  features  words  train  words  test  vocabulary  size  5000  cache  dir  cache  dir  cache  file  bow  features  pkl  Extract  Bag  of  Words  for  given  set  of  documents  already  preprocessed  into  words  If  cache  file  is  not  None  try  to  read  from  it  first  cache  data  None  if  cache  file  is  not  None  try  with  open  os  path  join  cache  dir  cache  file  rb  as  cache  data  joblib  load  print  Read  features  from  cache  file  cache  file  except  pass  una,amazon
ble  to  read  from  cache  but  that  okay  If  cache  is  missing  then  do  the  heavy  lifting  if  cache  data  is  None  Fit  vectorizer  to  training  documents  and  use  it  to  transform  them  NOTE  Training  documents  have  already  been  preprocessed  and  tokenized  into  words  pass  in  dummy  functions  to  skip  those  steps  preprocessor  lambda  vectorizer  CountVectorizer  max  features  vocabulary  size  preprocessor  lambda  tokenizer  lambda  already  preprocessed  features  train  vectorizer  fit  transform  words  train  toarray  Apply  the  same  vectorizer  to  transform  the  test  documents  ignore  unknown  words  features  test  vectorizer  transform  words  test  toarray  NOTE  Remember  to  convert  the  features  using  toarray  for  compact  representation  Write  to  cache  file  for  future  runs  store  vocabulary  as  well  if  cache  file  is  not  None  vocabulary  vectorizer  vocabulary  cache  data  dict  features  train  features  train  features  test  features  ,amazon
test  vocabulary  vocabulary  with  open  os  path  join  cache  dir  cache  file  wb  as  joblib  dump  cache  data  print  Wrote  features  to  cache  file  cache  file  else  Unpack  data  loaded  from  cache  file  features  train  features  test  vocabulary  cache  data  features  train  cache  data  features  test  cache  data  vocabulary  Return  both  the  extracted  features  as  well  as  the  vocabulary  return  features  train  features  test  vocabulary  Extract  Bag  of  Words  features  for  both  training  and  test  datasets  train  test  vocabulary  extract  BoW  features  train  test  import  pandas  as  pd  val  pd  DataFrame  train  10000  train  pd  DataFrame  train  10000  val  pd  DataFrame  train  10000  train  pd  DataFrame  train  10000  test  pd  DataFrame  test  test  pd  DataFrame  test  First  we  make  sure  that  the  local  directory  in  which  we  like  to  store  the  training  and  validation  csv  files  exists  data  dir  data  xgboost  if  not  os  path  exists  data  ,amazon
dir  os  makedirs  data  dir  First  save  the  test  data  to  test  csv  in  the  data  dir  directory  Note  that  we  do  not  save  the  associated  ground  truth  labels  instead  we  will  use  them  later  to  compare  with  our  model  output  pd  concat  test  test  axis  to  csv  os  path  join  data  dir  test  csv  header  False  index  False  pd  concat  val  val  axis  to  csv  os  path  join  data  dir  validation  csv  header  False  index  False  pd  concat  train  train  axis  to  csv  os  path  join  data  dir  train  csv  header  False  index  False  To  save  bit  of  memory  we  can  set  text  train  val  train  and  val  to  None  train  val  train  val  Noneimport  sagemaker  session  sagemaker  Session  Store  the  current  SageMaker  session  S3  prefix  which  folder  will  we  use  prefix  sentiment  xgboost  test  location  session  upload  data  os  path  join  data  dir  test  csv  key  prefix  prefix  val  location  session  upload  data  os  path  join  data  dir  validation,amazon
  csv  key  prefix  prefix  train  location  session  upload  data  os  path  join  data  dir  train  csv  key  prefix  prefix  from  sagemaker  import  get  execution  role  Our  current  execution  role  is  require  when  creating  the  model  as  the  training  and  inference  code  will  need  to  access  the  model  artifacts  role  get  execution  role  We  need  to  retrieve  the  location  of  the  container  which  is  provided  by  Amazon  for  using  XGBoost  As  matter  of  convenience  the  training  and  inference  code  both  use  the  same  container  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  session  boto  region  name  xgboost  TODO  Create  SageMaker  estimator  using  the  container  location  determined  in  the  previous  cell  It  is  recommended  that  you  use  single  training  instance  of  type  ml  m4  xlarge  It  is  also  recommended  that  you  use  s3  output  format  session  default  bucket  prefix  as  the  output  pat,amazon
h  xgb  None  TODO  Set  the  XGBoost  hyperparameters  in  the  xgb  object  Don  forget  that  in  this  case  we  have  binary  label  so  we  should  be  using  the  binary  logistic  objective  First  make  sure  to  import  the  relevant  objects  used  to  construct  the  tuner  from  sagemaker  tuner  import  IntegerParameter  ContinuousParameter  HyperparameterTuner  TODO  Create  the  hyperparameter  tuner  object  xgb  hyperparameter  tuner  None  s3  input  train  sagemaker  s3  input  s3  data  train  location  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  val  location  content  type  csv  xgb  hyperparameter  tuner  fit  train  s3  input  train  validation  s3  input  validation  xgb  hyperparameter  tuner  wait  TODO  Create  new  estimator  object  attached  to  the  best  training  job  found  during  hyperparameter  tuning  xgb  attached  None  TODO  Create  transformer  object  from  the  attached  estimator  Using  an  instance  count  of  and  an  instance  t,amazon
ype  of  ml  m4  xlarge  should  be  more  than  enough  xgb  transformer  None  TODO  Start  the  transform  job  Make  sure  to  specify  the  content  type  and  the  split  type  of  the  test  data  xgb  transformer  wait  aws  s3  cp  recursive  xgb  transformer  output  path  data  dirpredictions  pd  read  csv  os  path  join  data  dir  test  csv  out  header  None  predictions  round  num  for  num  in  predictions  squeeze  values  from  sklearn  metrics  import  accuracy  score  accuracy  score  test  predictions  First  we  will  remove  all  of  the  files  contained  in  the  data  dir  directory  rm  data  dir  And  then  we  delete  the  directory  itself  rmdir  data  dir  Similarly  we  will  remove  the  files  in  the  cache  dir  directory  and  the  directory  itself  rm  cache  dir  rmdir  cache  dir  ,amazon
import  os  import  sagemaker  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  DEMO  abalone  cat  abalone  py  from  sagemaker  tensorflow  import  TensorFlow  abalone  estimator  TensorFlow  entry  point  abalone  py  role  role  training  steps  100  evaluation  steps  100  hyperparameters  learning  rate  001  train  instance  count  train  instance  type  ml  c4  xlarge  abalone  estimator  fit  inputs  abalone  predictor  abalone  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  import  tensorflow  as  tf  import  numpy  as  np  prediction  set  tf  contrib  learn  datasets  base  load  csv  without  header  filename  os  path  join  data  abalone  predict  csv  target  dtype  np  int  features  dtype  np  float32  data  prediction  set  data  tensor  proto  tf  make  tensor  proto  values  np  asarray  data  shape  len  data  dtype  tf  ,amazon
float32  abalone  predictor  predict  tensor  proto  sagemaker  Session  delete  endpoint  abalone  predictor  endpoint  ,amazon
import  os  import  boto3  import  sagemaker  from  sagemaker  mxnet  import  MXNet  from  mxnet  import  gluon  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  gluon  data  vision  MNIST  data  train  train  True  gluon  data  vision  MNIST  data  test  train  False  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  DEMO  mnist  cat  mnist  py  MXNet  mnist  py  role  role  train  instance  count  train  instance  type  ml  c4  xlarge  framework  version  hyperparameters  batch  size  100  epochs  20  learning  rate  momentum  log  interval  100  fit  inputs  predictor  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  IPython  display  import  HTML  HTML  open  input  html  read  response  predictor  predict  data  print  int  response  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
import  numpy  as  np  import  matplotlib  pyplot  as  plt  from  sklearn  cross  validation  import  train  test  split  cross  val  score  from  sklearn  metrics  import  accuracy  score  roc  auc  score  average  precision  score  from  sklearn  preprocessing  import  LabelEncoder  import  seaborn  as  sns  from  sklearn  grid  search  import  RandomizedSearchCV  from  sklearn  ensemble  import  RandomForestClassifier  from  scipy  stats  import  randint  as  sp  randint  from  scikitplot  metrics  import  plot  lift  curve  plot  cumulative  gain  pip  install  scikit  plot  import  warnings  warnings  filterwarnings  ignore  Gr  fica  de  variable  predecir  churn  count  sns  countplot  df  data  Exited  churn  count  set  xlabel  El  cliente  abandon  el  banco  ylabel  mero  de  clientes  title  Distribuci  del  abandono  plt  show  print  Taza  de  abandono  str  100  sum  df  data  Exited  len  df  data  Exited  Se  crean  las  matrices  df  data  drop  Exited  axis  Eliminamos  columnas  df  data  ,ibm
Exited  Definimos  variable  predecir  Se  define  funci  que  crea  diccionarios  para  transformar  los  factores  de  las  variables  categ  ricas  meros  viceversa  def  label  encoder  cols  from  collections  import  namedtuple  encode  decode  for  in  cols  le  LabelEncoder  le  fit  transform  encode  dict  zip  le  classes  le  transform  le  classes  decode  dict  zip  le  transform  le  classes  le  classes  encoder  tuple  namedtuple  encoder  tuple  encode  decode  dictionaries  encoder  tuple  encode  decode  return  dictionaries  categoric  select  dtypes  include  object  columns  Guarda  el  nombre  de  las  columnas  categ  ricas  dictionaries  label  encoder  categoric  Label  encoder  dictionaries  replace  dictionaries  encode  inplace  True  Codificamos  las  variables  categ  ricas  train  test  train  test  train  test  split  test  size  random  state  Dividimos  los  datos  en  entrenamiento  prueba  train  shape  Extraemos  las  dimensiones  de  la  matriz  de  entrenamiento  head ,ibm
 Observamos  mo  quedan  las  primeras  filas  luego  de  la  transformaci  param  dist  max  features  sp  randint  int  np  sqrt  int  min  samples  leaf  sp  randint  20  Se  definen  los  hiperpar  metros  para  sintonizar  el  modelo  clf  RandomForestClassifier  jobs  estimators  COLOCAR  AQU  MERO  DE  RBOLES  Se  inicializa  la  instancia  de  Random  Forest  Corre  squeda  aleatorio  por  iteraciones  iter  search  COLOQUE  AQU  EL  MERO  DE  ITERACIONES  random  search  clf  RandomizedSearchCV  clf  param  distributions  param  dist  iter  iter  search  cv  scoring  accuracy  random  state  10  random  search  clf  fit  train  train  Se  entrena  el  modelo  con  los  mejores  par  metros  encontrados  best  estimator  random  search  clf  best  estimator  Guarda  el  modelo  best  score  random  search  clf  best  score  Guarda  el  puntaje  del  mejor  modelo  print  Best  params  best  estimator  Best  score  best  score  pred  best  estimator  predict  test  Predicciones  binarias  en  el  conju,ibm
nto  de  prueba  pred  proba  best  estimator  predict  proba  test  Predicciones  de  probabilidad  en  el  conjunto  de  prueba  accuracy  accuracy  score  pred  pred  true  test  Exactitud  AUC  roc  auc  score  score  pred  proba  true  test  AUC  average  precision  score  average  precision  score  true  test  score  pred  proba  print  Accuracy  str  np  round  accuracy  100  AUC  str  np  round  AUC  100  average  precision  score  str  np  round  average  precision  score  100  from  repository  mlrepositoryclient  import  MLRepositoryClient  from  repository  mlrepositoryartifact  import  MLRepositoryArtifact  from  repository  mlrepository  import  MetaProps  MetaNameswml  credentials  ml  repository  client  MLRepositoryClient  wml  credentials  url  ml  repository  client  authorize  wml  credentials  username  wml  credentials  password  Check  if  props  is  mandatory  props  MetaProps  MetaNames  AUTHOR  NAME  XXXXXXX  MetaNames  AUTHOR  EMAIL  XXXXXXX  model  artifact  MLRepositoryArtifact  b,ibm
est  estimator  name  XXXXXXX  meta  props  props  saved  model  ml  repository  client  models  save  model  artifact  saved  model  meta  available  props  import  urllib3  requests  json  headers  urllib3  util  make  headers  basic  auth  format  wml  credentials  username  wml  credentials  password  url  v3  identity  token  format  wml  credentials  url  response  requests  get  url  headers  headers  mltoken  json  loads  response  text  get  token  header  Content  Type  application  json  Authorization  Bearer  mltoken  endpoint  instance  wml  credentials  url  v3  wml  instances  wml  credentials  instance  id  header  Content  Type  application  json  Authorization  Bearer  mltoken  response  get  instance  requests  get  endpoint  instance  headers  header  endpoint  published  models  json  loads  response  get  instance  text  get  entity  get  published  models  get  url  print  endpoint  published  models  header  Content  Type  application  json  Authorization  Bearer  mltoken  response  ge,ibm
t  requests  get  endpoint  published  models  headers  header  endpoint  deployments  get  entity  get  deployments  get  url  for  in  json  loads  response  get  text  get  resources  if  get  metadata  get  guid  saved  model  uid  print  endpoint  deployments  payload  online  name  XXXXXXX  description  Churn  predction  using  Random  Forest  type  online  response  online  requests  post  endpoint  deployments  json  payload  online  headers  header  print  response  online  text  scoring  url  json  loads  response  online  text  get  entity  get  scoring  url  print  scoring  url  Credit  score  int  input  Credit  score  Geography  input  Geography  Gender  input  Gender  Age  int  input  Age  Tenure  int  input  Tenure  Balance  int  input  Balance  Num  products  int  input  Number  of  products  HasCrCard  input  Has  credit  card  IsActiveMember  input  Is  active  member  EstimadedSalary  int  input  Estimated  salary  new  observation  np  array  Credit  score  Geography  Gender  Age  Tenure ,ibm
 Balance  Num  products  HasCrCard  IsActiveMember  EstimadedSalary  dtype  object  new  observation  pd  DataFrame  new  observation  columns  columns  new  observation  replace  dictionaries  encode  inplace  True  payload  scoring  values  list  new  observation  values  print  payload  scoring  response  scoring  requests  post  scoring  url  json  payload  scoring  headers  header  response  json  loads  response  scoring  text  probabilidad  abandono  response  values  print  Este  cliente  tiene  una  probabilidad  de  np  round  probabilidad  abandono  de  abandonar  el  banco  def  uplift  true  pred  percentile  expected  response  rate  int  np  round  len  true  percentile  true  true  astype  int  temp  pd  DataFrame  pred  pred  true  true  temp  temp  sort  values  by  pred  ascending  False  iloc  response  rate  sum  temp  true  len  temp  true  lift  response  rate  expected  response  rate  return  lift  percentile  expected  response  rate  uplift  score  make  scorer  uplift  greater  is ,ibm
 better  True  needs  proba  True  percentile  percentile  expected  response  rate  expected  response  rate  uplift  cv  np  mean  cross  val  score  best  estimator  train  train  astype  int  cv  scoring  uplift  score  print  Cross  validation  uplift  uplift  cv  Lift  and  gain  cumulative  gain  charts  pred  best  estimator  predict  proba  test  print  uplift  pred  pred  true  test  percentile  percentile  expected  response  rate  expected  response  rate  plot  cumulative  gain  test  pred  title  Cumulative  Gains  Curve  plt  show  plot  lift  curve  test  pred  title  Lift  curve  plt  show  ,ibm
import  google  datalab  bigquery  as  bq  bq  tables  describe  bigquery  public  data  san  francisco  bikeshare  trips  bq  query  cyclesharing  standardSQL  SELECT  EXTRACT  YEAR  FROM  start  date  AS  year  ROUND  AVG  duration  sec  60  AS  avg  duration  min  COUNT  AS  yearly  trips  FROM  bigquery  public  data  san  francisco  bikeshare  trips  GROUP  BY  YEAR  ORDER  BY  YEAR  DESC  bq  execute  cyclesharing  chart  columns  data  cyclesharing  fields  year  yearly  trips  bq  tables  describe  ghtorrent  bq  ght  2017  01  19  commits  bq  query  github  standardSQL  WITH  commits  AS  SELECT  author  email  EXTRACT  DAYOFWEEK  FROM  author  date  BETWEEN  AND  is  weekday  LOWER  REGEXP  EXTRACT  diff  new  path  lang  diff  new  path  AS  path  author  date  FROM  bigquery  public  data  github  repos  commits  UNNEST  difference  diff  WHERE  EXTRACT  YEAR  FROM  author  date  2016  SELECT  lang  is  weekday  COUNT  path  AS  numcommits  FROM  commits  WHERE  LENGTH  lang  AND  lang  IS  NOT  ,google
NULL  AND  REGEXP  CONTAINS  lang  zA  GROUP  BY  lang  is  weekday  HAVING  numcommits  100  ORDER  BY  numcommits  DESC  Limit  10  bq  execute  github  chart  columns  data  github  fields  lang  numcommits  Testing  out  notebook  with  two  examples  Notebook  example  one  Looking  at  bikesharing  data  ,google
import  boto3  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  mxnet  as  mx  data  mx  test  utils  get  mnist  from  mnist  import  train  model  train  data  data  num  cpus  num  gpus  import  os  import  json  os  mkdir  model  model  save  checkpoint  model  model  0000  with  open  model  model  shapes  json  as  shapes  json  dump  shape  model  data  shapes  name  data  shapes  import  tarfile  def  flatten  tarinfo  tarinfo  name  os  path  basename  tarinfo  name  return  tarinfo  tar  tarfile  open  model  tar  gz  gz  tar  add  model  filter  flatten  tar  close  import  sagemaker  sagemaker  session  sagemaker  Session  inputs  sagemaker  session  upload  data  bucket  SageMakerS3Bucket  path  model  tar  gz  key  prefix  model  from  sagemaker  mxnet  model  import  MXNetModel  sagemaker  model  MXNetModel  model  data  s3  SageMakerS3Bucket  model  model  tar  gz  role  role  entry  point  mnist  py  import  logging  logging  getLogger  setLevel  logging ,amazon
 WARNING  predictor  sagemaker  model  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  IPython  display  import  HTML  HTML  open  input  html  read  response  predictor  predict  data  print  Raw  prediction  result  print  response  labeled  predictions  list  zip  range  10  response  print  Labeled  predictions  print  labeled  predictions  labeled  predictions  sort  key  lambda  label  and  prob  label  and  prob  print  Most  likely  answer  format  labeled  predictions  print  predictor  endpoint  sagemaker  Session  delete  endpoint  predictor  endpoint  os  remove  model  tar  gz  import  shutil  shutil  rmtree  model  ,amazon
bin  bash  setup  shimport  os  import  subprocess  import  sagemaker  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  instance  type  local  if  subprocess  call  nvidia  smi  Set  type  to  GPU  if  one  is  present  instance  type  local  gpu  print  Instance  type  instance  type  role  get  execution  role  import  utils  from  tensorflow  contrib  learn  python  learn  datasets  import  mnist  import  tensorflow  as  tf  data  sets  mnist  read  data  sets  data  dtype  tf  uint8  reshape  False  validation  size  5000  utils  convert  to  data  sets  train  train  data  utils  convert  to  data  sets  validation  validation  data  utils  convert  to  data  sets  test  test  data  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  mnist  cat  mnist  py  from  sagemaker  tensorflow  import  TensorFlow  mnist  estimator  TensorFlow  entry  point  mnist  py  role  role  training  steps  10  evaluation  steps  10  train  instance  count  train  in,amazon
stance  type  instance  type  mnist  estimator  fit  inputs  mnist  predictor  mnist  estimator  deploy  initial  instance  count  instance  type  instance  type  import  numpy  as  np  from  tensorflow  examples  tutorials  mnist  import  input  data  mnist  input  data  read  data  sets  tmp  data  one  hot  True  for  in  range  10  data  mnist  test  images  tolist  tensor  proto  tf  make  tensor  proto  values  np  asarray  data  shape  len  data  dtype  tf  float32  predict  response  mnist  predictor  predict  tensor  proto  print  label  np  argmax  mnist  test  labels  print  label  is  format  label  prediction  predict  response  outputs  classes  int64Val  print  prediction  is  format  prediction  mnist  estimator  delete  endpoint  ,amazon
import  mxnet  as  mx  import  boto3  define  training  batch  size  batch  size  100mnist  mx  test  utils  get  mnist  ntrain  int  mnist  train  data  shape  train  mnist  train  data  ntrain  train  mnist  train  label  ntrain  test  mnist  train  data  ntrain  test  mnist  train  label  ntrain  data  mx  sym  var  data  first  conv  layer  conv1  mx  sym  Convolution  data  data  kernel  num  filter  20  tanh1  mx  sym  Activation  data  conv1  act  type  tanh  pool1  mx  sym  Pooling  data  tanh1  pool  type  max  kernel  stride  second  conv  layer  conv2  mx  sym  Convolution  data  pool1  kernel  num  filter  50  tanh2  mx  sym  Activation  data  conv2  act  type  tanh  pool2  mx  sym  Pooling  data  tanh2  pool  type  max  kernel  stride  first  fullc  layer  flatten  mx  sym  flatten  data  pool2  fc1  mx  symbol  FullyConnected  data  flatten  num  hidden  500  tanh3  mx  sym  Activation  data  fc1  act  type  tanh  second  fullc  fc2  mx  sym  FullyConnected  data  tanh3  num  hidden  10  softmax,amazon
  loss  lenet  mx  sym  SoftmaxOutput  data  fc2  name  softmax  create  iterator  around  training  and  validation  data  train  iter  mx  io  NDArrayIter  mnist  train  data  ntrain  mnist  train  label  ntrain  batch  size  shuffle  True  val  iter  mx  io  NDArrayIter  mnist  train  data  ntrain  mnist  train  label  ntrain  batch  size  create  trainable  module  on  GPU  lenet  model  mx  mod  Module  symbol  lenet  context  mx  gpu  train  with  the  same  lenet  model  fit  train  iter  eval  data  val  iter  optimizer  sgd  optimizer  params  learning  rate  eval  metric  acc  batch  end  callback  mx  callback  Speedometer  batch  size  100  num  epoch  10  predict  accuracy  for  lenet  acc  mx  metric  Accuracy  lenet  model  score  val  iter  acc  print  acc  lenet  save  mnist  symbol  mxnet  lenet  model  save  params  mnist  module  mxnet  s3  boto3  client  s3  s3  upload  file  mnist  symbol  mxnet  jakechenawstemp  mnist  symbol  mxnet  s3  upload  file  mnist  module  mxnet  jakechenawste,amazon
mp  mnist  module  mxnet  test  mnist  test  data  test  mnist  test  label  test  tofile  test  csv  test  tofile  test  csv  s3  upload  file  test  csv  jakechenawstemp  test  csv  s3  upload  file  test  csv  jakechenawstemp  test  csv  ,amazon
import  sagemaker  from  sagemaker  import  get  execution  role  import  boto3  import  json  sess  sagemaker  Session  role  get  execution  role  print  role  This  is  the  role  that  SageMaker  would  use  to  leverage  AWS  resources  S3  CloudWatch  on  your  behalf  bucket  sess  default  bucket  Replace  with  your  own  bucket  name  if  needed  print  bucket  prefix  SagemakerBlazingText  Replace  with  the  prefix  under  which  you  want  to  store  the  data  if  neededs3  train  data  s3  format  sagemaker  test  ninja  BlazingTextInput  s3  output  location  s3  BlazingText  Model  Output  word2vec  pitchfork  2018  09  19  format  sagemaker  test  ninja  region  name  boto3  Session  region  namecontainer  sagemaker  amazon  amazon  estimator  get  image  uri  region  name  blazingtext  latest  print  Using  SageMaker  BlazingText  container  format  container  region  name  bt  model  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  c4  2x,amazon
large  train  volume  size  train  max  run  360000  input  mode  File  output  path  s3  output  location  sagemaker  session  sess  bt  model  set  hyperparameters  mode  batch  skipgram  epochs  min  count  sampling  threshold  0001  learning  rate  05  window  size  vector  dim  100  negative  samples  batch  size  11  window  size  Preferred  Used  only  if  mode  is  batch  skipgram  evaluation  True  Perform  similarity  evaluation  on  WS  353  dataset  at  the  end  of  training  subwords  False  Subword  embedding  learning  is  not  supported  by  batch  skipgramtrain  data  sagemaker  session  s3  input  s3  train  data  distribution  FullyReplicated  content  type  text  plain  s3  data  type  S3Prefix  data  channels  train  train  data  bt  model  fit  inputs  data  channels  logs  True  bt  endpoint  bt  model  deploy  initial  instance  count  instance  type  ml  m4  xlarge  words  addictive  recorded  payload  instances  words  response  bt  endpoint  predict  json  dumps  payload  vecs  jso,amazon
n  loads  response  print  vecs  s3  boto3  resource  s3  key  bt  model  model  data  bt  model  model  data  find  s3  Bucket  sagemaker  test  ninja  download  file  key  model  tar  gz  tar  xvzf  model  tar  gz  cat  eval  jsonimport  numpy  as  np  from  sklearn  preprocessing  import  normalize  Read  the  400  most  frequent  word  vectors  The  vectors  in  the  file  are  in  descending  order  of  frequency  num  points  400  first  line  True  index  to  word  with  open  vectors  txt  as  for  line  num  line  in  enumerate  if  first  line  dim  int  line  strip  split  word  vecs  np  zeros  num  points  dim  dtype  float  first  line  False  continue  line  line  strip  word  line  split  vec  word  vecs  line  num  for  index  vec  val  in  enumerate  line  split  vec  index  float  vec  val  index  to  word  append  word  if  line  num  num  points  break  word  vecs  normalize  word  vecs  copy  False  return  norm  False  from  sklearn  manifold  import  TSNE  tsne  TSNE  perplexity  40  c,amazon
omponents  init  pca  iter  10000  two  embeddings  tsne  fit  transform  word  vecs  num  points  labels  index  to  word  num  points  from  matplotlib  import  pylab  matplotlib  inline  def  plot  embeddings  labels  pylab  figure  figsize  20  20  for  label  in  enumerate  labels  embeddings  pylab  scatter  pylab  annotate  label  xy  xytext  textcoords  offset  points  ha  right  va  bottom  pylab  show  plot  two  embeddings  labels  sess  delete  endpoint  bt  endpoint  endpoint  ,amazon
import  pickle  import  pandas  as  pd  import  numpy  as  np  load  data  processed  from  related  notebooks  data  from  CSV  downloaded  from  Azure  portal  df  daily  usage  csv  pickle  load  open  df  daily  usage  rb  data  from  Azure  billing  API  Azure  df  invoice  daily  usage  pickle  load  open  df  invoice  daily  usage  rb  df  daily  usage  csv  columnsdf  invoice  daily  usage  columns  compare  the  two  datasets  joining  on  resource  and  usage  date  result  pd  merge  df  daily  usage  csv  df  invoice  daily  usage  left  on  Usage  Date  Meter  Id  right  on  usageStartTime  meterId  how  outer  indicator  True  len  result  result  dtypes  add  percent  differnce  column  to  compare  the  usage  result  pct  diff  result  apply  lambda  row  row  Consumed  Quantity  row  quantity  row  quantity  100  axis  The  resouces  and  dates  match  if  all  rows  merge  is  both  merge  would  be  right  only  or  left  only  if  given  resouce  date  tuple  is  only  in  the  CSV  or  o,microsoft
nly  in  the  API  result  Usage  Date  Meter  Id  usageStartTime  meterId  Consumed  Quantity  quantity  pct  diff  merge  show  results  with  percent  difference  between  CSV  source  and  API  source  df  diffs  result  loc  abs  result  pct  diff  0000001  Usage  Date  Meter  Id  meterCategory  meterRegion  meterName  meterSubCategory  Consumed  Quantity  quantity  pct  diff  df  diffs  columns  Usage  Date  Meter  Id  Meter  Category  Meter  Region  Meter  Name  Meter  Sub  Category  CSV  Quantity  API  Quantity  Percent  Difference  df  diffsdf  diffs  to  excel  diffs  xlsx  one  res  result  loc  result  meterId  65d4ded2  41ae  43a8  bb68  3c200e1ba864  Usage  Date  Meter  Id  usageStartTime  meterId  Consumed  Quantity  quantity  pct  diff  one  res  ,microsoft
import  boto3  import  pandas  as  pd  import  numpy  as  np  import  io  from  sklearn  metrics  import  accuracy  scoreruntime  boto3  Session  client  runtime  sagemaker  def  get  payload  df  csv  io  StringIO  df  to  csv  csv  index  None  header  None  return  csv  getvalue  rstrip  endpoint  reference  see  the  HowToDeploy  notebook  cntkdemo  endpoint  cntkdemo  poc  endpoint  201801240604  payload  get  payload  pd  read  csv  eval  data  csv  header  None  time  response  runtime  invoke  endpoint  EndpointName  cntkdemo  endpoint  ContentType  text  csv  Body  payload  result  response  Body  read  decode  pred  pd  read  csv  io  StringIO  result  sep  header  None  predy  true  pd  read  csv  eval  pred  csv  header  None  accuracy  score  requires  inputs  to  be  arrays  yy  true  np  argmax  for  in  true  values  yy  pred  np  argmax  for  in  pred  values  print  yy  true  print  yy  pred  overfitting  baby  print  accuracy  score  yy  true  yy  pred  import  sagemaker  as  sage  from  ti,amazon
me  import  gmtime  strftime  sess  sage  Session  sess  delete  endpoint  cntkdemo  endpoint  ,amazon
import  os  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  your  s3  bucket  name  here  enter  your  s3  bucket  where  you  will  copy  data  and  model  artifacts  prefix  sagemaker  DEMO  breast  cancer  prediction  place  to  upload  training  files  within  the  bucketimport  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  import  io  import  time  import  json  import  sagemaker  amazon  common  as  smacdata  pd  read  csv  https  archive  ics  uci  edu  ml  machine  learning  databases  breast  cancer  wisconsin  wdbc  data  header  None  specify  columns  extracted  from  wbdc  names  data  columns  id  diagnosis  radius  mean  texture  mean  perimeter  mean  area  mean  smoothness  mean  compactness  mean  concavity  mean  concave  points  mean  symmetry  mean  fractal  dimension  mean  radius  se  texture  se  perimeter  se  area  se  smoothness  se  compactness  se  concavity  se  concave  points  se  symmet,amazon
ry  se  fractal  dimension  se  radius  worst  texture  worst  perimeter  worst  area  worst  smoothness  worst  compactness  worst  concavity  worst  concave  points  worst  symmetry  worst  fractal  dimension  worst  save  the  data  data  to  csv  data  csv  sep  index  False  print  the  shape  of  the  data  file  print  data  shape  show  the  top  few  rows  display  data  head  describe  the  data  object  display  data  describe  we  will  also  summarize  the  categorical  field  diganosis  display  data  diagnosis  value  counts  rand  split  np  random  rand  len  data  train  list  rand  split  val  list  rand  split  rand  split  test  list  rand  split  data  train  data  train  list  data  val  data  val  list  data  test  data  test  list  train  data  train  iloc  as  matrix  train  data  train  iloc  as  matrix  val  data  val  iloc  as  matrix  val  data  val  iloc  as  matrix  test  data  test  iloc  as  matrix  test  data  test  iloc  as  matrix  train  file  linear  train  data  io  Byt,amazon
esIO  smac  write  numpy  to  dense  tensor  train  astype  float32  train  astype  float32  seek  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  train  file  upload  fileobj  validation  file  linear  validation  data  io  BytesIO  smac  write  numpy  to  dense  tensor  val  astype  float32  val  astype  float32  seek  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  train  file  upload  fileobj  See  Algorithms  Provided  by  Amazon  SageMaker  Common  Parameters  in  the  SageMaker  documentation  for  an  explanation  of  these  values  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  linear  learner  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  linear  learner  latest  linear  job  DEMO  linear  time  strftime  tim,amazon
e  gmtime  print  Job  name  is  linear  job  linear  training  params  RoleArn  role  TrainingJobName  linear  job  AlgorithmSpecification  TrainingImage  containers  boto3  Session  region  name  TrainingInputMode  File  ResourceConfig  InstanceCount  InstanceType  ml  c4  2xlarge  VolumeSizeInGB  10  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  S3DataDistributionType  ShardedByS3Key  CompressionType  None  RecordWrapperType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  prefix  S3DataDistributionType  FullyReplicated  CompressionType  None  RecordWrapperType  None  OutputDataConfig  S3OutputPath  s3  format  bucket  prefix  HyperParameters  feature  dim  30  mini  batch  size  100  predictor  type  regressor  epochs  10  num  models  32  loss  absolute  loss  StoppingCondition  MaxRuntimeInSeconds  60  60  time  region  boto3  Session  region  name  sm  b,amazon
oto3  client  sagemaker  sm  create  training  job  linear  training  params  status  sm  describe  training  job  TrainingJobName  linear  job  TrainingJobStatus  print  status  sm  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  linear  job  if  status  Failed  message  sm  describe  training  job  TrainingJobName  linear  job  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  linear  hosting  container  Image  containers  boto3  Session  region  name  ModelDataUrl  sm  describe  training  job  TrainingJobName  linear  job  ModelArtifacts  S3ModelArtifacts  create  model  response  sm  create  model  ModelName  linear  job  ExecutionRoleArn  role  PrimaryContainer  linear  hosting  container  print  create  model  response  ModelArn  linear  endpoint  config  DEMO  linear  endpoint  config  time  strftime  time  gmtime  print  linear  endpoint  config  create  endpoint  config  response  sm  create  endpo,amazon
int  config  EndpointConfigName  linear  endpoint  config  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  linear  job  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  linear  endpoint  DEMO  linear  endpoint  time  strftime  time  gmtime  print  linear  endpoint  create  endpoint  response  sm  create  endpoint  EndpointName  linear  endpoint  EndpointConfigName  linear  endpoint  config  print  create  endpoint  response  EndpointArn  resp  sm  describe  endpoint  EndpointName  linear  endpoint  status  resp  EndpointStatus  print  Status  status  sm  get  waiter  endpoint  in  service  wait  EndpointName  linear  endpoint  resp  sm  describe  endpoint  EndpointName  linear  endpoint  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Status  status  if  status  InService  raise  Exception  Endpoint  creation  did  not  succeed  def  np2csv  arr  csv  io  BytesIO  np  savetxt  csv  arr  de,amazon
limiter  fmt  return  csv  getvalue  decode  rstrip  runtime  boto3  client  runtime  sagemaker  payload  np2csv  test  response  runtime  invoke  endpoint  EndpointName  linear  endpoint  ContentType  text  csv  Body  payload  result  json  loads  response  Body  read  decode  test  pred  np  array  score  for  in  result  predictions  test  mae  linear  np  mean  np  abs  test  test  pred  test  mae  baseline  np  mean  np  abs  test  np  median  train  training  median  as  baseline  predictor  print  Test  MAE  Baseline  round  test  mae  baseline  print  Test  MAE  Linear  round  test  mae  linear  test  pred  class  test  pred  test  pred  baseline  np  repeat  np  median  train  len  test  prediction  accuracy  np  mean  test  test  pred  class  100  baseline  accuracy  np  mean  test  test  pred  baseline  100  print  Prediction  Accuracy  round  prediction  accuracy  print  Baseline  Accuracy  round  baseline  accuracy  sm  delete  endpoint  EndpointName  linear  endpoint  ,amazon
Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  sagemaker  periscopedata  demo  nyc  data  key  enhancedtotodataset  csv  data  location  s3  format  bucket  data  key  set  prefix  for  this  instance  please  input  your  name  in  the  following  set  of  square  brackets  making  sure  to  use  appropriate  directory  characters  prefix  sagemaker  your  name  here  xgboost  dm  import  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  from  io  import  BytesIO  import  os  import  time  import  json  import  sagemaker  amazon  common  as  smac  import  sagemaker  from  sagemaker  predictor  import  csv  serializer  json  deserializer  read  the  csv  from  S3  df  pd  read  csv  data  location  display  the  first  records  to  verify  the  import  df  head  some  of  the  categorical  variables  are  currently  encoded  as  numeric  The  number  of  categories  is  low  and  can  easily  be  one  h,amazon
ot  encoded  using  get  dummy  categorical  columns  max  dog  size  min  dog  size  requester  gender  provider  gender  experience  continuous  walk  count  dog  count  requester  fee  previous  client  count  price  per  walk  provider  fee  percent  morning  walks  percent  afternoon  walks  percent  evening  walks  df  pd  get  dummies  df  columns  max  dog  size  min  dog  size  requester  gender  provider  gender  experience  verify  that  the  one  hot  encoding  creation  of  boolean  for  each  categorical  variable  succeeded  df  head  train  data  validation  data  test  data  np  split  df  sample  frac  random  state  1729  int  len  df  int  len  df  pd  concat  train  data  lifetime  revenue  train  data  drop  lifetime  revenue  axis  axis  to  csv  train  csv  index  False  header  False  pd  concat  validation  data  lifetime  revenue  validation  data  drop  lifetime  revenue  axis  axis  to  csv  validation  csv  index  False  header  False  boto3  Session  resource  s3  Bucket  bucket,amazon
  Object  os  path  join  prefix  train  train  csv  upload  file  train  csv  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  validation  csv  upload  file  validation  csv  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  xgboost  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  xgboost  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  xgboost  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  xgboost  latest  ap  northeast  501404015308  dkr  ecr  ap  northeast  amazonaws  com  xgboost  latest  ap  northeast  306986355934  dkr  ecr  ap  northeast  amazonaws  com  xgboost  latest  s3  input  train  sagemaker  s3  input  s3  data  s3  train  format  bucket  prefix  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  s3  validation  format  bucket  prefix  content  type  csv  sess  sagemaker  Session  xgb  sagemaker  estimator  Estimator  containers  boto3  Session  r,amazon
egion  name  role  train  instance  count  train  instance  type  ml  m4  xlarge  output  path  s3  output  format  bucket  prefix  sagemaker  session  sess  xgb  set  hyperparameters  max  depth  eta  gamma  min  child  weight  subsample  silent  objective  reg  linear  use  linear  regression  to  create  continuous  output  num  round  100  xgb  fit  train  s3  input  train  validation  s3  input  validation  xgb  predictor  xgb  deploy  initial  instance  count  instance  type  ml  m4  xlarge  xgb  predictor  content  type  text  csv  xgb  predictor  serializer  csv  serializerdef  predict  data  rows  500  split  array  np  array  split  data  int  data  shape  float  rows  predictions  for  array  in  split  array  predictions  join  predictions  xgb  predictor  predict  array  decode  utf  return  np  fromstring  predictions  sep  predictions  predict  test  data  drop  lifetime  revenue  axis  values  check  the  top  predictions  to  make  sure  they  are  reasonable  print  predictions  export  test,amazon
  predictions  test  data  with  pred  test  data  test  data  with  pred  insert  predictions  predictions  test  data  with  pred  head  test  data  with  pred  to  csv  toto  test  predictions  csv  index  False  header  True  export  train  data  with  predictiona  train  predictions  predict  train  data  drop  lifetime  revenue  axis  values  train  data  with  pred  train  data  train  data  with  pred  insert  predictions  train  predictions  train  data  with  pred  head  train  data  with  pred  to  csv  toto  train  predictions  csv  index  False  header  True  def  rmse  predictions  actuals  rmse  predictions  actuals  mean  return  rmse  rmse  predictions  np  round  predictions  actuals  test  data  lifetime  revenue  import  seaborn  as  sns  import  matplotlib  pyplot  as  plt  import  numpy  as  np  from  scipy  import  stats  plt  figure  figsize  plt  gca  set  aspect  equal  adjustable  box  max  lim  max  int  np  max  np  round  predictions  int  np  max  test  data  lifetime  revenue  ,amazon
np  linspace  max  lim  10  plt  plot  linewidth  linestyle  alpha  label  Actual  regression  part  sns  regplot  np  round  predictions  test  data  lifetime  revenue  color  purple  label  Prediction  plt  xlabel  Predictions  plt  ylabel  Actual  plt  title  Predictive  vs  Actual  Lifetime  Revenue  plt  legend  plt  show  import  seaborn  as  sns  plt  figure  figsize  sns  residplot  np  round  predictions  test  data  lifetime  revenue  color  purple  plt  xlabel  LTV  plt  ylabel  Residual  plt  title  Residual  Plot  sagemaker  Session  delete  endpoint  xgb  predictor  endpoint  ,amazon
from  collections  import  defaultdict  from  sklearn  preprocessing  import  LabelEncoder  import  matplotlib  pyplot  as  plt  import  pandas  as  pd  import  numpy  as  np  import  sagemaker  import  datetime  import  tempfile  import  random  import  boto3  import  copy  import  uuid  import  json  import  os  matplotlib  inline  def  make  product  categories  num  years  Creates  random  product  with  unique  product  id  from  the  list  of  potential  category  and  subcategory  pairings  Args  categories  dict  Dictionary  containing  mappings  of  categories  to  lists  of  subcategories  num  years  int  Range  of  years  from  today  date  to  pick  start  date  for  time  series  data  from  Returns  dict  Dictionary  containing  product  information  for  the  generated  product  product  id  str  uuid  uuid4  fields  cat  subcats  random  choice  list  categories  items  subcat  random  choice  subcats  start  date  datetime  date  today  datetime  timedelta  random  randint  num  years  num  ,amazon
years  365  return  product  id  product  id  category  cat  subcategory  subcat  start  date  start  date  def  make  weights  categories  num  years  Creates  normalized  weights  in  the  interval  for  each  category  for  each  month  of  each  year  in  the  input  range  of  years  Args  categories  dict  Dictionary  containing  mappings  of  categories  to  lists  of  subcategories  num  years  int  Range  of  years  from  today  date  to  pick  start  date  for  time  series  data  from  Returns  dict  Dictionary  containing  weights  for  each  category  for  each  month  of  each  year  in  the  input  range  of  years  end  year  datetime  date  today  year  results  defaultdict  lambda  defaultdict  dict  for  year  in  range  end  year  num  years  end  year  for  month  in  range  13  rands  np  random  random  size  len  categories  weights  rands  rands  sum  weight  index  for  category  in  categories  results  category  year  month  weights  weight  index  weight  index  return  results  d,amazon
ef  make  category  weights  categories  num  years  Creates  random  weights  for  each  category  and  subcategory  to  further  augment  seasonality  within  these  groupings  Weights  are  randomly  created  for  every  category  and  subcategory  for  each  year  and  month  At  the  category  and  subcategory  levels  the  weights  are  normalized  to  sum  to  Args  categories  dict  Dictionary  containing  mappings  of  categories  to  lists  of  subcategories  num  years  int  Range  of  years  from  today  date  to  pick  start  date  for  time  series  data  from  Returns  dict  Dictionary  containing  weights  for  each  category  for  each  month  of  each  year  in  the  input  range  of  years  category  weights  make  weights  list  categories  keys  num  years  Create  weights  for  the  top  level  categories  for  category  subcategories  in  categories  items  Add  weights  for  the  corresponding  subcategories  subcat  weights  make  weights  subcategories  num  years  for  subcat  weigh,amazon
ts  in  subcat  weights  items  category  weights  category  subcat  weights  return  category  weights  def  make  time  point  value  date  cat  subcat  seasonality  weights  Creates  daily  time  point  sales  value  for  product  based  on  baseline  seasonality  and  product  weights  Args  date  datetime  Date  to  generate  time  point  value  for  cat  string  Category  of  product  subcat  string  Subcategory  of  product  seasonality  dict  Dictionary  containing  details  about  seasonality  of  sales  for  products  weights  dict  Dictionary  containing  weights  for  each  category  and  subcategory  for  each  month  of  each  year  Returns  int  Time  point  sales  value  for  given  product  on  given  day  month  date  month  year  date  year  baseline  seasonality  month  noise  np  random  normal  scale  cat  weight  weights  cat  year  month  subcat  weight  weights  cat  subcat  year  month  value  baseline  noise  baseline  cat  weight  subcat  weight  return  int  value  def  make  data,amazon
  for  product  product  seasonality  weights  Creates  DataFrame  containing  time  point  sales  values  for  input  product  based  on  baseline  seasonality  and  product  weights  Args  product  dict  Dictionary  containing  product  information  for  single  product  seasonality  dict  Dictionary  containing  details  about  seasonality  of  sales  for  products  weights  dict  Dictionary  containing  weights  for  each  category  and  subcategory  for  each  month  of  each  year  Returns  DataFrame  DataFrame  containing  time  point  sales  values  for  given  product  cat  product  category  subcat  product  subcategory  today  datetime  date  today  start  date  product  start  date  delta  today  start  date  data  for  in  range  delta  days  local  product  product  copy  local  product  pop  start  date  None  date  start  date  datetime  timedelta  days  local  product  date  date  local  product  sales  make  time  point  value  date  cat  subcat  seasonality  weights  data  append  local  pr,amazon
oduct  return  pd  DataFrame  data  def  make  data  for  products  products  seasonality  weights  Creates  DataFrame  containing  time  point  sales  values  for  all  input  products  based  on  baseline  return  seasonality  and  product  weights  Args  products  list  List  of  dictionaries  containing  product  information  for  all  products  seasonality  dict  Dictionary  containing  details  about  seasonality  of  sales  for  products  weights  dict  Dictionary  containing  weights  for  each  category  and  subcategory  for  each  month  of  each  year  Returns  DataFrame  DataFrame  containing  time  series  sales  values  for  given  product  df  pd  concat  make  data  for  product  product  seasonality  weights  for  product  in  products  df  df  product  id  date  category  subcategory  sales  df  product  id  df  product  id  apply  str  df  date  df  date  apply  lambda  pd  to  datetime  format  return  df  def  plot  dataset  ts  df  condition  Plots  summed  time  point  sales  values  o,amazon
f  data  at  monthly  granularity  grouped  by  condition  Args  df  DataFrame  DataFrame  containing  time  point  sales  data  for  products  condition  str  Condition  to  group  DataFrame  by  category  or  subcategory  Returns  None  data  df  copy  data  data  groupby  date  condition  sum  unstack  fillna  data  data  groupby  data  index  year  data  index  month  sum  fig  ax  plt  subplots  figsize  15  15  data  plot  ax  ax  def  ts  to  json  obj  series  cat  Converts  input  time  point  DataFrame  into  JSON  object  in  DeepAR  format  Args  series  DataFrame  DataFrame  containing  time  point  sales  data  for  product  cat  str  Label  encoded  category  for  product  Returns  dict  Dictionary  containing  time  point  sales  data  converted  to  DeepAR  format  ts  series  copy  Make  local  copy  of  series  so  as  not  to  modify  original  DataFrame  ts  ts  rename  sales  Rename  series  ts  ts  reset  index  Input  series  is  indexed  by  date  re  index  to  make  df  with  date  ,amazon
as  column  start  date  index  ts  sales  nonzero  Pull  out  index  of  first  non  zero  day  of  return  values  ts  ts  iloc  start  date  index  reset  index  drop  True  Truncate  leading  return  rows  from  time  series  and  reset  index  to  json  obj  start  str  ts  date  cat  int  cat  target  ts  sales  astype  int  tolist  return  json  obj  def  transform  to  json  objs  df  le  Converts  input  DataFrame  of  all  time  point  sales  values  for  all  products  into  list  of  JSON  objects  in  DeepAR  format  for  each  product  Args  df  DataFrame  DataFrame  containing  time  point  sales  data  for  all  products  le  LabelEncoder  SciKit  LabelEncoder  fit  on  product  categories  and  subcategories  to  label  encode  these  values  with  Returns  dict  Dictionary  containing  time  point  sales  data  converted  to  DeepAR  format  for  all  products  cats  df  df  product  id  category  subcategory  drop  duplicates  set  index  product  id  ts  df  df  groupby  date  product  id ,amazon
 sum  unstack  fillna  ts  df  columns  ts  df  columns  droplevel  Drop  unneeded  multi  index  level  json  objs  for  column  in  list  ts  df  columns  values  cat  cats  df  loc  column  category  subcat  cats  df  loc  column  subcategory  num  cat  le  transform  cat  num  subcat  le  transform  subcat  json  objs  append  ts  to  json  obj  ts  df  loc  column  num  cat  json  objs  append  ts  to  json  obj  ts  df  loc  column  num  subcat  return  json  objs  def  make  train  set  json  objs  prediction  length  Creates  training  set  from  an  input  dataset  by  removing  prediction  length  number  of  time  points  from  each  time  series  Args  json  objs  list  List  of  JSON  objects  in  DeepAR  format  prediction  length  int  Number  of  time  points  to  remove  from  each  time  series  Returns  list  List  of  JSON  objects  in  DeepAR  format  where  each  object  has  had  prediction  length  number  of  time  points  removed  objs  copy  deepcopy  json  objs  for  obj  in  objs ,amazon
 obj  target  obj  target  prediction  length  return  objs  def  write  json  to  file  json  objs  channel  Writes  list  of  JSON  objects  to  file  in  JSON  Lines  format  encoded  as  utf  Output  file  is  named  after  channel  Args  json  objs  list  List  of  JSON  objects  in  DeepAR  format  channel  str  Channel  that  JSON  Lines  data  will  be  used  for  train  or  test  Returns  str  Path  of  file  data  was  written  to  file  name  json  format  channel  file  path  os  path  join  os  getcwd  file  name  with  open  file  path  wb  as  for  obj  in  json  objs  line  json  dumps  obj  line  line  encode  utf  write  line  seek  flush  return  file  path  Categories  and  subcategories  to  generate  products  with  Each  product  id  is  generated  with  random  category  and  subcategory  from  that  category  options  categories  shoe  sneaker  boot  slipper  outerwear  coat  jacket  shell  top  shirt  shirt  sweater  knit  bottom  skirt  pant  short  leggings  accessories  belt  tie ,amazon
 scarf  hat  brooche  Seasonality  for  time  series  data  to  be  generated  around  seasonality  500  400  200  100  40  80  150  180  140  10  240  11  100  12  400  products  100  Number  of  products  to  generate  Increase  this  to  generate  more  individual  product  data  years  Number  of  years  in  past  from  current  date  to  generate  date  for  Increase  this  to  generate  longer  time  series  for  each  product  products  make  product  categories  years  for  in  range  products  category  weights  make  category  weights  categories  years  product  data  make  data  for  products  products  seasonality  category  weights  print  product  data  head  Visualize  monthly  sales  of  products  at  category  level  plot  dataset  ts  product  data  category  Visualize  monthly  sales  of  products  at  subcategory  level  plot  dataset  ts  product  data  subcategory  uniq  cats  product  data  category  unique  tolist  uniq  subcats  product  data  subcategory  unique  tolist  le  LabelEn,amazon
coder  fit  uniq  cats  uniq  subcats  print  le  classes  epochs  50  freq  prediction  length  28  context  length  28  cardinality  str  len  le  classes  embedding  dimension  hyperparameters  epochs  epochs  time  freq  freq  context  length  context  length  prediction  length  prediction  length  cardinality  cardinality  embedding  dimension  embedding  dimension  json  objs  transform  to  json  objs  product  data  le  train  set  make  train  set  json  objs  int  prediction  length  test  set  json  objs  resource  prefix  deepar  retail  forecast  sagemaker  session  sagemaker  Session  train  file  write  json  to  file  train  set  train  test  file  write  json  to  file  test  set  test  train  location  sagemaker  session  upload  data  train  file  key  prefix  resource  prefix  test  location  sagemaker  session  upload  data  test  file  key  prefix  resource  prefix  role  sagemaker  get  execution  role  region  ap  northeast  image  name  sagemaker  amazon  amazon  estimator  get  imag,amazon
e  uri  region  forecasting  deepar  latest  bucket  sagemaker  session  default  bucket  s3  output  path  output  format  bucket  resource  prefix  estimator  sagemaker  estimator  Estimator  sagemaker  session  sagemaker  session  image  name  image  name  role  role  train  instance  count  train  instance  type  ml  c5  2xlarge  base  job  name  resource  prefix  output  path  s3  s3  output  path  estimator  set  hyperparameters  hyperparameters  data  channels  train  train  location  test  test  location  estimator  fit  inputs  data  channels  import  cv2predictor  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  endpoint  name  resource  prefix  content  type  application  json  Prediction  for  item  with  existing  time  series  sales  history  start  date  datetime  date  today  datetime  timedelta  days  int  prediction  length  28  target  test  set  target  int  prediction  length  28  28  cat  test  set  cat  request  json  instances  start  str  start  date  targe,amazon
t  target  cat  cat  configuration  num  samples  10  output  types  samples  quantiles  payload  json  dumps  request  json  encode  utf  response  predictor  predict  payload  print  test  set  target  int  prediction  length  result  json  loads  response  print  result  predictions  samples  import  numpy  as  np  import  matplotlib  pyplot  as  plt  plt  plot  test  set  target  int  prediction  length  plt  plot  result  predictions  samples  Cold  start  prediction  for  new  item  that  has  no  time  series  sales  history  product  category  shoe  cat  int  le  transform  product  category  request  json  instances  start  2019  06  12  00  00  00  target  cat  cat  No  target  values  because  this  is  new  product  with  no  existing  time  series  sales  data  configuration  num  samples  10  output  types  quantiles  quantiles  payload  json  dumps  request  json  encode  utf  response  predictor  predict  payload  print  response  ,amazon
mkdir  data  wget  data  aclImdb  v1  tar  gz  http  ai  stanford  edu  amaas  data  sentiment  aclImdb  v1  tar  gz  tar  zxf  data  aclImdb  v1  tar  gz  dataimport  os  import  glob  def  read  imdb  data  data  dir  data  aclImdb  data  labels  for  data  type  in  train  test  data  data  type  labels  data  type  for  sentiment  in  pos  neg  data  data  type  sentiment  labels  data  type  sentiment  path  os  path  join  data  dir  data  type  sentiment  txt  files  glob  glob  path  for  in  files  with  open  as  review  data  data  type  sentiment  append  review  read  Here  we  represent  positive  review  by  and  negative  review  by  labels  data  type  sentiment  append  if  sentiment  pos  else  assert  len  data  data  type  sentiment  len  labels  data  type  sentiment  data  size  does  not  match  labels  size  format  data  type  sentiment  return  data  labelsdata  labels  read  imdb  data  print  IMDB  reviews  train  pos  neg  test  pos  neg  format  len  data  train  pos  len  data ,amazon
 train  neg  len  data  test  pos  len  data  test  neg  from  sklearn  utils  import  shuffle  def  prepare  imdb  data  data  labels  Prepare  training  and  test  sets  from  IMDb  movie  reviews  Combine  positive  and  negative  reviews  and  labels  data  train  data  train  pos  data  train  neg  data  test  data  test  pos  data  test  neg  labels  train  labels  train  pos  labels  train  neg  labels  test  labels  test  pos  labels  test  neg  Shuffle  reviews  and  corresponding  labels  within  training  and  test  sets  data  train  labels  train  shuffle  data  train  labels  train  data  test  labels  test  shuffle  data  test  labels  test  Return  unified  training  data  test  data  training  labels  test  labets  return  data  train  data  test  labels  train  labels  testtrain  test  train  test  prepare  imdb  data  data  labels  print  IMDb  reviews  combined  train  test  format  len  train  len  test  train  100  import  nltk  nltk  download  stopwords  from  nltk  corpus  import  stop,amazon
words  from  nltk  stem  porter  import  stemmer  PorterStemmer  import  re  from  bs4  import  BeautifulSoup  def  review  to  words  review  text  BeautifulSoup  review  html  parser  get  text  Remove  HTML  tags  text  re  sub  zA  Z0  text  lower  Convert  to  lower  case  words  text  split  Split  string  into  words  words  for  in  words  if  not  in  stopwords  words  english  Remove  stopwords  words  PorterStemmer  stem  for  in  words  stem  return  wordsimport  pickle  cache  dir  os  path  join  cache  sentiment  analysis  where  to  store  cache  files  os  makedirs  cache  dir  exist  ok  True  ensure  cache  directory  exists  def  preprocess  data  data  train  data  test  labels  train  labels  test  cache  dir  cache  dir  cache  file  preprocessed  data  pkl  Convert  each  review  to  words  read  from  cache  if  available  If  cache  file  is  not  None  try  to  read  from  it  first  cache  data  None  if  cache  file  is  not  None  try  with  open  os  path  join  cache  dir  cach,amazon
e  file  rb  as  cache  data  pickle  load  print  Read  preprocessed  data  from  cache  file  cache  file  except  pass  unable  to  read  from  cache  but  that  okay  If  cache  is  missing  then  do  the  heavy  lifting  if  cache  data  is  None  Preprocess  training  and  test  data  to  obtain  words  for  each  review  words  train  list  map  review  to  words  data  train  words  test  list  map  review  to  words  data  test  words  train  review  to  words  review  for  review  in  data  train  words  test  review  to  words  review  for  review  in  data  test  Write  to  cache  file  for  future  runs  if  cache  file  is  not  None  cache  data  dict  words  train  words  train  words  test  words  test  labels  train  labels  train  labels  test  labels  test  with  open  os  path  join  cache  dir  cache  file  wb  as  pickle  dump  cache  data  print  Wrote  preprocessed  data  to  cache  file  cache  file  else  Unpack  data  loaded  from  cache  file  words  train  words  test  labels  tr,amazon
ain  labels  test  cache  data  words  train  cache  data  words  test  cache  data  labels  train  cache  data  labels  test  return  words  train  words  test  labels  train  labels  test  Preprocess  data  train  test  train  test  preprocess  data  train  test  train  test  import  numpy  as  np  from  sklearn  feature  extraction  text  import  CountVectorizer  from  sklearn  externals  import  joblib  joblib  is  an  enhanced  version  of  pickle  that  is  more  efficient  for  storing  NumPy  arrays  def  extract  BoW  features  words  train  words  test  vocabulary  size  5000  cache  dir  cache  dir  cache  file  bow  features  pkl  Extract  Bag  of  Words  for  given  set  of  documents  already  preprocessed  into  words  If  cache  file  is  not  None  try  to  read  from  it  first  cache  data  None  if  cache  file  is  not  None  try  with  open  os  path  join  cache  dir  cache  file  rb  as  cache  data  joblib  load  print  Read  features  from  cache  file  cache  file  except  pass  una,amazon
ble  to  read  from  cache  but  that  okay  If  cache  is  missing  then  do  the  heavy  lifting  if  cache  data  is  None  Fit  vectorizer  to  training  documents  and  use  it  to  transform  them  NOTE  Training  documents  have  already  been  preprocessed  and  tokenized  into  words  pass  in  dummy  functions  to  skip  those  steps  preprocessor  lambda  vectorizer  CountVectorizer  max  features  vocabulary  size  preprocessor  lambda  tokenizer  lambda  already  preprocessed  features  train  vectorizer  fit  transform  words  train  toarray  Apply  the  same  vectorizer  to  transform  the  test  documents  ignore  unknown  words  features  test  vectorizer  transform  words  test  toarray  NOTE  Remember  to  convert  the  features  using  toarray  for  compact  representation  Write  to  cache  file  for  future  runs  store  vocabulary  as  well  if  cache  file  is  not  None  vocabulary  vectorizer  vocabulary  cache  data  dict  features  train  features  train  features  test  features  ,amazon
test  vocabulary  vocabulary  with  open  os  path  join  cache  dir  cache  file  wb  as  joblib  dump  cache  data  print  Wrote  features  to  cache  file  cache  file  else  Unpack  data  loaded  from  cache  file  features  train  features  test  vocabulary  cache  data  features  train  cache  data  features  test  cache  data  vocabulary  Return  both  the  extracted  features  as  well  as  the  vocabulary  return  features  train  features  test  vocabulary  Extract  Bag  of  Words  features  for  both  training  and  test  datasets  train  test  vocabulary  extract  BoW  features  train  test  import  pandas  as  pd  TODO  Split  the  train  and  train  arrays  into  the  DataFrames  val  train  and  val  train  Make  sure  that  val  and  val  contain  10  000  entires  while  train  and  train  contain  the  remaining  15  000  entries  First  we  make  sure  that  the  local  directory  in  which  we  like  to  store  the  training  and  validation  csv  files  exists  data  dir  data  xgboost  i,amazon
f  not  os  path  exists  data  dir  os  makedirs  data  dir  First  save  the  test  data  to  test  csv  in  the  data  dir  directory  Note  that  we  do  not  save  the  associated  ground  truth  labels  instead  we  will  use  them  later  to  compare  with  our  model  output  pd  DataFrame  test  to  csv  os  path  join  data  dir  test  csv  header  False  index  False  TODO  Save  the  training  and  validation  data  to  train  csv  and  validation  csv  in  the  data  dir  directory  Make  sure  that  the  files  you  create  are  in  the  correct  format  To  save  bit  of  memory  we  can  set  text  train  val  train  and  val  to  None  test  train  val  train  val  Noneimport  sagemaker  session  sagemaker  Session  Store  the  current  SageMaker  session  S3  prefix  which  folder  will  we  use  prefix  sentiment  xgboost  TODO  Upload  the  test  csv  train  csv  and  validation  csv  files  which  are  contained  in  data  dir  to  S3  using  sess  upload  data  test  location  None  val ,amazon
 location  None  train  location  Nonefrom  sagemaker  import  get  execution  role  Our  current  execution  role  is  require  when  creating  the  model  as  the  training  and  inference  code  will  need  to  access  the  model  artifacts  role  get  execution  role  We  need  to  retrieve  the  location  of  the  container  which  is  provided  by  Amazon  for  using  XGBoost  As  matter  of  convenience  the  training  and  inference  code  both  use  the  same  container  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  session  boto  region  name  xgboost  TODO  Create  SageMaker  estimator  using  the  container  location  determined  in  the  previous  cell  It  is  recommended  that  you  use  single  training  instance  of  type  ml  m4  xlarge  It  is  also  recommended  that  you  use  s3  output  format  session  default  bucket  prefix  as  the  output  path  xgb  None  TODO  Set  the  XGBoost  hyperparameters  in  the  xgb  object  Don  forget ,amazon
 that  in  this  case  we  have  binary  label  so  we  should  be  using  the  binary  logistic  objective  s3  input  train  sagemaker  s3  input  s3  data  train  location  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  val  location  content  type  csv  xgb  fit  train  s3  input  train  validation  s3  input  validation  TODO  Create  transformer  object  from  the  trained  model  Using  an  instance  count  of  and  an  instance  type  of  ml  m4  xlarge  should  be  more  than  enough  xgb  transformer  None  TODO  Start  the  transform  job  Make  sure  to  specify  the  content  type  and  the  split  type  of  the  test  data  xgb  transformer  wait  aws  s3  cp  recursive  xgb  transformer  output  path  data  dirpredictions  pd  read  csv  os  path  join  data  dir  test  csv  out  header  None  predictions  round  num  for  num  in  predictions  squeeze  values  from  sklearn  metrics  import  accuracy  score  accuracy  score  test  predictions  First  we  will  remov,amazon
e  all  of  the  files  contained  in  the  data  dir  directory  rm  data  dir  And  then  we  delete  the  directory  itself  rmdir  data  dir  Similarly  we  will  remove  the  files  in  the  cache  dir  directory  and  the  directory  itself  rm  cache  dir  rmdir  cache  dir  ,amazon
list  months  Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec  for  month  in  months  print  month  named  list  months  Jan  31  Feb  28  Mar  31  Apr  30  May  31  Jun  30  Jul  31  Aug  31  Sep  30  Oct  31  Nov  30  Dec  31  for  month  in  months  print  month  month  month  for  year  in  range  1900  2001  print  year  months  Jan  31  Feb  28  Mar  31  Apr  30  May  31  Jun  30  Jul  31  Aug  31  Sep  30  Oct  31  Nov  30  Dec  31  for  year  in  range  1900  2001  for  month  in  months  print  year  month  month  months  Jan  31  Feb  28  Mar  31  Apr  30  May  31  Jun  30  Jul  31  Aug  31  Sep  30  Oct  31  Nov  30  Dec  31  for  year  in  range  1900  2001  for  month  in  months  if  month  Feb  and  year  and  year  100  print  year  month  month  leap  year  else  print  year  month  month  sundayCount  totalDay  months  Jan  31  Feb  28  Mar  31  Apr  30  May  31  Jun  30  Jul  31  Aug  31  Sep  30  Oct  31  Nov  30  Dec  31  for  year  in  range  1900  2001  for  month  in  month,microsoft
s  if  month  Feb  and  year  and  year  100  print  year  month  month  leap  year  totalDay  totalDay  month  else  print  year  month  month  totalDay  totalDay  month  print  totalDay  totalDay  1900  sundayCount  totalDay  months  Jan  31  Feb  28  Mar  31  Apr  30  May  31  Jun  30  Jul  31  Aug  31  Sep  30  Oct  31  Nov  30  Dec  31  for  year  in  range  1900  2001  for  month  in  months  if  month  Feb  and  year  and  year  100  print  year  month  month  leap  year  totalDay  totalDay  month  else  print  year  month  month  totalDay  totalDay  month  print  totalDay  totalDay  if  totalDay  and  year  1900  sundayCount  print  sundayCount  print  sundayCount  2000  12  2001  ,microsoft
from  sagemaker  import  get  execution  role  import  sagemaker  as  sagerole  get  execution  role  get  user  IAM  Role  sess  sage  Session  my  work  data  context  input  data  training  from  local  s3  data  store  cntkdemo  data  to  S3  data  location  sess  upload  data  my  work  data  key  prefix  s3  data  store  account  sess  boto  session  client  sts  get  caller  identity  Account  region  sess  boto  session  region  name  image  dkr  ecr  amazonaws  com  cntkdemo  ecr  format  account  region  image  name  on  ECRestimator  sage  estimator  Estimator  image  role  ml  c4  2xlarge  output  path  s3  output  format  sess  default  bucket  model  artifact  repository  sagemaker  session  sess  estimator  set  hyperparameters  input  dim  num  classes  num  hidden  layers  hidden  layers  dim  50  epochs  200  minibatch  size  25  init  normal  verbose  time  estimator  fit  data  location  it  time  to  trainestimator  model  data  get  model  artifact  uri  ,amazon
import  os  import  boto3  import  sagemaker  from  sagemaker  mxnet  import  MXNet  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  bash  mkdir  data  curl  https  raw  githubusercontent  com  saurabh3949  Text  Classification  Datasets  master  stsa  binary  phrases  train  data  train  curl  https  raw  githubusercontent  com  saurabh3949  Text  Classification  Datasets  master  stsa  binary  test  data  test  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  DEMO  sentiment  cat  sentiment  py  MXNet  sentiment  py  role  role  train  instance  count  train  instance  type  ml  c4  2xlarge  framework  version  hyperparameters  batch  size  epochs  learning  rate  01  embedding  size  50  log  interval  1000  fit  inputs  predictor  deploy  initial  instance  count  instance  type  ml  c4  xlarge  data  this  movie  was  extremely  good  the  plot  was  very  boring  this  film  is  so  slick  superficial  and  trend ,amazon
 hoppy  just  could  not  watch  it  till  the  end  the  movie  was  so  enthralling  response  predictor  predict  data  print  responsesagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
import  os  from  pyspark  import  SparkContext  SparkConf  from  pyspark  sql  import  SparkSession  import  sagemaker  from  sagemaker  import  get  execution  role  import  sagemaker  pyspark  role  get  execution  role  Configure  Spark  to  use  the  SageMaker  Spark  dependency  jars  jars  sagemaker  pyspark  classpath  jars  classpath  join  sagemaker  pyspark  classpath  jars  See  the  SageMaker  Spark  Github  repo  under  sagemaker  pyspark  sdk  to  learn  how  to  connect  to  remote  EMR  cluster  running  Spark  from  Notebook  Instance  spark  SparkSession  builder  config  spark  driver  extraClassPath  classpath  master  local  getOrCreate  import  boto3  region  boto3  Session  region  name  trainingData  spark  read  format  libsvm  option  numFeatures  784  option  vectorType  dense  load  s3a  sagemaker  sample  data  spark  mnist  train  format  region  testData  spark  read  format  libsvm  option  numFeatures  784  option  vectorType  dense  load  s3a  sagemaker  sample  data  spark ,amazon
 mnist  test  format  region  trainingData  show  import  random  from  sagemaker  pyspark  import  IAMRole  S3DataPath  from  sagemaker  pyspark  algorithms  import  XGBoostSageMakerEstimator  xgboost  estimator  XGBoostSageMakerEstimator  sagemakerRole  IAMRole  role  trainingInstanceType  ml  m4  xlarge  trainingInstanceCount  endpointInstanceType  ml  m4  xlarge  endpointInitialInstanceCount  xgboost  estimator  setEta  xgboost  estimator  setGamma  xgboost  estimator  setMinChildWeight  xgboost  estimator  setSilent  xgboost  estimator  setObjective  multi  softmax  xgboost  estimator  setNumClasses  10  xgboost  estimator  setNumRound  10  train  model  xgboost  estimator  fit  trainingData  transformedData  model  transform  trainingData  transformedData  show  from  pyspark  sql  types  import  DoubleType  import  matplotlib  pyplot  as  plt  import  numpy  as  np  helper  function  to  display  digit  def  show  digit  img  caption  xlabel  subplot  None  if  subplot  None  subplot  plt  subplots  im,amazon
gr  img  reshape  28  28  subplot  axes  get  xaxis  set  ticks  subplot  axes  get  yaxis  set  ticks  plt  title  caption  plt  xlabel  xlabel  subplot  imshow  imgr  cmap  gray  images  np  array  transformedData  select  features  cache  take  250  clusters  transformedData  select  prediction  cache  take  250  for  cluster  in  range  10  print  nCluster  format  int  cluster  digits  img  for  img  in  zip  clusters  images  if  int  prediction  cluster  height  len  digits  width  plt  rcParams  figure  figsize  width  height  subplots  plt  subplots  height  width  subplots  np  ndarray  flatten  subplots  for  subplot  image  in  zip  subplots  digits  show  digit  image  subplot  subplot  for  subplot  in  subplots  len  digits  subplot  axis  off  plt  show  Delete  the  endpoint  from  sagemaker  pyspark  import  SageMakerResourceCleanup  resource  cleanup  SageMakerResourceCleanup  model  sagemakerClient  resource  cleanup  deleteResources  model  getCreatedResources  ,amazon
from  google  cloud  import  translate  import  sixdef  google  translate  target  text  translate  client  translate  Client  if  isinstance  text  six  binary  type  text  text  decode  utf  result  translate  client  translate  text  target  language  target  return  result  translatedText  input  text  google  result  google  translate  en  input  text  Print  translated  text  print  result  ,microsoft
import  wget  link  to  data  https  apsportal  ibm  com  exchange  api  v1  entries  c173693bf48aeb22e41bbe2b41d79c1f  data  accessKey  941eec501eadcdceb5abd25cf7c029d5  WisconsinDataSet  wget  download  link  to  data  print  WisconsinDataSet  import  pandas  as  pd  WisconsinDataSet  BreastCancerWisconsinDataSet  csv  df  data  pd  read  csv  WisconsinDataSet  df  data  head  df  data  shapedf  data  describe  df  data  diagnosis  map  lambda  if  else  df  data  drop  diagnosis  id  axis  from  sklearn  cross  validation  import  train  test  split  train  test  train  test  train  test  split  test  size  35  random  state  143  print  Number  of  training  records  str  train  size  print  Number  of  testing  records  str  test  size  import  xgboost  from  xgboost  sklearn  import  XGBClassifier  from  sklearn  pipeline  import  Pipeline  from  sklearn  preprocessing  import  StandardScaler  from  sklearn  cross  validation  import  cross  val  score  from  sklearn  metrics  import  accuracy  score  f,ibm
rom  xgboost  import  plot  importance  from  matplotlib  import  pyplot  import  pprint  matplotlib  inlinepipeline  Pipeline  scaler  StandardScaler  classifier  XGBClassifier  pipelinepipeline  fit  train  values  train  values  xgboost  plot  importance  pipeline  steps  pred  pipeline  predict  test  values  accuracy  accuracy  score  test  pred  print  Accuracy  2f  accuracy  100  pipeline  gs  Pipeline  scaler  StandardScaler  classifier  XGBClassifier  parameters  classifier  learning  rate  01  03  classifier  estimators  50  200  from  sklearn  grid  search  import  GridSearchCV  clf  GridSearchCV  pipeline  gs  parameters  clf  fit  train  values  train  values  clf  grid  scores  print  Best  score  clf  best  score  print  Best  parameter  set  clf  best  params  pred  clf  predict  test  values  accuracy  accuracy  score  test  pred  print  Accuracy  2f  accuracy  100  pred  clf  predict  test  values  55  print  pred  from  repository  mlrepository  import  MetaNames  from  repository  mlreposi,ibm
tory  import  MetaProps  from  repository  mlrepositoryclient  import  MLRepositoryClient  from  repository  mlrepositoryartifact  import  MLRepositoryArtifact  Enter  your  credentials  here  wml  credentials  url  https  ibm  watson  ml  mybluemix  net  access  key  username  password  instance  id  service  path  wml  credentials  url  username  wml  credentials  username  password  wml  credentials  password  instance  id  wml  credentials  instance  id  ml  repository  client  MLRepositoryClient  service  path  ml  repository  client  authorize  username  password  Check  if  props  is  mandatory  AUTHOR  NAME  and  AUTHOR  EMAIL  have  been  removed  for  Watson  Studio  sharing  props1  MetaProps  MetaNames  AUTHOR  NAME  MetaNames  AUTHOR  EMAIL  model  artifact  MLRepositoryArtifact  clf  name  Tumor  Type  Detection  v1  pipe  meta  props  props1  saved  model  ml  repository  client  models  save  model  artifact  dict  meta  saved  model  meta  get  pprint  pprint  dict  meta  import  urllib3  req,ibm
uests  json  headers  urllib3  util  make  headers  basic  auth  format  username  password  url  v3  identity  token  format  service  path  response  requests  get  url  headers  headers  mltoken  json  loads  response  text  get  token  header  Content  Type  application  json  Authorization  Bearer  mltoken  endpoint  instance  service  path  v3  wml  instances  instance  id  endpoint  instance  service  path  v3  wml  instances  instance  id  header  Content  Type  application  json  Authorization  Bearer  mltoken  response  get  instance  requests  get  endpoint  instance  headers  header  print  response  get  instance  print  response  get  instance  text  endpoint  published  models  json  loads  response  get  instance  text  get  entity  get  published  models  get  url  print  endpoint  published  models  header  Content  Type  application  json  Authorization  Bearer  mltoken  response  get  requests  get  endpoint  published  models  headers  header  print  response  get  print  response  get  t,ibm
ext  endpoint  deployments  get  entity  get  deployments  get  url  for  in  json  loads  response  get  text  get  resources  if  get  metadata  get  guid  saved  model  uid  print  endpoint  deployments  payload  online  name  skl  xgb  cancer1  pipe  description  skl  xgb  Cancer  type  online  response  online  requests  post  endpoint  deployments  json  payload  online  headers  header  print  response  online  print  response  online  text  scoring  url  json  loads  response  online  text  get  entity  get  scoring  url  print  scoring  url  payload  scoring  values  16  20  010  081  11  69  13  19  14  78  848  19  48  627  43  75  23  27  44  6697  208  324  139  153  2789  30  013  001  15  90  29  47  35  97  15  83  31  03  08  response  scoring  requests  post  scoring  url  json  payload  scoring  headers  header  pprint  pprint  response  scoring  text  ,ibm
import  boto3  re  from  sagemaker  import  get  execution  role  role  get  execution  role  cat  iris  dnn  classifier  pyfrom  iris  dnn  classifier  import  estimator  fn  classifier  estimator  fn  run  config  None  params  None  import  os  from  six  moves  urllib  request  import  urlopen  Data  sets  IRIS  TRAINING  iris  training  csv  IRIS  TRAINING  URL  http  download  tensorflow  org  data  iris  training  csv  IRIS  TEST  iris  test  csv  IRIS  TEST  URL  http  download  tensorflow  org  data  iris  test  csv  if  not  os  path  exists  IRIS  TRAINING  raw  urlopen  IRIS  TRAINING  URL  read  with  open  IRIS  TRAINING  wb  as  write  raw  if  not  os  path  exists  IRIS  TEST  raw  urlopen  IRIS  TEST  URL  read  with  open  IRIS  TEST  wb  as  write  raw  from  iris  dnn  classifier  import  train  input  fn  train  func  train  input  fn  params  None  classifier  train  input  fn  train  func  steps  1000  from  iris  dnn  classifier  import  serving  input  fn  exported  model  classifier,amazon
  export  savedmodel  export  dir  base  export  Servo  serving  input  receiver  fn  serving  input  fn  print  exported  model  import  tarfile  with  tarfile  open  model  tar  gz  mode  gz  as  archive  archive  add  export  recursive  True  import  sagemaker  sagemaker  session  sagemaker  Session  inputs  sagemaker  session  upload  data  path  model  tar  gz  key  prefix  model  from  sagemaker  tensorflow  model  import  TensorFlowModel  sagemaker  model  TensorFlowModel  model  data  s3  sagemaker  session  default  bucket  model  model  tar  gz  role  role  entry  point  iris  dnn  classifier  py  time  predictor  sagemaker  model  deploy  initial  instance  count  instance  type  ml  m4  xlarge  sample  predictor  predict  sample  os  remove  model  tar  gz  import  shutil  shutil  rmtree  export  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
matplotlib  inline  import  os  import  time  from  time  import  gmtime  strftime  import  numpy  as  np  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  from  sklearn  datasets  import  load  boston  import  sklearn  model  selectionimport  sagemaker  from  sagemaker  import  get  execution  role  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  This  is  an  object  that  represents  the  SageMaker  session  that  we  are  currently  operating  in  This  object  contains  some  useful  information  that  we  will  need  to  access  later  such  as  our  region  session  sagemaker  Session  This  is  an  object  that  represents  the  IAM  role  that  we  are  currently  assigned  When  we  construct  and  launch  the  training  job  later  we  will  need  to  tell  it  what  IAM  role  it  should  have  Since  our  use  case  is  relatively  simple  we  will  simply  assign  the  training  job  the  role  we  currently  have  role  get  execution  role  boston  load  boston,amazon
  First  we  package  up  the  input  data  and  the  target  variable  the  median  value  as  pandas  dataframes  This  will  make  saving  the  data  to  file  little  easier  later  on  bos  pd  pd  DataFrame  boston  data  columns  boston  feature  names  bos  pd  pd  DataFrame  boston  target  We  split  the  dataset  into  training  and  testing  sets  train  test  train  test  sklearn  model  selection  train  test  split  bos  pd  bos  pd  test  size  33  Then  we  split  the  training  set  further  into  training  and  validation  sets  train  val  train  val  sklearn  model  selection  train  test  split  train  train  test  size  33  This  is  our  local  data  directory  We  need  to  make  sure  that  it  exists  data  dir  data  boston  if  not  os  path  exists  data  dir  os  makedirs  data  dir  We  use  pandas  to  save  our  test  train  and  validation  data  to  csv  files  Note  that  we  make  sure  not  to  include  header  information  or  an  index  as  this  is  required  by  the ,amazon
 built  in  algorithms  provided  by  Amazon  Also  for  the  train  and  validation  data  it  is  assumed  that  the  first  entry  in  each  row  is  the  target  variable  test  to  csv  os  path  join  data  dir  test  csv  header  False  index  False  pd  concat  val  val  axis  to  csv  os  path  join  data  dir  validation  csv  header  False  index  False  pd  concat  train  train  axis  to  csv  os  path  join  data  dir  train  csv  header  False  index  False  prefix  boston  xgboost  LL  test  location  session  upload  data  os  path  join  data  dir  test  csv  key  prefix  prefix  val  location  session  upload  data  os  path  join  data  dir  validation  csv  key  prefix  prefix  train  location  session  upload  data  os  path  join  data  dir  train  csv  key  prefix  prefix  We  will  need  to  know  the  name  of  the  container  that  we  want  to  use  for  training  SageMaker  provides  nice  utility  method  to  construct  this  for  us  container  get  image  uri  session  boto  reg,amazon
ion  name  xgboost  We  now  specify  the  parameters  we  wish  to  use  for  our  training  job  training  params  We  need  to  specify  the  permissions  that  this  training  job  will  have  For  our  purposes  we  can  use  the  same  permissions  that  our  current  SageMaker  session  has  training  params  RoleArn  role  Here  we  describe  the  algorithm  we  wish  to  use  The  most  important  part  is  the  container  which  contains  the  training  code  training  params  AlgorithmSpecification  TrainingImage  container  TrainingInputMode  File  We  also  need  to  say  where  we  would  like  the  resulting  model  artifacst  stored  training  params  OutputDataConfig  S3OutputPath  s3  session  default  bucket  prefix  output  We  also  need  to  set  some  parameters  for  the  training  job  itself  Namely  we  need  to  describe  what  sort  of  compute  instance  we  wish  to  use  along  with  stopping  condition  to  handle  the  case  that  there  is  some  sort  of  error  and  the  t,amazon
raining  script  doesn  terminate  training  params  ResourceConfig  InstanceCount  InstanceType  ml  m4  xlarge  VolumeSizeInGB  training  params  StoppingCondition  MaxRuntimeInSeconds  86400  Next  we  set  the  algorithm  specific  hyperparameters  You  may  wish  to  change  these  to  see  what  effect  there  is  on  the  resulting  model  training  params  HyperParameters  max  depth  eta  gamma  min  child  weight  subsample  objective  reg  linear  early  stopping  rounds  10  num  round  200  Now  we  need  to  tell  SageMaker  where  the  data  should  be  retrieved  from  training  params  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  train  location  S3DataDistributionType  FullyReplicated  ContentType  csv  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  val  location  S3DataDistributionType  FullyReplicated  ContentType  csv  CompressionType  None  First  we  need  to  choose  training  job  na,amazon
me  This  is  useful  for  if  we  want  to  recall  information  about  our  training  job  at  later  date  Note  that  SageMaker  requires  training  job  name  and  that  the  name  needs  to  be  unique  which  we  accomplish  by  appending  the  current  timestamp  training  job  name  boston  xgboost  strftime  gmtime  training  params  TrainingJobName  training  job  name  And  now  we  ask  SageMaker  to  create  and  execute  the  training  job  training  job  session  sagemaker  client  create  training  job  training  params  session  logs  for  job  training  job  name  wait  True  We  begin  by  asking  SageMaker  to  describe  for  us  the  results  of  the  training  job  The  data  structure  returned  contains  lot  more  information  than  we  currently  need  try  checking  it  out  yourself  in  more  detail  training  job  info  session  sagemaker  client  describe  training  job  TrainingJobName  training  job  name  model  artifacts  training  job  info  ModelArtifacts  S3ModelArtifact,amazon
s  Just  like  when  we  created  training  job  the  model  name  must  be  unique  model  name  training  job  name  model  We  also  need  to  tell  SageMaker  which  container  should  be  used  for  inference  and  where  it  should  retrieve  the  model  artifacts  from  In  our  case  the  xgboost  container  that  we  used  for  training  can  also  be  used  for  inference  primary  container  Image  container  ModelDataUrl  model  artifacts  And  lastly  we  construct  the  SageMaker  model  model  info  session  sagemaker  client  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  Just  like  in  each  of  the  previous  steps  we  need  to  make  sure  to  name  our  job  and  the  name  should  be  unique  transform  job  name  boston  xgboost  batch  transform  strftime  gmtime  Now  we  construct  the  data  structure  which  will  describe  the  batch  transform  job  transform  request  TransformJobName  transform  job  name  This  is  the  na,amazon
me  of  the  model  that  we  created  earlier  ModelName  model  name  This  describes  how  many  compute  instances  should  be  used  at  once  If  you  happen  to  be  doing  very  large  batch  transform  job  it  may  be  worth  running  multiple  compute  instances  at  once  MaxConcurrentTransforms  This  says  how  big  each  individual  request  sent  to  the  model  should  be  at  most  One  of  the  things  that  SageMaker  does  in  the  background  is  to  split  our  data  up  into  chunks  so  that  each  chunks  stays  under  this  size  limit  MaxPayloadInMB  Sometimes  we  may  want  to  send  only  single  sample  to  our  endpoint  at  time  however  in  this  case  each  of  the  chunks  that  we  send  should  contain  multiple  samples  of  our  input  data  BatchStrategy  MultiRecord  This  next  object  describes  where  the  output  data  should  be  stored  Some  of  the  more  advanced  options  which  we  don  cover  here  also  describe  how  SageMaker  should  collect  output,amazon
  from  various  batches  TransformOutput  S3OutputPath  s3  batch  bransform  format  session  default  bucket  prefix  Here  we  describe  our  input  data  Of  course  we  need  to  tell  SageMaker  where  on  S3  our  input  data  is  stored  in  addition  we  need  to  detail  the  characteristics  of  our  input  data  In  particular  since  SageMaker  may  need  to  split  our  data  up  into  chunks  it  needs  to  know  how  the  individual  samples  in  our  data  file  appear  In  our  case  each  line  is  its  own  sample  and  so  we  set  the  split  type  to  line  We  also  need  to  tell  SageMaker  what  type  of  data  is  being  sent  in  this  case  csv  so  that  it  can  properly  serialize  the  data  TransformInput  ContentType  text  csv  SplitType  Line  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  test  location  And  lastly  we  tell  SageMaker  what  sort  of  compute  instance  we  would  like  it  to  use  TransformResources  InstanceType  ml  m4  xlarge  InstanceCou,amazon
nt  transform  response  session  sagemaker  client  create  transform  job  transform  request  transform  desc  session  wait  for  transform  job  transform  job  name  transform  output  s3  batch  bransform  format  session  default  bucket  prefix  aws  s3  cp  recursive  transform  output  data  dirY  pred  pd  read  csv  os  path  join  data  dir  test  csv  out  header  None  plt  scatter  test  pred  plt  xlabel  Median  Price  plt  ylabel  Predicted  Price  plt  title  Median  Price  vs  Predicted  Price  First  we  will  remove  all  of  the  files  contained  in  the  data  dir  directory  rm  data  dir  And  then  we  delete  the  directory  itself  rmdir  data  dir  ,amazon
import  boto3  sagemaker  boto3  client  sagemaker  model  name  spark  regression  model  009  endpoint  config  name  inference  server  009a  config  endpoint  name  inference  server  009a  Create  SageMaker  Model  role  arn  aws  iam  account  number  role  sagemaker  role  primary  container  Image  account  number  dkr  ecr  us  west  amazonaws  com  inference  server  latest  ModelDataUrl  s3  bucket  models  spark  regression  model  model  tgz  create  model  response  sagemaker  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  Create  EndPoint  Config  response  sagemaker  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  VariantName  default  variant  name  ModelName  model  name  InitialInstanceCount  InstanceType  ml  m4  xlarge  print  response  Create  Sagemaker  Endpoint  response  sagemaker  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  ,amazon
endpoint  config  name  print  response  time  Test  the  Endpoint  import  boto3  io  json  client  boto3  client  sagemaker  runtime  input  schema  fields  name  Price  type  double  name  Mileage  type  integer  name  Make  type  string  name  Model  type  string  name  Trim  type  string  name  Type  type  string  name  Cylinder  type  integer  name  Liter  type  double  name  Doors  type  integer  name  Cruise  type  integer  name  Sound  type  integer  name  Leather  type  integer  rows  9041  9062544231  26191  Chevrolet  AVEO  SVM  Sedan  4D  Sedan  response  client  invoke  endpoint  EndpointName  endpoint  name  Body  input  ContentType  application  json  Accept  application  json  res  json  json  loads  response  Body  read  decode  utf  print  json  dumps  res  json  indent  ,amazon
Guess  the  number  game  The  computer  selects  random  number  between  and  100  The  user  keeps  guessing  which  number  the  computer  has  chosen  until  they  get  it  right  The  computer  responds  bingo  or  too  high  or  too  low  after  each  guess  After  the  user  has  guessed  the  number  correctly  the  computer  output  how  many  attempts  it  has  made  import  random  random  num  random  randint  100  try  attempt  while  True  user  int  input  Enter  number  100  if  user  random  num  if  user  random  num  print  Too  low  else  print  Too  high  attempt  else  print  Bingo  attempt  print  No  of  attempt  format  attempt  break  except  TypeError  ValueError  EOFError  as  err  print  Invalid  input  Reverse  the  above  game  You  think  of  number  Get  the  computer  to  guess  number  and  you  respond  with  too  high  too  low  or  bingo  Make  sure  the  computer  has  game  plan  don  just  let  it  guess  at  random  import  random  first  random  randint  100  valid ,microsoft
 low  high  100  while  True  print  first  user  input  Enter  your  response  if  user  not  in  valid  print  Invalid  input  break  else  if  user  low  first  first  random  randint  low  high  elif  user  high  first  first  random  randint  low  high  else  print  Bingo  break  number  45  Write  function  that  will  convert  mark  into  grade  The  function  will  return  or  The  function  will  require  parameter  mark  The  formula  is  70  60  55  50  45  40  40  Assume  the  maximum  mark  is  100  Having  written  the  function  we  want  to  use  it  three  times  Write  program  with  the  function  that  allows  the  user  to  enter  two  module  marks  and  displays  the  grade  It  then  computes  the  average  and  displays  the  overall  grade  Enter  Module  mark  68  Enter  Module  mark  57  Result  Module  Module  Overall  def  grade  mark  if  mark  40  return  elif  mark  45  return  elif  mark  50  return  elif  mark  55  return  elif  mark  60  return  elif  mark  70  return  else,microsoft
  return  first  int  input  Enter  Module  mark  second  int  input  Enter  Module  mark  avg  first  second  print  Result  print  Module  format  grade  first  print  Module  format  grade  second  print  Overall  format  grade  avg  For  the  above  problem  allow  maximum  mark  other  than  100  This  will  require  the  user  to  enter  both  the  mark  and  maximum  possible  for  that  module  The  function  will  also  require  the  maximum  mark  in  order  to  calculate  the  grade  so  another  parameter  will  be  needed  def  grade  mark  max  mark  weight  mark  max  mark  if  weight  40  return  elif  weight  45  return  elif  weight  50  return  elif  weight  55  return  elif  weight  60  return  elif  weight  70  return  else  return  max  mark  int  input  Enter  maximum  mark  first  int  input  Enter  Module  mark  second  int  input  Enter  Module  mark  avg  first  second  print  Result  print  Module  format  grade  first  max  mark  print  Module  format  grade  second  max  mark  pr,microsoft
int  Overall  format  grade  avg  max  mark  Write  procedure  draw  stars  that  will  draw  sequence  of  spaces  followed  by  sequence  of  stars  It  should  accept  two  parameters  the  number  of  spaces  and  the  number  of  stars  draw  stars  would  produce  indicates  space  Use  your  procedure  to  draw  def  draw  stars  print  end  main  draw  stars  print  draw  stars  print  draw  stars  print  draw  stars  print  draw  stars  print  draw  stars  print  draw  stars  draw  stars  print  draw  stars  draw  stars  print  Write  program  using  the  above  draw  stars  procedure  that  will  draw  pyramid  whose  base  is  width  specified  by  the  user  Enter  base  Assume  base  is  an  odd  number  def  draw  stars  print  end  main  user  int  input  Enter  base  user  for  in  range  draw  stars  print  Write  program  to  perform  basic  encryption  and  decryption  on  text  This  algorithm  works  by  moving  letters  along  by  an  offset  If  the  offset  is  then  etc  Write  two  f,microsoft
unctions  encrypt  and  decrypt  Both  will  return  string  The  user  selects  whether  to  encrypt  or  decrypt  The  user  enters  sentence  to  encrypt  and  the  offset  The  program  responds  with  the  encrypted  or  decrypted  version  def  encryption  sentence  new  str  for  in  sentence  new  ascii  ord  if  new  ascii  122  new  ascii  96  new  ascii  122  new  char  chr  new  ascii  new  str  new  char  return  new  str  print  encryption  helloworld  The  above  algorithm  is  actually  easy  to  crack  much  better  algorithm  would  be  one  that  has  different  offset  for  every  letter  We  can  do  this  using  the  random  number  generator  because  it  generates  the  same  sequence  of  random  numbers  from  seed  position  As  long  as  sender  and  receiver  agree  where  to  seed  this  will  be  the  encryption  key  they  can  both  work  out  the  same  offsets  Algorithm  for  encrypt  function  function  Encrypt  text  key  Randomize  key  For  each  letter  in  text  Get  ,microsoft
its  ascii  code  Add  random  offset  int  rnd  10  to  ascii  code  Turn  this  new  ascii  code  back  to  character  Append  character  to  ciphertext  string  Endfor  return  ciphertext  end  from  random  import  randint  def  encrypt  text  key  def  Randomize  key  pos  return  key  pos  key  new  str  for  in  range  len  text  new  ascii  ord  text  Randomize  key  if  new  ascii  122  new  ascii  96  new  ascii  122  new  char  chr  new  ascii  new  str  new  char  return  new  str  print  encrypt  helloworld  10  For  the  above  task  use  single  function  with  an  extra  parameter  to  indicate  whether  the  text  is  being  encrypted  or  decrypted  rather  than  having  two  different  functions  from  random  import  randint  def  encrypt  text  key  encrypt  True  def  Randomize  key  pos  return  key  pos  key  if  encrypt  True  new  str  for  in  range  len  text  new  ascii  ord  text  Randomize  key  if  new  ascii  122  new  ascii  96  new  ascii  122  new  char  chr  new  ascii  ne,microsoft
w  str  new  char  return  new  str  else  decrypt  it  new  str  for  in  range  len  text  new  ascii  ord  text  Randomize  key  if  new  ascii  97  new  ascii  122  96  new  ascii  97  new  char  chr  new  ascii  new  str  new  char  return  new  str  main  print  encrypt  helloworld  10  print  encrypt  hfnosbuytm  10  False  ,microsoft
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  xgboost  churn  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  import  io  import  os  import  sys  import  time  import  json  from  IPython  display  import  display  from  time  import  strftime  gmtime  import  sagemaker  from  sagemaker  predictor  import  csv  serializer  wget  http  dataminingconsultant  com  DKD2e  data  sets  zip  unzip  DKD2e  data  sets  zipchurn  pd  read  csv  Data  sets  churn  txt  pd  set  option  display  max  columns  500  churn  Frequency  tables  for  each  categorical  feature  for  column  in  churn  select  dtypes  include  object  columns  display  pd  crosstab  index  churn  column  columns  observations  normalize  columns  Histograms  for  each  numeric  features  display  churn  describe  matplotlib  inline  hist  churn  hist  bins  30  sharey  Tru,amazon
e  figsize  10  10  churn  churn  drop  Phone  axis  churn  Area  Code  churn  Area  Code  astype  object  for  column  in  churn  select  dtypes  include  object  columns  if  column  Churn  display  pd  crosstab  index  churn  column  columns  churn  Churn  normalize  columns  for  column  in  churn  select  dtypes  exclude  object  columns  print  column  hist  churn  column  Churn  hist  by  Churn  bins  30  plt  show  display  churn  corr  pd  plotting  scatter  matrix  churn  figsize  12  12  plt  show  churn  churn  drop  Day  Charge  Eve  Charge  Night  Charge  Intl  Charge  axis  model  data  pd  get  dummies  churn  model  data  pd  concat  model  data  Churn  True  model  data  drop  Churn  False  Churn  True  axis  axis  train  data  validation  data  test  data  np  split  model  data  sample  frac  random  state  1729  int  len  model  data  int  len  model  data  train  data  to  csv  train  csv  header  False  index  False  validation  data  to  csv  validation  csv  header  False  index  Fals,amazon
e  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  train  csv  upload  file  train  csv  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  validation  csv  upload  file  validation  csv  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  boto3  Session  region  name  xgboost  s3  input  train  sagemaker  s3  input  s3  data  s3  train  format  bucket  prefix  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  s3  validation  format  bucket  prefix  content  type  csv  sess  sagemaker  Session  xgb  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  m4  xlarge  output  path  s3  output  format  bucket  prefix  sagemaker  session  sess  xgb  set  hyperparameters  max  depth  eta  gamma  min  child  weight  subsample  silent  objective  binary  logistic  num  round  100  xgb  fit  train  s3  input  train  validation  s,amazon
3  input  validation  xgb  predictor  xgb  deploy  initial  instance  count  instance  type  ml  m4  xlarge  xgb  predictor  content  type  text  csv  xgb  predictor  serializer  csv  serializer  xgb  predictor  deserializer  Nonedef  predict  data  rows  500  split  array  np  array  split  data  int  data  shape  float  rows  predictions  for  array  in  split  array  predictions  join  predictions  xgb  predictor  predict  array  decode  utf  return  np  fromstring  predictions  sep  predictions  predict  test  data  as  matrix  pd  crosstab  index  test  data  iloc  columns  np  round  predictions  rownames  actual  colnames  predictions  plt  hist  predictions  plt  show  pd  crosstab  index  test  data  iloc  columns  np  where  predictions  cutoffs  np  arange  01  01  costs  for  in  cutoffs  costs  append  np  sum  np  sum  np  array  100  500  100  pd  crosstab  index  test  data  iloc  columns  np  where  predictions  costs  np  array  costs  plt  plot  cutoffs  costs  plt  show  print  Cost  is  m,amazon
inimized  near  cutoff  of  cutoffs  np  argmin  costs  for  cost  of  np  min  costs  sagemaker  Session  delete  endpoint  xgb  predictor  endpoint  ,amazon
Check  core  SDK  version  number  import  azureml  core  print  SDK  version  azureml  core  VERSION  from  azureml  core  import  Workspace  ws  Workspace  from  config  print  ws  name  ws  location  ws  resource  group  sep  from  azureml  core  model  import  Model  model  Model  register  model  path  resnet50v2  resnet50v2  onnx  model  name  resnet50v2  tags  onnx  demo  description  ResNet50v2  from  ONNX  Model  Zoo  workspace  ws  models  ws  models  for  in  models  print  Name  name  tVersion  version  tDescription  description  tags  writefile  score  py  import  json  import  time  import  sys  import  os  from  azureml  core  model  import  Model  import  numpy  as  np  we  re  going  to  use  numpy  to  process  input  and  output  data  import  onnxruntime  to  inference  ONNX  models  we  use  the  ONNX  Runtime  def  softmax  reshape  np  exp  np  max  return  sum  axis  def  init  global  session  model  Model  get  model  path  model  name  resnet50v2  session  onnxruntime  InferenceSess,microsoft
ion  model  None  def  preprocess  input  data  json  convert  the  JSON  data  into  the  tensor  input  img  data  np  array  json  loads  input  data  json  data  astype  float32  normalize  mean  vec  np  array  485  456  406  stddev  vec  np  array  229  224  225  norm  img  data  np  zeros  img  data  shape  astype  float32  for  in  range  img  data  shape  norm  img  data  img  data  255  mean  vec  stddev  vec  return  norm  img  data  def  postprocess  result  return  softmax  np  array  result  tolist  def  run  input  data  json  try  start  time  time  load  in  our  data  which  is  expected  as  NCHW  224x224  image  input  data  preprocess  input  data  json  input  name  session  get  inputs  name  get  the  id  of  the  first  input  of  the  model  result  session  run  input  name  input  data  end  time  time  stop  timer  return  result  postprocess  result  time  end  start  except  Exception  as  result  str  return  error  result  from  azureml  core  conda  dependencies  import  Cond,microsoft
aDependencies  myenv  CondaDependencies  create  pip  packages  numpy  onnxruntime  with  open  myenv  yml  as  write  myenv  serialize  to  string  from  azureml  core  image  import  ContainerImage  image  config  ContainerImage  image  configuration  execution  script  score  py  runtime  python  conda  file  myenv  yml  description  ONNX  ResNet50  Demo  tags  demo  onnx  image  ContainerImage  create  name  onnxresnet50v2  models  model  image  config  image  config  workspace  ws  image  wait  for  creation  show  output  True  print  image  image  build  log  uri  from  azureml  core  webservice  import  AciWebservice  aciconfig  AciWebservice  deploy  configuration  cpu  cores  memory  gb  tags  demo  onnx  description  web  service  for  ResNet50  ONNX  model  from  azureml  core  webservice  import  Webservice  from  random  import  randint  aci  service  name  onnx  demo  resnet50  str  randint  100  print  Service  aci  service  name  aci  service  Webservice  deploy  from  image  deployment  conf,microsoft
ig  aciconfig  image  image  name  aci  service  name  workspace  ws  aci  service  wait  for  deployment  True  print  aci  service  state  if  aci  service  state  Healthy  run  this  command  for  debugging  print  aci  service  get  logs  aci  service  delete  print  aci  service  scoring  uri  aci  service  delete  ,microsoft
import  os  import  boto3  import  sagemaker  from  sagemaker  mxnet  import  MXNet  from  mxnet  import  gluon  nd  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  gluon  data  vision  FashionMNIST  data  train  train  True  gluon  data  vision  FashionMNIST  data  test  train  False  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  mnist  cat  fmnist  py  batch  size  100  epochs  10  learning  rate  01  momentum  log  interval  100m  MXNet  fmnist  py  role  role  train  instance  count  train  instance  type  ml  c4  xlarge  hyperparameters  batch  size  batch  size  epochs  epochs  learning  rate  learning  rate  momentum  momentum  log  interval  log  interval  fit  inputs  predictor  deploy  initial  instance  count  instance  type  ml  m4  xlarge  import  numpy  as  np  import  gzip  import  struct  import  matplotlib  pyplot  as  plt  matplotlib  inline  def  read  data  label  url  image  url  with  gzip  open,amazon
  label  url  as  flbl  magic  num  struct  unpack  II  flbl  read  label  np  fromstring  flbl  read  dtype  np  int8  with  gzip  open  image  url  rb  as  fimg  magic  num  rows  cols  struct  unpack  IIII  fimg  read  16  image  np  fromstring  fimg  read  dtype  np  uint8  reshape  len  label  rows  cols  return  label  image  val  lbl  val  img  read  data  data  test  t10k  labels  idx1  ubyte  gz  data  test  t10k  images  idx3  ubyte  gz  idx  32  This  number  can  be  changed  to  get  another  image  plt  imshow  val  img  idx  cmap  Greys  plt  axis  off  plt  show  image  nd  array  val  img  idx  reshape  28  28  asnumpy  tolist  response  predictor  predict  image  print  int  response  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
bash  SFC  DIR  home  ec2  user  snowflake  SFC  DIR  mkdir  SFC  DIR  cd  SFC  DIR  PRODUCTS  snowflake  jdbc  spark  snowflake  11  for  PRODUCT  in  PRODUCTS  do  wget  https  repo1  maven  org  maven2  net  snowflake  PRODUCT  maven  metadata  xml  dev  null  VERSION  grep  latest  maven  metadata  xml  awk  print  awk  print  DRIVER  PRODUCT  VERSION  jar  if  DRIVER  then  rm  PRODUCT  dev  null  wget  https  repo1  maven  org  maven2  net  snowflake  PRODUCT  VERSION  DRIVER  dev  null  fi  maven  metadata  xml  rm  maven  metadata  xml  done  sc  stop  from  pyspark  import  SparkContext  SparkConf  from  pyspark  sql  import  SQLContext  SparkSession  from  pyspark  sql  types  import  from  sagemaker  pyspark  import  IAMRole  classpath  jars  from  sagemaker  pyspark  algorithms  import  KMeansSageMakerEstimator  sfc  jars  ls  home  ec2  user  snowflake  jar  conf  SparkConf  set  spark  driver  extraClassPath  join  classpath  jars  join  sfc  jars  setMaster  local  setAppName  local  spark  tes,amazon
t  sc  SparkContext  conf  conf  spark  SQLContext  sc  scimport  boto3  params  SNOWFLAKE  URL  SNOWFLAKE  ACCOUNT  ID  SNOWFLAKE  USER  ID  SNOWFLAKE  PASSWORD  SNOWFLAKE  DATABASE  SNOWFLAKE  SCHEMA  SNOWFLAKE  WAREHOUSE  SNOWFLAKE  BUCKET  SNOWFLAKE  PREFIX  region  us  east  def  get  credentials  params  ssm  boto3  client  ssm  region  response  ssm  get  parameters  Names  params  WithDecryption  True  Build  dict  of  credentials  param  values  Name  Value  for  in  response  Parameters  return  param  values  param  values  get  credentials  params  sfOptions  sfURL  param  values  SNOWFLAKE  URL  sfAccount  param  values  SNOWFLAKE  ACCOUNT  ID  sfUser  param  values  SNOWFLAKE  USER  ID  sfPassword  param  values  SNOWFLAKE  PASSWORD  sfDatabase  param  values  SNOWFLAKE  DATABASE  sfSchema  param  values  SNOWFLAKE  SCHEMA  sfWarehouse  param  values  SNOWFLAKE  WAREHOUSE  SNOWFLAKE  SOURCE  NAME  net  snowflake  spark  snowflake  df  spark  read  format  SNOWFLAKE  SOURCE  NAME  options  sfOpti,amazon
ons  option  query  select  main  temp  max  273  15  8000  32  00  as  temp  max  far  main  temp  min  273  15  8000  32  00  as  temp  min  far  cast  time  as  timestamp  time  city  coord  lat  lat  city  coord  lon  lon  from  snowflake  sample  data  weather  weather  14  total  limit  5000000  load  df  describe  show  ,amazon
Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  import  sagemaker  bucket  sagemaker  Session  default  bucket  prefix  sagemaker  DEMO  xgboost  churn  role  get  execution  role  import  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  import  io  import  os  import  sys  import  time  import  json  from  IPython  display  import  display  from  time  import  strftime  gmtime  import  sagemaker  from  sagemaker  predictor  import  csv  serializer  wget  http  dataminingconsultant  com  DKD2e  data  sets  zip  unzip  DKD2e  data  sets  zipchurn  pd  read  csv  Data  sets  churn  txt  pd  set  option  display  max  columns  500  churn  Frequency  tables  for  each  categorical  feature  for  column  in  churn  select  dtypes  include  object  columns  display  pd  crosstab  index  churn  column  columns  observations  normalize  columns  Histograms  for  each  numeric  features  display  churn  describe  matplotlib  inline  hist  churn  h,amazon
ist  bins  30  sharey  True  figsize  10  10  churn  churn  drop  Phone  axis  churn  Area  Code  churn  Area  Code  astype  object  for  column  in  churn  select  dtypes  include  object  columns  if  column  Churn  display  pd  crosstab  index  churn  column  columns  churn  Churn  normalize  columns  for  column  in  churn  select  dtypes  exclude  object  columns  print  column  hist  churn  column  Churn  hist  by  Churn  bins  30  plt  show  display  churn  corr  pd  plotting  scatter  matrix  churn  figsize  12  12  plt  show  churn  churn  drop  Day  Charge  Eve  Charge  Night  Charge  Intl  Charge  axis  model  data  pd  get  dummies  churn  model  data  pd  concat  model  data  Churn  True  model  data  drop  Churn  False  Churn  True  axis  axis  train  data  validation  data  test  data  np  split  model  data  sample  frac  random  state  1729  int  len  model  data  int  len  model  data  train  data  to  csv  train  csv  header  False  index  False  validation  data  to  csv  validation  csv  ,amazon
header  False  index  False  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  train  csv  upload  file  train  csv  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  validation  csv  upload  file  validation  csv  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  training  image  get  image  uri  boto3  Session  region  name  xgboost  s3  input  train  s3  train  format  bucket  prefix  s3  input  validation  s3  validation  format  bucket  prefix  print  s3  input  train  from  time  import  gmtime  strftime  sleep  tuning  job  name  xgboost  tuningjob  strftime  gmtime  print  tuning  job  name  tuning  job  config  ParameterRanges  CategoricalParameterRanges  ContinuousParameterRanges  MaxValue  MinValue  Name  eta  MaxValue  10  MinValue  Name  min  child  weight  MaxValue  MinValue  Name  alpha  IntegerParameterRanges  MaxValue  10  MinValue  Name  max  depth  ResourceLimits  MaxNumberOfTrainingJobs  40  MaxPa,amazon
rallelTrainingJobs  Strategy  Bayesian  HyperParameterTuningJobObjective  MetricName  validation  auc  Type  Maximize  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  training  job  definition  AlgorithmSpecification  TrainingImage  training  image  TrainingInputMode  File  InputDataConfig  ChannelName  train  CompressionType  None  ContentType  csv  DataSource  S3DataSource  S3DataDistributionType  FullyReplicated  S3DataType  S3Prefix  S3Uri  s3  input  train  ChannelName  validation  CompressionType  None  ContentType  csv  DataSource  S3DataSource  S3DataDistributionType  FullyReplicated  S3DataType  S3Prefix  S3Uri  s3  input  validation  OutputDataConfig  S3OutputPath  s3  output  format  bucket  prefix  ResourceConfig  InstanceCount  InstanceType  ml  m4  xlarge  VolumeSizeInGB  10  RoleArn  role  StaticHyperParameters  eval  metric  auc  num  round  100  objective  binary  logistic  rate  drop  tweedie  variance  power  StoppingCondition  MaxRuntimeInSeconds  43200  region  boto3 ,amazon
 Session  region  name  client  boto3  Session  client  sagemaker  client  create  hyper  parameter  tuning  job  HyperParameterTuningJobName  tuning  job  name  HyperParameterTuningJobConfig  tuning  job  config  TrainingJobDefinition  training  job  definition  client  describe  hyper  parameter  tuning  job  HyperParameterTuningJobName  tuning  job  name  HyperParameterTuningJobStatus  xgb  predictor  sagemaker  predictor  RealTimePredictor  XGboost  churn  best2  endpoint  xgb  predictor  content  type  text  csv  xgb  predictor  serializer  csv  serializer  xgb  predictor  deserializer  Nonedef  predict  data  rows  500  split  array  np  array  split  data  int  data  shape  float  rows  predictions  for  array  in  split  array  predictions  join  predictions  xgb  predictor  predict  array  decode  utf  return  np  fromstring  predictions  sep  predictions  predict  test  data  as  matrix  pd  crosstab  index  test  data  iloc  columns  np  round  predictions  rownames  actual  colnames  predictions  ,amazon
plt  hist  predictions  plt  show  pd  crosstab  index  test  data  iloc  columns  np  where  predictions  target  clients  np  where  predictions  print  Following  clients  have  been  determined  as  high  risk  of  churn  and  should  be  engaged  target  clients  from  sklearn  metrics  import  accuracy  score  f1  score  precision  score  recall  score  classification  report  confusion  matrix  roc  curve  from  sklearn  metrics  import  precision  recall  curve  average  precision  score  precision  recall  fscore  supportdef  f1  get  label  bin  if  cont  else  for  cont  in  change  the  prob  to  class  output  return  f1  f1  score  bin  def  print  evaluation  metric  true  pred  precision  recall  fscore  support  precision  recall  fscore  support  true  pred  print  Precision  format  precision  print  Recall  format  recall  print  score  format  fscore  print  Support  format  support  return  def  plot  roc  curve  true  prob  fpr  tpr  threshold  roc  curve  true  prob  fig  plt  gcf  fig,amazon
  set  size  inches  10  plt  title  Receiver  Operating  Characteristic  ROC  plt  plot  fpr  tpr  plt  plot  plt  xlim  plt  ylim  plt  ylabel  True  Positive  Rate  plt  xlabel  False  Positive  Rate  plt  show  returnprint  evaluation  metric  test  data  iloc  np  where  predictions  plot  roc  curve  test  data  iloc  predictions  sagemaker  Session  delete  endpoint  xgb  predictor  endpoint  ,amazon
import  sagemaker  role  sagemaker  get  execution  role  import  sagemaker  import  boto3  from  sagemaker  mxnet  import  MXNet  from  sagemaker  tuner  import  IntegerParameter  CategoricalParameter  ContinuousParameter  HyperparameterTunerregion  boto3  Session  region  name  train  data  location  s3  sagemaker  sample  data  mxnet  mnist  train  format  region  test  data  location  s3  sagemaker  sample  data  mxnet  mnist  test  format  region  cat  mnist  pyestimator  MXNet  entry  point  mnist  py  role  role  train  instance  count  train  instance  type  ml  m4  xlarge  sagemaker  session  sagemaker  Session  base  job  name  DEMO  hpo  mxnet  hyperparameters  batch  size  100  hyperparameter  ranges  optimizer  CategoricalParameter  sgd  Adam  learning  rate  ContinuousParameter  01  num  epoch  IntegerParameter  10  50  objective  metric  name  Validation  accuracy  metric  definitions  Name  Validation  accuracy  Regex  Validation  accuracy  tuner  HyperparameterTuner  estimator  objective  met,amazon
ric  name  hyperparameter  ranges  metric  definitions  max  jobs  max  parallel  jobs  tuner  fit  train  train  data  location  test  test  data  location  boto3  client  sagemaker  describe  hyper  parameter  tuning  job  HyperParameterTuningJobName  tuner  latest  tuning  job  job  name  HyperParameterTuningJobStatus  ,amazon
matplotlib  inline  import  sys  from  urllib  request  import  urlretrieve  import  zipfile  from  dateutil  parser  import  parse  import  json  from  random  import  shuffle  import  random  import  datetime  import  os  import  boto3  import  s3fs  import  sagemaker  import  numpy  as  np  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  from  future  import  print  function  from  ipywidgets  import  interact  interactive  fixed  interact  manual  import  ipywidgets  as  widgets  from  ipywidgets  import  IntSlider  FloatSlider  Checkbox  set  random  seeds  for  reproducibility  np  random  seed  42  random  seed  42  col  names  for  in  range  1000  col  names  append  patient  str  df  pd  DataFrame  columns  col  names  df  loc  100  1000  np  random  randint  10  100  1000  for  in  range  25000  randn  np  random  randint  1000  np  random  randint  df  loc  df  loc  randn  df  loc  df  loc  100df  index  pd  to  datetime  df  index  unit  origin  pd  Timestamp  2015  01  01  freq  sag,amazon
emaker  session  sagemaker  Session  s3  bucket  sagemaker  Session  default  bucket  replace  with  an  existing  bucket  if  needed  s3  prefix  deepar  electricity  demo  notebook  prefix  used  for  all  data  stored  within  the  bucket  role  sagemaker  get  execution  role  IAM  role  to  use  by  SageMakerregion  sagemaker  session  boto  region  name  s3  data  path  s3  data  format  s3  bucket  s3  prefix  s3  output  path  s3  output  format  s3  bucket  s3  prefix  image  name  sagemaker  amazon  amazon  estimator  get  image  uri  region  forecasting  deepar  latest  num  timeseries  df  shape  timeseries  for  in  range  num  timeseries  timeseries  append  np  trim  zeros  df  iloc  trim  fig  axs  plt  subplots  figsize  20  20  sharex  True  axx  axs  ravel  for  in  range  10  timeseries  loc  2015  01  01  2017  06  01  plot  ax  axx  axx  set  xlabel  date  axx  set  ylabel  Inhaler  consumption  axx  grid  which  minor  axis  we  use  hour  frequency  for  the  time  series  freq  we  pr,amazon
edict  for  days  prediction  length  12  we  also  use  days  as  context  length  this  is  the  number  of  state  updates  accomplished  before  making  predictions  context  length  12start  dataset  pd  Timestamp  2015  01  01  00  00  00  freq  freq  end  training  pd  Timestamp  2017  01  01  00  00  00  freq  freq  training  data  start  str  start  dataset  target  ts  start  dataset  end  training  tolist  We  use  because  pandas  indexing  includes  the  upper  bound  for  ts  in  timeseries  print  len  training  data  num  test  windows  test  data  start  str  start  dataset  target  ts  start  dataset  end  training  prediction  length  tolist  for  in  range  num  test  windows  for  ts  in  timeseries  print  len  test  data  def  write  dicts  to  file  path  data  with  open  path  wb  as  fp  for  in  data  fp  write  json  dumps  encode  utf  fp  write  encode  utf  time  write  dicts  to  file  train  json  training  data  write  dicts  to  file  test  json  test  data  s3  boto3  reso,amazon
urce  s3  def  copy  to  s3  local  file  s3  path  override  False  assert  s3  path  startswith  s3  split  s3  path  split  bucket  split  path  join  split  buk  s3  Bucket  bucket  if  len  list  buk  objects  filter  Prefix  path  if  not  override  print  File  s3  already  exists  nSet  override  to  upload  anyway  format  s3  bucket  s3  path  return  else  print  Overwriting  existing  file  with  open  local  file  rb  as  data  print  Uploading  file  to  format  s3  path  buk  put  object  Key  path  Body  data  time  copy  to  s3  train  json  s3  data  path  train  train  json  override  True  copy  to  s3  test  json  s3  data  path  test  test  json  override  True  s3filesystem  s3fs  S3FileSystem  with  s3filesystem  open  s3  data  path  train  train  json  rb  as  fp  print  fp  readline  decode  utf  100  estimator  sagemaker  estimator  Estimator  sagemaker  session  sagemaker  session  image  name  image  name  role  role  train  instance  count  train  instance  type  ml  c4  2xlarge,amazon
  base  job  name  deepar  electricity  demo  output  path  s3  output  path  hyperparameters  time  freq  freq  epochs  400  early  stopping  patience  40  mini  batch  size  64  learning  rate  5E  context  length  str  context  length  prediction  length  str  prediction  length  estimator  set  hyperparameters  hyperparameters  time  data  channels  train  train  format  s3  data  path  test  test  format  s3  data  path  estimator  fit  inputs  data  channels  wait  True  class  DeepARPredictor  sagemaker  predictor  RealTimePredictor  def  init  self  args  kwargs  super  init  args  content  type  sagemaker  content  types  CONTENT  TYPE  JSON  kwargs  def  predict  self  ts  cat  None  dynamic  feat  None  num  samples  100  return  samples  False  quantiles  Requests  the  prediction  of  for  the  time  series  listed  in  ts  each  with  the  optional  corresponding  category  listed  in  cat  ts  pandas  Series  object  the  time  series  to  predict  cat  integer  the  group  associated  to  the ,amazon
 time  series  default  None  num  samples  integer  number  of  samples  to  compute  at  prediction  time  default  100  return  samples  boolean  indicating  whether  to  include  samples  in  the  response  default  False  quantiles  list  of  strings  specifying  the  quantiles  to  compute  default  Return  value  list  of  pandas  DataFrame  objects  each  containing  the  predictions  prediction  time  ts  index  quantiles  str  for  in  quantiles  req  self  encode  request  ts  cat  dynamic  feat  num  samples  return  samples  quantiles  res  super  DeepARPredictor  self  predict  req  return  self  decode  response  res  ts  index  freq  prediction  time  return  samples  def  encode  request  self  ts  cat  dynamic  feat  num  samples  return  samples  quantiles  instance  series  to  dict  ts  cat  if  cat  is  not  None  else  None  dynamic  feat  if  dynamic  feat  else  None  configuration  num  samples  num  samples  output  types  quantiles  samples  if  return  samples  else  quantiles  qu,amazon
antiles  quantiles  http  request  data  instances  instance  configuration  configuration  return  json  dumps  http  request  data  encode  utf  def  decode  response  self  response  freq  prediction  time  return  samples  we  only  sent  one  time  series  so  we  only  receive  one  in  return  however  if  possible  one  will  pass  multiple  time  series  as  predictions  will  then  be  faster  predictions  json  loads  response  decode  utf  predictions  prediction  length  len  next  iter  predictions  quantiles  values  prediction  index  pd  DatetimeIndex  start  prediction  time  freq  freq  periods  prediction  length  if  return  samples  dict  of  samples  sample  str  for  in  enumerate  predictions  samples  else  dict  of  samples  return  pd  DataFrame  data  predictions  quantiles  dict  of  samples  index  prediction  index  def  set  frequency  self  freq  self  freq  freq  def  encode  target  ts  return  if  np  isfinite  else  NaN  for  in  ts  def  series  to  dict  ts  cat  None  ,amazon
dynamic  feat  None  Given  pandas  Series  object  returns  dictionary  encoding  the  time  series  ts  pands  Series  object  with  the  target  time  series  cat  an  integer  indicating  the  time  series  category  Return  value  dictionary  obj  start  str  ts  index  target  encode  target  ts  if  cat  is  not  None  obj  cat  cat  if  dynamic  feat  is  not  None  obj  dynamic  feat  dynamic  feat  return  objpredictor  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  predictor  cls  DeepARPredictor  predictor  predict  ts  timeseries  120  asfreq  quantiles  10  90  tail  def  plot  predictor  target  ts  cat  None  dynamic  feat  None  forecast  date  end  training  show  samples  False  plot  history  12  confidence  80  print  calling  served  model  to  generate  predictions  starting  from  format  str  forecast  date  assert  confidence  50  and  confidence  100  low  quantile  confidence  005  up  quantile  confidence  005  we  first  construct  the  argument  to ,amazon
 call  our  model  args  ts  target  ts  forecast  date  return  samples  show  samples  quantiles  low  quantile  up  quantile  num  samples  100  if  dynamic  feat  is  not  None  args  dynamic  feat  dynamic  feat  fig  plt  figure  figsize  20  ax  plt  subplot  else  fig  plt  figure  figsize  20  ax  plt  subplot  if  cat  is  not  None  args  cat  cat  ax  text  cat  format  cat  transform  ax  transAxes  call  the  end  point  to  get  the  prediction  prediction  predictor  predict  args  plot  the  samples  if  show  samples  for  key  in  prediction  keys  if  sample  in  key  prediction  key  plot  color  lightskyblue  alpha  label  nolegend  plot  the  target  target  section  target  ts  forecast  date  plot  history  forecast  date  prediction  length  target  section  plot  color  black  label  target  plot  the  confidence  interval  and  the  median  predicted  ax  fill  between  prediction  str  low  quantile  index  prediction  str  low  quantile  values  prediction  str  up  quantile  val,amazon
ues  color  alpha  label  confidence  interval  format  confidence  prediction  plot  color  label  P50  ax  legend  loc  for  in  prediction  if  30  ax  axvline  prediction  index  color  linestyle  lw  fix  the  scale  as  the  samples  may  change  it  ax  set  ylim  target  section  min  target  section  max  if  dynamic  feat  is  not  None  for  in  enumerate  dynamic  feat  start  ax  plt  subplot  len  dynamic  feat  len  dynamic  feat  sharex  ax  feat  ts  pd  Series  index  pd  DatetimeIndex  start  target  ts  index  freq  target  ts  index  freq  periods  len  data  feat  ts  forecast  date  plot  history  forecast  date  prediction  length  plot  ax  ax  color  style  description  width  initial  interact  manual  patient  id  IntSlider  min  max  369  value  94  style  style  forecast  hour  IntSlider  min  max  300  value  20  style  style  confidence  IntSlider  min  60  max  95  value  80  step  style  style  history  weeks  plot  IntSlider  min  max  30  value  10  style  style  show  samp,amazon
les  Checkbox  value  False  continuous  update  False  def  plot  interact  patient  id  forecast  hour  confidence  history  weeks  plot  show  samples  plot  predictor  target  ts  timeseries  patient  id  asfreq  forecast  date  end  training  datetime  timedelta  days  forecast  hour  show  samples  show  samples  plot  history  history  weeks  plot  12  confidence  confidence  def  create  special  day  feature  ts  fraction  05  First  select  random  day  indices  plus  the  forecast  day  num  days  ts  index  ts  index  days  rand  indices  list  np  random  randint  num  days  int  num  days  num  days  feature  value  np  zeros  like  ts  for  in  rand  indices  feature  value  12  12  feature  pd  Series  index  ts  index  data  feature  value  return  feature  def  drop  at  random  ts  drop  probability  assert  drop  probability  random  mask  np  random  random  len  ts  drop  probability  return  ts  mask  random  mask  special  day  features  create  special  day  feature  ts  for  ts  in  ,amazon
timeseries  timeseries  uplift  ts  feat  for  ts  feat  in  zip  timeseries  special  day  features  time  series  processed  drop  at  random  ts  for  ts  in  timeseries  uplift  fig  axs  plt  subplots  figsize  20  20  sharex  True  axx  axs  ravel  for  in  range  10  ax  axx  ts  time  series  processed  400  ts  plot  ax  ax  ax  set  ylim  ts  max  ts  max  ax2  ax  twinx  special  day  features  400  plot  ax  ax2  color  ax2  set  ylim  time  training  data  new  features  start  str  start  dataset  target  encode  target  ts  start  dataset  end  training  dynamic  feat  special  day  features  start  dataset  end  training  tolist  for  ts  in  enumerate  time  series  processed  print  len  training  data  new  features  as  in  our  previous  example  we  do  rolling  evaluation  over  the  next  days  num  test  windows  test  data  new  features  start  str  start  dataset  target  encode  target  ts  start  dataset  end  training  prediction  length  dynamic  feat  special  day  features  s,amazon
tart  dataset  end  training  prediction  length  tolist  for  in  range  num  test  windows  for  ts  in  enumerate  timeseries  uplift  def  check  dataset  consistency  train  dataset  test  dataset  None  train  dataset  has  dynamic  feat  dynamic  feat  in  if  has  dynamic  feat  num  dynamic  feat  len  dynamic  feat  has  cat  cat  in  if  has  cat  num  cat  len  cat  def  check  ds  ds  for  in  enumerate  ds  if  has  dynamic  feat  assert  dynamic  feat  in  assert  num  dynamic  feat  len  dynamic  feat  for  in  dynamic  feat  assert  len  target  len  if  has  cat  assert  cat  in  assert  len  cat  num  cat  check  ds  train  dataset  if  test  dataset  is  not  None  check  ds  test  dataset  check  dataset  consistency  training  data  new  features  test  data  new  features  time  write  dicts  to  file  train  new  features  json  training  data  new  features  write  dicts  to  file  test  new  features  json  test  data  new  features  time  s3  data  path  new  features  s3  new  feat,amazon
ures  data  format  s3  bucket  s3  prefix  s3  output  path  new  features  s3  new  features  output  format  s3  bucket  s3  prefix  print  Uploading  to  S3  this  may  take  few  minutes  depending  on  your  connection  copy  to  s3  train  new  features  json  s3  data  path  new  features  train  train  new  features  json  override  True  copy  to  s3  test  new  features  json  s3  data  path  new  features  test  test  new  features  json  override  True  time  estimator  new  features  sagemaker  estimator  Estimator  sagemaker  session  sagemaker  session  image  name  image  name  role  role  train  instance  count  train  instance  type  ml  c4  2xlarge  base  job  name  deepar  electricity  demo  new  features  output  path  s3  output  path  new  features  hyperparameters  time  freq  freq  context  length  str  context  length  prediction  length  str  prediction  length  epochs  400  learning  rate  5E  mini  batch  size  64  early  stopping  patience  40  num  dynamic  feat  auto  this  wi,amazon
ll  use  the  dynamic  feat  field  if  it  present  in  the  data  estimator  new  features  set  hyperparameters  hyperparameters  estimator  new  features  fit  inputs  train  train  format  s3  data  path  new  features  test  test  format  s3  data  path  new  features  wait  True  time  predictor  new  features  estimator  new  features  deploy  initial  instance  count  instance  type  ml  m4  xlarge  predictor  cls  DeepARPredictor  customer  id  120  predictor  new  features  predict  ts  time  series  processed  customer  id  prediction  length  dynamic  feat  special  day  features  customer  id  tolist  quantiles  head  interact  manual  customer  id  IntSlider  min  max  369  value  13  style  style  forecast  day  IntSlider  min  max  100  value  21  style  style  confidence  IntSlider  min  60  max  95  value  80  step  style  style  missing  ratio  FloatSlider  min  max  95  value  step  05  style  style  show  samples  Checkbox  value  False  continuous  update  False  def  plot  interact  cu,amazon
stomer  id  forecast  day  confidence  missing  ratio  show  samples  forecast  date  end  training  datetime  timedelta  days  forecast  day  target  time  series  processed  customer  id  start  dataset  forecast  date  prediction  length  target  drop  at  random  target  missing  ratio  dynamic  feat  special  day  features  customer  id  start  dataset  forecast  date  prediction  length  tolist  plot  predictor  new  features  target  ts  target  dynamic  feat  dynamic  feat  forecast  date  forecast  date  show  samples  show  samples  plot  history  12  confidence  confidence  predictor  delete  endpoint  predictor  new  features  delete  endpoint  ,amazon
import  sagemaker  bucket  sagemaker  Session  default  bucket  prefix  sagemaker  DEMO  hpo  byo  role  sagemaker  get  execution  role  import  os  import  boto3  import  sagemaker  from  sagemaker  tuner  import  IntegerParameter  CategoricalParameter  ContinuousParameter  HyperparameterTuner  sh  The  name  of  our  algorithm  algorithm  name  rmars  set  stop  if  anything  fails  account  aws  sts  get  caller  identity  query  Account  output  text  Get  the  region  defined  in  the  current  configuration  default  to  us  west  if  none  defined  region  aws  configure  get  region  region  region  us  west  fullname  account  dkr  ecr  region  amazonaws  com  algorithm  name  latest  If  the  repository  doesn  exist  in  ECR  create  it  aws  ecr  describe  repositories  repository  names  algorithm  name  dev  null  if  ne  then  aws  ecr  create  repository  repository  name  algorithm  name  dev  null  fi  Get  the  login  command  from  ECR  and  execute  it  directly  aws  ecr  get  login  re,amazon
gion  region  no  include  email  Build  the  docker  image  locally  with  the  image  name  and  then  push  it  to  ECR  with  the  full  name  docker  build  algorithm  name  docker  tag  algorithm  name  fullname  docker  push  fullname  train  file  iris  csv  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  train  file  upload  file  train  file  region  boto3  Session  region  name  account  boto3  client  sts  get  caller  identity  get  Account  estimator  sagemaker  estimator  Estimator  image  name  dkr  ecr  amazonaws  com  rmars  latest  format  account  region  role  role  train  instance  count  train  instance  type  ml  m4  xlarge  output  path  s3  output  format  bucket  prefix  sagemaker  session  sagemaker  Session  hyperparameters  target  Sepal  Length  hyperparameter  ranges  degree  IntegerParameter  thresh  ContinuousParameter  001  01  prune  CategoricalParameter  TRUE  FALSE  objective  metric  name  mse  metric  definitions  Name  mse  Regex  m,amazon
se  tuner  HyperparameterTuner  estimator  objective  metric  name  hyperparameter  ranges  metric  definitions  objective  type  Minimize  max  jobs  max  parallel  jobs  tuner  fit  train  s3  train  format  bucket  prefix  boto3  client  sagemaker  describe  hyper  parameter  tuning  job  HyperParameterTuningJobName  tuner  latest  tuning  job  job  name  HyperParameterTuningJobStatus  ,amazon
sh  docker  rmi  docker  images  true  docker  ps  status  exited  xargs  100  docker  rm  run  this  to  clear  out  stale  docker  images  and  containers  sometimes  they  bake  bad  things  into  thier  layers  ignore  errors  bash  cd  container  chmod  755  build  push  sh  build  push  sh  breast  cancer  nb  bh3  you  can  it  appears  you  just  needed  to  chmod  the  build  push  sh  also  sh  might  be  little  more  standard  although  totally  not  sure  and  the  script  headlines  usr  bin  env  bash  source  so  instead  of  source  script  you  can  just  script  for  convenience  bh  S3  prefix  prefix  breastCancerNB  Define  IAM  role  import  boto3  import  re  import  os  import  numpy  as  np  import  pandas  as  pd  import  sagemaker  as  sage  from  time  import  gmtime  strftime  sess  sage  Session  role  sage  get  execution  role  if  not  os  path  exists  training  data  breast  cancer  csv  from  sklearn  datasets  import  load  breast  cancer  import  pandas  as  pd  breast  ,amazon
cancer  load  breast  cancer  data  pd  concat  pd  Series  breast  cancer  target  pd  DataFrame  breast  cancer  data  axis  data  to  csv  training  data  breast  cancer  csv  header  False  index  False  WORK  DIRECTORY  training  data  data  location  sess  upload  data  WORK  DIRECTORY  key  prefix  prefix  print  Contents  of  directory  now  in  S3  at  format  WORK  DIRECTORY  data  location  account  sess  boto  session  client  sts  get  caller  identity  Account  region  sess  boto  session  region  name  image  dkr  ecr  amazonaws  com  breast  cancer  nb  format  account  region  print  Location  of  Docker  image  is  format  image  time  from  sagemaker  estimator  import  Estimator  output  path  s3  output  format  sess  default  bucket  clf  Estimator  image  name  image  role  role  train  instance  count  train  instance  type  ml  c4  2xlarge  output  path  output  path  sagemaker  session  sess  clf  fit  data  location  time  from  sagemaker  predictor  import  csv  serializer  predict,amazon
or  clf  deploy  initial  instance  count  instance  type  ml  m4  xlarge  serializer  csv  serializer  ,amazon
import  os  import  sagemaker  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  import  utils  utils  cifar10  download  inputs  sagemaker  session  upload  data  path  tmp  cifar10  data  key  prefix  data  DEMO  cifar10  from  sagemaker  tensorflow  import  TensorFlow  source  dir  os  path  join  os  getcwd  source  dir  estimator  TensorFlow  entry  point  resnet  cifar  10  py  source  dir  source  dir  role  role  hyperparameters  min  eval  frequency  10  training  steps  1000  evaluation  steps  100  train  instance  count  train  instance  type  ml  c4  xlarge  base  job  name  tensorboard  example  estimator  fit  inputs  run  tensorboard  locally  True  predictor  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  import  numpy  as  np  random  image  data  np  random  rand  32  32  predictor  predict  random  image  data  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
git  clone  git  github  com  marcobardoscia  neva  git  cd  neva  python  setup  py  install  cd  home  nbuser  library  Move  back  up  to  home  nbuser  library  Append  the  package  to  the  path  import  sys  sys  path  append  home  nbuser  library  neva  data  Still  can  find  the  data  Have  to  cd  there  sys  path  append  home  nbuser  anaconda3  501  lib  python3  site  packages  neva  py3  egg  Change  directory  to  where  the  neva  data  files  are  cd  home  nbuser  library  neva  dataimport  nevahelp  neva  parsing  data  bsys  params  neva  parse  csv  balance  sheets  csv  exposures  table  csv  running  Eisenberg  and  Noe  without  any  shock  to  equities  equity  shock  for  in  bsys  neva  shock  and  solve  bsys  equity  shock  method  eisenberg  noe  solve  assets  False  reading  final  equities  equity  final  bsys  history  equity  final  computing  payment  vectors  pay  vec  bnk  ibliabtot  if  bnk  equity  else  max  bnk  equity  bnk  ibliabtot  for  bnk  in  bsys  pay  vec,microsoft
  Change  directory  to  where  the  neva  data  files  are  cd  home  nbuser  library  neva  dataimport  neva  parsing  data  bsys  params  neva  parse  csv  balance  sheets  csv  exposures  table  csv  Geometric  Browianian  Motion  on  external  assets  whose  volatility  is  estimated  via  the  volatility  of  equities  sigma  equity  float  params  bnk  sigma  equity  for  bnk  in  params  bsys  neva  BankingSystemGBMse  with  sigma  equity  bsys  sigma  equity  storing  initial  equity  equity  start  bsys  get  equity  shocks  to  initial  equity  50  equity  delta  equity  start  equity  delta  for  in  equity  start  running  ex  ante  Black  and  Cox  as  in  with  recovery  rate  equal  to  60  recovery  rate  for  in  bsys  neva  shock  and  solve  bsys  equity  delta  exante  en  blackcox  gbm  solve  assets  False  recovery  rate  recovery  rate  reading  equities  after  one  round  and  after  all  rounds  equity  direct  bsys  history  equity  final  bsys  history  ,microsoft
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  videogames  xgboost  Define  IAM  role  import  sagemaker  role  sagemaker  get  execution  role  import  numpy  as  np  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  from  IPython  display  import  Image  from  IPython  display  import  display  from  sklearn  datasets  import  dump  svmlight  file  from  time  import  gmtime  strftime  import  sys  import  math  import  json  import  boto3raw  data  filename  Video  Games  Sales  as  at  22  Dec  2016  csv  s3  boto3  resource  s3  s3  Bucket  bucket  download  file  prefix  raw  data  filename  raw  data  csv  data  pd  read  csv  raw  data  csv  pd  set  option  display  max  rows  20  datadata  data  Global  Sales  plt  bar  not  hit  hit  data  value  counts  plt  show  viz  data  filter  User  Score  Critic  Score  Global  Sales  axis  viz  User  Score  pd  Series  viz  User  Score  apply  pd  to  numeric  errors  coerce  viz  User  Score  viz  User  Score  mask  np  isnan  v,amazon
iz  User  Score  viz  Critic  Score  10  viz  plot  kind  scatter  logx  True  logy  True  Critic  Score  Global  Sales  viz  plot  kind  scatter  logx  True  logy  True  User  Score  Global  Sales  plt  show  data  data  drop  Name  Year  of  Release  NA  Sales  EU  Sales  JP  Sales  Other  Sales  Global  Sales  Critic  Count  User  Count  Developer  axis  data  isnull  sum  data  data  dropna  data  User  Score  data  User  Score  apply  pd  to  numeric  errors  coerce  data  User  Score  data  User  Score  mask  np  isnan  data  User  Score  data  Critic  Score  10  data  data  apply  lambda  yes  if  True  else  no  model  data  pd  get  dummies  data  train  data  validation  data  test  data  np  split  model  data  sample  frac  random  state  1729  int  len  model  data  int  len  model  data  dump  svmlight  file  train  data  drop  no  yes  axis  train  data  yes  train  libsvm  dump  svmlight  file  validation  data  drop  no  yes  axis  validation  data  yes  validation  libsvm  dump  svmlight  fi,amazon
le  test  data  drop  no  yes  axis  test  data  yes  test  libsvm  boto3  Session  resource  s3  Bucket  bucket  Object  prefix  train  train  libsvm  upload  file  train  libsvm  boto3  Session  resource  s3  Bucket  bucket  Object  prefix  validation  validation  libsvm  upload  file  validation  libsvm  job  name  DEMO  videogames  xgboost  strftime  gmtime  print  Training  job  job  name  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  xgboost  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  xgboost  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  xgboost  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  xgboost  latest  create  training  params  RoleArn  role  TrainingJobName  job  name  AlgorithmSpecification  TrainingImage  containers  boto3  Session  region  name  TrainingInputMode  File  ResourceConfig  InstanceCount  InstanceType  ml  c4  xlarge  VolumeSizeInGB  10  InputDataConfig  ChannelName  train  DataSource  S,amazon
3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  S3DataDistributionType  FullyReplicated  ContentType  libsvm  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  prefix  S3DataDistributionType  FullyReplicated  ContentType  libsvm  CompressionType  None  OutputDataConfig  S3OutputPath  s3  xgboost  video  games  output  format  bucket  prefix  HyperParameters  max  depth  eta  eval  metric  auc  scale  pos  weight  subsample  objective  binary  logistic  num  round  100  StoppingCondition  MaxRuntimeInSeconds  60  60  time  sm  boto3  client  sagemaker  sm  create  training  job  create  training  params  status  sm  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  status  try  sm  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  finally  status  sm  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  Train,amazon
ing  job  ended  with  status  status  if  status  Failed  message  sm  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  create  model  response  sm  create  model  ModelName  job  name  ExecutionRoleArn  role  PrimaryContainer  Image  containers  boto3  Session  region  name  ModelDataUrl  sm  describe  training  job  TrainingJobName  job  name  ModelArtifacts  S3ModelArtifacts  print  create  model  response  ModelArn  xgboost  endpoint  config  DEMO  videogames  xgboost  config  strftime  gmtime  print  xgboost  endpoint  config  create  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  xgboost  endpoint  config  ProductionVariants  InstanceType  ml  t2  medium  InitialInstanceCount  ModelName  job  name  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  xgboost  endpoint  DEMO  videog,amazon
ames  xgboost  endpoint  strftime  gmtime  print  xgboost  endpoint  create  endpoint  response  sm  create  endpoint  EndpointName  xgboost  endpoint  EndpointConfigName  xgboost  endpoint  config  print  create  endpoint  response  EndpointArn  resp  sm  describe  endpoint  EndpointName  xgboost  endpoint  status  resp  EndpointStatus  print  Status  status  try  sm  get  waiter  endpoint  in  service  wait  EndpointName  xgboost  endpoint  finally  resp  sm  describe  endpoint  EndpointName  xgboost  endpoint  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Status  status  if  status  InService  message  sm  describe  endpoint  EndpointName  xgboost  endpoint  FailureReason  print  Endpoint  creation  failed  with  the  following  error  format  message  raise  Exception  Endpoint  creation  did  not  succeed  runtime  boto3  client  runtime  sagemaker  def  do  predict  data  endpoint  name  content  type  payload  join  data  response  runtime  invoke  endpoint  EndpointName  endpoint,amazon
  name  ContentType  content  type  Body  payload  result  response  Body  read  result  result  decode  utf  result  result  split  preds  float  num  for  num  in  result  preds  round  num  for  num  in  preds  return  preds  def  batch  predict  data  batch  size  endpoint  name  content  type  items  len  data  arrs  for  offset  in  range  items  batch  size  if  offset  batch  size  items  results  do  predict  data  offset  offset  batch  size  endpoint  name  content  type  arrs  extend  results  else  arrs  extend  do  predict  data  offset  items  endpoint  name  content  type  sys  stdout  write  return  arrs  time  import  json  with  open  test  libsvm  as  payload  read  strip  labels  int  line  split  for  line  in  payload  split  test  data  line  for  line  in  payload  split  preds  batch  predict  test  data  100  xgboost  endpoint  text  libsvm  print  nerror  rate  sum  for  in  range  len  preds  if  preds  labels  float  len  preds  pd  crosstab  index  np  array  labels  columns  np,amazon
  array  preds  sm  delete  endpoint  EndpointName  xgboost  endpoint  ,amazon
change  these  to  try  this  notebook  out  BUCKET  mlroadshowdelhi  PROJECT  qwiklabs  gcp  4cb3310e6378752b  REGION  us  central1  import  os  os  environ  BUCKET  BUCKET  os  environ  PROJECT  PROJECT  os  environ  REGION  REGION  bash  if  gsutil  ls  grep  gs  BUCKET  then  gsutil  mb  REGION  gs  BUCKET  fiquery  SELECT  weight  pounds  is  male  mother  age  mother  race  plurality  gestation  weeks  mother  married  ever  born  cigarette  use  alcohol  use  FARM  FINGERPRINT  CONCAT  CAST  YEAR  AS  STRING  CAST  month  AS  STRING  AS  hashmonth  FROM  publicdata  samples  natality  WHERE  year  2000  import  google  datalab  bigquery  as  bq  df  bq  Query  query  LIMIT  100  execute  result  to  dataframe  df  head  def  get  distinct  values  column  name  sql  SELECT  COUNT  AS  num  babies  AVG  weight  pounds  AS  avg  wt  FROM  publicdata  samples  natality  WHERE  year  2000  GROUP  BY  format  column  name  return  bq  Query  sql  execute  result  to  dataframe  df  get  distinct  values  mo,google
ther  race  df  plot  mother  race  logy  num  babies  kind  bar  df  plot  mother  race  avg  wt  kind  bar  df  get  distinct  values  is  male  df  plot  is  male  logy  num  babies  kind  bar  df  plot  is  male  avg  wt  kind  bar  df  get  distinct  values  mother  age  df  df  sort  values  mother  age  df  plot  mother  age  num  babies  df  plot  mother  age  avg  wt  df  get  distinct  values  plurality  df  df  sort  values  plurality  df  plot  plurality  logy  num  babies  kind  bar  df  plot  plurality  avg  wt  kind  bar  df  get  distinct  values  gestation  weeks  df  df  sort  values  gestation  weeks  df  plot  gestation  weeks  logy  num  babies  kind  bar  df  plot  gestation  weeks  avg  wt  kind  bar  df  get  distinct  values  mother  married  df  plot  mother  married  logy  num  babies  kind  bar  df  plot  mother  married  avg  wt  kind  bar  df  get  distinct  values  ever  born  df  df  sort  values  ever  born  df  plot  ever  born  logy  num  babies  kind  bar  df  plot  ever  b,google
orn  avg  wt  kind  bar  df  get  distinct  values  cigarette  use  df  plot  cigarette  use  logy  num  babies  kind  bar  df  plot  cigarette  use  avg  wt  kind  bar  df  get  distinct  values  alcohol  use  df  plot  alcohol  use  logy  num  babies  kind  bar  df  plot  alcohol  use  avg  wt  kind  bar  import  apache  beam  as  beam  import  datetime  def  to  csv  rowdict  pull  columns  from  BQ  and  create  line  import  hashlib  import  copy  CSV  COLUMNS  weight  pounds  is  male  mother  age  mother  race  plurality  gestation  weeks  mother  married  cigarette  use  alcohol  use  split  modify  opaque  numeric  race  code  into  human  readable  data  races  dict  zip  18  28  39  48  White  Black  American  Indian  Chinese  Japanese  Hawaiian  Filipino  Asian  Indian  Korean  Samaon  Vietnamese  result  copy  deepcopy  rowdict  if  mother  race  in  rowdict  and  rowdict  mother  race  in  races  result  mother  race  races  rowdict  mother  race  else  result  mother  race  Unknown  data  join ,google
 str  result  if  in  result  else  None  for  in  CSV  COLUMNS  key  hashlib  sha224  data  hexdigest  hash  the  columns  to  form  key  return  str  format  data  key  def  preprocess  in  test  mode  job  name  preprocess  babyweight  features  datetime  datetime  now  strftime  if  in  test  mode  print  Launching  local  job  hang  on  OUTPUT  DIR  preproc  else  print  Launching  Dataflow  job  hang  on  format  job  name  OUTPUT  DIR  gs  babyweight  preproc  format  BUCKET  options  staging  location  os  path  join  OUTPUT  DIR  tmp  staging  temp  location  os  path  join  OUTPUT  DIR  tmp  job  name  job  name  project  PROJECT  teardown  policy  TEARDOWN  ALWAYS  no  save  main  session  True  opts  beam  pipeline  PipelineOptions  flags  options  if  in  test  mode  RUNNER  DirectRunner  else  RUNNER  DataflowRunner  beam  Pipeline  RUNNER  options  opts  query  SELECT  weight  pounds  is  male  mother  age  mother  race  plurality  gestation  weeks  mother  married  ever  born  cigarette  use  ,google
alcohol  use  FARM  FINGERPRINT  CONCAT  CAST  YEAR  AS  STRING  CAST  month  AS  STRING  AS  hashmonth  FROM  publicdata  samples  natality  WHERE  year  2000  AND  weight  pounds  AND  mother  age  AND  plurality  AND  gestation  weeks  AND  month  if  in  test  mode  query  query  LIMIT  100  for  step  in  train  eval  if  step  train  selquery  SELECT  FROM  WHERE  MOD  ABS  hashmonth  format  query  else  selquery  SELECT  FROM  WHERE  MOD  ABS  hashmonth  format  query  read  format  step  beam  io  Read  beam  io  BigQuerySource  query  selquery  use  standard  sql  True  csv  format  step  beam  Map  to  csv  out  format  step  beam  io  Write  beam  io  WriteToText  os  path  join  OUTPUT  DIR  csv  format  step  job  run  preprocess  in  test  mode  False  bash  gsutil  ls  gs  BUCKET  babyweight  preproc  00000  import  shutil  import  tensorflow  as  tf  import  tensorflow  contrib  learn  as  tflearn  import  tensorflow  contrib  layers  as  tflayers  from  tensorflow  contrib  learn  python  le,google
arn  import  learn  runner  import  tensorflow  contrib  metrics  as  metricsCSV  COLUMNS  weight  pounds  is  male  mother  age  mother  race  plurality  gestation  weeks  mother  married  cigarette  use  alcohol  use  key  split  LABEL  COLUMN  weight  pounds  KEY  COLUMN  key  DEFAULTS  null  null  null  null  null  nokey  TRAIN  STEPS  1000  def  read  dataset  prefix  pattern  batch  size  512  use  prefix  to  create  filename  filename  gs  babyweight  preproc  format  BUCKET  prefix  pattern  if  prefix  train  mode  tf  contrib  learn  ModeKeys  TRAIN  else  mode  tf  contrib  learn  ModeKeys  EVAL  the  actual  input  function  passed  to  TensorFlow  def  input  fn  could  be  path  to  one  file  or  file  pattern  input  file  names  tf  train  match  filenames  once  filename  filename  queue  tf  train  string  input  producer  input  file  names  shuffle  True  read  CSV  reader  tf  TextLineReader  value  reader  read  up  to  filename  queue  num  records  batch  size  value  column  tf  exp,google
and  dims  value  columns  tf  decode  csv  value  column  record  defaults  DEFAULTS  features  dict  zip  CSV  COLUMNS  columns  features  pop  KEY  COLUMN  label  features  pop  LABEL  COLUMN  return  features  label  return  input  fndef  get  wide  deep  define  column  types  races  White  Black  American  Indian  Chinese  Japanese  Hawaiian  Filipino  Unknown  Asian  Indian  Korean  Samaon  Vietnamese  is  male  mother  age  mother  race  plurality  gestation  weeks  mother  married  cigarette  use  alcohol  use  tflayers  sparse  column  with  keys  is  male  keys  True  False  tflayers  real  valued  column  mother  age  tflayers  sparse  column  with  keys  mother  race  keys  races  tflayers  real  valued  column  plurality  tflayers  real  valued  column  gestation  weeks  tflayers  sparse  column  with  keys  mother  married  keys  True  False  tflayers  sparse  column  with  keys  cigarette  use  keys  True  False  None  tflayers  sparse  column  with  keys  alcohol  use  keys  True  False  None,google
  which  columns  are  wide  sparse  linear  relationship  to  output  and  which  are  deep  complex  relationship  to  output  wide  is  male  mother  race  plurality  mother  married  cigarette  use  alcohol  use  deep  mother  age  gestation  weeks  tflayers  embedding  column  mother  race  return  wide  deepdef  serving  input  fn  feature  placeholders  is  male  tf  placeholder  tf  string  None  mother  age  tf  placeholder  tf  float32  None  mother  race  tf  placeholder  tf  string  None  plurality  tf  placeholder  tf  float32  None  gestation  weeks  tf  placeholder  tf  float32  None  mother  married  tf  placeholder  tf  string  None  cigarette  use  tf  placeholder  tf  string  None  alcohol  use  tf  placeholder  tf  string  None  features  key  tf  expand  dims  tensor  for  key  tensor  in  feature  placeholders  items  return  tflearn  utils  input  fn  utils  InputFnOps  features  None  feature  placeholders  from  tensorflow  contrib  learn  python  learn  utils  import  saved  model  e,google
xport  utils  pattern  00001  of  process  only  one  of  the  shards  for  testing  purposes  def  experiment  fn  output  dir  wide  deep  get  wide  deep  return  tflearn  Experiment  tflearn  DNNLinearCombinedRegressor  model  dir  output  dir  linear  feature  columns  wide  dnn  feature  columns  deep  dnn  hidden  units  64  32  train  input  fn  read  dataset  train  pattern  eval  input  fn  read  dataset  eval  pattern  eval  metrics  rmse  tflearn  MetricSpec  metric  fn  metrics  streaming  root  mean  squared  error  export  strategies  saved  model  export  utils  make  export  strategy  serving  input  fn  default  output  alternative  key  None  exports  to  keep  train  steps  TRAIN  STEPS  shutil  rmtree  babyweight  trained  ignore  errors  True  start  fresh  each  time  learn  runner  run  experiment  fn  babyweight  trained  bash  grep  def  babyweight  trainer  model  py  bash  echo  bucket  BUCKET  rm  rf  babyweight  trained  export  PYTHONPATH  PYTHONPATH  PWD  babyweight  python  tr,google
ainer  task  bucket  BUCKET  output  dir  babyweight  trained  job  dir  tmp  pattern  00001  of  train  steps  1000  bash  OUTDIR  gs  BUCKET  babyweight  trained  model  JOBNAME  babyweight  date  echo  OUTDIR  REGION  JOBNAME  gsutil  rm  rf  OUTDIR  gcloud  ml  engine  jobs  submit  training  JOBNAME  region  REGION  module  name  trainer  task  package  path  pwd  babyweight  trainer  job  dir  OUTDIR  staging  bucket  gs  BUCKET  scale  tier  STANDARD  bucket  BUCKET  output  dir  OUTDIR  train  steps  2000000from  google  datalab  ml  import  TensorBoard  TensorBoard  start  gs  babyweight  trained  model  format  BUCKET  for  pid  in  TensorBoard  list  pid  TensorBoard  stop  pid  print  Stopped  TensorBoard  with  pid  format  pid  bash  gsutil  ls  gs  BUCKET  babyweight  trained  model  export  Servo  bash  MODEL  NAME  babyweight  MODEL  VERSION  v1  MODEL  LOCATION  gsutil  ls  gs  BUCKET  babyweight  trained  model  export  Servo  tail  echo  Deleting  and  deploying  MODEL  NAME  MODEL  VERSIO,google
N  from  MODEL  LOCATION  this  will  take  few  minutes  gcloud  ml  engine  versions  delete  MODEL  VERSION  model  MODEL  NAME  gcloud  ml  engine  models  delete  MODEL  NAME  gcloud  ml  engine  models  create  MODEL  NAME  regions  REGION  gcloud  ml  engine  versions  create  MODEL  VERSION  model  MODEL  NAME  origin  MODEL  LOCATION  from  googleapiclient  import  discovery  from  oauth2client  client  import  GoogleCredentials  import  json  credentials  GoogleCredentials  get  application  default  api  discovery  build  ml  v1  credentials  credentials  request  data  instances  is  male  True  mother  age  26  mother  race  Asian  Indian  plurality  gestation  weeks  39  mother  married  True  cigarette  use  False  alcohol  use  False  is  male  False  mother  age  29  mother  race  Asian  Indian  plurality  gestation  weeks  38  mother  married  True  cigarette  use  False  alcohol  use  False  is  male  True  mother  age  26  mother  race  White  plurality  gestation  weeks  39  mother  marri,google
ed  True  cigarette  use  False  alcohol  use  False  is  male  True  mother  age  26  mother  race  White  plurality  gestation  weeks  37  mother  married  True  cigarette  use  False  alcohol  use  False  parent  projects  models  versions  PROJECT  babyweight  v1  response  api  projects  predict  body  request  data  name  parent  execute  print  response  format  response  ,google
pip  install  PyMySQLimport  pymysql  cursors  ls  al  Enter  the  values  for  you  database  connection  dsn  database  compose  32e9f9ad  32e9  49ad  b8fa  c10d3e18954a  for  example  BLUDB  dsn  hostname  sl  us  south  portal  23  dblayer  com  for  example  mydbinstance  cz6pjylrdjko  us  east  rds  amazonaws  com  dsn  port  39024  for  example  3306  without  quotation  marks  dsn  uid  admin  for  example  user1  dsn  pwd  moreal  for  example  7dBZ3jWt9xN6  o0JiX  conn  MySQLdb  connect  host  dsn  hostname  port  dsn  port  user  dsn  uid  passwd  dsn  pwd  db  dsn  database  conn  pymysql  connect  host  dsn  hostname  port  dsn  port  user  dsn  uid  passwd  dsn  pwd  db  dsn  database  conn  query  DROP  TABLE  IF  EXISTS  Cars  conn  query  CREATE  TABLE  Cars  Id  INTEGER  PRIMARY  KEY  Name  VARCHAR  20  Price  INT  conn  query  INSERT  INTO  Cars  VALUES  Audi  52642  conn  query  INSERT  INTO  Cars  VALUES  Mercedes  57127  conn  query  INSERT  INTO  Cars  VALUES  Skoda  9000  conn  query  ,ibm
INSERT  INTO  Cars  VALUES  Volvo  29000  conn  query  INSERT  INTO  Cars  VALUES  Bentley  350000  conn  query  INSERT  INTO  Cars  VALUES  Citroen  21000  conn  query  INSERT  INTO  Cars  VALUES  Hummer  41400  conn  query  INSERT  INTO  Cars  VALUES  Volkswagen  21600  cursor  conn  cursor  cursor  execute  SELECT  FROM  Cars  cursor  fetchone  print  nShow  me  the  databases  rows  cursor  fetchall  import  pprint  pprint  pprint  rows  conn  close  ,ibm
time  import  os  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  region  boto3  Session  region  name  bucket  bucket  name  put  your  s3  bucket  name  here  and  create  s3  bucket  prefix  sagemaker  DEMO  xgboost  regression  customize  to  your  bucket  where  you  have  stored  the  data  bucket  path  https  s3  amazonaws  com  format  region  bucket  time  import  io  import  boto3  import  random  def  data  split  FILE  DATA  FILE  TRAIN  FILE  VALIDATION  FILE  TEST  PERCENT  TRAIN  PERCENT  VALIDATION  PERCENT  TEST  data  for  in  open  FILE  DATA  train  file  open  FILE  TRAIN  valid  file  open  FILE  VALIDATION  tests  file  open  FILE  TEST  num  of  data  len  data  num  train  int  PERCENT  TRAIN  100  num  of  data  num  valid  int  PERCENT  VALIDATION  100  num  of  data  num  tests  int  PERCENT  TEST  100  num  of  data  data  fractions  num  train  num  valid  num  tests  split  data  rand  data  ind  for  split  ind  fraction  i,amazon
n  enumerate  data  fractions  for  in  range  fraction  rand  data  ind  random  randint  len  data  split  data  split  ind  append  data  rand  data  ind  data  pop  rand  data  ind  for  in  split  data  train  file  write  for  in  split  data  valid  file  write  for  in  split  data  tests  file  write  train  file  close  valid  file  close  tests  file  close  def  write  to  s3  fobj  bucket  key  return  boto3  Session  resource  s3  Bucket  bucket  Object  key  upload  fileobj  fobj  def  upload  to  s3  bucket  channel  filename  fobj  open  filename  rb  key  prefix  channel  url  s3  format  bucket  key  filename  print  Writing  to  format  url  write  to  s3  fobj  bucket  key  time  import  urllib  request  Load  the  dataset  FILE  DATA  abalone  urllib  request  urlretrieve  https  www  csie  ntu  edu  tw  cjlin  libsvmtools  datasets  regression  abalone  FILE  DATA  split  the  downloaded  data  into  train  test  validation  files  FILE  TRAIN  abalone  train  FILE  VALIDATION  abalone ,amazon
 validation  FILE  TEST  abalone  test  PERCENT  TRAIN  70  PERCENT  VALIDATION  15  PERCENT  TEST  15  data  split  FILE  DATA  FILE  TRAIN  FILE  VALIDATION  FILE  TEST  PERCENT  TRAIN  PERCENT  VALIDATION  PERCENT  TEST  upload  the  files  to  the  S3  bucket  upload  to  s3  bucket  train  FILE  TRAIN  upload  to  s3  bucket  validation  FILE  VALIDATION  upload  to  s3  bucket  test  FILE  TEST  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  xgboost  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  xgboost  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  xgboost  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  xgboost  latest  container  containers  boto3  Session  region  name  time  import  boto3  from  time  import  gmtime  strftime  job  name  DEMO  xgboost  regression  strftime  gmtime  print  Training  job  job  name  Ensure  that  the  training  and  validation  data  folders  generated  above  are  reflected  in ,amazon
 the  InputDataConfig  parameter  below  create  training  params  AlgorithmSpecification  TrainingImage  container  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  bucket  path  prefix  single  xgboost  ResourceConfig  InstanceCount  InstanceType  ml  m4  4xlarge  VolumeSizeInGB  TrainingJobName  job  name  HyperParameters  max  depth  eta  gamma  min  child  weight  subsample  silent  objective  reg  linear  num  round  50  StoppingCondition  MaxRuntimeInSeconds  3600  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  bucket  path  prefix  train  S3DataDistributionType  FullyReplicated  ContentType  libsvm  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  bucket  path  prefix  validation  S3DataDistributionType  FullyReplicated  ContentType  libsvm  CompressionType  None  client  boto3  client  sagemaker  client  create  training  job  create  training  params  import  time  status  client,amazon
  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  status  while  status  Completed  and  status  Failed  time  sleep  60  status  client  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  status  time  import  boto3  from  time  import  gmtime  strftime  model  name  job  name  model  print  model  name  info  client  describe  training  job  TrainingJobName  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  data  primary  container  Image  container  ModelDataUrl  model  data  create  model  response  client  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  endpoint  config  name  DEMO  XGBoostEndpointConfig  strftime  gmtime  print  endpoint  config  name  create  endpoint  config  response  client  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  Ins,amazon
tanceType  ml  m4  xlarge  InitialVariantWeight  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  import  time  endpoint  name  DEMO  XGBoostEndpoint  strftime  gmtime  print  endpoint  name  create  endpoint  response  client  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  print  create  endpoint  response  EndpointArn  resp  client  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  while  status  Creating  time  sleep  60  resp  client  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  print  Arn  resp  EndpointArn  print  Status  status  runtime  client  boto3  client  runtime  sagemaker  head  abalone  test  abalone  single  test  time  import  json  from  itertools  import  islice  import  math  import  struct  file  name  abalone  single  test ,amazon
 customize  to  your  test  file  with  open  file  name  as  payload  read  strip  response  runtime  client  invoke  endpoint  EndpointName  endpoint  name  ContentType  text  libsvm  Body  payload  result  response  Body  read  result  result  decode  utf  result  result  split  result  math  ceil  float  for  in  result  label  payload  strip  split  print  Label  label  nPrediction  result  import  sys  import  math  def  do  predict  data  endpoint  name  content  type  payload  join  data  response  runtime  client  invoke  endpoint  EndpointName  endpoint  name  ContentType  content  type  Body  payload  result  response  Body  read  result  result  decode  utf  result  result  split  preds  float  num  for  num  in  result  preds  math  ceil  num  for  num  in  preds  return  preds  def  batch  predict  data  batch  size  endpoint  name  content  type  items  len  data  arrs  for  offset  in  range  items  batch  size  if  offset  batch  size  items  results  do  predict  data  offset  offset  batch ,amazon
 size  endpoint  name  content  type  arrs  extend  results  else  arrs  extend  do  predict  data  offset  items  endpoint  name  content  type  sys  stdout  write  return  arrs  time  import  json  import  numpy  as  np  with  open  FILE  TEST  as  payload  read  strip  labels  int  line  split  for  line  in  payload  split  test  data  line  for  line  in  payload  split  preds  batch  predict  test  data  100  endpoint  name  text  libsvm  print  Median  Absolute  Percent  Error  MdAPE  np  median  np  abs  np  array  labels  np  array  preds  np  array  labels  client  delete  endpoint  EndpointName  endpoint  name  ,amazon
import  numpy  as  np  def  numerical  gradient  1e  0001  grad  np  zeros  like  it  np  nditer  flags  multi  index  op  flags  readwrite  while  not  it  finished  idx  it  multi  index  tmp  val  idx  idx  float  tmp  val  fxh1  idx  float  tmp  val  fxh2  grad  idx  fxh1  fxh2  idx  tmp  val  it  iternext  return  graddef  gradient  descent  init  lr  01  step  num  100  init  for  in  range  step  num  grad  numerical  gradient  lr  grad  return  wfrom  functions  import  functions  class  simpleNet  def  init  self  self  np  random  randn  2x3  def  predict  self  return  np  dot  self  def  loss  self  self  predict  softmax  Activation  function  at  output  layer  loss  cross  entropy  error  general  Loss  function  return  loss  np  array  np  array  net  simpleNet  print  net  print  lambda  net  loss  dW  numerical  gradient  net  Loss  function  print  dW  print  newW  gradient  descent  net  print  newW  class  TwoLayerNet  def  init  self  input  size  hidden  size  output  size  weight  ini,amazon
t  std  01  self  params  self  params  W1  weight  init  std  np  random  randn  input  size  hidden  size  self  params  b1  np  zeros  hidden  size  self  params  W2  weight  init  std  np  random  randn  hidden  size  output  size  self  params  b2  np  zeros  output  size  def  predict  self  W1  W2  self  params  W1  self  params  W2  b1  b2  self  params  b1  self  params  b2  a1  np  dot  W1  b1  z1  sigmoid  a1  a2  np  dot  z1  W2  b2  softmax  a2  return  def  loss  self  self  predict  return  cross  entropy  error  general  def  accuracy  self  self  predict  np  argmax  axis  np  argmax  axis  accuracy  np  sum  float  shape  return  accuracy  def  numerical  gradient  complex  self  loss  lambda  self  loss  grads  grads  W1  numerical  gradient  loss  self  params  W1  grads  b1  numerical  gradient  loss  self  params  b1  grads  W2  numerical  gradient  loss  self  params  W2  grads  b2  numerical  gradient  loss  self  params  b2  return  grads  def  gradient  self  W1  W2  self  params  W1,amazon
  self  params  W2  b1  b2  self  params  b1  self  params  b2  grads  batch  num  shape  forward  a1  np  dot  W1  b1  z1  sigmoid  a1  a2  np  dot  z1  W2  b2  softmax  a2  backward  dy  batch  num  grads  W2  np  dot  z1  dy  grads  b2  np  sum  dy  axis  da1  np  dot  dy  W2  dz1  sigmoid  grad  a1  da1  grads  W1  np  dot  dz1  grads  b1  np  sum  dz1  axis  return  gradsimport  import  ipynb  import  matplotlib  pylab  as  plt  from  mnist  import  load  mnist  train  train  test  test  load  mnist  normalize  True  one  hot  label  True  network  TwoLayerNet  input  size  784  hidden  size  50  output  size  10  iters  num  15000  train  size  train  shape  batch  size  100  learning  rate  train  loss  list  train  acc  list  test  acc  list  iter  per  epoch  max  train  size  batch  size  for  in  range  iters  num  batch  mask  np  random  choice  train  size  batch  size  batch  train  batch  mask  batch  train  batch  mask  grad  network  numerical  gradient  complex  batch  batch  grad  network ,amazon
 gradient  batch  batch  for  key  in  W1  b1  W2  b2  network  params  key  learning  rate  grad  key  loss  network  loss  batch  batch  train  loss  list  append  loss  if  iter  per  epoch  train  acc  network  accuracy  train  train  test  acc  network  accuracy  test  test  train  acc  list  append  train  acc  test  acc  list  append  test  acc  print  train  acc  test  acc  str  train  acc  str  test  acc  markers  train  test  np  arange  len  train  acc  list  plt  plot  train  acc  list  label  train  acc  plt  plot  test  acc  list  label  test  acc  linestyle  plt  xlabel  epochs  plt  ylabel  accuracy  plt  ylim  plt  legend  loc  lower  right  plt  show  ,amazon
update  config  import  myconfig  print  myconfig  blobacct  myconfig  blobkey  from  subprocess  import  call  import  osblobcont  upload  blobpath  blobcont  models  localpath  os  path  abspath  models  print  blobxfer  upload  storage  account  storage  account  key  remote  path  local  path  format  myconfig  blobacct  myconfig  blobkey  blobpath  localpath  call  blobxfer  upload  storage  account  myconfig  blobacct  storage  account  key  myconfig  blobkey  remote  path  blobpath  local  path  localpath  blobcont  upload  blobpath  blobcont  models  downpath  os  path  abspath  models  if  not  os  path  exists  downpath  os  makedirs  downpath  print  blobxfer  download  storage  account  storage  account  key  remote  path  local  path  skip  on  lmt  ge  strip  components  100  format  myconfig  blobacct  myconfig  blobkey  blobpath  downpath  call  blobxfer  download  storage  account  myconfig  blobacct  storage  account  key  myconfig  blobkey  remote  path  blobpath  local  path  downpath  ski,microsoft
p  on  lmt  ge  strip  components  100  ,microsoft
Read  the  Data  from  S3  import  boto3  import  pandas  as  pd  from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  apjprintsupply  data  locationsellTo  s3  APJ  Supplies  SampleData  MVC  transformed  xlsx  format  bucket  sellThru  pd  read  excel  data  locationsellTo  sheet  name  Sell  To  Rebates  Look  at  rows  to  ensure  data  is  correct  sellThru  Look  at  data  stats  sellThru  describe  Import  packages  prepare  data  for  machine  learning  import  numpy  as  np  from  sklearn  cluster  import  KMeans  import  matplotlib  pyplot  as  plt  matplotlib  inline  np  array  sellThru  SKUID  ROI  Visualize  Data  plt  scatter  label  True  Position  Apply  means  to  fit  SKUs  into  Clusters  kmeans  KMeans  clusters  kmeans  fit  print  kmeans  cluster  centers  print  kmeans  labels  plt  scatter  kmeans  labels  cmap  rainbow  plt  scatter  kmeans  cluster  centers  kmeans  cluster  centers  color  black  ,amazon
import  sagemaker  from  sagemaker  import  get  execution  role  import  boto3  import  json  sess  sagemaker  Session  role  get  execution  role  print  role  This  is  the  role  that  SageMaker  would  use  to  leverage  AWS  resources  S3  CloudWatch  on  your  behalf  bucket  sess  default  bucket  Replace  with  your  own  bucket  name  if  needed  print  bucket  prefix  blazingtext  subwords  Replace  with  the  prefix  under  which  you  want  to  store  the  data  if  needed  wget  http  mattmahoney  net  dc  text8  zip  text8  gz  Uncompressing  gzip  text8  gz  ftrain  channel  prefix  train  sess  upload  data  path  text8  bucket  bucket  key  prefix  train  channel  s3  train  data  s3  format  bucket  train  channel  s3  output  location  s3  output  format  bucket  prefix  region  name  boto3  Session  region  namecontainer  sagemaker  amazon  amazon  estimator  get  image  uri  region  name  blazingtext  latest  print  Using  SageMaker  BlazingText  container  format  container  region  nam,amazon
e  bt  model  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  c4  2xlarge  Use  of  ml  p3  2xlarge  is  highly  recommended  for  highest  speed  and  cost  efficiency  train  volume  size  30  train  max  run  360000  input  mode  File  output  path  s3  output  location  sagemaker  session  sess  bt  model  set  hyperparameters  mode  skipgram  epochs  min  count  sampling  threshold  0001  learning  rate  05  window  size  vector  dim  100  negative  samples  subwords  True  Enables  learning  of  subword  embeddings  for  OOV  word  vector  generation  min  char  min  length  of  char  ngrams  max  char  max  length  of  char  ngrams  batch  size  11  window  size  Preferred  Used  only  if  mode  is  batch  skipgram  evaluation  True  Perform  similarity  evaluation  on  WS  353  dataset  at  the  end  of  trainingtrain  data  sagemaker  session  s3  input  s3  train  data  distribution  FullyReplicated  content  type  text  plain  s3  data  type  S3P,amazon
refix  data  channels  train  train  data  bt  model  fit  inputs  data  channels  logs  True  bt  endpoint  bt  model  deploy  initial  instance  count  instance  type  ml  m4  xlarge  words  awesome  awweeesome  payload  instances  words  response  bt  endpoint  predict  json  dumps  payload  vecs  json  loads  response  print  vecs  wget  http  www  nlp  stanford  edu  lmthang  morphoNLM  rw  zip  unzip  rw  zip  cut  rw  rw  txt  awk  print  tolower  tr  query  words  txtquery  words  with  open  query  words  txt  as  for  line  in  readlines  query  words  append  line  strip  query  words  list  set  query  words  total  words  len  query  words  vectors  import  numpy  as  np  import  math  from  scipy  import  stats  batch  size  500  batch  start  batch  end  batch  start  batch  size  while  len  vectors  total  words  batch  end  min  batch  end  total  words  subset  words  query  words  batch  start  batch  end  payload  instances  subset  words  response  bt  endpoint  predict  json  dumps  pay,amazon
load  vecs  json  loads  response  for  in  vecs  arr  np  array  vector  dtype  float  if  np  linalg  norm  arr  continue  vectors  word  arr  batch  start  batch  size  batch  end  batch  sizemysim  gold  dropped  nwords  def  similarity  v1  v2  n1  np  linalg  norm  v1  n2  np  linalg  norm  v2  return  np  dot  v1  v2  n1  n2  fin  open  rw  rw  txt  rb  for  line  in  fin  tline  line  decode  utf8  split  word1  tline  lower  word2  tline  lower  nwords  if  word1  in  vectors  and  word2  in  vectors  v1  vectors  word1  v2  vectors  word2  similarity  v1  v2  mysim  append  gold  append  float  tline  else  dropped  fin  close  corr  stats  spearmanr  mysim  gold  print  Correlation  Dropped  words  corr  100  math  ceil  dropped  nwords  100  sess  delete  endpoint  bt  endpoint  endpoint  ,amazon
import  os  import  sagemaker  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  DEMO  adult  from  sagemaker  tensorflow  import  TensorFlow  estimator  TensorFlow  entry  point  LogReg  py  role  role  training  steps  100  evaluation  steps  100  train  instance  count  train  instance  type  ml  c4  xlarge  estimator  fit  inputs  ,amazon
from  IPython  display  import  HTML  HTML  iframe  width  560  height  315  src  https  www  youtube  com  embed  ZSMaMKbw1y0  rel  amp  controls  amp  showinfo  frameborder  allowfullscreen  iframe  from  IPython  display  import  HTML  HTML  iframe  width  560  height  315  src  https  www  youtube  com  embed  5sDVprory3c  rel  amp  controls  amp  showinfo  frameborder  allowfullscreen  iframe  ,amazon
import  tensorflow  as  tf  import  numpy  as  np  tf  set  random  seed  777  for  reproducibility  xy  np  array  828  659973  833  450012  908100  828  349976  831  659973  823  02002  828  070007  1828100  821  655029  828  070007  819  929993  824  400024  1438100  818  97998  824  159973  816  820  958984  1008100  815  48999  819  23999  819  359985  823  1188100  818  469971  818  97998  819  823  1198100  816  820  450012  811  700012  815  25  1098100  809  780029  813  669983  809  51001  816  659973  1398100  804  539978  809  559998  data  xy  data  xy  from  scipy  import  stats  print  stats  describe  data  print  stats  describe  data  placeholders  for  tensor  that  will  be  always  fed  tf  placeholder  tf  float32  shape  None  tf  placeholder  tf  float32  shape  None  tf  Variable  tf  random  normal  name  weight  tf  Variable  tf  random  normal  name  bias  Hypothesis  hypothesis  tf  matmul  Simplified  cost  loss  function  cost  tf  reduce  mean  tf  square  hypothesis  Minimize ,ibm
 optimizer  tf  train  GradientDescentOptimizer  learning  rate  1e  10  train  optimizer  minimize  cost  Launch  the  graph  in  session  sess  tf  Session  Initializes  global  variables  in  the  graph  sess  run  tf  global  variables  initializer  for  step  in  range  101  cost  val  hy  val  sess  run  cost  hypothesis  train  feed  dict  data  data  if  step  print  step  Cost  cost  val  print  Prediction  hy  val  Cost  45533e  12  Prediction  1104436  375  2224342  75  1749606  75  1226179  375  1445287  125  1457459  1335740  1700924  625  Cost  69762e  27  Prediction  66371490e  13  37543360e  13  80198785e  13  06716290e  13  79336847e  13  83371348e  13  43026590e  13  64060907e  13  Cost  inf  Prediction  21438790e  21  44468702e  21  92314724e  21  34811610e  21  58882674e  21  60219962e  21  46847142e  21  86965602e  21  Cost  inf  Prediction  02525216e  28  10324465e  28  37453079e  28  46851237e  28  26638074e  28  31070676e  28  86744608e  28  19722623e  28  Cost  inf  Prediction  334224,ibm
28e  36  68593010e  36  11292430e  36  48114879e  36  74561303e  36  76030542e  36  61338091e  36  05415459e  36  Cost  inf  Prediction  inf  inf  inf  inf  inf  inf  inf  inf  Cost  nan  Prediction  nan  nan  nan  nan  nan  nan  nan  nan  ,ibm
import  tensorflow  as  tf  tf  set  random  seed  777  for  reproducibility  data  data  placeholders  for  tensor  that  will  be  always  fed  tf  placeholder  tf  float32  shape  None  tf  placeholder  tf  float32  shape  None  tf  Variable  tf  random  normal  name  weight  tf  Variable  tf  random  normal  name  bias  Hypothesis  using  sigmoid  tf  div  tf  exp  tf  matmul  hypothesis  tf  sigmoid  tf  matmul  cost  loss  function  cost  tf  reduce  mean  tf  log  hypothesis  tf  log  hypothesis  train  tf  train  GradientDescentOptimizer  learning  rate  01  minimize  cost  check  cast  boolean  return  sess  tf  Session  sess  run  tf  global  variables  initializer  print  sess  run  tf  cast  10  dtype  tf  float32  print  sess  run  tf  equal  Accuracy  computation  True  if  hypothesis  else  False  predicted  tf  cast  hypothesis  dtype  tf  float32  accuracy  tf  reduce  mean  tf  cast  tf  equal  predicted  dtype  tf  float32  Launch  graph  with  tf  Session  as  sess  Initialize  TensorFlow ,ibm
 variables  sess  run  tf  global  variables  initializer  for  step  in  range  10001  cost  val  sess  run  cost  train  feed  dict  data  data  if  step  500  print  step  cost  val  Accuracy  report  sess  run  hypothesis  predicted  accuracy  feed  dict  data  data  print  nHypothesis  nCorrect  nAccuracy  73078  200  571512  400  507414  600  471824  800  447585  9200  159066  9400  15656  9600  154132  9800  151778  10000  149496  Hypothesis  03074029  15884677  30486736  78138196  93957496  98016882  Correct  Accuracy  ,ibm
bq  query  SELECT  count  FROM  starry  sunup  200204  crypto  test  bq  query  SELECT  count  FROM  starry  sunup  200204  crypto  test  WHERE  price  usd  6000  bq  query  SELECT  FROM  starry  sunup  200204  crypto  test  WHERE  price  usd  6000  bq  query  SELECT  sum  market  cap  usd  FROM  starry  sunup  200204  crypto  test  bq  query  SELECT  FROM  starry  sunup  200204  crypto  test  WHERE  available  supply  5000000  bq  query  SELECT  FROM  starry  sunup  200204  crypto  test  ORDER  BY  percent  change  7d  desc  LIMIT  bq  query  SELECT  FROM  starry  sunup  200204  crypto  test  WHERE  symbol  LIKE  import  google  datalab  bigquery  as  bqq1  bq  Query  SELECT  FROM  starry  sunup  200204  crypto  test  WHERE  price  usd  6000  output  options  bq  QueryOutput  table  use  cache  False  result  q1  execute  output  options  output  options  result  print  list  result  convert  to  Pandas  dataframe  df1  result  to  dataframe  type  df1  df1import  matplotlib  as  mpl  import  matplotlib  pyp,google
lot  as  plt  import  numpy  as  npq2  bq  Query  SELECT  symbol  percent  change  7d  FROM  starry  sunup  200204  crypto  test  ORDER  BY  percent  change  7d  desc  LIMIT  output  options  bq  QueryOutput  table  use  cache  False  result  q2  execute  output  options  output  options  result  df2  result  to  dataframe  df2  matplotlib  inlinempl  rcParams  font  family  DejaVu  Sans  df2  percent  change  7d  plot  kind  bar  plt  title  Crypto  Currencies  Weekly  Growth  plt  ylabel  change  plt  xticks  np  arange  df2  symbol  plt  show  import  urllib  request  import  json  import  csv  with  urllib  request  urlopen  https  api  coinmarketcap  com  v1  ticker  as  url  data  json  loads  url  read  decode  print  data  with  open  CoinMarketCap  csv  as  fileout  csvwriter  csv  writer  fileout  header  data  keys  csvwriter  writerow  header  for  in  data  csvwriter  writerow  values  pwdfrom  google  cloud  import  storageclient  storage  Client  bucket  client  get  bucket  batanov  blob2  buc,google
ket  blob  CoinMarketCap  csv  blob2  upload  from  filename  filename  content  datalab  notebooks  CoinMarketCap  csv  from  google  cloud  import  bigqueryclient  bigquery  Client  dataset  id  CoinMarketCap  dataset  ref  client  dataset  dataset  id  job  config  bigquery  LoadJobConfig  job  config  schema  bigquery  SchemaField  price  btc  STRING  bigquery  SchemaField  price  usd  STRING  bigquery  SchemaField  last  updated  STRING  bigquery  SchemaField  24h  volume  usd  STRING  bigquery  SchemaField  market  cap  usd  STRING  bigquery  SchemaField  symbol  STRING  bigquery  SchemaField  id  STRING  bigquery  SchemaField  total  supply  STRING  bigquery  SchemaField  rank  STRING  bigquery  SchemaField  max  supply  STRING  bigquery  SchemaField  percent  change  1h  STRING  bigquery  SchemaField  percent  change  7d  STRING  bigquery  SchemaField  name  STRING  bigquery  SchemaField  available  supply  STRING  bigquery  SchemaField  percent  change  24h  STRING  job  config  skip  leading  rows  ,google
load  job  client  load  table  from  uri  gs  batanov  CoinMarketCap  csv  dataset  ref  table  CoinMarketCap  job  config  job  config  assert  load  job  job  type  load  load  job  result  Waits  for  table  load  to  complete  assert  load  job  state  DONE  assert  client  get  table  dataset  ref  table  CoinMarketCap  num  rows  100  bq  query  SELECT  symbol  percent  change  7d  FROM  starry  sunup  200204  crypto  test  ORDER  BY  percent  change  7d  desc  LIMIT  bq  query  name  q1  SELECT  symbol  percent  change  7d  FROM  starry  sunup  200204  crypto  test  ORDER  BY  percent  change  7d  desc  LIMIT  chart  columns  data  q1  fields  symbol  percent  change  7d  chart  help  ,google
import  numpy  as  np  import  tensorflow  as  tffrom  IPython  display  import  clear  output  Image  display  HTML  def  strip  consts  graph  def  max  const  size  32  Strip  large  constant  values  from  graph  def  strip  def  tf  GraphDef  for  n0  in  graph  def  node  strip  def  node  add  MergeFrom  n0  if  op  Const  tensor  attr  value  tensor  size  len  tensor  tensor  content  if  size  max  const  size  tensor  tensor  content  stripped  bytes  size  return  strip  def  def  show  graph  graph  def  max  const  size  32  Visualize  TensorFlow  graph  if  hasattr  graph  def  as  graph  def  graph  def  graph  def  as  graph  def  strip  def  strip  consts  graph  def  max  const  size  max  const  size  code  script  function  load  document  getElementById  id  pbtxt  data  script  link  rel  import  href  https  tensorboard  appspot  com  tf  graph  basic  build  html  onload  load  div  style  height  600px  tf  graph  basic  id  id  tf  graph  basic  div  format  data  repr  str  strip  ,amazon
def  id  graph  str  np  random  rand  iframe  iframe  seamless  style  width  1200px  height  620px  border  srcdoc  iframe  format  code  replace  quot  display  HTML  iframe  Tensors  and  nodes  with  tf  name  scope  sample  nodes  node  tf  placeholder  tf  float32  shape  None  28  name  tensor  tensor  tf  fill  10  name  tensor  tensor  tf  fill  10  name  tensor  Sessions  and  operation  overload  with  tf  name  scope  sample  op  res1  tf  matmul  tensor  tensor  name  matrix  multiplication  Variables  and  Operations  with  tf  name  scope  sample  graph  tf  Variable  initial  value  dtype  tf  float32  name  tf  Variable  initial  value  dtype  tf  float32  name  tf  Variable  initial  value  25  dtype  tf  float32  name  Operation  overload  tf  add  name  tf  add  name  Variables  initialization  and  session  execution  init  tf  global  variables  initializer  with  tf  Session  as  init  run  result1  run  res1  result2  run  result1result2show  graph  tf  get  default  graph  ,amazon
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  videogames  xgboost  Define  IAM  role  import  sagemaker  role  sagemaker  get  execution  role  import  numpy  as  np  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  from  IPython  display  import  Image  from  IPython  display  import  display  from  sklearn  datasets  import  dump  svmlight  file  from  time  import  gmtime  strftime  import  sys  import  math  import  json  import  boto3raw  data  filename  Video  Games  Sales  as  at  22  Dec  2016  csv  s3  boto3  resource  s3  s3  Bucket  bucket  download  file  prefix  raw  data  filename  raw  data  csv  data  pd  read  csv  raw  data  csv  pd  set  option  display  max  rows  20  datadata  data  Global  Sales  plt  bar  not  hit  hit  data  value  counts  plt  show  viz  data  filter  User  Score  Critic  Score  Global  Sales  axis  viz  User  Score  pd  Series  viz  User  Score  apply  pd  to  numeric  errors  coerce  viz  User  Score  viz  User  Score  mask  np  isnan  v,amazon
iz  User  Score  viz  Critic  Score  10  viz  plot  kind  scatter  logx  True  logy  True  Critic  Score  Global  Sales  viz  plot  kind  scatter  logx  True  logy  True  User  Score  Global  Sales  plt  show  data  data  drop  Name  Year  of  Release  NA  Sales  EU  Sales  JP  Sales  Other  Sales  Global  Sales  Critic  Count  User  Count  Developer  axis  data  isnull  sum  data  data  dropna  data  User  Score  data  User  Score  apply  pd  to  numeric  errors  coerce  data  User  Score  data  User  Score  mask  np  isnan  data  User  Score  data  Critic  Score  10  data  data  apply  lambda  yes  if  True  else  no  model  data  pd  get  dummies  data  train  data  validation  data  test  data  np  split  model  data  sample  frac  random  state  1729  int  len  model  data  int  len  model  data  dump  svmlight  file  train  data  drop  no  yes  axis  train  data  yes  train  libsvm  dump  svmlight  file  validation  data  drop  no  yes  axis  validation  data  yes  validation  libsvm  dump  svmlight  fi,amazon
le  test  data  drop  no  yes  axis  test  data  yes  test  libsvm  boto3  Session  resource  s3  Bucket  bucket  Object  prefix  train  train  libsvm  upload  file  train  libsvm  boto3  Session  resource  s3  Bucket  bucket  Object  prefix  validation  validation  libsvm  upload  file  validation  libsvm  job  name  DEMO  videogames  xgboost  strftime  gmtime  print  Training  job  job  name  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  xgboost  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  xgboost  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  xgboost  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  xgboost  latest  create  training  params  RoleArn  role  TrainingJobName  job  name  AlgorithmSpecification  TrainingImage  containers  boto3  Session  region  name  TrainingInputMode  File  ResourceConfig  InstanceCount  InstanceType  ml  c4  xlarge  VolumeSizeInGB  10  InputDataConfig  ChannelName  train  DataSource  S,amazon
3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  S3DataDistributionType  FullyReplicated  ContentType  libsvm  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  prefix  S3DataDistributionType  FullyReplicated  ContentType  libsvm  CompressionType  None  OutputDataConfig  S3OutputPath  s3  xgboost  video  games  output  format  bucket  prefix  HyperParameters  max  depth  eta  eval  metric  auc  scale  pos  weight  subsample  objective  binary  logistic  num  round  100  StoppingCondition  MaxRuntimeInSeconds  60  60  time  sm  boto3  client  sagemaker  sm  create  training  job  create  training  params  status  sm  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  status  try  sm  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  finally  status  sm  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  Train,amazon
ing  job  ended  with  status  status  if  status  Failed  message  sm  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  create  model  response  sm  create  model  ModelName  job  name  ExecutionRoleArn  role  PrimaryContainer  Image  containers  boto3  Session  region  name  ModelDataUrl  sm  describe  training  job  TrainingJobName  job  name  ModelArtifacts  S3ModelArtifacts  print  create  model  response  ModelArn  xgboost  endpoint  config  DEMO  videogames  xgboost  config  strftime  gmtime  print  xgboost  endpoint  config  create  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  xgboost  endpoint  config  ProductionVariants  InstanceType  ml  t2  medium  InitialInstanceCount  ModelName  job  name  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  xgboost  endpoint  DEMO  videog,amazon
ames  xgboost  endpoint  strftime  gmtime  print  xgboost  endpoint  create  endpoint  response  sm  create  endpoint  EndpointName  xgboost  endpoint  EndpointConfigName  xgboost  endpoint  config  print  create  endpoint  response  EndpointArn  resp  sm  describe  endpoint  EndpointName  xgboost  endpoint  status  resp  EndpointStatus  print  Status  status  try  sm  get  waiter  endpoint  in  service  wait  EndpointName  xgboost  endpoint  finally  resp  sm  describe  endpoint  EndpointName  xgboost  endpoint  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Status  status  if  status  InService  message  sm  describe  endpoint  EndpointName  xgboost  endpoint  FailureReason  print  Endpoint  creation  failed  with  the  following  error  format  message  raise  Exception  Endpoint  creation  did  not  succeed  runtime  boto3  client  runtime  sagemaker  def  do  predict  data  endpoint  name  content  type  payload  join  data  response  runtime  invoke  endpoint  EndpointName  endpoint,amazon
  name  ContentType  content  type  Body  payload  result  response  Body  read  result  result  decode  utf  result  result  split  preds  float  num  for  num  in  result  preds  round  num  for  num  in  preds  return  preds  def  batch  predict  data  batch  size  endpoint  name  content  type  items  len  data  arrs  for  offset  in  range  items  batch  size  if  offset  batch  size  items  results  do  predict  data  offset  offset  batch  size  endpoint  name  content  type  arrs  extend  results  else  arrs  extend  do  predict  data  offset  items  endpoint  name  content  type  sys  stdout  write  return  arrs  time  import  json  with  open  test  libsvm  as  payload  read  strip  labels  int  line  split  for  line  in  payload  split  test  data  line  for  line  in  payload  split  preds  batch  predict  test  data  100  xgboost  endpoint  text  libsvm  print  nerror  rate  sum  for  in  range  len  preds  if  preds  labels  float  len  preds  pd  crosstab  index  np  array  labels  columns  np,amazon
  array  preds  sm  delete  endpoint  EndpointName  xgboost  endpoint  ,amazon
matplotlib  inline  import  sys  import  numpy  as  np  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  import  math  import  osimport  boto3  import  re  from  sagemaker  import  get  execution  role  import  sagemaker  Acquire  realtime  endpoint  endpoint  name  xgboost  iris  v1  predictor  sagemaker  predictor  RealTimePredictor  endpoint  endpoint  name  from  sagemaker  predictor  import  csv  serializer  json  deserializer  predictor  content  type  text  csv  predictor  serializer  csv  serializer  predictor  deserializer  Nonedf  all  pd  read  csv  iris  all  csv  df  all  head  df  all  columns  Need  to  pass  an  array  to  the  prediction  can  pass  numpy  array  or  list  of  values  19  20  arr  test  df  all  as  matrix  df  all  columns  type  arr  test  arr  test  shapearr  test  result  predictor  predict  arr  test  arr  test  shaperesultint  float  For  large  number  of  predictions  we  can  split  the  input  data  and  Query  the  prediction  service  array  split  is ,amazon
 convenient  to  specify  how  many  splits  are  needed  predictions  for  arr  in  np  array  split  arr  test  10  result  predictor  predict  arr  result  result  decode  utf  result  result  split  print  arr  shape  predictions  int  float  for  in  result  len  predictions  predictions  from  sklearn  import  preprocessing  le  preprocessing  LabelEncoder  le  fit  Iris  setosa  Iris  versicolor  Iris  virginica  df  all  predicted  class  le  inverse  transform  predictions  df  all  head  print  Confusion  matrix  Actual  versus  Predicted  pd  crosstab  df  all  class  df  all  predicted  class  import  sklearn  metrics  as  metrics  print  metrics  classification  report  df  all  class  df  all  predicted  class  ,amazon
import  mxnet  as  mx  import  sagemaker  from  sagemaker  mxnet  import  MXNet  as  MXNetEstimator  sagemaker  session  sagemaker  Session  role  sagemaker  get  execution  role  mx  test  utils  get  cifar10  Downloads  Cifar  10  dataset  to  data  cifarinputs  sagemaker  session  upload  data  path  data  cifar  key  prefix  data  cifar10  estimator  MXNetEstimator  entry  point  train  py  role  sagemaker  get  execution  role  train  instance  count  train  instance  type  ml  p3  2xlarge  hyperparameters  batch  size  1024  epochs  30  estimator  fit  inputs  predictor  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  load  the  CIFAR  10  samples  and  convert  them  into  format  we  can  use  with  the  prediction  endpoint  from  readimage  utils  import  read  images  filenames  images  airplane1  png  images  automobile1  png  images  bird1  png  images  cat1  png  images  deer1  png  images  dog1  png  images  frog1  png  images  horse1  png  images  ship1  png  image,amazon
s  truck1  png  image  data  read  images  filenames  for  img  in  enumerate  image  data  response  predictor  predict  img  print  image  class  format  int  response  estimator  delete  endpoint  ,amazon
pip  install  jaydebeapi  user  pip  install  ibmdbpy  user  import  jaydebeapi  from  ibmdbpy  import  IdaDataBase  from  ibmdbpy  import  IdaDataFrameimport  os  os  environ  CLASSPATH  usr  local  src  data  connectors  db2jcc4  10  jar  for  Watson  Studio  Environments  please  use  the  following  location  os  environ  CLASSPATH  opt  ibm  jdbc  db2jcc4  jar  import  jpype  args  Djava  class  path  os  environ  CLASSPATH  jvm  jpype  getDefaultJVMPath  ls  al  opt  ibm  jdbc  db2jcc4  jar  ls  al  opt  ibm  java  x86  64  80  jre  bin  classic  libjvm  so  argsjpype  startJVM  jvm  args  dsn  uid  dash33218  db104434  dsn  pwd  r9ltKnMVW  xxxx  dsn  hostname  dashdb  entry  yp  syd01  01  services  au  syd  bluemix  net  awh  yp  small03  services  dal  bluemix  net  dsn  port  50000  50001  dsn  database  BLUDB  BLUDB  connection  string  jdbc  db2  dsn  hostname  dsn  port  dsn  database  user  dsn  uid  password  dsn  pwd  idadb  IdaDataBase  dsn  connection  string  df  idadb  show  tables  show  ,ibm
all  True  df  head  idadb  exists  table  or  view  GOSALESDW  EMP  EXPENSE  FACT  idadf  IdaDataFrame  idadb  GOSALESDW  EMP  EXPENSE  FACT  idadf  head  idadf  tail  idadf  shapeidadf  columnsidadf  cov  idadf  new  idadf  Select  the  first  10  rows  idadf  new  head  idadb  close  ,ibm
install  packages  rugarch  if  require  ccgarch  install  packages  ccgarch  library  ccgarch  nobs  50  003  005  001  diag  15  diag  75  uncR  matrix  12  12  dcc  para  01  98  dcc  data  dcc  sim  nobs  uncR  dcc  para  model  diagonal  dcc  datadcc  results  dcc  estimation  inia  iniA  iniB  ini  dcc  dcc  para  dvar  dcc  data  eps  model  extended  or  model  diagonal  head  dcc  results  head  dcc  results  DCC  dim  dcc  results  DCC  dcc  results  DCC  matrix  dcc  results  DCC  nrow  ncol  matrix  dcc  results  DCC  nrow  ncol  array  dcc  results  DCC  dim  50  Returns  for  the  market  index  the  given  asset  data  index  asset  data  data  isnan  data  data  select  only  elements  which  are  not  NaN  data  center  data  ones  size  data  mean  data  demeaned  returns  GJR  GARCH  and  DCC  parameters  loglikelihood  Ht  Rt  Qt  stdresid  likelihoods  stderrors  jointscores  dcc  mvgarch  data  center  ht  sqrt  market  conditional  volatility  ht  sqrt  asset  conditional  volatility  r,microsoft
ho  squeeze  Rt  conditional  correlation  quantile  data  center  alpha  HS  VaR  nonparametric  it  our  systemic  event  it  scalar  here  MES  fct  MES  data  center  ht  ht  rho  LRMES  exp  18  MES  without  simulation  SRISK  LTQ  LRMES  MV  parameters  loglikelihood  Ht  Rt  Qt  likelihoods  stdresid  stderrors  jointscores  dcc  mvgarch  data  dccQ  dccP  archQ  garchP  parameters  vector  of  parameters  estimated  from  the  model  of  the  form  GarchParams  GarchParams  GarchParams  DCCParams  where  the  Garch  Parameters  from  each  estimation  are  of  the  form  omega  alpha  i1  alpha  i2  alpha  ip  beta  i1  beta  i2  beta  iq  loglikelihood  The  log  likelihood  evaluated  at  the  optimum  Ht  by  by  array  of  conditional  variances  Rt  by  by  array  of  Rt  elements  Qt  by  by  array  of  Qt  elements  stdresid  by  matrix  of  standardized  residuals  likelihoods  the  estimated  likelihoods  by  stderrors  length  parameters  matrix  of  estimated  correct  standard  errors  Th,microsoft
e  estimated  from  the  robust  standard  errors  The  estimated  from  the  standard  errors  jointscores  The  estimated  scores  of  the  likelihood  by  length  parameters  Conditional  Volatility  univariate  function  MES  fct  MES  data  ht  ht  rho  em  data  ht  market  first  column  xi  data  ht  rho  em  sqrt  rho  asset  second  column  bwd  size  data  Scaillet  bwd  p21  put  instead  of  the  standard  deviation  because  our  shocks  are  iid  with  unit  variance  K1  sum  em  normcdf  ht  em  bwd  sum  normcdf  ht  em  bwd  K2  sum  xi  normcdf  ht  em  bwd  sum  normcdf  ht  em  bwd  MES  ht  rho  K1  ht  sqrt  rho  K2  Supply  series  of  returns  with  means  subtracted  dcc  gjrgarch  ret0  ret0  An  by  by  matrix  of  floats  containing  the  DCC  coefficients  by  matrix  of  floats  containing  the  conditional  variances  sqrt  sqrt  mx  squeeze  Pull  out  the  off  diagonal  correlation  between  market  firm  beta  mx  var  quantile  ret0  data  Find  the  variance  of  the  fi,microsoft
rm  mes  lrmes  calculate  mes  ret0  ret0  beta  mx  data  data  Hopefully  package  ccgarch  can  do  this  step  srisk  calculate  srisk  lrmes  data  FrmsLia  data  FrmsCap  data  SRISK  needs  the  LR  MES  balance  sheet  data  crash  level  40  function  mes  lrmes  calculate  mes  internal  ret0  ret0  beta  mx  quantile  ret0  length  ret0  ret0  den  sqrt  mx  num  ret0  mx  num  den  normcdf  k1  sum  sum  k2  sum  sum  mes  mx  k1  den  k2  lrmes  exp  log  beta  function  srisk  calculate  srisk  internal  lrmes  tl  mc  srisk  tl  lrmes  mc  srisk  srisk  ,microsoft
c1  general  conv2d  input  64  c2  general  conv2d  c1  64  c3  general  conv2d  c2  64  ,amazon
import  os  from  os  import  path  import  json  docker  run  rm  hello  world  mkdir  script  mkdir  script  code  cp  process  cifar  data  ipynb  script  code  cp  cntk  cifar10  ipynb  script  code  jupyter  nbconvert  to  python  script  code  process  cifar  data  ipynb  ExecutePreprocessor  kernel  name  cntk  py34  jupyter  nbconvert  to  python  script  code  cntk  cifar10  ipynb  ExecutePreprocessor  kernel  name  cntk  py34  az  login  tableselected  subscription  My  Subscription  ADD  THE  NAME  OR  ID  OF  THE  SUBSCRIPTION  YOU  WANT  TO  USE  az  account  set  subscription  selected  subscription  az  account  list  tabledocker  registry  mscontainer  docker  registry  group  mscontainergorup  az  group  create  docker  registry  group  southcentralus  table  az  acr  create  docker  registry  docker  registry  group  southcentralus  table  az  acr  update  docker  registry  admin  enabled  true  tablejson  data  az  acr  credential  show  docker  registry  docker  username  json  loads  join,microsoft
  json  data  username  docker  password  json  loads  join  json  data  password  json  data  az  acr  show  docker  registry  docker  registry  server  json  loads  join  json  data  loginServer  mkdir  script  docker  writefile  script  docker  dockerfile  Dockerfile  for  CNTK  GPU  OpenMPI  for  use  with  Batch  Shipyard  on  Azure  Batch  FROM  microsoft  cntk  beta8  runtime  gpu  python3  cuda8  cudnn5  MAINTAINER  Mathew  Salvaris  ADD  code  code  ENV  PATH  root  anaconda3  bin  PATH  CMD  bin  bash  container  name  docker  registry  server  masalvar  cntkbatch  application  path  script  docker  file  location  path  join  application  path  docker  dockerfile  docker  login  docker  registry  server  docker  username  docker  password  docker  build  container  name  docker  file  location  application  path  no  cache  docker  push  container  name  git  clone  https  github  com  Azure  batch  shipyard  gitbatchshipyard  batch  shipyard  shipyard  group  name  msbatchexample  location  southc,microsoft
entralus  time  az  group  create  group  name  location  table  time  az  group  list  tablebatch  account  name  msbatchex  storage  account  name  msbatchstoreex  template  dict  schema  https  schema  management  azure  com  schemas  2015  01  01  deploymentTemplate  json  contentVersion  parameters  batchAccounts  name  defaultValue  batch  account  name  type  String  storageAccounts  name  defaultValue  storage  account  name  type  String  variables  resources  type  Microsoft  Batch  batchAccounts  name  parameters  batchAccounts  name  apiVersion  2015  12  01  location  location  properties  autoStorage  storageAccountId  resourceId  Microsoft  Storage  storageAccounts  parameters  storageAccounts  name  resources  dependsOn  resourceId  Microsoft  Storage  storageAccounts  parameters  storageAccounts  name  type  Microsoft  Storage  storageAccounts  sku  name  Standard  LRS  tier  Standard  kind  Storage  name  parameters  storageAccounts  name  apiVersion  2016  01  01  location  location  tags  ,microsoft
properties  resources  dependsOn  template  filename  template  json  with  open  template  filename  as  outfile  json  dump  template  dict  outfile  az  group  deployment  validate  template  file  template  filename  group  name  time  az  group  deployment  create  template  file  template  filename  group  name  verbosejson  data  az  batch  account  keys  list  batch  account  name  group  name  batch  account  key  json  loads  join  json  data  primary  json  data  az  batch  account  list  group  name  batch  service  url  https  json  loads  join  json  data  accountEndpoint  json  data  az  storage  account  keys  list  storage  account  name  group  name  storage  account  key  json  loads  join  json  data  value  storage  alias  mystorageaccount  storage  endpoint  core  windows  net  credentials  credentials  batch  account  batch  account  name  account  key  batch  account  key  account  service  url  batch  service  url  storage  storage  alias  account  storage  account  name  account  key,microsoft
  storage  account  key  endpoint  storage  endpoint  docker  registry  docker  registry  server  username  docker  username  password  docker  password  config  batch  shipyard  storage  account  settings  storage  alias  docker  registry  private  allow  public  docker  hub  pull  on  missing  True  server  docker  registry  server  global  resources  docker  images  container  name  pool  pool  specification  id  scikit  vm  size  STANDARD  NC6  vm  count  publisher  Canonical  offer  UbuntuServer  sku  16  04  LTS  ssh  username  docker  reboot  on  start  task  failed  False  block  until  all  global  resources  loaded  True  jobs  job  specifications  id  cntkjob  tasks  id  run  cifar  This  should  be  changed  per  task  image  container  name  remove  container  after  exit  True  command  bash  source  cntk  activate  cntk  python  code  process  cifar  data  py  ipython  code  cntk  cifar10  py  gpu  True  mkdir  configdef  write  json  to  file  json  dict  filename  with  open  filename  as  ou,microsoft
tfile  json  dump  json  dict  outfile  write  json  to  file  credentials  path  join  config  credentials  json  write  json  to  file  config  path  join  config  config  json  write  json  to  file  pool  path  join  config  pool  json  write  json  to  file  jobs  path  join  config  jobs  json  bash  bg  proc  pool  proc  batch  shipyard  shipyard  pool  add  yes  configdir  config  batchshipyard  pool  list  configdir  config  time  batchshipyard  jobs  add  configdir  config  tail  stdout  txt  batchshipyard  data  stream  configdir  config  filespec  cntkjob  run  cifar  stdout  txt  batchshipyard  data  stream  configdir  config  filespec  cntkjob  run  cifar  stderr  txt  batchshipyard  jobs  del  configdir  config  wait  batchshipyard  pool  del  configdir  config  az  group  delete  group  name  yes  verbose  az  group  delete  docker  registry  group  yes  verbose  ,microsoft
project  id  datalab  project  id  bucket  gs  image  classifier  lite  project  id  gsutil  mb  bucket  print  Execution  complete  import  tensorflow  as  tf  from  tensorflow  python  platform  import  gfile  def  load  graph  graph  filename  with  tf  gfile  GFile  graph  filename  rb  as  graph  def  tf  GraphDef  graph  def  ParseFromString  read  with  tf  Graph  as  default  as  graph  tf  import  graph  def  graph  def  input  map  None  return  elements  None  name  prefix  op  dict  None  producer  op  list  None  input  tensor  graph  get  tensor  by  name  prefix  input  input  tensor  output  tensor  graph  get  tensor  by  name  prefix  final  result  output  tensor  return  graph  input  tensor  output  tensor  print  Execution  complete  def  convert  graph  frozen  graph  input  tensor  output  tensor  lite  file  with  tf  Session  graph  frozen  graph  as  sess  Following  the  TOCO  Python  API  documentation  see  below  convert  the  trained  model  to  TFLite  format  and  save  it  i,google
nto  the  lite  file  file  Documentation  https  github  com  tensorflow  tensorflow  blob  master  tensorflow  contrib  lite  toco  g3doc  python  api  md  YOUR  CODE  HERE  raise  NotImplementedError  print  Execution  complete  def  convert  graph  frozen  graph  input  tensor  output  tensor  lite  file  with  tf  Session  graph  graph  as  sess  tflite  model  tf  contrib  lite  toco  convert  sess  graph  def  input  tensor  output  tensor  with  gfile  GFile  lite  file  wb  as  timeline  file  timeline  file  write  tflite  model  print  Execution  complete  tf  logging  set  verbosity  tf  logging  DEBUG  model  file  gs  candies  ml  model  v2  retrained  graph  pb  graph  input  tensor  output  tensor  load  graph  model  file  lite  file  retrained  graph  tflite  bucket  convert  graph  graph  input  tensor  output  tensor  lite  file  labels  file  gs  candies  ml  model  v2  retrained  labels  txt  gsutil  cp  labels  file  bucket  print  Execution  complete  ,google
import  mxnet  as  mx  from  movielens  data  import  get  data  iter  max  id  load  mldata  iter  from  matrix  fact  import  train  import  matplotlib  pyplot  as  plt  If  MXNet  is  not  compiled  with  GPU  support  on  OSX  set  to  mx  cpu  Can  be  changed  to  mx  gpu  mx  gpu  mx  gpu  if  there  are  GPUs  ctx  mx  cpu  mx  gpu  train  test  data  get  data  iter  batch  size  50  max  user  max  item  max  id  ml  100k  data  max  user  max  item  def  plain  net  input  user  mx  symbol  Variable  user  item  mx  symbol  Variable  item  score  mx  symbol  Variable  score  user  feature  lookup  user  mx  symbol  Embedding  data  user  input  dim  max  user  output  dim  item  feature  lookup  item  mx  symbol  Embedding  data  item  input  dim  max  item  output  dim  predict  by  the  inner  product  which  is  elementwise  product  and  then  sum  pred  user  item  pred  mx  symbol  sum  data  pred  axis  pred  mx  symbol  Flatten  data  pred  loss  layer  pred  mx  symbol  LinearRegressionOut,amazon
put  data  pred  label  score  return  pred  net1  plain  net  64  mx  viz  plot  network  net1  results1  train  net1  train  test  data  num  epoch  15  learning  rate  02  ctx  ctx  plt  plot  results1  plt  ylabel  RMSE  plt  xlabel  iterations  per  500  mini  batch  plt  title  RMSE  History  of  Liner  MF  plt  show  def  get  one  layer  mlp  hidden  input  user  mx  symbol  Variable  user  item  mx  symbol  Variable  item  score  mx  symbol  Variable  score  user  latent  features  user  mx  symbol  Embedding  data  user  input  dim  max  user  output  dim  user  mx  symbol  Activation  data  user  act  type  relu  user  mx  symbol  FullyConnected  data  user  num  hidden  hidden  item  latent  features  item  mx  symbol  Embedding  data  item  input  dim  max  item  output  dim  item  mx  symbol  Activation  data  item  act  type  relu  item  mx  symbol  FullyConnected  data  item  num  hidden  hidden  predict  by  the  inner  product  pred  user  item  pred  mx  symbol  sum  data  pred  axis  pred ,amazon
 mx  symbol  Flatten  data  pred  loss  layer  pred  mx  symbol  LinearRegressionOutput  data  pred  label  score  return  pred  net2  get  one  layer  mlp  64  64  mx  viz  plot  network  net2  results2  train  net2  train  test  data  num  epoch  15  learning  rate  02  ctx  ctx  plt  plot  results1  label  Linear  MF  plt  plot  results2  label  Non  Linear  MF  plt  ylabel  RMSE  plt  xlabel  iterations  per  500  mini  batch  plt  title  RMSE  History  plt  legend  bbox  to  anchor  05  loc  borderaxespad  plt  show  What  if  we  let  the  linear  model  train  for  longer  time  results3  train  net1  train  test  data  num  epoch  30  learning  rate  02  ctx  ctx  plt  plot  results1  label  Linear  MF  plt  plot  results2  label  Non  Linear  MF  plt  plot  results3  label  Linear  MF  with  30  epoch  plt  ylabel  RMSE  plt  xlabel  iterations  per  500  mini  batch  plt  title  RMSE  History  plt  legend  bbox  to  anchor  05  loc  borderaxespad  plt  show  ,amazon
time  sh  The  name  of  our  algorithm  algorithm  name  decision  trees  cd  container  chmod  decision  trees  train  chmod  decision  trees  serve  account  aws  sts  get  caller  identity  query  Account  output  text  Get  the  region  defined  in  the  current  configuration  default  to  us  west  if  none  defined  region  aws  configure  get  region  region  region  us  east  fullname  account  dkr  ecr  region  amazonaws  com  algorithm  name  latest  If  the  repository  doesn  exist  in  ECR  create  it  aws  ecr  describe  repositories  repository  names  algorithm  name  dev  null  if  ne  then  aws  ecr  create  repository  repository  name  algorithm  name  dev  null  fi  Get  the  login  command  from  ECR  and  execute  it  directly  aws  ecr  get  login  region  region  no  include  email  Build  the  docker  image  locally  with  the  image  name  and  then  push  it  to  ECR  with  the  full  name  docker  build  algorithm  name  docker  tag  algorithm  name  fullname  docker  push  full,amazon
name  S3  prefix  prefix  tflearn  planesnet  Define  IAM  role  import  boto3  import  re  import  os  import  numpy  as  np  import  pandas  as  pd  from  sagemaker  import  get  execution  role  role  get  execution  role  import  sagemaker  as  sage  from  time  import  gmtime  strftime  sess  sage  Session  unzip  data  planesnet  json  zip  data  time  WORK  DIRECTORY  data  data  location  sess  upload  data  WORK  DIRECTORY  key  prefix  prefix  time  account  sess  boto  session  client  sts  get  caller  identity  Account  region  sess  boto  session  region  name  image  dkr  ecr  amazonaws  com  decision  trees  latest  format  account  region  tree  sage  estimator  Estimator  image  role  ml  c5  9xlarge  output  path  s3  output  format  sess  default  bucket  sagemaker  session  sess  tree  fit  data  location  time  from  sagemaker  predictor  import  csv  serializer  predictor  tree  deploy  initial  instance  count  instance  type  ml  m4  xlarge  serializer  csv  serializer  time  import  ,amazon
sys  import  os  import  numpy  as  np  from  PIL  import  Image  from  scipy  import  ndimage  in  fname  scene  png  out  fname  None  Load  trained  model  model  load  model  fname  Read  input  image  data  im  Image  open  in  fname  arr  np  array  im  shape  arr  shape  Set  output  fname  if  not  out  fname  out  fname  os  path  splitext  in  fname  detection  png  Create  detection  variables  detections  np  zeros  shape  shape  dtype  uint8  output  np  copy  arr  Sliding  window  parameters  step  win  20  Loop  through  pixel  positions  print  Processing  for  in  range  shape  win  step  print  row  0f  of  0f  shape  win  for  in  range  shape  win  step  Extract  sub  chip  chip  arr  win  win  chip  tostring  prediction  int  predictor  predict  decode  utf  Record  positive  detections  if  prediction  detections  int  win  int  win  Process  detection  locations  dilation  ndimage  binary  dilation  detections  structure  np  ones  labels  labels  ndimage  label  dilation  center  mass ,amazon
 ndimage  center  of  mass  dilation  labels  np  arange  labels  Loop  through  detection  locations  if  type  center  mass  tuple  center  mass  center  mass  for  in  center  mass  int  win  int  win  Draw  bounding  boxes  in  output  array  output  win  255  output  win  win  win  255  output  win  255  output  win  win  win  255  Save  output  image  outIm  Image  fromarray  output  outIm  save  out  fname  Display  aircraft  detected  from  scene  png  from  IPython  display  import  Image  Image  scene  detection  png  sess  delete  endpoint  predictor  endpoint  ,amazon
import  tensorflow  as  tffrom  tensorflow  examples  tutorials  mnist  import  input  datamnist  input  data  read  data  sets  MNIST  data  one  hot  True  type  mnist  mnist  train  imagesmnist  train  num  examplesmnist  test  num  examplesmnist  validation  num  examplesimport  matplotlib  pyplot  as  plt  matplotlib  inlinemnist  train  images  shapeplt  imshow  mnist  train  images  reshape  28  28  plt  imshow  mnist  train  images  reshape  28  28  cmap  gist  gray  mnist  train  images  max  plt  imshow  mnist  train  images  reshape  784  plt  imshow  mnist  train  images  reshape  784  cmap  gist  gray  aspect  02  tf  placeholder  tf  float32  shape  None  784  10  because  possible  numbers  tf  Variable  tf  zeros  784  10  tf  Variable  tf  zeros  10  Create  the  Graph  tf  matmul  true  tf  placeholder  tf  float32  None  10  Cross  Entropycross  entropy  tf  reduce  mean  tf  nn  softmax  cross  entropy  with  logits  labels  true  logits  optimizer  tf  train  GradientDescentOptimizer  lea,amazon
rning  rate  train  optimizer  minimize  cross  entropy  init  tf  global  variables  initializer  with  tf  Session  as  sess  sess  run  init  Train  the  model  for  1000  steps  on  the  training  set  Using  built  in  batch  feeder  from  mnist  for  convenience  for  step  in  range  1000  batch  batch  mnist  train  next  batch  100  sess  run  train  feed  dict  batch  true  batch  Test  the  Train  Model  matches  tf  equal  tf  argmax  tf  argmax  true  acc  tf  reduce  mean  tf  cast  matches  tf  float32  print  sess  run  acc  feed  dict  mnist  test  images  true  mnist  test  labels  ,amazon
import  boto3  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  mxnet  as  mx  data  mx  test  utils  get  mnist  from  mnist  import  train  model  train  data  data  num  cpus  num  gpus  import  os  import  json  os  mkdir  model  model  save  checkpoint  model  model  0000  with  open  model  model  shapes  json  as  shapes  json  dump  shape  model  data  shapes  name  data  shapes  import  tarfile  def  flatten  tarinfo  tarinfo  name  os  path  basename  tarinfo  name  return  tarinfo  tar  tarfile  open  model  tar  gz  gz  tar  add  model  filter  flatten  tar  close  import  sagemaker  sagemaker  session  sagemaker  Session  inputs  sagemaker  session  upload  data  path  model  tar  gz  key  prefix  model  from  sagemaker  mxnet  model  import  MXNetModel  sagemaker  model  MXNetModel  model  data  s3  sagemaker  session  default  bucket  model  model  tar  gz  role  role  entry  point  mnist  py  import  logging  logging  getLogger  setLevel  logging  WARNING ,amazon
 predictor  sagemaker  model  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  IPython  display  import  HTML  HTML  open  input  html  read  response  predictor  predict  data  print  Raw  prediction  result  print  response  labeled  predictions  list  zip  range  10  response  print  Labeled  predictions  print  labeled  predictions  labeled  predictions  sort  key  lambda  label  and  prob  label  and  prob  print  Most  likely  answer  format  labeled  predictions  print  predictor  endpoint  sagemaker  Session  delete  endpoint  predictor  endpoint  os  remove  model  tar  gz  import  shutil  shutil  rmtree  model  ,amazon
Timing  function  from  datetime  import  datetime  Timings  def  timing  tag  fromTag  None  Timings  tag  datetime  now  if  fromTag  print  at  elapsed  since  format  tag  Timings  tag  fromTag  Timings  tag  Timings  fromTag  else  print  at  format  tag  Timings  tag  timing  Start  hidden  cell  The  following  code  contains  the  credentials  for  file  in  your  IBM  Cloud  Object  Storage  You  might  want  to  remove  those  credentials  before  you  share  your  notebook  YourCredentials  IBM  API  KEY  ID  ZATI1oq  TlsWqN  oEz3Wo1IPXWwOkCx0PXV3gj0d5eui  IAM  SERVICE  ID  iam  ServiceId  521e4bd0  5161  49f0  9e11  474c674a5fe7  ENDPOINT  https  s3  api  us  geo  objectstorage  service  networklayer  com  IBM  AUTH  ENDPOINT  https  iam  ng  bluemix  net  oidc  token  BUCKET  watstudworkshop  donotdelete  pr  basx79wonvxlys  FILE  201701  citibike  tripdata  csv  NOTTHISONE  Insert  YOUR  OWN  Credentials  as  YourCredentials  in  the  cell  above  Setup  access  to  COS  import  sys  import  typ,ibm
es  import  pandas  as  pd  from  botocore  client  import  Config  import  ibm  boto3  def  iter  self  return  try  raise  Exception  YourCredentials  NOTTHISONE  except  KeyError  pass  bucket  name  YourCredentials  BUCKET  object  name  2017  csv  cos  ibm  boto3  client  s3  ibm  api  key  id  YourCredentials  IBM  API  KEY  ID  ibm  auth  endpoint  YourCredentials  IBM  AUTH  ENDPOINT  config  Config  signature  version  oauth  endpoint  url  YourCredentials  ENDPOINT  timing  BeforeFTPAndCOS  Get  the  file  via  FTP  download  rm  2017  csv  wget  ftp  ftp  ncdc  noaa  gov  pub  data  ghcn  daily  by  year  2017  csv  gz  ls  2017  Unzip  the  file  gunzip  2017  csv  gz  ls  2017  Send  the  file  to  COS  from  file  system  with  open  object  name  rb  as  cos  upload  fileobj  bucket  name  object  name  print  Done  file  uploaded  to  COS  bucket  format  object  name  bucket  name  timing  AfterFTPAndCOS  BeforeFTPAndCOS  import  ibmos2spark  timing  BeforeDownloadingFromCOS  Create  credenti,ibm
als  in  the  format  required  by  CloudObjectStorage  credentials  endpoint  YourCredentials  ENDPOINT  api  key  YourCredentials  IBM  API  KEY  ID  service  id  YourCredentials  IAM  SERVICE  ID  configuration  name  wheather  data  2017  config  os  See  https  github  com  ibm  watson  data  lab  ibmos2spark  tree  master  python  cos  ibmos2spark  CloudObjectStorage  sc  credentials  configuration  name  configuration  name  cos  type  bluemix  cos  The  sc  object  is  your  SparkContext  object  The  cos  object  will  provide  the  URL  for  SparkContext  to  retrieve  your  data  Get  the  URL  data  url  cos  url  object  name  bucket  name  timing  AfterDownloadingFromCOS  BeforeDownloadingFromCOS  print  COS  URL  for  the  data  asset  format  data  url  timing  BeforeLoadingData  Now  we  load  the  data  Note  that  Spark  uses  lazy  evaluation  so  the  lengthy  operation  will  be  take  weather  sc  textFile  data  url  take  triggers  the  Spark  job  that  loads  the  data  See  the  Sp,ibm
ark  Job  Progress  gauge  weather  take  timing  AfterLoadingData  BeforeLoadingData  print  Total  records  in  the  data  set  format  weather  count  print  The  first  row  in  the  data  set  format  weather  first  Create  RDD  from  Python  array  to  represent  Header  see  https  spark  apache  org  docs  latest  api  python  pyspark  html  pyspark  SparkContext  parallelize  header  sc  parallelize  STATION  DATE  METRIC  VALUE  C5  C6  C7  C8  Append  the  header  and  data  into  single  RDD  weather  header  union  weather  weather  take  timing  BeforeParsing  weatherParse  weather  map  lambda  line  line  split  weatherParse  first  weatherParse  first  weatherParse  first  timing  AfterParsing  BeforeParsing  Create  new  RDD  which  holds  only  rows  which  represent  precipitation  events  weatherPrecp  weatherParse  filter  lambda  PRCP  Display  first  lines  weatherPrecp  take  Map  to  tuples  with  station  ID  as  first  element  then  tuple  made  of  the  precipitation  measure  a,ibm
nd  constant  is  the  station  is  the  precipitation  value  weatherPrecpCountByKey  weatherPrecp  map  lambda  int  weatherPrecpCountByKey  take  timing  AfterFiltering  AfterParsing  weatherPrecpAddByKey  weatherPrecpCountByKey  reduceByKey  lambda  v1  v2  v1  v2  v1  v2  weatherPrecpAddByKey  first  weatherAverages  weatherPrecpAddByKey  map  lambda  float  weatherAverages  first  for  pair  in  weatherAverages  top  10  print  Station  had  average  precipitations  of  2f  format  pair  pair  precTop10  stationsTop10  for  pair  in  weatherAverages  map  lambda  top  10  precTop10  append  pair  stationsTop10  append  pair  print  Station  had  average  precipitations  of  2f  format  pair  pair  timing  AfterAverages  AfterFiltering  matplotlib  inline  import  numpy  as  np  import  matplotlib  pyplot  as  plt  10  index  np  arange  bar  width  plt  bar  index  precTop10  bar  width  color  plt  xlabel  Stations  plt  ylabel  Precipitations  plt  title  10  stations  with  the  highest  average  pre,ibm
cipitation  plt  xticks  index  bar  width  stationsTop10  rotation  90  plt  show  timing  BeginSparkSQL  Filter  where  type  is  snow  weatherSnow  weatherParse  filter  lambda  SNOW  print  There  are  SNOW  events  format  weatherSnow  count  timing  BeginBuildSparkSQLDataFrame  from  datetime  import  datetime  from  pyspark  sql  import  Row  spark  SparkSession  builder  getOrCreate  Convert  each  line  of  snowWeather  RDD  into  Row  object  snowRows  weatherSnow  map  lambda  Row  station  month  datetime  strptime  month  date  datetime  strptime  day  metric  value  int  Apply  Row  schema  to  create  Spark  DataFrame  snowSchema  spark  createDataFrame  snowRows  Register  snow2017  table  with  columns  station  month  date  metric  and  value  snowSchema  registerTempTable  snow2017  timing  EndBuildSparkSQLDataFrame  BeginBuildSparkSQLDataFrame  timing  BeginCountSnowDays  snow  US10chey021  spark  sql  SELECT  month  COUNT  AS  snowdays  FROM  snow2017  WHERE  station  US10chey021  GROUP  ,ibm
BY  month  ORDER  BY  month  collect  timing  EndCountSnowDays  BeginCountSnowDays  snow  US10chey021  Convert  to  python  array  of  12  elements  initialized  to  US10chey021  snowdays  12  fill  in  array  with  snow  days  per  month  notice  the  indexed  array  versus  indexed  months  ranks  for  row  in  snow  US10chey021  US10chey021  snowdays  row  month  row  snowdays  print  Snow  days  per  month  US10chey021  snowdays  timing  BeginCountSnowDays  snow  USW00094985  spark  sql  SELECT  month  COUNT  AS  snowdays  FROM  snow2017  WHERE  station  USW00094985  GROUP  BY  month  ORDER  BY  month  collect  timing  EndCountSnowDays  BeginCountSnowDays  Create  array  of  12  to  start  with  USW00094985  snowdays  12  For  each  row  compute  number  of  snow  days  for  row  in  snow  USW00094985  USW00094985  snowdays  row  month  row  snowdays  print  USW00094985  snowdays  matplotlib  inline  import  matplotlib  import  numpy  as  np  import  matplotlib  pyplot  as  plt  12  ind  np  arange  width,ibm
  35  pUS10chey021  plt  bar  ind  US10chey021  snowdays  width  color  label  US10chey021  pUSW00094985  plt  bar  ind  width  USW00094985  snowdays  width  color  label  USW00094985  plt  ylabel  SNOW  DAYS  plt  xlabel  MONTH  plt  title  Snow  Days  in  2017  at  Stations  US10chey021  vs  USW00094985  plt  xticks  ind  width  Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec  plt  legend  plt  show  timing  BeginCountSnowDays  snowStations  spark  sql  SELECT  station  COUNT  AS  snowdays  FROM  snow2017  WHERE  station  LIKE  US  GROUP  BY  station  ORDER  BY  station  LIMIT  100  snowStations  head  timing  EndCountSnowDays  BeginCountSnowDays  snowStations  registerTempTable  snowdays  2017  snowStations  top5  spark  sql  SELECT  station  snowdays  FROM  snowdays  2017  ORDER  BY  snowdays  DESC  LIMIT  collect  for  row  in  snowStations  top5  print  row  Query  the  station  snowdays  station  snowdays  spark  sql  SELECT  station  snowdays  FROM  snowdays  2017  ORDER  BY  snowdays  Make,ibm
  RDD  with  snowdays  as  first  column  used  as  key  snowday  station  station  snowdays  rdd  map  lambda  snowdays  station  Collapse  by  key  snowdays  and  make  list  of  stations  as  second  column  snowday  stationsList  snowday  station  reduceByKey  lambda  sortByKey  collect  for  snowday  in  snowday  stationsList  print  Snow  days  Stations  format  snowday  snowday  timing  EndSparkSQLQueries  BeginSparkSQL  Save  as  parquet  file  If  you  are  running  this  cell  multiple  times  you  will  need  to  overwrite  the  data  in  the  parquet  file  snowStations  write  mode  overwrite  parquet  bmos  url  CONTAINER  snowStations  parquet  snowStations  url  cos  url  snowStations  parquet  format  int  datetime  now  timestamp  bucket  name  snowStations  write  parquet  snowStations  url  timing  EndWriteToCOS  EndSparkSQLQueries  snowDaysParquetFile  spark  read  parquet  cos  url  snowStations  parquet  bucket  name  snowDaysParquetFile  registerTempTable  snow  from  parquet  timing  ,ibm
ReadFromCOS  EndWriteToCOS  Display  structure  of  the  DataFrame  snowDaysParquetFile  describe  station  snowdays  spark  sql  SELECT  DISTINCT  COUNT  AS  countSnow  FROM  snow  from  parquet  print  There  are  stations  format  station  snowdays  first  countSnow  timing  EndSQLFromCOS  ReadFromCOS  timing  End  Start  ,ibm
from  sklearn  feature  extraction  text  import  CountVectorizer  from  sklearn  cross  validation  import  train  test  split  from  sklearn  linear  model  import  LogisticRegression  from  sklearn  metrics  import  accuracy  score  from  sklearn  externals  import  joblib  import  random  import  boto3  import  warnings  warnings  filterwarnings  action  ignore  category  Warning  s3  boto3  resource  s3  BUCKET  NAME  aiml  rumi  MODEL  FINE  KEY  ServerlessAIWorkshop  SentimentAnalysis  sentiment  analysis  tweet  pkl  TEST  FILE  KEY  ServerlessAIWorkshop  SentimentAnalysis  test  data  TEST  FILE  KEY  ServerlessAIWorkshop  SentimentAnalysis  test  data  data  data  labels  label  will  be  either  pos  or  neg  with  open  pos  tweets  txt  as  for  in  data  append  data  labels  append  pos  with  open  neg  tweets  txt  as  for  in  data  append  data  labels  append  neg  vectorizer  CountVectorizer  analyzer  word  exclude  common  words  such  as  the  or  and  lowercase  False  features  vecto,amazon
rizer  fit  transform  data  features  nd  features  toarray  train  size  seed  train  test  train  test  train  test  split  features  nd  data  labels  train  size  train  size  random  state  seed  log  model  LogisticRegression  using  LogisticRegression  class  from  Sciki  learn  log  model  log  model  fit  train  train  train  the  model  using  the  training  datasety  pred  log  model  predict  test  print  accuracy  score  test  pred  save  the  trained  model  in  the  memory  to  pickle  file  joblib  dump  log  model  sentiment  analysis  tweet  pkl  joblib  dump  test  test  csv  joblib  dump  test  test  csv  upload  the  trailed  model  pickle  file  as  well  as  test  dataset  to  S3  bucket  s3  meta  client  upload  file  sentiment  analysis  tweet  pkl  BUCKET  NAME  MODEL  FINE  KEY  s3  meta  client  upload  file  test  csv  BUCKET  NAME  TEST  FILE  KEY  s3  meta  client  upload  file  test  csv  BUCKET  NAME  TEST  FILE  KEY  ,amazon
import  tensorflow  as  tffrom  tensorflow  examples  tutorials  mnist  import  input  datamnist  input  data  read  data  sets  MNIST  data  one  hot  True  def  init  weights  shape  init  random  dist  tf  truncated  normal  shape  stddev  return  tf  Variable  init  random  dist  def  init  bias  shape  init  bias  vals  tf  constant  shape  shape  return  tf  Variable  init  bias  vals  def  conv2d  return  tf  nn  conv2d  strides  padding  SAME  def  max  pool  2by2  return  tf  nn  max  pool  ksize  strides  padding  SAME  def  convolutional  layer  input  shape  init  weights  shape  init  bias  shape  return  tf  nn  relu  conv2d  input  def  normal  full  layer  input  layer  size  input  size  int  input  layer  get  shape  init  weights  input  size  size  init  bias  size  return  tf  matmul  input  layer  bx  tf  placeholder  tf  float32  shape  None  784  true  tf  placeholder  tf  float32  shape  None  10  image  tf  reshape  28  28  Using  6by6  filter  here  used  5by5  in  video  you  can  ,amazon
play  around  with  the  filter  size  You  can  change  the  32  output  that  essentially  represents  the  amount  of  filters  used  You  need  to  pass  in  32  to  the  next  input  though  the  comes  from  the  original  input  of  single  image  convo  convolutional  layer  image  shape  32  convo  pooling  max  pool  2by2  convo  Using  6by6  filter  here  used  5by5  in  video  you  can  play  around  with  the  filter  size  You  can  actually  change  the  64  output  if  you  want  you  can  think  of  that  as  representation  of  the  amount  of  6by6  filters  used  convo  convolutional  layer  convo  pooling  shape  32  64  convo  pooling  max  pool  2by2  convo  Why  by  image  Because  we  did  pooling  layers  so  28  64  then  just  comes  from  the  output  of  the  previous  Convolution  convo  flat  tf  reshape  convo  pooling  64  full  layer  one  tf  nn  relu  normal  full  layer  convo  flat  1024  NOTE  THE  PLACEHOLDER  HERE  hold  prob  tf  placeholder  tf  float32  full  one  ,amazon
dropout  tf  nn  dropout  full  layer  one  keep  prob  hold  prob  pred  normal  full  layer  full  one  dropout  10  cross  entropy  tf  reduce  mean  tf  nn  softmax  cross  entropy  with  logits  labels  true  logits  pred  optimizer  tf  train  AdamOptimizer  learning  rate  0001  train  optimizer  minimize  cross  entropy  init  tf  global  variables  initializer  steps  5000  with  tf  Session  as  sess  sess  run  init  for  in  range  steps  batch  batch  mnist  train  next  batch  50  sess  run  train  feed  dict  batch  true  batch  hold  prob  PRINT  OUT  MESSAGE  EVERY  100  STEPS  if  100  print  Currently  on  step  format  print  Accuracy  is  Test  the  Train  Model  matches  tf  equal  tf  argmax  pred  tf  argmax  true  acc  tf  reduce  mean  tf  cast  matches  tf  float32  print  sess  run  acc  feed  dict  mnist  test  images  true  mnist  test  labels  hold  prob  print  ,amazon
import  tensorflow  as  tf  import  matplotlib  pyplot  as  pltprint  TF  Version  tf  version  fashion  mnist  tf  keras  datasets  fashion  mnist  train  images  train  labels  test  images  test  labels  fashion  mnist  load  data  print  Fashion  MNIST  print  Training  set  images  shape  shape  format  shape  train  images  shape  print  Training  set  labels  shape  shape  format  shape  train  labels  shape  print  Test  set  images  shape  shape  format  shape  test  images  shape  print  Test  set  labels  shape  shape  format  shape  test  labels  shape  labels  lookup  shirt  top  Trouser  Pullover  Dress  Coat  Sandal  Shirt  Sneaker  Bag  Ankle  boot  plt  figure  figsize  10  10  for  in  range  25  plt  subplot  plt  xticks  plt  yticks  plt  grid  False  plt  imshow  train  images  cmap  plt  cm  binary  plt  xlabel  labels  lookup  train  labels  Converts  MNIST  data  to  TFRecords  file  format  import  os  def  int64  feature  value  return  tf  train  Feature  int64  list  tf  train  Int,amazon
64List  value  value  def  bytes  feature  value  return  tf  train  Feature  bytes  list  tf  train  BytesList  value  value  def  float  feature  value  return  tf  train  Feature  float  list  tf  train  FloatList  value  value  def  convert  mnist  fashion  dataset  images  labels  name  directory  import  pathlib  height  width  images  shape  filename  os  path  join  directory  name  tfrecords  pathlib  Path  directory  mkdir  parents  True  exist  ok  True  print  Writing  filename  with  tf  python  io  TFRecordWriter  filename  as  writer  for  index  in  range  len  images  image  raw  images  index  tostring  example  tf  train  Example  features  tf  train  Features  feature  height  int64  feature  height  width  int64  feature  width  channels  int64  feature  label  int64  feature  int  labels  index  image  raw  bytes  feature  image  raw  writer  write  example  SerializeToString  convert  mnist  fashion  dataset  train  images  train  labels  train  data  convert  mnist  fashion  dataset  t,amazon
est  images  test  labels  validation  data  import  sagemaker  bucket  sagemaker  Session  default  bucket  Automatically  create  bucket  prefix  radix  mnist  fashion  tutorial  Subfolder  prefix  role  sagemaker  get  execution  role  s3  url  sagemaker  Session  upload  data  path  data  bucket  bucket  key  prefix  prefix  data  mnist  print  s3  url  ,amazon
Adding  variables  rollingChangeFeed  False  startFromTheBeginning  False  useNextToken  True  database  changefeedsource  collection  tweet  new  tweetsConfig  Endpoint  https  dbstreamdemo  documents  azure  com  443  Masterkey  ekRLXkETPJ93s6XZz4YubZOw1mjSnoO5Bhz1Gk29bVxCbtgtKmiyRz4SogOSxLOGTouXbwlaAHcHOzct4JVwtQ  Database  database  Collection  collection  Database  changefeedsource  Collection  tweet  new  ReadChangeFeed  true  ChangeFeedQueryName  database  collection  ChangeFeedStartFromTheBeginning  str  startFromTheBeginning  ChangeFeedUseNextToken  str  useNextToken  RollingChangeFeed  str  rollingChangeFeed  ChangeFeedCheckpointLocation  changefeedcheckpointlocation  SamplingRatio  Adding  Read  DataFrame  SparkSession  available  as  spark  tweets  spark  read  format  com  microsoft  azure  cosmosdb  spark  options  tweetsConfig  load  Get  the  number  of  tweets  tweets  count  display  tweets  tweets  printSchema  Create  tweets  TempView  This  way  we  can  run  SQL  statements  within  the ,microsoft
 notebook  tweets  createOrReplaceTempView  tweets  sql  select  count  from  tweets  sql  select  id  created  at  user  screen  name  user  location  text  retweet  count  entities  hashtags  entities  user  mentions  favorited  source  from  tweets  limit  20  sql  select  concat  concat  dense  rank  OVER  PARTITION  BY  ORDER  BY  tweets  DESC  text  as  hashtags  tweets  from  select  hashtags  text  count  distinct  id  as  tweets  from  select  explode  entities  hashtags  as  hashtags  id  from  tweets  group  by  hashtags  text  order  by  tweets  desc  limit  10  Import  Necessary  Libraries  import  pydocumentdb  from  pydocumentdb  import  document  client  from  pydocumentdb  import  documents  import  datetime  Configuring  the  connection  policy  allowing  for  endpoint  discovery  connectionPolicy  documents  ConnectionPolicy  connectionPolicy  EnableEndpointDiscovery  connectionPolicy  PreferredLocations  Japan  East  Japan  West  Set  keys  to  connect  to  Cosmos  DB  masterKey  b3KPBHQvW,microsoft
TD8prYsQDiHlaM8kDzBholipD1sgshjT60ayDK9WkvRAT0Qywsi5FkcyKsYcvF4iIrUEBBzaZwJKw  host  https  videoanalytics  documents  azure  com  443  client  document  client  DocumentClient  host  masterKey  masterKey  connectionPolicy  Configure  Database  and  Collections  databaseId  asset  collectionId  meta  Configurations  the  Cosmos  DB  client  will  use  to  connect  to  the  database  and  collection  dbLink  dbs  databaseId  collLink  dbLink  colls  collectionId  Set  query  parameter  querystr  SELECT  City  FROM  WHERE  State  WA  querystr  SELECT  FROM  Query  documents  query  client  QueryDocuments  collLink  querystr  options  None  partition  key  None  Query  for  partitioned  collections  query  client  QueryDocuments  collLink  query  options  enableCrossPartitionQuery  True  partition  key  None  Push  into  list  elements  elements  list  query  print  elements  ,microsoft
import  os  import  sagemaker  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  DEMO  abalone  cat  abalone  py  from  sagemaker  tensorflow  import  TensorFlow  abalone  estimator  TensorFlow  entry  point  abalone  py  role  role  training  steps  100  evaluation  steps  100  hyperparameters  learning  rate  001  train  instance  count  train  instance  type  ml  c4  xlarge  abalone  estimator  fit  inputs  abalone  predictor  abalone  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  import  tensorflow  as  tf  import  numpy  as  np  prediction  set  tf  contrib  learn  datasets  base  load  csv  without  header  filename  os  path  join  data  abalone  predict  csv  target  dtype  np  int  features  dtype  np  float32  data  prediction  set  data  tensor  proto  tf  make  tensor  proto  values  np  asarray  data  shape  len  data  dtype  tf  ,amazon
float32  abalone  predictor  predict  tensor  proto  sagemaker  Session  delete  endpoint  abalone  predictor  endpoint  ,amazon
cat  container  Dockerfile  sh  The  name  of  our  algorithm  algorithm  name  decision  trees  sample  cd  container  chmod  decision  trees  train  chmod  decision  trees  serve  account  aws  sts  get  caller  identity  query  Account  output  text  Get  the  region  defined  in  the  current  configuration  default  to  us  west  if  none  defined  region  aws  configure  get  region  region  region  us  west  fullname  account  dkr  ecr  region  amazonaws  com  algorithm  name  latest  If  the  repository  doesn  exist  in  ECR  create  it  aws  ecr  describe  repositories  repository  names  algorithm  name  dev  null  if  ne  then  aws  ecr  create  repository  repository  name  algorithm  name  dev  null  fi  Get  the  login  command  from  ECR  and  execute  it  directly  aws  ecr  get  login  region  region  no  include  email  Build  the  docker  image  locally  with  the  image  name  and  then  push  it  to  ECR  with  the  full  name  docker  build  algorithm  name  docker  tag  algorithm  name,amazon
  fullname  docker  push  fullname  S3  prefix  prefix  DEMO  scikit  byo  iris  Define  IAM  role  import  boto3  import  re  import  os  import  numpy  as  np  import  pandas  as  pd  from  sagemaker  import  get  execution  role  role  get  execution  role  import  sagemaker  as  sage  from  time  import  gmtime  strftime  sess  sage  Session  WORK  DIRECTORY  data  data  location  sess  upload  data  WORK  DIRECTORY  key  prefix  prefix  account  sess  boto  session  client  sts  get  caller  identity  Account  region  sess  boto  session  region  name  image  dkr  ecr  amazonaws  com  decision  trees  sample  latest  format  account  region  tree  sage  estimator  Estimator  image  role  ml  c4  2xlarge  output  path  s3  output  format  sess  default  bucket  sagemaker  session  sess  tree  fit  data  location  from  sagemaker  predictor  import  csv  serializer  predictor  tree  deploy  ml  m4  xlarge  serializer  csv  serializer  shape  pd  read  csv  data  iris  csv  header  None  import  itertools  ,amazon
50  for  in  range  40  for  in  range  10  indices  for  in  itertools  product  test  data  shape  iloc  indices  test  test  data  iloc  test  test  data  iloc  print  predictor  predict  test  values  decode  utf  sess  delete  endpoint  predictor  endpoint  ,amazon
Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  import  sagemaker  bucket  sagemaker  Session  default  bucket  prefix  sagemaker  DEMO  xgboost  churn  role  get  execution  role  import  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  import  io  import  os  import  sys  import  time  import  json  from  IPython  display  import  display  from  time  import  strftime  gmtime  import  sagemaker  from  sagemaker  predictor  import  csv  serializer  wget  http  dataminingconsultant  com  DKD2e  data  sets  zip  unzip  DKD2e  data  sets  zipchurn  pd  read  csv  Data  sets  churn  txt  pd  set  option  display  max  columns  500  churn  head  len  churn  Frequency  tables  for  each  categorical  feature  for  column  in  churn  select  dtypes  include  object  columns  display  pd  crosstab  index  churn  column  columns  observations  normalize  columns  Histograms  for  each  numeric  features  display  churn  describe  matplotlib  inli,amazon
ne  hist  churn  hist  bins  30  sharey  True  figsize  10  10  churn  churn  drop  Phone  axis  churn  Area  Code  churn  Area  Code  astype  object  for  column  in  churn  select  dtypes  include  object  columns  if  column  Churn  display  pd  crosstab  index  churn  column  columns  churn  Churn  normalize  columns  for  column  in  churn  select  dtypes  exclude  object  columns  print  column  hist  churn  column  Churn  hist  by  Churn  bins  30  plt  show  display  churn  corr  pd  plotting  scatter  matrix  churn  figsize  12  12  plt  show  churn  churn  drop  Day  Charge  Eve  Charge  Night  Charge  Intl  Charge  axis  model  data  pd  get  dummies  churn  model  data  pd  concat  model  data  Churn  True  model  data  drop  Churn  False  Churn  True  axis  axis  train  data  validation  data  test  data  np  split  model  data  sample  frac  random  state  1729  int  len  model  data  int  len  model  data  train  data  to  csv  train  csv  header  False  index  False  validation  data  to  csv ,amazon
 validation  csv  header  False  index  False  test  data  to  csv  test  csv  header  False  index  False  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  train  csv  upload  file  train  csv  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  validation  csv  upload  file  validation  csv  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  boto3  Session  region  name  xgboost  s3  input  train  sagemaker  s3  input  s3  data  s3  train  format  bucket  prefix  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  s3  validation  format  bucket  prefix  content  type  csv  sess  sagemaker  Session  xgb  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  m4  xlarge  output  path  s3  output  format  bucket  prefix  sagemaker  session  sess  xgb  set  hyperparameters  max  depth  eta  min  child  weight  subsample  sile,amazon
nt  objective  binary  logistic  num  round  70  xgb  fit  train  s3  input  train  validation  s3  input  validation  xgb  predictor  xgb  deploy  initial  instance  count  instance  type  ml  m4  xlarge  xgb  predictor  content  type  text  csv  xgb  predictor  serializer  csv  serializer  xgb  predictor  deserializer  Nonetest  data  head  def  predict  data  rows  500  split  array  np  array  split  data  int  data  shape  float  rows  predictions  for  array  in  split  array  predictions  join  predictions  xgb  predictor  predict  array  decode  utf  return  np  fromstring  predictions  sep  predictions  predict  test  data  as  matrix  pd  crosstab  index  test  data  iloc  columns  np  round  predictions  rownames  actual  colnames  predictions  plt  hist  predictions  plt  show  pd  crosstab  index  test  data  iloc  columns  np  where  predictions  from  sklearn  metrics  import  accuracy  score  f1  score  precision  score  recall  score  classification  report  confusion  matrix  roc  curve  fro,amazon
m  sklearn  metrics  import  precision  recall  curve  average  precision  score  precision  recall  fscore  supportdef  f1  get  label  bin  if  cont  else  for  cont  in  change  the  prob  to  class  output  return  f1  f1  score  bin  def  print  evaluation  metric  true  pred  precision  recall  fscore  support  precision  recall  fscore  support  true  pred  print  Precision  format  precision  print  Recall  format  recall  print  score  format  fscore  print  Support  format  support  return  def  plot  roc  curve  true  prob  fpr  tpr  threshold  roc  curve  true  prob  fig  plt  gcf  fig  set  size  inches  10  plt  title  Receiver  Operating  Characteristic  ROC  plt  plot  fpr  tpr  plt  plot  plt  xlim  plt  ylim  plt  ylabel  True  Positive  Rate  plt  xlabel  False  Positive  Rate  plt  show  returnprint  evaluation  metric  test  data  iloc  np  where  predictions  plot  roc  curve  test  data  iloc  predictions  sagemaker  Session  delete  endpoint  xgb  predictor  endpoint  ,amazon
import  os  import  csv  from  urlparse  import  urlparse  import  boto3  import  numpy  as  np  from  scipy  sparse  import  lil  matrix  BUCKET  proserve  emea  ai  workshops  s3  boto3  client  s3  def  download  file  s3  source  dest  if  not  os  path  exists  dest  os  makedirs  dest  url  urlparse  s3  source  bucket  key  url  netloc  url  path  lstrip  file  name  key  split  with  open  dest  file  name  wb  as  data  s3  download  fileobj  bucket  key  data  def  loadDataset  filename  lines  columns  Features  are  one  hot  encoded  in  sparse  matrix  lil  matrix  lines  columns  astype  float32  Labels  are  stored  in  vector  line  with  open  filename  as  samples  csv  reader  delimiter  for  userId  movieId  rating  timestamp  in  samples  line  int  userId  line  int  nbUsers  int  movieId  append  int  rating  line  line  np  array  astype  float32  return  nbUsers  943  nbMovies  1682  nbFeatures  nbUsers  nbMovies  nbRatingsTrain  80000  nbRatingsTest  20000  input  dir  input  100k  ,amazon
download  file  s3  recommenders  workshop  artifacts  data  inputs  movielens100k  u1  base  BUCKET  input  dir  download  file  s3  recommenders  workshop  artifacts  data  inputs  movielens100k  u1  test  BUCKET  input  dir  train  train  loadDataset  u1  base  input  dir  nbRatingsTrain  nbFeatures  test  test  loadDataset  u1  test  input  dir  nbRatingsTest  nbFeatures  prefix  exercise4  fm  movielens100k  train  key  train  protobuf  train  prefix  format  prefix  train  test  key  test  protobuf  test  prefix  format  prefix  test  output  prefix  s3  output  format  BUCKET  prefix  def  writeDatasetToProtobuf  bucket  prefix  key  import  io  boto3  import  sagemaker  amazon  common  as  smac  buf  io  BytesIO  smac  write  spmatrix  to  sparse  tensor  buf  buf  seek  print  buf  obj  format  prefix  key  boto3  resource  s3  Bucket  bucket  Object  obj  upload  fileobj  buf  print  Wrote  dataset  format  bucket  obj  writeDatasetToProtobuf  train  train  BUCKET  train  prefix  train  key  writeDa,amazon
tasetToProtobuf  test  test  BUCKET  test  prefix  test  key  print  Output  format  output  prefix  import  sagemaker  from  sagemaker  import  get  execution  role  train  data  s3  exercise4  fm  movielens100k  train  train  protobuf  BUCKET  test  data  s3  exercise4  fm  movielens100k  test  test  protobuf  BUCKET  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  factorization  machines  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  factorization  machines  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  factorization  machines  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  factorization  machines  latest  fm  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  get  execution  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  output  prefix  sagemaker  session  sagemaker  Session  fm  set  hyperparameters  feature  dim  nbFeatures  predictor  type  regressor  mini,amazon
  batch  size  1000  num  factors  64  speedometer  period  10  epochs  50  fm  fit  train  train  data  test  test  data  fm  predictor  fm  deploy  instance  type  ml  c4  xlarge  initial  instance  count  import  json  import  numpy  as  np  from  sagemaker  predictor  import  json  deserializer  nbUsers  943  nbMovies  1682  nbFeatures  nbUsers  nbMovies  def  fm  serializer  data  js  instances  for  row  in  data  keys  np  argwhere  row  np  amax  row  flatten  tolist  js  instances  append  data  features  keys  keys  shape  nbFeatures  values  len  keys  print  js  return  json  dumps  js  fm  predictor  content  type  application  json  fm  predictor  serializer  fm  serializer  fm  predictor  deserializer  json  deserializer  result  fm  predictor  predict  test  1000  1010  toarray  print  result  print  print  test  1000  1010  ,amazon
sh  Installing  dependencies  from  git  pip  install  folium  rm  fr  data  mkdir  data  cd  data  wget  https  raw  githubusercontent  com  codeforamerica  click  that  hood  master  public  data  california  counties  geojsonimport  os  boto3  time  import  folium  from  io  import  StringIO  import  pandas  as  pd  print  folium  version  results  bucket  aws  athena  query  results  accountid  us  east  region  us  east  def  executeAthenaQuery  database  queryString  client  boto3  client  service  name  athena  region  name  region  s3c  boto3  client  s3  response  client  start  query  execution  QueryString  queryString  QueryExecutionContext  Database  database  ResultConfiguration  OutputLocation  s3  results  bucket  queryStatus  RUNNING  while  queryStatus  RUNNING  time  sleep  10  response1  client  get  query  execution  QueryExecutionId  response  QueryExecutionId  print  response  queryStatus  response1  QueryExecution  Status  State  print  queryStatus  if  queryStatus  SUCCEEDED  query  i,amazon
d  response  QueryExecutionId  results  key  query  id  csv  obj  s3c  get  object  Bucket  results  bucket  Key  results  key  content  obj  Body  read  csv  StringIO  content  decode  utf  df  pd  read  csv  csv  sep  return  df  return  Nonecounties  geo  os  path  join  data  california  counties  geojson  folium  Map  location  37  120  zoom  start  tiles  OpenStreetMap  folium  GeoJson  counties  geo  name  geojson  add  to  kdatabase  geospatial  query  SELECT  counties  name  as  county  COUNT  as  cnt  FROM  counties  CROSS  JOIN  earthquakes  WHERE  ST  CONTAINS  counties  boundaryshape  ST  POINT  earthquakes  longitude  earthquakes  latitude  GROUP  BY  counties  name  ORDER  BY  cnt  DESC  df  executeAthenaQuery  database  query  dfk  folium  Map  location  37  120  zoom  start  tiles  OpenStreetMap  choropleth  counties  geo  data  df  columns  county  cnt  key  on  feature  properties  name  fill  color  YlGn  fill  opacity  line  opacity  legend  name  Earthquakes  in  CA  ,amazon
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  linear  mnist  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  30  This  is  format  train  set  30  import  io  import  numpy  as  np  import  sagemaker  amazon  common  as  smac  vectors  np  array  tolist  for  in  train  set  astype  float32  labels  np  where  np  array  tolist  for ,amazon
 in  train  set  astype  float32  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  vectors  labels  buf  seek  import  boto3  import  os  key  recordio  pb  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  buf  s3  train  data  s3  train  format  bucket  prefix  key  print  uploaded  training  data  location  format  s3  train  data  output  location  s3  output  format  bucket  prefix  print  training  artifacts  will  be  uploaded  to  format  output  location  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  linear  learner  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  linear  learner  latest  import  boto3  import  sagemaker  sess  sagemaker  Session  linear  sagemaker  estimator  Estimator  containers  boto3  Session  region ,amazon
 name  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  output  location  sagemaker  session  sess  linear  set  hyperparameters  feature  dim  784  predictor  type  binary  classifier  mini  batch  size  200  linear  fit  train  s3  train  data  linear  predictor  linear  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  sagemaker  predictor  import  csv  serializer  json  deserializer  linear  predictor  content  type  text  csv  linear  predictor  serializer  csv  serializer  linear  predictor  deserializer  json  deserializerresult  linear  predictor  predict  train  set  30  31  print  result  import  numpy  as  np  predictions  for  array  in  np  array  split  test  set  100  result  linear  predictor  predict  array  predictions  predicted  label  for  in  result  predictions  predictions  np  array  predictions  import  pandas  as  pd  pd  crosstab  np  where  test  set  predictions  rownames  actuals  colnames  predictions  import  sagemaker  s,amazon
agemaker  Session  delete  endpoint  linear  predictor  endpoint  ,amazon
bucket  your  s3  bucket  name  here  prefix  sagemaker  linear  mnist  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  30  This  is  format  train  set  30  import  io  import  numpy  as  np  import  sagemaker  amazon  common  as  smac  vectors  np  array  tolist  for  in  train  set  astype  float32  labels  np  where  np  array  tolist  for  in  t,amazon
rain  set  astype  float32  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  vectors  labels  buf  seek  import  boto3  import  os  key  recordio  pb  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  buf  s3  train  data  s3  train  format  bucket  prefix  key  print  uploaded  training  data  location  format  s3  train  data  output  location  s3  output  format  bucket  prefix  print  training  artifacts  will  be  uploaded  to  format  output  location  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  linear  learner  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  linear  learner  latest  import  boto3  import  sagemaker  sess  sagemaker  Session  linear  sagemaker  estimator  Estimator  containers  boto3  Session  region  name ,amazon
 role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  output  location  sagemaker  session  sess  linear  set  hyperparameters  feature  dim  784  predictor  type  binary  classifier  mini  batch  size  200  linear  fit  train  s3  train  data  linear  predictor  linear  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  sagemaker  predictor  import  csv  serializer  json  deserializer  linear  predictor  content  type  text  csv  linear  predictor  serializer  csv  serializer  linear  predictor  deserializer  json  deserializerresult  linear  predictor  predict  train  set  30  31  print  result  import  numpy  as  np  predictions  for  array  in  np  array  split  test  set  100  result  linear  predictor  predict  array  predictions  predicted  label  for  in  result  predictions  predictions  np  array  predictions  import  pandas  as  pd  pd  crosstab  np  where  test  set  predictions  rownames  actuals  colnames  predictions  import  sagemaker  sagemak,amazon
er  Session  delete  endpoint  linear  predictor  endpoint  ,amazon
pip  install  sfc  modelsimport  os  sfc  models  from  sfc  models  gl  book  chapter3  import  SIMEX1  SIM  from  sfc  models  examples  Quick2DPlot  import  Quick2DPlotlogfile  sfc  models  sfc  models  register  standard  logs  output  logfile  builder  SIMEX  SIMEX1  country  code  C1  use  book  exogenous  True  model  SIMEX  builder  SIMEX  build  model  print  model  SIMEX  main  model  SIMEX  TimeSeriesCutoff  20Y  SIMEX  model  SIMEX  GetTimeSeries  GOOD  SUP  GOOD  time  model  SIMEX  GetTimeSeries  income  model  SIMEX  GetTimeSeries  HH  AfterTax  expected  income  model  SIMEX  GetTimeSeries  HH  EXP  AfterTax  SIMEX  model  SIMEX  GetTimeSeries  HH  Quick2DPlot  time  SIMEX  Output  Model  SIMEX  builder  SIM  SIM  country  code  C1  use  book  exogenous  True  model  SIM  builder  SIM  build  model  print  model  SIM  main  model  SIM  TimeSeriesCutoff  20Y  SIM  model  SIM  GetTimeSeries  GOOD  SUP  GOOD  SIM  model  SIM  GetTimeSeries  HH  Quick2DPlot  time  time  expected  income  income  H,microsoft
ousehold  Income  in  Model  SIMEX  run  now  False  filename  SIMEX1  output  png  Legend  Expected  Realised  DoPlot  Quick2DPlot  time  time  SIMEX  SIM  Output  run  now  False  Legend  Model  SIMEX  Model  SIM  DoPlot  Quick2DPlot  time  time  SIMEX  SIM  Household  Financial  Assets  run  now  False  Legend  Model  SIMEX  Model  SIM  DoPlot  ,microsoft
bucket  bucket  name  Put  your  s3  bucket  name  here  and  create  the  s3  bucket  if  it  does  not  exist  already  prefix  sagemaker  DEMO  blazingtext  text8  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  from  time  import  gmtime  strftime  import  time  import  numpy  as  np  import  os  import  json  wget  http  mattmahoney  net  dc  text8  zip  text8  gz  Uncompressing  gzip  text8  gz  fdef  upload  to  s3  bucket  prefix  channel  file  s3  boto3  resource  s3  data  open  file  rb  key  prefix  channel  file  s3  Bucket  bucket  put  object  Key  key  Body  data  upload  to  s3  bucket  prefix  train  text8  region  name  boto3  Session  region  namecontainers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  blazingtext  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  blazingtext  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  blazingtext  latest  eu  west  685385470294  dkr  ecr  eu  w,amazon
est  amazonaws  com  blazingtext  latest  container  containers  region  name  print  Using  SageMaker  BlazingText  container  format  container  region  name  resource  config  InstanceCount  InstanceType  ml  c4  2xlarge  VolumeSizeInGB  hyperparameters  mode  batch  skipgram  epochs  min  count  sampling  threshold  0001  learning  rate  025  window  size  vector  dim  100  negative  samples  batch  size  11  window  size  Preferred  evaluation  true  Perform  similarity  evaluation  on  WS  353  dataset  at  the  end  of  training  from  validator  import  validate  params  validate  params  resource  config  hyperparameters  job  name  DEMO  BT  text8  format  resource  config  InstanceCount  resource  config  InstanceType  replace  hyperparameters  mode  replace  strftime  gmtime  print  Training  job  job  name  create  training  params  TrainingJobName  job  name  ResourceConfig  resource  config  HyperParameters  hyperparameters  AlgorithmSpecification  TrainingImage  container  TrainingInputMode  F,amazon
ile  RoleArn  role  OutputDataConfig  S3OutputPath  s3  format  bucket  prefix  StoppingCondition  MaxRuntimeInSeconds  3600  24  Hours  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  S3DataDistributionType  FullyReplicated  Always  keep  FullyReplicated  for  BlazingText  sagemaker  client  boto3  Session  client  service  name  sagemaker  sagemaker  client  create  training  job  create  training  params  status  sagemaker  client  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  status  sagemaker  client  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  status  sagemaker  client  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  status  if  the  job  failed  determine  why  if  status  Failed  message  sage  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  forma,amazon
t  message  raise  Exception  Training  job  failed  info  sagemaker  client  describe  training  job  TrainingJobName  job  name  if  ModelArtifacts  not  in  info  raise  Exception  Could  not  find  model  artifacts  Please  wait  for  the  job  to  finish  key  output  model  tar  gz  format  prefix  info  TrainingJobName  s3  boto3  resource  s3  s3  Bucket  bucket  download  file  key  model  tar  gz  tar  xvzf  model  tar  gz  cat  eval  jsonimport  numpy  as  np  from  sklearn  preprocessing  import  normalize  Read  the  400  most  frequent  word  vectors  The  vectors  in  the  file  are  in  descending  order  of  frequency  num  points  400  first  line  True  index  to  word  with  open  vectors  txt  as  for  line  num  line  in  enumerate  if  first  line  dim  int  line  strip  split  word  vecs  np  zeros  num  points  dim  dtype  float  first  line  False  continue  line  line  strip  word  line  split  vec  word  vecs  line  num  for  index  vec  val  in  enumerate  line  split  vec  index ,amazon
 float  vec  val  index  to  word  append  word  if  line  num  num  points  break  word  vecs  normalize  word  vecs  copy  False  return  norm  False  from  sklearn  manifold  import  TSNE  tsne  TSNE  perplexity  40  components  init  pca  iter  10000  two  embeddings  tsne  fit  transform  word  vecs  num  points  labels  index  to  word  num  points  from  matplotlib  import  pylab  matplotlib  inline  def  plot  embeddings  labels  pylab  figure  figsize  20  20  for  label  in  enumerate  labels  embeddings  pylab  scatter  pylab  annotate  label  xy  xytext  textcoords  offset  points  ha  right  va  bottom  pylab  show  plot  two  embeddings  labels  ,amazon
Imports  the  Google  Cloud  client  library  from  google  cloud  import  language  from  google  cloud  language  import  enums  from  google  cloud  language  import  typesdef  sentiment  analysis  file  doc  open  file  output  doc  readlines  whole  doc  join  map  str  output  client  language  LanguageServiceClient  document  types  Document  content  whole  doc  type  enums  Document  Type  PLAIN  TEXT  entities  client  analyze  entities  document  entities  return  entitiesfile  the  little  prince  txt  sentiment  analysis  file  ,amazon
import  os  import  boto3  import  sagemaker  from  sagemaker  mxnet  import  MXNet  from  mxnet  import  gluon  nd  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  bash  sample  notebooks  sagemaker  python  sdk  mxnet  gluon  mnist  setup  sh  role  get  execution  role  def  input  transformer  data  label  return  nd  transpose  data  astype  np  float32  255  label  astype  np  float32  train  gluon  data  vision  FashionMNIST  data  fmnist  train  train  True  transform  input  transformer  test  gluon  data  vision  FashionMNIST  data  fmnist  test  train  False  transform  input  transformer  inputs  sagemaker  session  upload  data  path  data  fmnist  key  prefix  data  fminst  fmnist  cat  fmnist  cnn  py  batch  size  100  epochs  10  learning  rate  01  momentum  log  interval  100m  MXNet  fmnist  cnn  py  role  role  train  instance  count  train  instance  type  local  hyperparameters  batch  size  batch  size  epochs  epochs  learning  rate  learning  ra,amazon
te  momentum  momentum  log  interval  log  interval  fit  inputs  predictor  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  sagemaker  predictor  import  csv  serializer  json  deserializer  predictor  content  type  text  csv  predictor  serializer  csv  serializer  predictor  deserializer  json  deserializerimport  numpy  as  np  import  gzip  import  struct  import  matplotlib  pyplot  as  plt  matplotlib  inline  def  read  data  label  url  image  url  with  gzip  open  label  url  as  flbl  magic  num  struct  unpack  II  flbl  read  label  np  fromstring  flbl  read  dtype  np  int8  with  gzip  open  image  url  rb  as  fimg  magic  num  rows  cols  struct  unpack  IIII  fimg  read  16  image  np  fromstring  fimg  read  dtype  np  uint8  reshape  len  label  rows  cols  return  label  image  val  lbl  val  img  read  data  data  fmnist  test  t10k  labels  idx1  ubyte  gz  data  fmnist  test  t10k  images  idx3  ubyte  gz  idx  32  This  number  can  be  changed  to  get  an,amazon
other  image  plt  imshow  val  img  idx  cmap  Greys  plt  axis  off  plt  show  image  nd  array  val  img  idx  reshape  28  28  asnumpy  tolist  predictor  predict  str  image  import  boto3  client  boto3  client  sagemaker  runtime  response  client  invoke  endpoint  EndpointName  predictor  endpoint  Body  str  image  ContentType  text  csv  response  Body  read  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
bucket  your  s3  bucket  name  here  prefix  sagemaker  linear  mnist  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  30  This  is  format  train  set  30  import  io  import  numpy  as  np  import  sagemaker  amazon  common  as  smac  vectors  np  array  tolist  for  in  train  set  astype  float32  labels  np  where  np  array  tolist  for  in  t,amazon
rain  set  astype  float32  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  vectors  labels  buf  seek  import  boto3  import  os  key  recordio  pb  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  buf  s3  train  data  s3  train  format  bucket  prefix  key  print  uploaded  training  data  location  format  s3  train  data  output  location  s3  output  format  bucket  prefix  print  training  artifacts  will  be  uploaded  to  format  output  location  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  linear  learner  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  linear  learner  latest  import  boto3  import  sagemaker  sess  sagemaker  Session  linear  sagemaker  estimator  Estimator  containers  boto3  Session  region  name ,amazon
 role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  output  location  sagemaker  session  sess  linear  set  hyperparameters  feature  dim  784  predictor  type  binary  classifier  mini  batch  size  200  linear  fit  train  s3  train  data  linear  predictor  linear  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  sagemaker  predictor  import  csv  serializer  json  deserializer  linear  predictor  content  type  text  csv  linear  predictor  serializer  csv  serializer  linear  predictor  deserializer  json  deserializerresult  linear  predictor  predict  train  set  30  31  print  result  import  numpy  as  np  predictions  for  array  in  np  array  split  test  set  100  result  linear  predictor  predict  array  predictions  predicted  label  for  in  result  predictions  predictions  np  array  predictions  import  pandas  as  pd  pd  crosstab  np  where  test  set  predictions  rownames  actuals  colnames  predictions  import  sagemaker  sagemak,amazon
er  Session  delete  endpoint  linear  predictor  endpoint  ,amazon
Do  not  update  source  input  file  path  input  file  path  s3  ruchika  wibd  west  mleap  ml  data  data  csv  Update  your  bucket  name  and  model  location  path  s3  model  bucket  ruchika  wibd  west  spark  model  location  s3  s3  model  bucket  mleap  models  car  price  prediction  model  pipeline  name  pipeline1  model  name  model  from  pyspark  ml  import  Pipeline  from  pyspark  ml  regression  import  GBTRegressor  data  spark  read  csv  path  input  file  path  header  True  quote  sep  inferSchema  True  data  printSchema  from  pyspark  sql  functions  import  col  from  pyspark  sql  import  DataFrame  from  pyspark  ml  feature  import  StringIndexer  VectorAssembler  def  get  indexer  input  data  str  cols  value  for  in  data  data  columns  dtypes  if  string  str  cols  value  StringIndexer  inputCol  outputCol  indexed  fit  data  return  str  cols  valuedata  test  data  train  data  randomSplit  weights  seed  10  get  indexer  input  get  indexer  input  data  print  ge,amazon
t  indexer  input  def  model  training  data  train  indexer  input  cols  list  set  data  train  columns  set  indexer  input  keys  Price  str  ind  cols  indexed  column  for  column  in  indexer  input  keys  indexers  indexer  input  values  pipeline  tr  Pipeline  stages  indexers  data  tr  pipeline  tr  fit  data  train  transform  data  train  assembler  VectorAssembler  inputCols  cols  outputCol  features  gbt  GBTRegressor  featuresCol  features  labelCol  Price  stepSize  008  maxDepth  subsamplingRate  75  seed  10  maxIter  20  minInstancesPerNode  checkpointInterval  100  maxBins  64  pipeline  training  Pipeline  stages  assembler  gbt  model  pipeline  training  fit  data  tr  return  model  def  model  testing  model  data  test  indexer  input  indexers  indexer  input  values  pipeline  te  Pipeline  stages  indexers  data  te  pipeline  te  fit  data  test  transform  data  test  predictions  model  transform  data  te  predictions  select  prediction  show  10  False  model  model  tr,amazon
aining  data  train  get  indexer  input  model  write  overwrite  save  spark  model  location  Load  the  saved  model  and  Test  the  model  from  pyspark  ml  import  PipelineModel  from  pyspark  ml  import  Pipeline  import  json  from  pyspark  sql  functions  import  col  from  pyspark  ml  feature  import  StringIndexer  VectorAssembler  def  get  indexer  input  data  str  cols  value  for  in  data  data  columns  dtypes  if  string  str  cols  value  StringIndexer  inputCol  outputCol  indexed  fit  data  return  str  cols  value  def  model  testing  model  data  test  indexer  input  indexers  indexer  input  values  pipeline  te  Pipeline  stages  indexers  data  te  pipeline  te  fit  data  test  transform  data  test  data  te  show  False  predictions  model  transform  data  test  predictions  select  prediction  show  10  False  sameModel  PipelineModel  load  path  s3  ruchika  wibd  west  mleap  models  car  price  prediction  model  Price  9041  9062544231  Mileage  26191  Make  Chevro,amazon
let  Model  AVEO  Trim  SVM  Sedan  4D  Type  Sedan  Cylinder  Liter  Doors  Cruise  Sound  Leather  json  dumps  jsonRDD  sc  parallelize  df  spark  read  json  jsonRDD  get  indexer  input  get  indexer  input  df  model  testing  sameModel  df  get  indexer  input  row  df  data  test  limit  row  df  show  Serialize  to  MLeap  Bundle  import  mleap  pyspark  from  mleap  pyspark  spark  support  import  SimpleSparkSerializer  model  serializeToBundle  jar  file  tmp  model  name  zip  model  transform  row  df  Save  the  Bundle  to  S3  import  boto3  s3  boto3  resource  s3  data  open  tmp  model  name  zip  rb  s3  Bucket  s3  model  bucket  put  object  Key  mleap  models  pipeline  name  model  name  zip  Body  data  python  CHANGE  ME  s3  model  bucket  ruchika  wibd  west  spark  model  location  s3  s3  model  bucket  mleap  models  car  price  prediction  model  pipeline  name  pipeline1  model  name  model  home  home  ec2  user  models  Tar  Zip  the  model  and  save  back  to  S3  import ,amazon
 boto3  os  Check  if  local  directory  exists  otherwise  create  it  os  makedirs  home  exist  ok  True  s3  boto3  resource  s3  s3  Bucket  s3  model  bucket  download  file  mleap  models  pipeline  name  model  name  zip  home  model  name  zip  cmd  cd  home  tar  czvf  model  name  tgz  model  name  zip  print  cmd  print  os  system  cmd  data  open  home  model  name  tgz  rb  s3  Bucket  s3  model  bucket  put  object  Key  mleap  models  pipeline  name  model  name  tgz  Body  data  ,amazon
import  boto3  import  json  import  sagemaker  as  sage  import  argparseparser  argparse  ArgumentParser  description  Train  language  model  with  Amazon  SageMaker  parser  add  argument  image  name  type  str  default  languagemodel  help  the  image  repository  name  set  to  the  default  in  the  blog  parser  add  argument  role  name  type  str  default  AmazonSageMakerExecutionRole  help  the  SakeMaker  execution  role  name  set  to  the  default  in  the  blog  parser  add  argument  region  name  type  str  default  us  east  help  us  east  us  east  us  west  eu  west  set  to  the  default  in  the  blog  args  parser  parse  args  SAGEMAKER  REGIONS  us  east  us  east  us  west  eu  west  start  sagemaker  session  sess  sage  Session  Get  our  account  id  and  our  region  account  sess  boto  session  client  sts  get  caller  identity  Account  region  sess  boto  session  region  name  override  region  to  blog  default  or  if  not  in  acceptable  location  if  region  not  in ,amazon
 SAGEMAKER  REGIONS  region  args  region  name  Get  our  image  URI  and  the  role  we  created  in  our  CloudFormation  Template  image  dkr  ecr  amazonaws  com  format  account  region  args  image  name  role  arn  aws  iam  role  format  account  args  role  name  Create  sagemaker  training  instance  using  our  image  URI  languagemodel  sage  estimator  Estimator  image  role  ml  p2  xlarge  output  path  s3  output  format  sess  default  bucket  sagemaker  session  sess  read  our  local  config  information  config  json  load  open  config  config  json  set  our  training  configuration  for  the  model  languagemodel  hyperparam  dict  config  upload  our  training  data  to  s3  the  output  will  be  something  like  this  s3  sagemaker  us  east  account  id  data  train  csv  data  location  sess  upload  data  path  data  train  csv  finally  we  fit  our  data  sit  back  and  read  the  stream  languagemodel  fit  data  location  ,amazon
import  boto3  import  io  import  matplotlib  pyplot  as  plt  import  numpy  as  np  import  os  import  pandas  as  pd  import  sagemaker  import  sagemaker  amazon  common  as  smac  from  sagemaker  import  get  execution  role  from  sagemaker  predictor  import  csv  serializer  json  deserializer  Set  data  locations  bucket  your  s3  bucket  here  replace  this  with  your  own  bucket  prefix  sagemaker  DEMO  linear  learner  loss  weights  replace  this  with  your  own  prefix  s3  train  key  train  recordio  pb  data  format  prefix  s3  train  path  os  path  join  s3  bucket  s3  train  key  local  raw  data  creditcard  csv  zip  role  get  execution  role  Confirm  access  to  s3  bucket  for  obj  in  boto3  resource  s3  Bucket  bucket  objects  all  print  obj  key  Read  the  data  shuffle  and  split  into  train  and  test  sets  separating  the  labels  last  column  from  the  features  raw  data  pd  read  csv  local  raw  data  as  matrix  np  random  seed  np  random  shuffle  ,amazon
raw  data  train  size  int  raw  data  shape  train  features  raw  data  train  size  train  labels  raw  data  train  size  test  features  raw  data  train  size  test  labels  raw  data  train  size  Convert  the  processed  training  data  to  protobuf  and  write  to  S3  for  linear  learner  vectors  np  array  tolist  for  in  train  features  astype  float32  labels  np  array  tolist  for  in  train  labels  astype  float32  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  vectors  labels  buf  seek  boto3  resource  s3  Bucket  bucket  Object  s3  train  key  upload  fileobj  buf  def  predictor  from  hyperparams  s3  train  data  hyperparams  output  path  Create  an  Estimator  from  the  given  hyperparams  fit  to  training  data  and  return  deployed  predictor  specify  algorithm  containers  and  instantiate  an  Estimator  with  given  hyperparams  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  linear  learner  latest  us  east  382416733822  dkr  e,amazon
cr  us  east  amazonaws  com  linear  learner  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  linear  learner  latest  linear  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  role  train  instance  count  train  instance  type  ml  m4  xlarge  output  path  output  path  sagemaker  session  sagemaker  Session  linear  set  hyperparameters  hyperparams  train  model  linear  fit  train  s3  train  data  deploy  predictor  linear  predictor  linear  deploy  initial  instance  count  instance  type  ml  m4  xlarge  linear  predictor  content  type  text  csv  linear  predictor  serializer  csv  serializer  linear  predictor  deserializer  json  deserializer  return  linear  predictordef  evaluate  linear  predictor  test  features  test  labels  model  name  verbose  True  Evaluate  model  on  test  set  given  the  prediction  endpoint  Return  binary  classification  metrics  spl,amazon
it  the  test  data  set  into  100  batches  and  evaluate  using  prediction  endpoint  prediction  batches  linear  predictor  predict  batch  predictions  for  batch  in  np  array  split  test  features  100  parse  raw  predictions  json  to  exctract  predicted  label  test  preds  np  concatenate  np  array  predicted  label  for  in  batch  for  batch  in  prediction  batches  calculate  true  positives  false  positives  true  negatives  false  negatives  tp  np  logical  and  test  labels  test  preds  sum  fp  np  logical  and  test  labels  test  preds  sum  tn  np  logical  and  test  labels  test  preds  sum  fn  np  logical  and  test  labels  test  preds  sum  calculate  binary  classification  metrics  recall  tp  tp  fn  precision  tp  tp  fp  accuracy  tp  tn  tp  fp  tn  fn  f1  precision  recall  precision  recall  if  verbose  print  pd  crosstab  test  labels  test  preds  rownames  actuals  colnames  predictions  print  11  3f  format  Recall  recall  print  11  3f  format  Precision ,amazon
 precision  print  11  3f  format  Accuracy  accuracy  print  11  3f  format  F1  f1  return  TP  tp  FP  fp  FN  fn  TN  tn  Precision  precision  Recall  recall  Accuracy  accuracy  F1  f1  Model  model  name  def  delete  endpoint  predictor  try  boto3  client  sagemaker  delete  endpoint  EndpointName  predictor  endpoint  print  Deleted  format  predictor  endpoint  except  print  Already  deleted  format  predictor  endpoint  Training  binary  classifier  with  default  settings  logistic  regression  defaults  hyperparams  feature  dim  30  predictor  type  binary  classifier  epochs  40  defaults  output  path  s3  defaults  output  format  bucket  prefix  defaults  predictor  predictor  from  hyperparams  s3  train  path  defaults  hyperparams  defaults  output  path  Training  binary  classifier  with  automated  threshold  tuning  autothresh  hyperparams  feature  dim  30  predictor  type  binary  classifier  binary  classifier  model  selection  criteria  precision  at  target  recall  target  re,amazon
call  epochs  40  autothresh  output  path  s3  autothresh  output  format  bucket  prefix  autothresh  predictor  predictor  from  hyperparams  s3  train  path  autothresh  hyperparams  autothresh  output  path  Training  binary  classifier  with  class  weights  and  automated  threshold  tuning  class  weights  hyperparams  feature  dim  30  predictor  type  binary  classifier  binary  classifier  model  selection  criteria  precision  at  target  recall  target  recall  positive  example  weight  mult  balanced  epochs  40  class  weights  output  path  s3  class  weights  output  format  bucket  prefix  class  weights  predictor  predictor  from  hyperparams  s3  train  path  class  weights  hyperparams  class  weights  output  path  Training  binary  classifier  with  hinge  loss  and  automated  threshold  tuning  svm  hyperparams  feature  dim  30  predictor  type  binary  classifier  loss  hinge  loss  binary  classifier  model  selection  criteria  precision  at  target  recall  target  recall  epoc,amazon
hs  40  svm  output  path  s3  svm  output  format  bucket  prefix  svm  predictor  predictor  from  hyperparams  s3  train  path  svm  hyperparams  svm  output  path  Training  binary  classifier  with  hinge  loss  balanced  class  weights  and  automated  threshold  tuning  svm  balanced  hyperparams  feature  dim  30  predictor  type  binary  classifier  loss  hinge  loss  binary  classifier  model  selection  criteria  precision  at  target  recall  target  recall  positive  example  weight  mult  balanced  epochs  40  svm  balanced  output  path  s3  svm  balanced  output  format  bucket  prefix  svm  balanced  predictor  predictor  from  hyperparams  s3  train  path  svm  balanced  hyperparams  svm  balanced  output  path  Evaluate  the  trained  models  predictors  Logistic  defaults  predictor  Logistic  with  auto  threshold  autothresh  predictor  Logistic  with  class  weights  class  weights  predictor  Hinge  with  auto  threshold  svm  predictor  Hinge  with  class  weights  svm  balanced  pred,amazon
ictor  metrics  key  evaluate  predictor  test  features  test  labels  key  False  for  key  predictor  in  predictors  items  pd  set  option  display  float  format  lambda  3f  display  pd  DataFrame  list  metrics  values  loc  Model  Recall  Precision  Accuracy  F1  for  predictor  in  defaults  predictor  autothresh  predictor  class  weights  predictor  svm  predictor  svm  balanced  predictor  delete  endpoint  predictor  ,amazon
import  sys  sys  version  pip  install  tensorflowimport  tensorflow  as  tf  import  numpy  as  np  import  matplotlib  pyplot  as  plt  numpy  100  data  np  random  rand  100  astype  np  float32  data  data  Try  to  find  values  for  and  that  compute  data  data  We  know  that  should  be  and  but  TensorFlow  will  figure  that  out  for  us  tensorflow  fitting  tf  Variable  tf  random  uniform  tf  Variable  tf  zeros  data  Minimize  the  mean  squared  errors  loss  tf  reduce  mean  tf  square  data  optimizer  tf  train  GradientDescentOptimizer  train  optimizer  minimize  loss  Before  starting  initialize  the  variables  We  will  run  this  first  init  tf  global  variables  initializer  Launch  the  graph  sess  tf  Session  sess  run  init  Fit  the  line  for  step  in  range  201  sess  run  train  if  step  20  print  step  sess  run  sess  run  plt  plot  data  data  ro  label  Original  data  plt  plot  data  sess  run  data  sess  run  label  Fitted  line  plt  legend  plt  sh,microsoft
ow  Learns  best  fit  is  ,microsoft
import  os  import  io  import  re  import  boto3  import  pandas  as  pd  import  numpy  as  np  import  time  from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  S3  bucket  prefix  sagemaker  DEMO  parquet  conda  install  conda  forge  fastparquet  scikit  learn  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  from  fastparquet  import  write  from  fastparquet  import  ParquetFile  def  save  as  parquet  file  dataset  filename  label  col  dataset  dataset  data  pd  DataFrame  data  label  col  data  columns  data  columns  astype  str  Parquet  expexts  the  column  names  to  be  strings  write  filename  data  def  read  parquet  file  filename  pf  ParquetFile  filename  return  pf  to  pandas  def  features  and  target  df  label ,amazon
 col  df  loc  df  columns  label  col  values  df  label  col  values  return  trainFile  train  parquet  validFile  valid  parquet  testFile  test  parquet  label  col  target  save  as  parquet  file  train  set  trainFile  label  col  save  as  parquet  file  valid  set  validFile  label  col  save  as  parquet  file  test  set  testFile  label  col  dfTrain  read  parquet  file  trainFile  dfValid  read  parquet  file  validFile  dfTest  read  parquet  file  testFile  train  train  features  and  target  dfTrain  label  col  valid  valid  features  and  target  dfValid  label  col  test  test  features  and  target  dfTest  label  col  import  io  import  numpy  as  np  import  sagemaker  amazon  common  as  smac  trainVectors  np  array  tolist  for  in  train  astype  float32  trainLabels  np  where  np  array  tolist  for  in  train  astype  float32  bufTrain  io  BytesIO  smac  write  numpy  to  dense  tensor  bufTrain  trainVectors  trainLabels  bufTrain  seek  validVectors  np  array  tolist  for  ,amazon
in  valid  astype  float32  validLabels  np  where  np  array  tolist  for  in  valid  astype  float32  bufValid  io  BytesIO  smac  write  numpy  to  dense  tensor  bufValid  validVectors  validLabels  bufValid  seek  import  boto3  import  os  key  recordio  pb  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  bufTrain  s3  train  data  s3  train  format  bucket  prefix  key  print  uploaded  training  data  location  format  s3  train  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  key  upload  fileobj  bufValid  s3  validation  data  s3  validation  format  bucket  prefix  key  print  uploaded  validation  data  location  format  s3  validation  data  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  linear  learner  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  linear  learner  latest,amazon
  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  linear  learner  latest  linear  job  DEMO  linear  time  strftime  time  gmtime  print  Job  name  is  linear  job  linear  training  params  RoleArn  role  TrainingJobName  linear  job  AlgorithmSpecification  TrainingImage  containers  boto3  Session  region  name  TrainingInputMode  File  ResourceConfig  InstanceCount  InstanceType  ml  c4  2xlarge  VolumeSizeInGB  10  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  S3DataDistributionType  FullyReplicated  CompressionType  None  RecordWrapperType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  prefix  S3DataDistributionType  FullyReplicated  CompressionType  None  RecordWrapperType  None  OutputDataConfig  S3OutputPath  s3  format  bucket  prefix  HyperParameters  feature  dim  784  mini  batch  size  200  predictor  type  binary  classifier  epoc,amazon
hs  10  num  models  32  loss  absolute  loss  StoppingCondition  MaxRuntimeInSeconds  60  60  time  sm  boto3  Session  client  sagemaker  sm  create  training  job  linear  training  params  status  sm  describe  training  job  TrainingJobName  linear  job  TrainingJobStatus  print  status  sm  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  linear  job  if  status  Failed  message  sm  describe  training  job  TrainingJobName  linear  job  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  sm  describe  training  job  TrainingJobName  linear  job  TrainingJobStatus  ,amazon
cat  container  Dockerfile  sh  The  name  of  our  algorithm  algorithm  name  decision  trees  sample  cd  container  chmod  decision  trees  train  chmod  decision  trees  serve  account  aws  sts  get  caller  identity  query  Account  output  text  Get  the  region  defined  in  the  current  configuration  default  to  us  west  if  none  defined  region  aws  configure  get  region  region  region  us  west  fullname  account  dkr  ecr  region  amazonaws  com  algorithm  name  latest  If  the  repository  doesn  exist  in  ECR  create  it  aws  ecr  describe  repositories  repository  names  algorithm  name  dev  null  if  ne  then  aws  ecr  create  repository  repository  name  algorithm  name  dev  null  fi  Get  the  login  command  from  ECR  and  execute  it  directly  aws  ecr  get  login  region  region  no  include  email  Build  the  docker  image  locally  with  the  image  name  and  then  push  it  to  ECR  with  the  full  name  docker  build  algorithm  name  docker  tag  algorithm  name,amazon
  fullname  docker  push  fullname  S3  prefix  prefix  DEMO  scikit  byo  iris  Define  IAM  role  import  boto3  import  re  import  os  import  numpy  as  np  import  pandas  as  pd  from  sagemaker  import  get  execution  role  role  get  execution  role  import  sagemaker  as  sage  from  time  import  gmtime  strftime  sess  sage  Session  WORK  DIRECTORY  data  data  location  sess  upload  data  WORK  DIRECTORY  key  prefix  prefix  account  sess  boto  session  client  sts  get  caller  identity  Account  region  sess  boto  session  region  name  image  dkr  ecr  amazonaws  com  decision  trees  sample  latest  format  account  region  tree  sage  estimator  Estimator  image  role  ml  c4  2xlarge  output  path  s3  output  format  sess  default  bucket  sagemaker  session  sess  tree  fit  data  location  from  sagemaker  predictor  import  csv  serializer  predictor  tree  deploy  ml  m4  xlarge  serializer  csv  serializer  shape  pd  read  csv  data  iris  csv  header  None  import  itertools  ,amazon
50  for  in  range  40  for  in  range  10  indices  for  in  itertools  product  test  data  shape  iloc  indices  test  test  data  iloc  test  test  data  iloc  print  predictor  predict  test  values  decode  utf  sess  delete  endpoint  predictor  endpoint  ,amazon
import  sagemaker  from  sagemaker  mxnet  import  MXNet  from  sagemaker  tuner  import  IntegerParameter  CategoricalParameter  ContinuousParameter  HyperparameterTuner  sagemaker  session  sagemaker  Session  role  sagemaker  get  execution  role  Download  the  data  on  local  disk  from  mxnet  gluon  data  vision  import  datasets  trainset  datasets  CIFAR10  root  data  train  True  testset  datasets  CIFAR10  root  data  train  False  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  dawnbench  gluon  cifar10  print  input  spec  in  this  case  just  an  S3  path  format  inputs  import  os  source  dir  os  path  join  os  getcwd  training  code  train  instance  type  ml  p3  2xlarge  Configuration  for  the  stable  convergence  This  should  get  to  94  in  around  35  epochs  taking  little  more  than  minutes  hyperparameters  batch  size  512  lr  20117947903692157  momentum  8871656440409877  epochs  31  extra  wd  0013947371011895455  peak  Configuration  for  the ,amazon
 fastest  job  This  has  converged  in  30  epochs  taking  about  3min47s  However  the  results  are  not  as  easily  reproducible  Uncomment  if  you  want  to  run  this  configuration  hyperparameters  batch  size  512  lr  momentum  8989643139985577  epochs  30  extra  wd  000982971006274194  peak  10  estimator  MXNet  entry  point  run  py  source  dir  source  dir  role  role  train  instance  count  train  instance  type  train  instance  type  framework  version  hyperparameters  hyperparameters  estimator  fit  inputs  Uncomment  this  cell  if  you  wish  to  perform  single  run  For  tuning  job  proceed  to  next  cell  An  example  of  hyperparameters  and  ranges  that  you  would  like  to  optimize  hyperparameter  ranges  lr  ContinuousParameter  65  momentum  ContinuousParameter  87  92  wd  ContinuousParameter  0005  0017  metric  definitions  Name  Validation  accuracy  Regex  Validation  accuracy  We  can  are  the  validation  accuracy  objective  metric  name  Validation  accuracy,amazon
  tuner  HyperparameterTuner  estimator  objective  metric  name  hyperparameter  ranges  metric  definitions  max  jobs  120  max  parallel  jobs  base  tuning  job  name  my  tuning  job  tuner  fit  inputs  Launch  the  tuning  job  import  boto3  import  sagemaker  import  os  region  boto3  Session  region  name  sage  client  boto3  Session  client  sagemaker  Enter  you  tuning  job  name  tuning  job  name  my  tuning  job  tuning  job  result  sage  client  describe  hyper  parameter  tuning  job  HyperParameterTuningJobName  tuning  job  name  status  tuning  job  result  HyperParameterTuningJobStatus  if  status  Completed  print  Reminder  the  tuning  job  has  not  been  completed  job  count  tuning  job  result  TrainingJobStatusCounters  Completed  print  training  jobs  have  completed  job  count  is  minimize  tuning  job  result  HyperParameterTuningJobConfig  HyperParameterTuningJobObjective  Type  Maximize  objective  name  tuning  job  result  HyperParameterTuningJobConfig  HyperParame,amazon
terTuningJobObjective  MetricName  import  pandas  as  pd  tuner  sagemaker  HyperparameterTuningJobAnalytics  tuning  job  name  full  df  tuner  dataframe  if  len  full  df  df  full  df  full  df  FinalObjectiveValue  float  inf  if  len  df  df  df  sort  values  FinalObjectiveValue  ascending  is  minimize  print  Number  of  training  jobs  with  valid  objective  len  df  print  lowest  min  df  FinalObjectiveValue  highest  max  df  FinalObjectiveValue  pd  set  option  display  max  colwidth  Don  truncate  TrainingJobName  else  print  No  training  jobs  have  reported  valid  results  yet  import  bokeh  import  bokeh  io  bokeh  io  output  notebook  from  bokeh  plotting  import  figure  show  from  bokeh  models  import  HoverTool  class  HoverHelper  def  init  self  tuning  analytics  self  tuner  tuning  analytics  def  hovertool  self  tooltips  FinalObjectiveValue  FinalObjectiveValue  TrainingJobName  TrainingJobName  for  in  self  tuner  tuning  ranges  keys  tooltips  append  ht  Hove,amazon
rTool  tooltips  tooltips  return  ht  def  tools  self  standard  tools  pan  crosshair  wheel  zoom  zoom  in  zoom  out  undo  reset  return  self  hovertool  standard  tools  hover  HoverHelper  tuner  figure  plot  width  900  plot  height  400  tools  hover  tools  axis  type  datetime  circle  source  df  TrainingStartTime  FinalObjectiveValue  show  ranges  tuner  tuning  ranges  figures  for  hp  name  hp  range  in  ranges  items  categorical  args  if  hp  range  get  Values  This  is  marked  as  categorical  Check  if  all  options  are  actually  numbers  def  is  num  try  float  return  except  return  vals  hp  range  Values  if  sum  is  num  for  in  vals  len  vals  Bokeh  has  issues  plotting  categorical  range  that  actually  numeric  so  plot  as  numeric  print  Hyperparameter  is  tuned  as  categorical  but  all  values  are  numeric  hp  name  else  Set  up  extra  options  for  plotting  categoricals  bit  tricky  when  they  re  actually  numbers  categorical  args  range  vals,amazon
  Now  plot  it  figure  plot  width  500  plot  height  500  title  Objective  vs  hp  name  tools  hover  tools  axis  label  hp  name  axis  label  objective  name  categorical  args  circle  source  df  hp  name  FinalObjectiveValue  figures  append  show  bokeh  layouts  Column  figures  ,amazon
matplotlib  inline  import  os  import  numpy  as  np  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  from  sklearn  datasets  import  load  boston  import  sklearn  model  selectionimport  sagemaker  from  sagemaker  import  get  execution  role  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  from  sagemaker  predictor  import  csv  serializer  This  is  an  object  that  represents  the  SageMaker  session  that  we  are  currently  operating  in  This  object  contains  some  useful  information  that  we  will  need  to  access  later  such  as  our  region  session  sagemaker  Session  This  is  an  object  that  represents  the  IAM  role  that  we  are  currently  assigned  When  we  construct  and  launch  the  training  job  later  we  will  need  to  tell  it  what  IAM  role  it  should  have  Since  our  use  case  is  relatively  simple  we  will  simply  assign  the  training  job  the  role  we  currently  have  role  get  execution  role  boston  load  bosto,amazon
n  First  we  package  up  the  input  data  and  the  target  variable  the  median  value  as  pandas  dataframes  This  will  make  saving  the  data  to  file  little  easier  later  on  bos  pd  pd  DataFrame  boston  data  columns  boston  feature  names  bos  pd  pd  DataFrame  boston  target  We  split  the  dataset  into  training  and  testing  sets  train  test  train  test  sklearn  model  selection  train  test  split  bos  pd  bos  pd  test  size  33  Then  we  split  the  training  set  further  into  training  and  validation  sets  train  val  train  val  sklearn  model  selection  train  test  split  train  train  test  size  33  This  is  our  local  data  directory  We  need  to  make  sure  that  it  exists  data  dir  data  boston  if  not  os  path  exists  data  dir  os  makedirs  data  dir  We  use  pandas  to  save  our  test  train  and  validation  data  to  csv  files  Note  that  we  make  sure  not  to  include  header  information  or  an  index  as  this  is  required  by  the,amazon
  built  in  algorithms  provided  by  Amazon  Also  for  the  train  and  validation  data  it  is  assumed  that  the  first  entry  in  each  row  is  the  target  variable  test  to  csv  os  path  join  data  dir  test  csv  header  False  index  False  pd  concat  val  val  axis  to  csv  os  path  join  data  dir  validation  csv  header  False  index  False  pd  concat  train  train  axis  to  csv  os  path  join  data  dir  train  csv  header  False  index  False  prefix  boston  xgboost  HL  test  location  session  upload  data  os  path  join  data  dir  test  csv  key  prefix  prefix  val  location  session  upload  data  os  path  join  data  dir  validation  csv  key  prefix  prefix  train  location  session  upload  data  os  path  join  data  dir  train  csv  key  prefix  prefix  As  stated  above  we  use  this  utility  method  to  construct  the  image  name  for  the  training  container  container  get  image  uri  session  boto  region  name  xgboost  Now  that  we  know  which  contain,amazon
er  to  use  we  can  construct  the  estimator  object  xgb  sagemaker  estimator  Estimator  container  The  name  of  the  training  container  role  The  IAM  role  to  use  our  current  role  in  this  case  train  instance  count  The  number  of  instances  to  use  for  training  train  instance  type  ml  m4  xlarge  The  type  of  instance  ot  use  for  training  output  path  s3  output  format  session  default  bucket  prefix  Where  to  save  the  output  the  model  artifacts  sagemaker  session  session  The  current  SageMaker  sessionxgb  set  hyperparameters  max  depth  eta  gamma  min  child  weight  subsample  objective  reg  linear  early  stopping  rounds  10  num  round  200  This  is  wrapper  around  the  location  of  our  train  and  validation  data  to  make  sure  that  SageMaker  knows  our  data  is  in  csv  format  s3  input  train  sagemaker  s3  input  s3  data  train  location  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  val  location  co,amazon
ntent  type  csv  xgb  fit  train  s3  input  train  validation  s3  input  validation  xgb  transformer  xgb  transformer  instance  count  instance  type  ml  m4  xlarge  xgb  transformer  transform  test  location  content  type  text  csv  split  type  Line  xgb  transformer  wait  aws  s3  cp  recursive  xgb  transformer  output  path  data  dirY  pred  pd  read  csv  os  path  join  data  dir  test  csv  out  header  None  plt  scatter  test  pred  plt  xlabel  Median  Price  plt  ylabel  Predicted  Price  plt  title  Median  Price  vs  Predicted  Price  First  we  will  remove  all  of  the  files  contained  in  the  data  dir  directory  rm  data  dir  And  then  we  delete  the  directory  itself  rmdir  data  dir  ,amazon
Export  as  slides  command  jupyter  nbconvert  Jupyter  Slides  ipynb  to  slides  post  serveimport  os  import  pandas  as  pd  import  numpy  as  np  import  seaborn  as  sns  import  matplotlib  pyplot  as  plt  from  sklearn  import  preprocessing  svm  from  itertools  import  combinations  from  sklearn  preprocessing  import  PolynomialFeatures  LabelEncoder  StandardScaler  import  sklearn  feature  selection  from  sklearn  model  selection  import  train  test  split  from  collections  import  defaultdict  from  sklearn  import  metrics  The  code  was  removed  by  DSX  for  sharing  Checking  that  everything  is  correct  pd  set  option  display  max  columns  30  credit  status  head  10  After  running  this  cell  we  will  see  that  we  have  no  missing  values  credit  status  info  Check  if  we  have  any  NaN  values  credit  status  isnull  values  any  Describe  columns  with  numerical  values  pd  set  option  precision  credit  status  describe  Find  correlations  credit  sta,ibm
tus  corr  method  pearson  Create  Grid  for  pairwise  relationships  gr  sns  PairGrid  credit  status  size  hue  class  gr  gr  map  diag  plt  hist  gr  gr  map  offdiag  plt  scatter  gr  gr  add  legend  Set  up  plot  size  fig  ax  plt  subplots  figsize  20  10  Attributes  destribution  sns  boxplot  orient  palette  hls  data  credit  status  credit  amount  fliersize  14  Tenure  data  distribution  histogram  sns  distplot  credit  status  credit  amount  hist  True  plt  show  Use  pandas  get  dummies  credit  status  encoded  pd  get  dummies  credit  status  credit  status  encoded  head  10  Create  training  data  for  that  will  undergo  preprocessing  credit  status  encoded  iloc  head  Extract  labels  from  sklearn  preprocessing  import  LabelEncoder  Split  last  column  from  original  dataset  as  the  labels  column  credit  status  class  Apply  encoder  to  transform  strings  to  numeric  values  and  le  LabelEncoder  fit  enc  le  transform  pd  DataFrame  enc  head  10  D,ibm
etect  outlier  using  interquartile  method  and  remove  them  def  find  outliers  df  quartile  quartile  np  percentile  df  25  75  iqr  quartile  quartile  lower  bound  quartile  iqr  upper  bound  quartile  iqr  outlier  indices  list  df  index  df  lower  bound  df  upper  bound  outlier  values  list  df  outlier  indices  df  outlier  indices  np  NaN  return  df  Find  outliers  in  first  column  continuous  values  print  find  outliers  duration  Find  outliers  in  first  column  continuous  values  print  find  outliers  credit  amount  Find  outliers  in  first  column  continuous  values  print  find  outliers  age  Check  for  null  values  isnull  values  any  Define  the  values  to  replce  and  the  strategy  of  choosing  the  replacement  value  from  sklearn  preprocessing  import  Imputer  suspected  cols  duration  credit  amount  age  imp  Imputer  missing  values  NaN  strategy  mean  pd  DataFrame  suspected  cols  imp  fit  transform  pd  DataFrame  suspected  cols  pd  Data,ibm
Frame  head  10  Check  for  null  values  pd  DataFrame  isnull  values  any  train  test  train  test  train  test  split  enc  test  size  random  state  42  print  train  shape  train  shape  print  test  shape  test  shape  print  train  columns  np  set  printoptions  precision  list  train  iloc  10  values  Use  StandardScaler  scaler  preprocessing  StandardScaler  fit  train  train  train  scaled  scaler  transform  train  pd  DataFrame  train  scaled  columns  train  columns  head  pd  DataFrame  train  head  from  sklearn  linear  model  import  LogisticRegression  clf  lr  LogisticRegression  model  clf  lr  fit  train  scaled  train  model  Use  the  scaler  fit  on  trained  data  to  scale  our  test  data  test  scaled  scaler  transform  test  pd  DataFrame  test  scaled  columns  train  columns  head  score  lr  clf  lr  decision  function  test  scaled  score  lry  pred  lr  clf  lr  predict  test  scaled  acc  lr  accuracy  score  test  pred  lr  print  acc  lr  average  precision  lr  av,ibm
erage  precision  score  test  score  lr  print  Average  precision  recall  score  2f  format  average  precision  lr  Plot  SVC  ROC  Curve  plt  figure  figsize  15  10  clf  fpr  lr  tpr  lr  thresh  lr  metrics  roc  curve  test  score  lr  auc  lr  metrics  roc  auc  score  test  score  lr  plt  plot  fpr  lr  tpr  lr  label  Logistic  Regression  on  Preprocessed  Data  auc  str  auc  lr  plt  legend  loc  plt  xlabel  False  Positives  plt  ylabel  True  Positives  The  code  was  removed  by  DSX  for  sharing  To  work  with  the  Watson  Machine  Learning  REST  API  you  must  generate  Bearer  access  token  import  urllib3  requests  json  headers  urllib3  util  make  headers  basic  auth  format  credentials  username  credentials  password  url  v3  identity  token  format  credentials  url  response  requests  get  url  headers  headers  ml  token  Bearer  json  loads  response  text  get  token  print  ml  token  Create  an  online  scoring  endpoint  endpoint  instance  credentials  url  v,ibm
3  wml  instances  credentials  instance  id  header  Content  Type  application  json  Authorization  ml  token  response  get  instance  requests  get  endpoint  instance  headers  header  print  response  get  instance  print  response  get  instance  text  Create  API  client  from  watson  machine  learning  client  import  WatsonMachineLearningAPIClient  client  WatsonMachineLearningAPIClient  credentials  Publish  model  in  Watson  Machine  Learning  repository  on  Cloud  model  props  client  repository  ModelMetaNames  AUTHOR  NAME  Heba  El  Shimy  client  repository  ModelMetaNames  NAME  Loan  Approval  Model  published  model  client  repository  store  model  model  model  meta  props  model  props  training  data  train  scaled  training  target  train  Create  model  deployment  published  model  uid  client  repository  get  model  uid  published  model  created  deployment  client  deployments  create  published  model  uid  Deployment  of  Loan  Approval  Model  Get  Scoring  URL  scoring,ibm
  endpoint  client  deployments  get  scoring  url  created  deployment  print  scoring  endpoint  Get  model  details  and  expected  input  model  details  client  repository  get  details  published  model  uid  print  json  dumps  model  details  indent  ,ibm
from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  bucket  name  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  30  This  is  format  train  set  30  time  from  sagemaker  amazon  common  import  write  numpy  to  dense  tensor  import  io  import  boto3  data  key  kmeans  lowlevel  example  data  data  location  s3  format  bucket  data  key  print  training  data  will  be  uploaded  to  format  data  location  Convert  the  trainin,amazon
g  data  into  the  format  required  by  the  SageMaker  KMeans  algorithm  buf  io  BytesIO  write  numpy  to  dense  tensor  buf  train  set  train  set  buf  seek  boto3  resource  s3  Bucket  bucket  Object  data  key  upload  fileobj  buf  time  import  boto3  from  time  import  gmtime  strftime  job  name  kmeans  lowlevel  strftime  gmtime  print  Training  job  job  name  images  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  kmeans  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  kmeans  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  kmeans  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  kmeans  latest  image  images  boto3  Session  region  name  output  location  s3  kmeans  example  output  format  bucket  print  training  artifacts  will  be  uploaded  to  format  output  location  create  training  params  AlgorithmSpecification  TrainingImage  image  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPat,amazon
h  output  location  ResourceConfig  InstanceCount  InstanceType  ml  c4  8xlarge  VolumeSizeInGB  50  TrainingJobName  job  name  HyperParameters  10  feature  dim  784  mini  batch  size  500  force  dense  True  StoppingCondition  MaxRuntimeInSeconds  60  60  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  data  location  S3DataDistributionType  FullyReplicated  CompressionType  None  RecordWrapperType  None  sagemaker  boto3  client  sagemaker  sagemaker  create  training  job  create  training  params  status  sagemaker  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  status  try  sagemaker  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  finally  status  sagemaker  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  Training  job  ended  with  status  status  if  status  Failed  message  sagemaker  describe  training  job  TrainingJobName  job  name  FailureReason  ,amazon
print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  time  import  boto3  from  time  import  gmtime  strftime  model  name  job  name  print  model  name  info  sagemaker  describe  training  job  TrainingJobName  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  primary  container  Image  image  ModelDataUrl  model  data  create  model  response  sagemaker  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  endpoint  config  name  KMeansEndpointConfig  strftime  gmtime  print  endpoint  config  name  create  endpoint  config  response  sagemaker  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointC,amazon
onfigArn  time  import  time  endpoint  name  KMeansEndpoint  strftime  gmtime  print  endpoint  name  create  endpoint  response  sagemaker  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  print  create  endpoint  response  EndpointArn  resp  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  try  sagemaker  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  finally  resp  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Create  endpoint  ended  with  status  status  if  status  InService  message  sagemaker  describe  endpoint  EndpointName  endpoint  name  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Endpoint  creation  did  not  succeed  Simple  function  to  create  csv  from  our  numpy  array  def  np2csv  arr  csv  io  BytesIO  numpy  save,amazon
txt  csv  arr  delimiter  fmt  return  csv  getvalue  decode  rstrip  runtime  boto3  Session  client  runtime  sagemaker  import  json  payload  np2csv  train  set  30  31  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  text  csv  Body  payload  result  json  loads  response  Body  read  decode  print  result  time  payload  np2csv  valid  set  100  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  text  csv  Body  payload  result  json  loads  response  Body  read  decode  clusters  closest  cluster  for  in  result  predictions  for  cluster  in  range  10  print  nCluster  format  int  cluster  digits  img  for  img  in  zip  clusters  valid  set  if  int  cluster  height  len  digits  width  plt  rcParams  figure  figsize  width  height  subplots  plt  subplots  height  width  subplots  numpy  ndarray  flatten  subplots  for  subplot  image  in  zip  subplots  digits  show  digit  image  subplot  subplot  for  subplot  in  subplots  len  digi,amazon
ts  subplot  axis  off  plt  show  sagemaker  delete  endpoint  EndpointName  endpoint  name  ,amazon
import  tensorflow  as  tf  from  matplotlib  import  pyplot  import  pandas  as  pd  import  seaborn  as  sns  iris  pd  read  csv  dataset  iris  csv  iris  head  iris  Species  value  counts  iris  describe  sns  pairplot  iris  drop  Id  axis  hue  Species  size  diag  kind  kde  ,microsoft
time  import  boto3  from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  veera  sagemaker  catdog  customize  to  your  bucket  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  image  classification  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  image  classification  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  image  classification  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  image  classification  latest  ap  northeast  501404015308  dkr  ecr  ap  northeast  amazonaws  com  image  classification  latest  training  image  containers  boto3  Session  region  name  import  os  import  urllib  request  import  zipfile  def  download  url  filename  url  split  if  not  os  path  exists  filename  urllib  request  urlretrieve  url  filename  Cat  Dog  Image  files  download  https  www  kaggle  com  dogs  vs  cats  download  train  zip  If  the  above  dosen  work  Use  Kaggle  CLI  to  dow,amazon
nload  dataset  kg  download  username  password  dogs  vs  cats  train  zip  Tool  for  creating  lst  file  download  https  raw  githubusercontent  com  apache  incubator  mxnet  master  tools  im2rec  py  Unzip  the  dataset  zip  ref  zipfile  ZipFile  train  zip  zip  ref  extractall  train  zip  ref  close  Pre  process  data  to  creat  LST  file  bash  mkdir  catDog  dog  mkdir  catDog  cat  for  in  ls  train  train  jpg  do  if  dog  then  mv  catDog  dog  fi  if  cat  then  mv  catDog  cat  fi  done  Create  validation  set  80  Train  dataset  20  Validation  dataset  mkdir  catDog  train  val  dog  mkdir  catDog  train  val  cat  for  in  ls  catDog  dog  jpg  shuf  head  2500  do  mv  catDog  train  val  dog  done  for  in  ls  catDog  cat  jpg  shuf  head  2500  do  mv  catDog  train  val  cat  done  Create  LST  files  python  im2rec  py  list  recursive  catdog  12500  2500  train  catDog  python  im2rec  py  list  recursive  catdog  12500  2500  val  catDog  train  val  head  catdog  12500 ,amazon
 2500  train  lst  example  lst  open  example  lst  lst  content  read  print  lst  content  Four  channels  train  validation  train  lst  and  validation  lst  s3train  s3  train  format  bucket  s3validation  s3  validation  format  bucket  s3train  lst  s3  train  lst  format  bucket  s3validation  lst  s3  validation  lst  format  bucket  upload  the  image  files  to  train  and  validation  channels  aws  s3  cp  catDog  s3train  recursive  quiet  aws  s3  cp  catDog  train  val  s3validation  recursive  quiet  upload  the  lst  files  to  train  lst  and  validation  lst  channels  aws  s3  cp  catdog  12500  2500  train  lst  s3train  lst  quiet  aws  s3  cp  catdog  12500  2500  val  lst  s3validation  lst  quiet  The  algorithm  supports  multiple  network  depth  number  of  layers  They  are  18  34  50  101  152  and  200  For  this  training  we  will  use  18  layers  num  layers  18  we  need  to  specify  the  input  image  shape  for  the  training  data  image  shape  224  224  we  also  ,amazon
need  to  specify  the  number  of  training  samples  in  the  training  set  num  training  samples  20000  specify  the  number  of  output  classes  num  classes  batch  size  for  training  mini  batch  size  128  number  of  epochs  epochs  learning  rate  learning  rate  01  report  top  accuracy  top  resize  image  before  training  resize  256  period  to  store  model  parameters  in  number  of  epochs  in  this  case  we  will  save  parameters  from  epoch  and  checkpoint  frequency  Since  we  are  using  transfer  learning  we  set  use  pretrained  model  to  so  that  weights  can  be  initialized  with  pre  trained  weights  use  pretrained  model  time  import  time  import  boto3  from  time  import  gmtime  strftime  s3  boto3  client  s3  create  unique  job  name  job  name  prefix  sagemaker  imageclassification  catdog  timestamp  time  strftime  time  gmtime  job  name  job  name  prefix  timestamp  training  params  specify  the  training  docker  image  AlgorithmSpecification  T,amazon
rainingImage  training  image  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  s3  output  format  bucket  job  name  prefix  ResourceConfig  InstanceCount  InstanceType  ml  p2  xlarge  VolumeSizeInGB  50  TrainingJobName  job  name  HyperParameters  image  shape  image  shape  num  layers  str  num  layers  num  training  samples  str  num  training  samples  num  classes  str  num  classes  mini  batch  size  str  mini  batch  size  epochs  str  epochs  learning  rate  str  learning  rate  top  str  top  resize  str  resize  checkpoint  frequency  str  checkpoint  frequency  use  pretrained  model  str  use  pretrained  model  StoppingCondition  MaxRuntimeInSeconds  360000  Training  data  should  be  inside  subdirectory  called  train  Validation  data  should  be  inside  subdirectory  called  validation  The  algorithm  currently  only  supports  fullyreplicated  model  where  data  is  copied  onto  each  machine  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S,amazon
3DataType  S3Prefix  S3Uri  s3  train  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  image  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  image  CompressionType  None  ChannelName  train  lst  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  lst  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  image  CompressionType  None  ChannelName  validation  lst  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  lst  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  image  CompressionType  None  print  Training  job  name  format  job  name  print  nInput  Data  Location  format  training  params  InputDataConfig  DataSource  S3DataSource  create  the  Amazon  SageMaker  training  job  sagemaker  boto3  client  service  name  ,amazon
sagemaker  sagemaker  create  training  job  training  params  confirm  that  the  training  job  has  started  status  sagemaker  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  Training  job  current  status  format  status  try  wait  for  the  job  to  finish  and  report  the  ending  status  sagemaker  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  except  print  Training  failed  to  start  if  exception  is  raised  that  means  it  has  failed  message  sagemaker  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  statu,amazon
s  status  print  training  info  time  import  boto3  from  time  import  gmtime  strftime  sage  boto3  Session  client  service  name  sagemaker  timestamp  time  strftime  time  gmtime  model  name  image  classification  model  catdog  timestamp  print  model  name  info  sage  describe  training  job  TrainingJobName  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  data  hosting  image  containers  boto3  Session  region  name  primary  container  Image  hosting  image  ModelDataUrl  model  data  create  model  response  sage  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  timestamp  time  strftime  time  gmtime  endpoint  config  name  job  name  prefix  epc  timestamp  endpoint  config  response  sage  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  p2  xlarge  InitialInstanceCount,amazon
  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  configuration  name  format  endpoint  config  name  print  Endpoint  configuration  arn  format  endpoint  config  response  EndpointConfigArn  time  import  time  timestamp  time  strftime  time  gmtime  endpoint  name  job  name  prefix  ep  timestamp  print  Endpoint  name  format  endpoint  name  endpoint  params  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  endpoint  response  sagemaker  create  endpoint  endpoint  params  print  EndpointArn  format  endpoint  response  EndpointArn  get  the  status  of  the  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  response  EndpointStatus  print  EndpointStatus  format  status  try  sagemaker  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  finally  resp  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Create  endpoint  en,amazon
ded  with  status  status  if  status  InService  message  sagemaker  describe  endpoint  EndpointName  endpoint  name  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Endpoint  creation  did  not  succeed  import  boto3  runtime  boto3  Session  client  service  name  runtime  sagemaker  file  name  tmp  test  jpg  test  image  from  IPython  display  import  Image  Image  file  name  import  json  import  numpy  as  np  with  open  file  name  rb  as  payload  read  payload  bytearray  payload  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  image  Body  payload  result  response  Body  read  result  will  be  in  json  format  and  convert  it  to  ndarray  result  json  loads  result  the  result  will  output  the  probabilities  for  all  classes  find  the  class  with  maximum  probability  and  print  the  class  index  index  np  argmax  result  object  categories  cat  dog  print  Result  label  obj,amazon
ect  categories  index  probability  str  result  index  sage  delete  endpoint  EndpointName  endpoint  name  ,amazon
10  def  get  factorial  number  product  while  True  if  number  return  product  product  product  number  number  number  print  get  factorial  10  10  def  get  factorial  number  product  while  True  if  number  return  product  product  product  number  number  number  result  get  factorial  10  string  str  result  total  for  in  string  total  total  int  print  total  100  def  get  factorial  number  product  while  True  if  number  return  product  product  product  number  number  number  result  get  factorial  100  print  result  string  str  result  total  for  in  string  total  total  int  print  total  ,microsoft
import  boto3  import  botocore  import  sagemaker  import  sys  bucket  specify  bucket  you  have  access  to  prefix  sagemaker  rcf  benchmarks  execution  role  sagemaker  get  execution  role  check  if  the  bucket  exists  try  boto3  Session  client  s3  head  bucket  Bucket  bucket  except  botocore  exceptions  ParamValidationError  as  print  Hey  You  either  forgot  to  specify  your  S3  bucket  or  you  gave  your  bucket  an  invalid  name  except  botocore  exceptions  ClientError  as  if  response  Error  Code  403  print  Hey  You  don  have  permission  to  access  the  bucket  format  bucket  elif  response  Error  Code  404  print  Hey  Your  bucket  doesn  exist  format  bucket  else  raise  else  print  Training  input  output  will  be  stored  in  s3  format  bucket  prefix  time  import  pandas  as  pd  import  urllib  request  data  filename  nyc  taxi  csv  data  source  https  raw  githubusercontent  com  numenta  NAB  master  data  realKnownCause  nyc  taxi  csv  urllib  reques,amazon
t  urlretrieve  data  source  data  filename  taxi  data  pd  read  csv  data  filename  delimiter  taxi  data  head  matplotlib  inline  import  matplotlib  import  matplotlib  pyplot  as  plt  matplotlib  rcParams  figure  dpi  100  taxi  data  plot  taxi  data  5500  6500  plot  taxi  data  5952  6000  from  sagemaker  import  RandomCutForest  session  sagemaker  Session  specify  general  training  job  information  rcf  RandomCutForest  role  execution  role  train  instance  count  train  instance  type  ml  m4  xlarge  data  location  s3  format  bucket  prefix  output  path  s3  output  format  bucket  prefix  num  samples  per  tree  512  num  trees  50  automatically  upload  the  training  data  to  S3  and  run  the  training  job  rcf  fit  rcf  record  set  taxi  data  value  as  matrix  reshape  print  Training  job  name  format  rcf  latest  training  job  job  name  rcf  inference  rcf  deploy  initial  instance  count  instance  type  ml  m4  xlarge  print  Endpoint  name  format  rcf  infe,amazon
rence  endpoint  from  sagemaker  predictor  import  csv  serializer  json  deserializer  rcf  inference  content  type  text  csv  rcf  inference  serializer  csv  serializer  rcf  inference  accept  application  json  rcf  inference  deserializer  json  deserializertaxi  data  numpy  taxi  data  value  as  matrix  reshape  print  taxi  data  numpy  results  rcf  inference  predict  taxi  data  numpy  results  rcf  inference  predict  taxi  data  numpy  scores  datum  score  for  datum  in  results  scores  add  scores  to  taxi  data  frame  and  print  first  few  values  taxi  data  score  pd  Series  scores  index  taxi  data  index  taxi  data  head  fig  ax1  plt  subplots  ax2  ax1  twinx  Try  this  out  change  start  and  end  to  zoom  in  on  the  anomaly  found  earlier  in  this  notebook  start  end  len  taxi  data  start  end  5500  6500  taxi  data  subset  taxi  data  start  end  ax1  plot  taxi  data  subset  value  color  C0  alpha  ax2  plot  taxi  data  subset  score  color  C1  ax1  g,amazon
rid  which  major  axis  both  ax1  set  ylabel  Taxi  Ridership  color  C0  ax2  set  ylabel  Anomaly  Score  color  C1  ax1  tick  params  colors  C0  ax2  tick  params  colors  C1  ax1  set  ylim  40000  ax2  set  ylim  min  scores  max  scores  fig  set  figwidth  10  score  mean  taxi  data  score  mean  score  std  taxi  data  score  std  score  cutoff  score  mean  score  std  anomalies  taxi  data  subset  taxi  data  subset  score  score  cutoff  anomaliesax2  plot  anomalies  index  anomalies  score  ko  figsagemaker  Session  delete  endpoint  rcf  inference  endpoint  import  numpy  as  np  def  shingle  data  shingle  size  num  data  len  data  shingled  data  np  zeros  num  data  shingle  size  shingle  size  for  in  range  num  data  shingle  size  shingled  data  data  shingle  size  return  shingled  data  single  data  with  shingle  size  48  one  day  shingle  size  48  prefix  shingled  sagemaker  randomcutforest  shingled  taxi  data  shingled  shingle  taxi  data  values  shingle  si,amazon
ze  print  taxi  data  shingled  session  sagemaker  Session  specify  general  training  job  information  rcf  RandomCutForest  role  execution  role  train  instance  count  train  instance  type  ml  m4  xlarge  data  location  s3  format  bucket  prefix  shingled  output  path  s3  output  format  bucket  prefix  shingled  num  samples  per  tree  512  num  trees  50  automatically  upload  the  training  data  to  S3  and  run  the  training  job  rcf  fit  rcf  record  set  taxi  data  shingled  from  sagemaker  predictor  import  csv  serializer  json  deserializer  rcf  inference  rcf  deploy  initial  instance  count  instance  type  ml  m4  xlarge  rcf  inference  content  type  text  csv  rcf  inference  serializer  csv  serializer  rcf  inference  accept  appliation  json  rcf  inference  deserializer  json  deserializer  Score  the  shingled  datapoints  results  rcf  inference  predict  taxi  data  shingled  scores  np  array  datum  score  for  datum  in  results  scores  compute  the  shingle,amazon
d  score  distribution  and  cutoff  and  determine  anomalous  scores  score  mean  scores  mean  score  std  scores  std  score  cutoff  score  mean  score  std  anomalies  scores  scores  score  cutoff  anomaly  indices  np  arange  len  scores  scores  score  cutoff  print  anomalies  fig  ax1  plt  subplots  ax2  ax1  twinx  Try  this  out  change  start  and  end  to  zoom  in  on  the  anomaly  found  earlier  in  this  notebook  start  end  len  taxi  data  taxi  data  subset  taxi  data  start  end  ax1  plot  taxi  data  value  color  C0  alpha  ax2  plot  scores  color  C1  ax2  scatter  anomaly  indices  anomalies  color  ax1  grid  which  major  axis  both  ax1  set  ylabel  Taxi  Ridership  color  C0  ax2  set  ylabel  Anomaly  Score  color  C1  ax1  tick  params  colors  C0  ax2  tick  params  colors  C1  ax1  set  ylim  40000  ax2  set  ylim  min  scores  max  scores  fig  set  figwidth  10  sagemaker  Session  delete  endpoint  rcf  inference  endpoint  ,amazon
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  gluon  recsys  import  sagemaker  role  sagemaker  get  execution  role  import  os  import  mxnet  as  mx  from  mxnet  import  gluon  nd  ndarray  from  mxnet  metric  import  MSE  import  pandas  as  pd  import  numpy  as  np  import  sagemaker  from  sagemaker  mxnet  import  MXNet  import  boto3  import  json  import  matplotlib  pyplot  as  plt  mkdir  tmp  recsys  aws  s3  cp  s3  amazon  reviews  pds  tsv  amazon  reviews  us  Digital  Video  Download  v1  00  tsv  gz  tmp  recsys  df  pd  read  csv  tmp  recsys  amazon  reviews  us  Digital  Video  Download  v1  00  tsv  gz  delimiter  error  bad  lines  False  df  head  df  df  customer  id  product  id  star  rating  product  title  customers  df  customer  id  value  counts  products  df  product  id  value  counts  quantiles  01  02  03  04  05  25  75  95  96  97  98  99  print  customers  customers  quantile  quantiles  print  products  products  quantile  quantiles  customers  cust,amazon
omers  customers  products  products  products  10  reduced  df  df  merge  pd  DataFrame  customer  id  customers  index  merge  pd  DataFrame  product  id  products  index  customers  reduced  df  customer  id  value  counts  products  reduced  df  product  id  value  counts  customer  index  pd  DataFrame  customer  id  customers  index  user  np  arange  customers  shape  product  index  pd  DataFrame  product  id  products  index  item  np  arange  products  shape  reduced  df  reduced  df  merge  customer  index  merge  product  index  reduced  df  head  test  df  reduced  df  groupby  customer  id  last  reset  index  train  df  reduced  df  merge  test  df  customer  id  product  id  on  customer  id  product  id  how  outer  indicator  True  train  df  train  df  train  df  merge  left  only  class  SparseMatrixDataset  gluon  data  Dataset  def  init  self  data  label  assert  data  shape  len  label  self  data  data  self  label  label  if  isinstance  label  ndarray  NDArray  and  len  label  sh,amazon
ape  self  label  label  asnumpy  else  self  label  label  def  getitem  self  idx  return  self  data  idx  self  data  idx  self  label  idx  def  len  self  return  self  data  shape  batch  size  40  train  iter  gluon  data  DataLoader  SparseMatrixDataset  nd  array  train  df  user  item  values  dtype  np  float32  nd  array  train  df  star  rating  values  dtype  np  float32  shuffle  True  batch  size  batch  size  test  iter  gluon  data  DataLoader  SparseMatrixDataset  nd  array  test  df  user  item  values  dtype  np  float32  nd  array  test  df  star  rating  values  dtype  np  float32  shuffle  True  batch  size  batch  size  class  MFBlock  gluon  HybridBlock  def  init  self  max  users  max  items  num  emb  dropout  super  MFBlock  self  init  self  max  users  max  users  self  max  items  max  items  self  dropout  dropout  self  num  emb  num  emb  with  self  name  scope  self  user  embeddings  gluon  nn  Embedding  max  users  num  emb  self  item  embeddings  gluon  nn  Embeddin,amazon
g  max  items  num  emb  self  dropout  gluon  nn  Dropout  dropout  self  dense  gluon  nn  Dense  num  emb  activation  relu  def  hybrid  forward  self  users  items  self  user  embeddings  users  self  dense  self  item  embeddings  items  self  dense  predictions  self  dropout  self  dropout  predictions  sum  predictions  axis  return  predictionsnum  embeddings  64  net  MFBlock  max  users  customer  index  shape  max  items  product  index  shape  num  emb  num  embeddings  dropout  net  collect  params  Initialize  network  parameters  ctx  mx  gpu  net  collect  params  initialize  mx  init  Xavier  magnitude  24  ctx  ctx  force  reinit  True  net  hybridize  Set  optimization  parameters  opt  sgd  lr  02  momentum  wd  trainer  gluon  Trainer  net  collect  params  opt  learning  rate  lr  wd  wd  momentum  momentum  def  execute  train  iter  test  iter  net  epochs  ctx  loss  function  gluon  loss  L2Loss  for  in  range  epochs  print  epoch  format  for  user  item  label  in  enumerate  ,amazon
train  iter  try  user  user  as  in  context  ctx  reshape  batch  size  item  item  as  in  context  ctx  reshape  batch  size  label  label  as  in  context  ctx  reshape  batch  size  with  mx  autograd  record  output  net  user  item  loss  loss  function  output  label  loss  backward  trainer  step  batch  size  except  pass  print  EPOCH  MSE  ON  TRAINING  and  TEST  format  eval  net  train  iter  net  ctx  loss  function  eval  net  test  iter  net  ctx  loss  function  print  end  of  training  return  netdef  eval  net  data  net  ctx  loss  function  acc  MSE  for  user  item  label  in  enumerate  data  try  user  user  as  in  context  ctx  reshape  batch  size  item  item  as  in  context  ctx  reshape  batch  size  label  label  as  in  context  ctx  reshape  batch  size  predictions  net  user  item  reshape  batch  size  acc  update  preds  predictions  labels  label  except  pass  return  acc  get  time  epochs  trained  net  execute  train  iter  test  iter  net  epochs  ctx  product  i,amazon
ndex  u6  predictions  trained  net  nd  array  product  index  shape  as  in  context  ctx  nd  array  product  index  item  values  as  in  context  ctx  asnumpy  product  index  sort  values  u6  predictions  ascending  False  product  index  u7  predictions  trained  net  nd  array  product  index  shape  as  in  context  ctx  nd  array  product  index  item  values  as  in  context  ctx  asnumpy  product  index  sort  values  u7  predictions  ascending  False  product  index  u6  predictions  u7  predictions  plot  scatter  u6  predictions  u7  predictions  plt  show  cat  recommender  py  time  import  recommender  local  test  net  local  customer  index  local  product  index  recommender  train  train  tmp  recsys  num  embeddings  64  opt  sgd  lr  02  momentum  wd  epochs  local  boto3  client  s3  copy  Bucket  amazon  reviews  pds  Key  tsv  amazon  reviews  us  Digital  Video  Download  v1  00  tsv  gz  bucket  prefix  train  amazon  reviews  us  Digital  Video  Download  v1  00  tsv  gz  MXNet ,amazon
 recommender  py  py  version  py3  role  role  train  instance  count  train  instance  type  ml  p2  xlarge  output  path  s3  output  format  bucket  prefix  hyperparameters  num  embeddings  512  opt  opt  lr  lr  momentum  momentum  wd  wd  epochs  10  fit  train  s3  train  format  bucket  prefix  predictor  deploy  initial  instance  count  instance  type  ml  m4  xlarge  predictor  serializer  Nonepredictor  predict  json  dumps  customer  id  customer  index  customer  index  user  customer  id  values  tolist  product  id  B00KH1O9HW  B00M5KODWO  print  Naive  MSE  np  mean  test  df  star  rating  np  mean  train  df  star  rating  test  preds  for  array  in  np  array  split  test  df  customer  id  product  id  values  40  test  preds  predictor  predict  json  dumps  customer  id  array  tolist  product  id  array  tolist  test  preds  np  array  test  preds  print  MSE  np  mean  test  df  star  rating  test  preds  reduced  df  reduced  df  user  sort  values  star  rating  item  ascending  F,amazon
alse  True  predictions  for  array  in  np  array  split  product  index  product  id  values  40  predictions  predictor  predict  json  dumps  customer  id  customer  index  customer  index  user  customer  id  values  tolist  array  shape  product  id  array  tolist  predictions  pd  DataFrame  product  id  product  index  product  id  prediction  predictions  titles  reduced  df  groupby  product  id  product  title  last  reset  index  predictions  titles  predictions  merge  titles  predictions  titles  sort  values  prediction  product  id  ascending  False  True  predictions  user7  for  array  in  np  array  split  product  index  product  id  values  40  predictions  user7  predictor  predict  json  dumps  customer  id  customer  index  customer  index  user  customer  id  values  tolist  array  shape  product  id  array  tolist  plt  scatter  predictions  prediction  np  array  predictions  user7  plt  show  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
Use  this  only  for  Azure  AD  service  to  service  authentication  from  azure  common  credentials  import  ServicePrincipalCredentials  Use  this  only  for  Azure  AD  end  user  authentication  from  azure  common  credentials  import  UserPassCredentials  Use  this  only  for  Azure  AD  multi  factor  authentication  from  msrestazure  azure  active  directory  import  AADTokenCredentials  Required  for  Azure  Data  Lake  Store  account  management  from  azure  mgmt  datalake  store  import  DataLakeStoreAccountManagementClient  from  azure  mgmt  datalake  store  models  import  DataLakeStoreAccount  Required  for  Azure  Data  Lake  Store  filesystem  management  from  azure  datalake  store  import  core  lib  multithread  Common  Azure  imports  from  azure  mgmt  resource  resources  import  ResourceManagementClient  from  azure  mgmt  resource  resources  models  import  ResourceGroup  Use  these  as  needed  for  your  application  import  logging  getpass  pprint  uuid  time  You  need  to,microsoft
  be  interactive  in  order  to  log  on  in  Microsoft  Device  Log  in  Page  https  microsoft  com  devicelogin  credentials  lib  auth  Create  filesystem  client  object  adlsFileSystemClient  core  AzureDLFileSystem  credentials  store  name  jacktestsbdls  Create  directory  adlsFileSystemClient  mkdir  mysampledirectory  Upload  file  multithread  ADLUploader  adlsFileSystemClient  lpath  Users  xinxue  data  test  txt  rpath  mysampledirectory  mysamplefile  txt  nthreads  64  overwrite  True  buffersize  4194304  blocksize  4194304  Download  file  multithread  ADLDownloader  adlsFileSystemClient  lpath  Users  xinxue  data  mysamplefile  txt  out  rpath  mysampledirectory  mysamplefile  txt  nthreads  64  overwrite  True  buffersize  4194304  blocksize  4194304  Delete  directory  recusively  so  we  are  totally  cleaned  out  adlsFileSystemClient  rm  mysampledirectory  recursive  True  ,microsoft
import  boto3  import  io  import  matplotlib  pyplot  as  plt  import  numpy  as  np  import  os  import  pandas  as  pd  import  sagemaker  import  sagemaker  amazon  common  as  smac  from  sagemaker  import  get  execution  role  from  sagemaker  predictor  import  csv  serializer  json  deserializer  Set  data  locations  bucket  your  s3  bucket  here  replace  this  with  your  own  bucket  prefix  sagemaker  DEMO  linear  learner  loss  weights  replace  this  with  your  own  prefix  s3  train  key  train  recordio  pb  data  format  prefix  s3  train  path  os  path  join  s3  bucket  s3  train  key  local  raw  data  creditcard  csv  zip  role  get  execution  role  Confirm  access  to  s3  bucket  for  obj  in  boto3  resource  s3  Bucket  bucket  objects  all  print  obj  key  Read  the  data  shuffle  and  split  into  train  and  test  sets  separating  the  labels  last  column  from  the  features  raw  data  pd  read  csv  local  raw  data  as  matrix  np  random  seed  np  random  shuffle  ,amazon
raw  data  train  size  int  raw  data  shape  train  features  raw  data  train  size  train  labels  raw  data  train  size  test  features  raw  data  train  size  test  labels  raw  data  train  size  Convert  the  processed  training  data  to  protobuf  and  write  to  S3  for  linear  learner  vectors  np  array  tolist  for  in  train  features  astype  float32  labels  np  array  tolist  for  in  train  labels  astype  float32  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  vectors  labels  buf  seek  boto3  resource  s3  Bucket  bucket  Object  s3  train  key  upload  fileobj  buf  def  predictor  from  hyperparams  s3  train  data  hyperparams  output  path  Create  an  Estimator  from  the  given  hyperparams  fit  to  training  data  and  return  deployed  predictor  specify  algorithm  containers  and  instantiate  an  Estimator  with  given  hyperparams  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  linear  learner  latest  us  east  382416733822  dkr  e,amazon
cr  us  east  amazonaws  com  linear  learner  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  linear  learner  latest  linear  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  role  train  instance  count  train  instance  type  ml  m4  xlarge  output  path  output  path  sagemaker  session  sagemaker  Session  linear  set  hyperparameters  hyperparams  train  model  linear  fit  train  s3  train  data  deploy  predictor  linear  predictor  linear  deploy  initial  instance  count  instance  type  ml  m4  xlarge  linear  predictor  content  type  text  csv  linear  predictor  serializer  csv  serializer  linear  predictor  deserializer  json  deserializer  return  linear  predictordef  evaluate  linear  predictor  test  features  test  labels  model  name  verbose  True  Evaluate  model  on  test  set  given  the  prediction  endpoint  Return  binary  classification  metrics  spl,amazon
it  the  test  data  set  into  100  batches  and  evaluate  using  prediction  endpoint  prediction  batches  linear  predictor  predict  batch  predictions  for  batch  in  np  array  split  test  features  100  parse  raw  predictions  json  to  exctract  predicted  label  test  preds  np  concatenate  np  array  predicted  label  for  in  batch  for  batch  in  prediction  batches  calculate  true  positives  false  positives  true  negatives  false  negatives  tp  np  logical  and  test  labels  test  preds  sum  fp  np  logical  and  test  labels  test  preds  sum  tn  np  logical  and  test  labels  test  preds  sum  fn  np  logical  and  test  labels  test  preds  sum  calculate  binary  classification  metrics  recall  tp  tp  fn  precision  tp  tp  fp  accuracy  tp  tn  tp  fp  tn  fn  f1  precision  recall  precision  recall  if  verbose  print  pd  crosstab  test  labels  test  preds  rownames  actuals  colnames  predictions  print  11  3f  format  Recall  recall  print  11  3f  format  Precision ,amazon
 precision  print  11  3f  format  Accuracy  accuracy  print  11  3f  format  F1  f1  return  TP  tp  FP  fp  FN  fn  TN  tn  Precision  precision  Recall  recall  Accuracy  accuracy  F1  f1  Model  model  name  def  delete  endpoint  predictor  try  boto3  client  sagemaker  delete  endpoint  EndpointName  predictor  endpoint  print  Deleted  format  predictor  endpoint  except  print  Already  deleted  format  predictor  endpoint  Training  binary  classifier  with  default  settings  logistic  regression  defaults  hyperparams  feature  dim  30  predictor  type  binary  classifier  epochs  40  defaults  output  path  s3  defaults  output  format  bucket  prefix  defaults  predictor  predictor  from  hyperparams  s3  train  path  defaults  hyperparams  defaults  output  path  Training  binary  classifier  with  automated  threshold  tuning  autothresh  hyperparams  feature  dim  30  predictor  type  binary  classifier  binary  classifier  model  selection  criteria  precision  at  target  recall  target  re,amazon
call  epochs  40  autothresh  output  path  s3  autothresh  output  format  bucket  prefix  autothresh  predictor  predictor  from  hyperparams  s3  train  path  autothresh  hyperparams  autothresh  output  path  Training  binary  classifier  with  class  weights  and  automated  threshold  tuning  class  weights  hyperparams  feature  dim  30  predictor  type  binary  classifier  binary  classifier  model  selection  criteria  precision  at  target  recall  target  recall  positive  example  weight  mult  balanced  epochs  40  class  weights  output  path  s3  class  weights  output  format  bucket  prefix  class  weights  predictor  predictor  from  hyperparams  s3  train  path  class  weights  hyperparams  class  weights  output  path  Training  binary  classifier  with  hinge  loss  and  automated  threshold  tuning  svm  hyperparams  feature  dim  30  predictor  type  binary  classifier  loss  hinge  loss  binary  classifier  model  selection  criteria  precision  at  target  recall  target  recall  epoc,amazon
hs  40  svm  output  path  s3  svm  output  format  bucket  prefix  svm  predictor  predictor  from  hyperparams  s3  train  path  svm  hyperparams  svm  output  path  Training  binary  classifier  with  hinge  loss  balanced  class  weights  and  automated  threshold  tuning  svm  balanced  hyperparams  feature  dim  30  predictor  type  binary  classifier  loss  hinge  loss  binary  classifier  model  selection  criteria  precision  at  target  recall  target  recall  positive  example  weight  mult  balanced  epochs  40  svm  balanced  output  path  s3  svm  balanced  output  format  bucket  prefix  svm  balanced  predictor  predictor  from  hyperparams  s3  train  path  svm  balanced  hyperparams  svm  balanced  output  path  Evaluate  the  trained  models  predictors  Logistic  defaults  predictor  Logistic  with  auto  threshold  autothresh  predictor  Logistic  with  class  weights  class  weights  predictor  Hinge  with  auto  threshold  svm  predictor  Hinge  with  class  weights  svm  balanced  pred,amazon
ictor  metrics  key  evaluate  predictor  test  features  test  labels  key  False  for  key  predictor  in  predictors  items  pd  set  option  display  float  format  lambda  3f  display  pd  DataFrame  list  metrics  values  loc  Model  Recall  Precision  Accuracy  F1  for  predictor  in  defaults  predictor  autothresh  predictor  class  weights  predictor  svm  predictor  svm  balanced  predictor  delete  endpoint  predictor  ,amazon
import  pandasdata  pandas  read  csv  Datasets  userssharedsdfperc18to24yowhonethenrlschlnorworksre20002012  csv  data  ,microsoft
import  tensorflow  as  tf  import  pandas  as  pd  from  matplotlib  import  pyplottrain  data  all  pd  read  csv  dataset  titanic  train  csv  train  data  all  head  train  data  all  info  test  data  all  pd  read  csv  dataset  titanic  test  csv  test  data  all  head  train  data  one  hot  pd  get  dummies  train  data  all  train  data  one  hot  head  train  data  one  hot  describe  input  columns  Pclass  SibSp  Parch  label  columns  Survived  input  count  len  input  columns  label  count  len  label  columns  test  data  sample  train  data  one  hot  sample  frac  random  state  162  test  data  input  test  data  sample  filter  input  columns  test  data  label  test  data  sample  filter  label  columns  train  data  sample  train  data  one  hot  drop  test  data  sample  index  train  data  input  train  data  sample  filter  input  columns  train  data  label  train  data  sample  filter  label  columns  print  format  input  count  label  count  statistics  tf  placeholder  tf  floa,microsoft
t32  None  input  count  classification  tf  placeholder  tf  float32  None  label  count  input  tf  Variable  tf  zeros  input  count  label  count  input  tf  Variable  tf  zeros  label  count  prediction  input  tf  nn  softmax  tf  matmul  statistics  input  input  error  rate  tf  reduce  mean  tf  reduce  sum  classification  tf  log  prediction  input  reduction  indices  optimizer  tf  train  GradientDescentOptimizer  001  minimize  error  rate  iterations  10000  sess  tf  Session  init  tf  global  variables  initializer  sess  run  init  correct  prediction  tf  equal  tf  argmax  prediction  input  tf  argmax  classification  accuracy  tf  reduce  mean  tf  cast  correct  prediction  tf  float32  for  step  in  range  iterations  sess  run  optimizer  feed  dict  statistics  train  data  input  classification  train  data  label  if  step  iterations  print  accuracy  format  sess  run  accuracy  statistics  test  data  input  classification  test  data  label  print  check  sample  sess  run  pr,microsoft
ediction  input  statistics  check  sample  ,microsoft
time  import  boto3  from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  veera  sagemaker  catdog  customize  to  your  bucket  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  image  classification  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  image  classification  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  image  classification  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  image  classification  latest  ap  northeast  501404015308  dkr  ecr  ap  northeast  amazonaws  com  image  classification  latest  training  image  containers  boto3  Session  region  name  import  os  import  urllib  request  import  zipfile  def  download  url  filename  url  split  if  not  os  path  exists  filename  urllib  request  urlretrieve  url  filename  Cat  Dog  Image  files  download  https  www  kaggle  com  dogs  vs  cats  download  train  zip  If  the  above  dosen  work  Use  Kaggle  CLI  to  dow,amazon
nload  dataset  kg  download  username  password  dogs  vs  cats  train  zip  Tool  for  creating  lst  file  download  https  raw  githubusercontent  com  apache  incubator  mxnet  master  tools  im2rec  py  Unzip  the  dataset  zip  ref  zipfile  ZipFile  train  zip  zip  ref  extractall  train  zip  ref  close  Pre  process  data  to  creat  LST  file  bash  mkdir  catDog  dog  mkdir  catDog  cat  for  in  ls  train  train  jpg  do  if  dog  then  mv  catDog  dog  fi  if  cat  then  mv  catDog  cat  fi  done  Create  validation  set  80  Train  dataset  20  Validation  dataset  mkdir  catDog  train  val  dog  mkdir  catDog  train  val  cat  for  in  ls  catDog  dog  jpg  shuf  head  2500  do  mv  catDog  train  val  dog  done  for  in  ls  catDog  cat  jpg  shuf  head  2500  do  mv  catDog  train  val  cat  done  Create  LST  files  python  im2rec  py  list  recursive  catdog  12500  2500  train  catDog  python  im2rec  py  list  recursive  catdog  12500  2500  val  catDog  train  val  head  catdog  12500 ,amazon
 2500  train  lst  example  lst  open  example  lst  lst  content  read  print  lst  content  Four  channels  train  validation  train  lst  and  validation  lst  s3train  s3  train  format  bucket  s3validation  s3  validation  format  bucket  s3train  lst  s3  train  lst  format  bucket  s3validation  lst  s3  validation  lst  format  bucket  upload  the  image  files  to  train  and  validation  channels  aws  s3  cp  catDog  s3train  recursive  quiet  aws  s3  cp  catDog  train  val  s3validation  recursive  quiet  upload  the  lst  files  to  train  lst  and  validation  lst  channels  aws  s3  cp  catdog  12500  2500  train  lst  s3train  lst  quiet  aws  s3  cp  catdog  12500  2500  val  lst  s3validation  lst  quiet  The  algorithm  supports  multiple  network  depth  number  of  layers  They  are  18  34  50  101  152  and  200  For  this  training  we  will  use  18  layers  num  layers  18  we  need  to  specify  the  input  image  shape  for  the  training  data  image  shape  224  224  we  also  ,amazon
need  to  specify  the  number  of  training  samples  in  the  training  set  num  training  samples  20000  specify  the  number  of  output  classes  num  classes  batch  size  for  training  mini  batch  size  128  number  of  epochs  epochs  learning  rate  learning  rate  01  report  top  accuracy  top  resize  image  before  training  resize  256  period  to  store  model  parameters  in  number  of  epochs  in  this  case  we  will  save  parameters  from  epoch  and  checkpoint  frequency  Since  we  are  using  transfer  learning  we  set  use  pretrained  model  to  so  that  weights  can  be  initialized  with  pre  trained  weights  use  pretrained  model  time  import  time  import  boto3  from  time  import  gmtime  strftime  s3  boto3  client  s3  create  unique  job  name  job  name  prefix  sagemaker  imageclassification  catdog  timestamp  time  strftime  time  gmtime  job  name  job  name  prefix  timestamp  training  params  specify  the  training  docker  image  AlgorithmSpecification  T,amazon
rainingImage  training  image  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  s3  output  format  bucket  job  name  prefix  ResourceConfig  InstanceCount  InstanceType  ml  p2  xlarge  VolumeSizeInGB  50  TrainingJobName  job  name  HyperParameters  image  shape  image  shape  num  layers  str  num  layers  num  training  samples  str  num  training  samples  num  classes  str  num  classes  mini  batch  size  str  mini  batch  size  epochs  str  epochs  learning  rate  str  learning  rate  top  str  top  resize  str  resize  checkpoint  frequency  str  checkpoint  frequency  use  pretrained  model  str  use  pretrained  model  StoppingCondition  MaxRuntimeInSeconds  360000  Training  data  should  be  inside  subdirectory  called  train  Validation  data  should  be  inside  subdirectory  called  validation  The  algorithm  currently  only  supports  fullyreplicated  model  where  data  is  copied  onto  each  machine  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S,amazon
3DataType  S3Prefix  S3Uri  s3  train  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  image  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  image  CompressionType  None  ChannelName  train  lst  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  lst  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  image  CompressionType  None  ChannelName  validation  lst  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  lst  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  image  CompressionType  None  print  Training  job  name  format  job  name  print  nInput  Data  Location  format  training  params  InputDataConfig  DataSource  S3DataSource  create  the  Amazon  SageMaker  training  job  sagemaker  boto3  client  service  name  ,amazon
sagemaker  sagemaker  create  training  job  training  params  confirm  that  the  training  job  has  started  status  sagemaker  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  Training  job  current  status  format  status  try  wait  for  the  job  to  finish  and  report  the  ending  status  sagemaker  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  except  print  Training  failed  to  start  if  exception  is  raised  that  means  it  has  failed  message  sagemaker  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  statu,amazon
s  status  print  training  info  time  import  boto3  from  time  import  gmtime  strftime  sage  boto3  Session  client  service  name  sagemaker  timestamp  time  strftime  time  gmtime  model  name  image  classification  model  catdog  timestamp  print  model  name  info  sage  describe  training  job  TrainingJobName  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  data  hosting  image  containers  boto3  Session  region  name  primary  container  Image  hosting  image  ModelDataUrl  model  data  create  model  response  sage  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  timestamp  time  strftime  time  gmtime  endpoint  config  name  job  name  prefix  epc  timestamp  endpoint  config  response  sage  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  p2  xlarge  InitialInstanceCount,amazon
  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  configuration  name  format  endpoint  config  name  print  Endpoint  configuration  arn  format  endpoint  config  response  EndpointConfigArn  time  import  time  timestamp  time  strftime  time  gmtime  endpoint  name  job  name  prefix  ep  timestamp  print  Endpoint  name  format  endpoint  name  endpoint  params  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  endpoint  response  sagemaker  create  endpoint  endpoint  params  print  EndpointArn  format  endpoint  response  EndpointArn  get  the  status  of  the  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  response  EndpointStatus  print  EndpointStatus  format  status  try  sagemaker  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  finally  resp  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Create  endpoint  en,amazon
ded  with  status  status  if  status  InService  message  sagemaker  describe  endpoint  EndpointName  endpoint  name  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Endpoint  creation  did  not  succeed  import  boto3  runtime  boto3  Session  client  service  name  runtime  sagemaker  file  name  tmp  test  jpg  test  image  from  IPython  display  import  Image  Image  file  name  import  json  import  numpy  as  np  with  open  file  name  rb  as  payload  read  payload  bytearray  payload  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  image  Body  payload  result  response  Body  read  result  will  be  in  json  format  and  convert  it  to  ndarray  result  json  loads  result  the  result  will  output  the  probabilities  for  all  classes  find  the  class  with  maximum  probability  and  print  the  class  index  index  np  argmax  result  object  categories  cat  dog  print  Result  label  obj,amazon
ect  categories  index  probability  str  result  index  sage  delete  endpoint  EndpointName  endpoint  name  ,amazon
bash  EMR  MASTER  INTERNAL  IP  ip  172  31  58  190  ec2  internal  CONF  home  ec2  user  sparkmagic  config  json  if  CONF  bk  then  wget  https  raw  githubusercontent  com  jupyter  incubator  sparkmagic  master  sparkmagic  example  config  json  home  ec2  user  sparkmagic  home  ec2  user  sparkmagic  config  json  bk  dev  null  fi  cat  CONF  bk  sed  localhost  EMR  MASTER  INTERNAL  IP  CONF  new  if  diff  CONF  new  CONF  then  echo  Configuration  has  changed  Restart  Kernel  fi  cp  CONF  new  CONFimport  boto3  params  SNOWFLAKE  URL  SNOWFLAKE  ACCOUNT  ID  SNOWFLAKE  USER  ID  SNOWFLAKE  PASSWORD  SNOWFLAKE  DATABASE  SNOWFLAKE  SCHEMA  SNOWFLAKE  WAREHOUSE  SNOWFLAKE  BUCKET  SNOWFLAKE  PREFIX  region  us  east  def  get  credentials  params  ssm  boto3  client  ssm  region  response  ssm  get  parameters  Names  params  WithDecryption  True  Build  dict  of  credentials  param  values  Name  Value  for  in  response  Parameters  return  param  values  param  values  get  credentials ,amazon
 params  sfOptions  sfURL  param  values  SNOWFLAKE  URL  sfAccount  param  values  SNOWFLAKE  ACCOUNT  ID  sfUser  param  values  SNOWFLAKE  USER  ID  sfPassword  param  values  SNOWFLAKE  PASSWORD  sfDatabase  param  values  SNOWFLAKE  DATABASE  sfSchema  param  values  SNOWFLAKE  SCHEMA  sfWarehouse  param  values  SNOWFLAKE  WAREHOUSE  SNOWFLAKE  SOURCE  NAME  net  snowflake  spark  snowflake  df  spark  read  format  SNOWFLAKE  SOURCE  NAME  options  sfOptions  option  query  select  main  temp  max  273  15  8000  32  00  as  temp  max  far  main  temp  min  273  15  8000  32  00  as  temp  min  far  cast  time  as  timestamp  time  city  coord  lat  lat  city  coord  lon  lon  from  snowflake  sample  data  weather  weather  14  total  load  df  describe  show  ,amazon
time  wget  http  www  vision  caltech  edu  Image  Datasets  Caltech101  101  ObjectCategories  tar  gz  tar  xvzf  101  ObjectCategories  tar  gz  wget  https  raw  githubusercontent  com  apache  incubator  mxnet  master  tools  im2rec  py  python  im2rec  py  list  recursive  train  ratio  80  test  ratio  10  caltech101  101  ObjectCategories  class  index  python  im2rec  py  num  thread  16  resize  200  caltech101  101  ObjectCategoriesimport  os  import  urllib  request  import  boto3  import  re  import  sagemaker  from  sagemaker  import  get  execution  role  sess  sagemaker  Session  role  get  execution  role  bucket  sess  default  bucket  Upload  files  to  S3  prefix  notebook  transfer  caltech  train  input  sess  upload  data  path  caltech101  train  rec  key  prefix  prefix  valid  input  sess  upload  data  path  caltech101  val  rec  key  prefix  prefix  test  input  sess  upload  data  path  caltech101  test  rec  key  prefix  prefix  Show  S3  path  print  Training  data  is  uploade,amazon
d  to  train  input  print  Validation  data  is  uploaded  to  valid  input  print  Test  data  is  uploaded  to  valid  input  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  training  image  get  image  uri  boto3  Session  region  name  image  classification  img  transfer  sagemaker  estimator  Estimator  training  image  role  train  instance  count  train  instance  type  ml  p2  xlarge  output  path  s3  output  format  bucket  prefix  sagemaker  session  sess  img  transfer  set  hyperparameters  num  classes  101  num  training  samples  7315  use  pretrained  model  num  layers  18  epochs  learning  rate  01  mini  batch  size  128  image  shape  200  200  top  from  sagemaker  session  import  s3  input  train  rec  s3  input  s3  data  train  input  content  type  application  recordio  valid  rec  s3  input  s3  data  valid  input  content  type  application  recordio  img  transfer  fit  train  train  rec  validation  valid  rec  img  predictor  img  transfer  deploy  init,amazon
ial  instance  count  instance  type  ml  m4  xlarge  from  sagemaker  predictor  import  json  deserializer  img  predictor  content  type  application  image  img  predictor  deserializer  json  deserializer  import  pandas  as  pd  import  numpy  as  np  test  list  pd  read  table  caltech101  test  lst  header  None  rand  index  np  random  choice  test  list  index  file  path  101  ObjectCategories  test  list  iat  int  rand  index  class  index  pd  read  csv  class  index  sep  header  None  index  col  matplotlib  inline  from  skimage  import  io  transform  from  matplotlib  import  pyplot  as  plt  io  imshow  file  path  plt  show  with  open  file  path  rb  as  response  img  predictor  predict  read  np  resp  np  array  response  rank  np  resp  argsort  Reversing  makes  the  array  descending  Show  top  for  in  range  print  Class  Confidence  3f  format  class  index  loc  rank  values  np  resp  rank  img  predictor  delete  endpoint  ,amazon
import  boto3  re  from  sagemaker  import  get  execution  role  role  get  execution  role  cat  iris  dnn  classifier  pyfrom  iris  dnn  classifier  import  estimator  fn  classifier  estimator  fn  run  config  None  params  None  import  os  from  six  moves  urllib  request  import  urlopen  Data  sets  IRIS  TRAINING  iris  training  csv  IRIS  TRAINING  URL  http  download  tensorflow  org  data  iris  training  csv  IRIS  TEST  iris  test  csv  IRIS  TEST  URL  http  download  tensorflow  org  data  iris  test  csv  if  not  os  path  exists  IRIS  TRAINING  raw  urlopen  IRIS  TRAINING  URL  read  with  open  IRIS  TRAINING  wb  as  write  raw  if  not  os  path  exists  IRIS  TEST  raw  urlopen  IRIS  TEST  URL  read  with  open  IRIS  TEST  wb  as  write  raw  from  iris  dnn  classifier  import  train  input  fn  train  func  train  input  fn  params  None  classifier  train  input  fn  train  func  steps  1000  from  iris  dnn  classifier  import  serving  input  fn  exported  model  classifier,amazon
  export  savedmodel  export  dir  base  export  Servo  serving  input  receiver  fn  serving  input  fn  print  exported  model  import  tarfile  with  tarfile  open  model  tar  gz  mode  gz  as  archive  archive  add  export  recursive  True  import  sagemaker  sagemaker  session  sagemaker  Session  inputs  sagemaker  session  upload  data  path  model  tar  gz  key  prefix  model  from  sagemaker  tensorflow  model  import  TensorFlowModel  sagemaker  model  TensorFlowModel  model  data  s3  sagemaker  session  default  bucket  model  model  tar  gz  role  role  entry  point  iris  dnn  classifier  py  time  predictor  sagemaker  model  deploy  initial  instance  count  instance  type  ml  m4  xlarge  sample  predictor  predict  sample  os  remove  model  tar  gz  import  shutil  shutil  rmtree  export  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
import  essential  libs  import  google  datalab  bigquery  as  bq  import  matplotlib  pyplot  as  plot  import  numpy  as  np  import  pandas  as  pd  html  Enter  your  own  key  script  src  https  maps  googleapis  com  maps  api  js  key  YOUR  KEY  callback  initMap  async  defer  script  bq  query  select  count  distinct  bikeid  from  bigquery  public  data  new  york  citibike  trips  bq  query  select  count  from  bigquery  public  data  new  york  citibike  stations  bq  query  name  stations  select  name  latitude  longitude  from  bigquery  public  data  new  york  citibike  stations  results  stations  execute  result  chart  map  fields  latitude  longitude  name  data  results  showTip  true  mapType  normal  bq  query  name  trips  by  year  select  cast  extract  YEAR  from  starttime  as  string  as  year  count  as  rent  count  from  bigquery  public  data  new  york  citibike  trips  group  by  year  order  by  yearresults  trips  by  year  execute  result  chart  bars  data  results,google
  height  600  bq  query  name  trips  by  hour  select  extract  HOUR  from  starttime  as  hour  count  as  rent  count  from  bigquery  public  data  new  york  citibike  trips  where  extract  DAYOFWEEK  from  starttime  between  and  group  by  hour  order  by  hourresults  trips  by  hour  execute  result  chart  columns  data  results  height  600  bq  query  name  trips  by  hour  select  extract  HOUR  from  starttime  as  hour  count  as  rent  count  from  bigquery  public  data  new  york  citibike  trips  where  extract  DAYOFWEEK  from  starttime  between  and  group  by  hour  order  by  hourresults  trips  by  hour  execute  result  chart  columns  data  results  height  600  bq  query  select  trips  bikeid  count  as  rent  count  from  bigquery  public  data  new  york  citibike  trips  trips  group  by  trips  bikeid  order  by  rent  count  desc  limit  bq  query  select  count  distinct  start  station  id  from  bigquery  public  data  new  york  citibike  trips  trips  where  trips  bi,google
keid  17526  bq  query  name  visitedStations  select  distinct  latitude  longitude  from  bigquery  public  data  new  york  citibike  stations  stations  inner  join  bigquery  public  data  new  york  citibike  trips  trips  on  stations  station  id  trips  start  station  id  where  trips  bikeid  17526results  visitedStations  execute  result  chart  map  fields  latitude  longitude  data  results  showTip  false  mapType  normal  bq  query  SELECT  usertype  COUNT  as  num  trips  ROUND  AVG  cast  tripduration  as  int64  60  as  duration  FROM  bigquery  public  data  new  york  citibike  trips  GROUP  BY  bq  query  name  topStations  SELECT  count  as  rentCount  end  station  id  latitude  longitude  from  bigquery  public  data  new  york  citibike  trips  trips  inner  join  bigquery  public  data  new  york  citibike  stations  stations  on  trips  end  station  id  stations  station  id  where  trips  usertype  Customer  group  by  end  station  id  latitude  longitude  order  by  rentCount  ,google
desc  limit  results  topStations  execute  result  chart  map  fields  latitude  longitude  end  station  id  data  results  showTip  true  mapType  normal  bq  query  name  topStations  SELECT  count  as  rentCount  end  station  id  latitude  longitude  from  bigquery  public  data  new  york  citibike  trips  trips  inner  join  bigquery  public  data  new  york  citibike  stations  stations  on  trips  end  station  id  stations  station  id  where  trips  usertype  Subscriber  group  by  end  station  id  latitude  longitude  order  by  rentCount  desc  limit  results  topStations  execute  result  chart  map  fields  latitude  longitude  end  station  id  data  results  showTip  true  mapType  normal  bq  query  name  trips  by  day  519  select  EXTRACT  DAYOFWEEK  FROM  starttime  as  day  count  as  trip  from  bigquery  public  data  new  york  citibike  trips  where  start  station  id  519  and  usertype  Subscriber  group  by  day  order  by  dayresults  trips  by  day  519  execute  result  cha,google
rt  columns  data  results  height  600  bq  query  name  trips  by  day  497  select  EXTRACT  DAYOFWEEK  FROM  starttime  as  day  count  as  trip  from  bigquery  public  data  new  york  citibike  trips  where  start  station  id  497  and  usertype  Subscriber  group  by  day  order  by  dayresults  trips  by  day  497  execute  result  chart  columns  data  results  height  600  ,google
import  os  import  unittest  import  src  tester  utils  as  tester  utilsimport  model  mnist  utils  as  utils  utils  get  train  data  dataset  train  32  utils  get  val  data  dataset  eval  32  python  unittesttester  utils  export  project  ,amazon
import  os  import  sagemaker  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  post  from  sagemaker  tensorflow  import  TensorFlow  custom  estimator  TensorFlow  entry  point  custom  estimator  py  role  role  training  steps  1000  evaluation  steps  100  hyperparameters  learning  rate  001  train  instance  count  train  instance  type  ml  c4  xlarge  custom  estimator  fit  inputs  custom  predictor  custom  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  import  tensorflow  as  tf  import  numpy  as  np  prediction  set  tf  contrib  learn  datasets  base  load  csv  without  header  filename  os  path  join  data  post  holdout  csv  target  dtype  np  int  features  dtype  np  float32  data  prediction  set  data  tensor  proto  tf  make  tensor  proto  values  np  asarray  data  shape  len  data  dtype  tf  float32  custom  predic,amazon
tor  predict  tensor  proto  sagemaker  Session  delete  endpoint  custom  predictor  endpoint  ,amazon
import  subprocess  print  subprocess  check  output  build  and  push  sh  sph  decode  utf  flush  True  import  numpy  as  np  from  sagemaker  import  get  execution  role  import  sagemaker  as  sage  import  ParticleFluidPy  as  pfpparam  DENSITY  1000  kg  m3  Cs  100  100  kg  VISC  001  Pa  param  DT  param  param  Cs  param  DT  25  param  param  param  INSTIFF  param  Cs  param  DENSITY  Tait  param  STIFF  param  DT  param  param  10  param  DH  param  area  param  param  param  pfpy  pfp  ParticleFluidPy  pfpy  setConst  str  str  param  for  in  param  pbelow  param  pow  param  param  DENSITY  poslist  pfpy  make  box  np  array  pbelow  np  array  param  param  param  pbelow  param  TIMELIMIT  import  pickle  tempfile  tmp  tmp  pckl  pickle  dump  poslist  open  tempfile  wb  import  os  import  boto3  S3  resource  boto3  resource  s3  my  bucket  resource  Bucket  sagemaker  bucket03  S3  my  bucket  upload  file  tempfile  Key  sph  initial  state  init  pckl  os  remove  tempfile  role  g,amazon
et  execution  role  sess  sage  Session  ECS  Docker  image  latest  imagename  sph  account  sess  boto  session  client  sts  get  caller  identity  Account  region  sess  boto  session  region  name  image  dkr  ecr  amazonaws  com  format  account  region  imagename  superradiance  sage  estimator  Estimator  image  role  ml  p2  xlarge  output  path  s3  sagemaker  bucket03  sph  output  sagemaker  session  sess  superradiance  hyperparam  dict  param  superradiance  fit  s3  sagemaker  bucket03  sph  initial  state  import  os  import  boto3  resource  boto3  resource  s3  my  bucket  resource  Bucket  sagemaker  bucket03  S3  import  tarfile  job  name  superradiance  latest  training  job  name  job  name  sph  2018  07  31  04  11  20  855  results  my  bucket  Object  sph  output  output  model  tar  gz  format  job  name  tempfile  home  ec2  user  SageMaker  model  tar  gz  results  download  file  tempfile  tar  tarfile  open  tempfile  gz  tar  extractall  path  home  ec2  user  SageMaker  Data,amazon
  import  numpy  as  np  import  pickle  pickle  with  open  home  ec2  user  SageMaker  Data  out  pckl  rb  as  result  pickle  load  len  result  import  matplotlib  pyplot  as  plt  from  mpl  toolkits  mplot3d  import  Axes3D  fig  plt  figure  ax  fig  add  subplot  111  projection  3d  id  10  pos  result  id  pos  transpose  prs  result  id  prs  transpose  ax  set  xlim  20  ax  set  ylim  20  ax  set  zlim  20  ax  scatter3D  pos  pos  pos  prs  cmap  jet  ,amazon
Export  as  slides  command  jupyter  nbconvert  Jupyter  Slides  ipynb  to  slides  post  serveimport  os  import  pandas  as  pd  import  numpy  as  np  import  seaborn  as  sns  import  matplotlib  pyplot  as  plt  from  sklearn  import  preprocessing  svm  from  itertools  import  combinations  from  sklearn  preprocessing  import  PolynomialFeatures  LabelEncoder  StandardScaler  import  sklearn  feature  selection  from  sklearn  model  selection  import  train  test  split  from  collections  import  defaultdict  from  sklearn  import  metrics  The  code  was  removed  by  DSX  for  sharing  Checking  that  everything  is  correct  pd  set  option  display  max  columns  30  applicants  head  10  After  running  this  cell  we  will  see  that  we  have  no  missing  values  applicants  info  Convert  columns  with  numbers  as  values  but  object  as  datatype  into  numeric  cols  13  Set  error  level  to  coerce  so  any  string  value  will  be  replaced  with  NaN  applicants  cols  applicants  ,ibm
cols  apply  pd  to  numeric  errors  coerce  applicants  head  10  Check  if  we  have  any  NaN  values  applicants  isnull  values  any  Handle  missing  values  using  scikit  learn  Imputer  from  sklearn  preprocessing  import  Imputer  Define  the  values  to  replce  and  the  strategy  of  choosing  the  replacement  value  imp  Imputer  missing  values  NaN  strategy  mean  applicants  cols  imp  fit  transform  applicants  cols  applicants  head  10  Check  if  we  have  any  NaN  values  applicants  isnull  values  any  applicants  info  Describe  columns  with  numerical  values  pd  set  option  precision  applicants  describe  Find  correlations  applicants  corr  method  pearson  Create  Grid  for  pairwise  relationships  gr  sns  PairGrid  applicants  size  hue  15  gr  gr  map  diag  plt  hist  gr  gr  map  offdiag  plt  scatter  gr  gr  add  legend  Set  up  plot  size  fig  ax  plt  subplots  figsize  20  10  Attributes  destribution  sns  boxplot  orient  palette  hls  data  applicants  ,ibm
iloc  13  fliersize  14  Tenure  data  distribution  histogram  sns  distplot  applicants  iloc  hist  True  plt  show  Use  pandas  get  dummies  applicants  encoded  pd  get  dummies  applicants  applicants  encoded  head  10  Create  training  data  for  non  preprocessed  approach  npp  applicants  encoded  iloc  pd  DataFrame  npp  head  10  Create  training  data  for  that  will  undergo  preprocessing  applicants  encoded  iloc  head  Extract  labels  from  sklearn  preprocessing  import  LabelEncoder  Split  last  column  from  original  dataset  as  the  labels  column  applicants  15  Apply  encoder  to  transform  strings  to  numeric  values  and  le  LabelEncoder  fit  enc  le  transform  pd  DataFrame  enc  head  10  Detect  outlier  using  interquartile  method  and  remove  them  def  find  outliers  df  quartile  quartile  np  percentile  df  25  75  iqr  quartile  quartile  lower  bound  quartile  iqr  upper  bound  quartile  iqr  outlier  indices  list  df  index  df  lower  bound  df  upp,ibm
er  bound  outlier  values  list  df  outlier  indices  df  outlier  indices  np  NaN  return  df  Find  outliers  in  first  column  continuous  values  print  find  outliers  Find  outliers  in  first  column  continuous  values  print  find  outliers  Find  outliers  in  first  column  continuous  values  print  find  outliers  Find  outliers  in  first  column  continuous  values  print  find  outliers  10  Find  outliers  in  first  column  continuous  values  print  find  outliers  13  Check  for  null  values  isnull  values  any  Find  outliers  in  first  column  continuous  values  print  find  outliers  14  Define  the  values  to  replce  and  the  strategy  of  choosing  the  replacement  value  suspected  cols  10  13  14  imp  Imputer  missing  values  NaN  strategy  mean  pd  DataFrame  suspected  cols  imp  fit  transform  pd  DataFrame  suspected  cols  pd  DataFrame  head  10  Check  for  null  values  pd  DataFrame  isnull  values  any  Select  best  features  select  sklearn  feature  sel,ibm
ection  SelectKBest  20  selected  features  select  fit  enc  indexes  selected  features  get  support  indices  True  col  names  selected  pd  DataFrame  columns  for  in  indexes  selected  pd  DataFrame  col  names  selected  pd  DataFrame  selected  head  10  train  npp  test  npp  train  npp  test  npp  train  test  split  npp  enc  test  size  random  state  42  print  train  npp  shape  train  npp  shape  print  test  npp  shape  test  npp  shape  train  test  train  test  train  test  split  selected  enc  test  size  random  state  42  print  train  shape  train  shape  print  test  shape  test  shape  Use  StandardScaler  scaler  preprocessing  StandardScaler  fit  train  train  train  scaled  scaler  transform  train  pd  DataFrame  train  scaled  columns  pd  DataFrame  train  columns  head  pd  DataFrame  train  head  from  sklearn  linear  model  import  LogisticRegression  clf  lr  npp  LogisticRegression  clf  lr  npp  fit  train  npp  train  npp  from  sklearn  linear  model  import  Logis,ibm
ticRegression  clf  lr  LogisticRegression  model  clf  lr  fit  train  scaled  train  model  Use  the  scaler  fit  on  trained  data  to  scale  our  test  data  test  scaled  scaler  transform  test  pd  DataFrame  test  scaled  columns  pd  DataFrame  train  columns  head  score  lr  npp  clf  lr  npp  decision  function  test  npp  score  lr  npp  Get  accuracy  score  from  sklearn  metrics  import  accuracy  score  pred  lr  npp  clf  lr  npp  predict  test  npp  acc  lr  npp  accuracy  score  test  npp  pred  lr  npp  print  acc  lr  npp  Get  Precision  vs  Recall  score  from  sklearn  metrics  import  average  precision  score  average  precision  lr  npp  average  precision  score  test  npp  score  lr  npp  print  Average  precision  recall  score  2f  format  average  precision  lr  npp  score  lr  clf  lr  decision  function  test  scaled  score  lry  pred  lr  clf  lr  predict  test  scaled  acc  lr  accuracy  score  test  pred  lr  print  acc  lr  average  precision  lr  average  precision  s,ibm
core  test  score  lr  print  Average  precision  recall  score  2f  format  average  precision  lr  Plot  SVC  ROC  Curve  plt  figure  figsize  15  10  clf  fpr  lr  npp  tpr  lr  npp  thresh  lr  npp  metrics  roc  curve  test  npp  score  lr  npp  auc  lr  npp  metrics  roc  auc  score  test  npp  score  lr  npp  plt  plot  fpr  lr  npp  tpr  lr  npp  label  Logistic  Regression  on  Non  preprocessed  Data  auc  str  auc  lr  npp  fpr  lr  tpr  lr  thresh  lr  metrics  roc  curve  test  score  lr  auc  lr  metrics  roc  auc  score  test  score  lr  plt  plot  fpr  lr  tpr  lr  label  Logistic  Regression  on  Preprocessed  Data  auc  str  auc  lr  plt  legend  loc  plt  xlabel  False  Positives  plt  ylabel  True  Positives  The  code  was  removed  by  DSX  for  sharing  To  work  with  the  Watson  Machine  Learning  REST  API  you  must  generate  Bearer  access  token  import  urllib3  requests  json  headers  urllib3  util  make  headers  basic  auth  format  credentials  username  credentials  pass,ibm
word  url  v3  identity  token  format  credentials  url  response  requests  get  url  headers  headers  ml  token  Bearer  json  loads  response  text  get  token  print  ml  token  Create  an  online  scoring  endpoint  endpoint  instance  credentials  url  v3  wml  instances  credentials  instance  id  header  Content  Type  application  json  Authorization  ml  token  response  get  instance  requests  get  endpoint  instance  headers  header  print  response  get  instance  print  response  get  instance  text  Create  API  client  from  watson  machine  learning  client  import  WatsonMachineLearningAPIClient  client  WatsonMachineLearningAPIClient  credentials  Publish  model  in  Watson  Machine  Learning  repository  on  Cloud  model  props  client  repository  ModelMetaNames  AUTHOR  NAME  Heba  El  Shimy  client  repository  ModelMetaNames  NAME  Credit  Card  Approval  Model  published  model  client  repository  store  model  model  model  meta  props  model  props  training  data  train  scaled,ibm
  training  target  train  Create  model  deployment  published  model  uid  client  repository  get  model  uid  published  model  created  deployment  client  deployments  create  published  model  uid  Deployment  of  Credit  Card  Approval  Model  Get  Scoring  URL  scoring  endpoint  client  deployments  get  scoring  url  created  deployment  print  scoring  endpoint  Get  model  details  and  expected  input  model  details  client  repository  get  details  published  model  uid  print  json  dumps  model  details  indent  ,ibm
time  import  os  import  io  import  boto3  import  pandas  as  pd  import  numpy  as  np  import  re  from  sagemaker  import  get  execution  role  region  boto3  Session  region  name  role  get  execution  role  kms  key  id  your  kms  key  id  bucket  s3  bucket  put  your  s3  bucket  name  here  and  create  s3  bucket  prefix  sagemaker  DEMO  kms  customize  to  your  bucket  where  you  have  stored  the  data  bucket  path  https  s3  amazonaws  com  format  region  bucket  from  sklearn  datasets  import  load  boston  boston  load  boston  boston  data  boston  target  feature  names  boston  feature  names  data  pd  DataFrame  columns  feature  names  target  pd  DataFrame  columns  MEDV  data  MEDV  local  file  name  boston  csv  data  to  csv  local  file  name  header  False  index  False  from  sklearn  model  selection  import  train  test  split  train  test  train  test  train  test  split  test  size  random  state  test  val  test  val  train  test  split  test  test  test  size  ra,amazon
ndom  state  def  write  file  fname  feature  names  boston  feature  names  data  pd  DataFrame  columns  feature  names  target  pd  DataFrame  columns  MEDV  data  MEDV  bring  this  column  to  the  front  before  writing  the  files  cols  data  columns  tolist  cols  cols  cols  data  data  cols  data  to  csv  fname  header  False  index  False  train  file  train  csv  validation  file  val  csv  test  file  test  csv  write  file  train  train  train  file  write  file  val  val  validation  file  write  file  test  test  test  file  s3  boto3  client  s3  data  train  open  train  file  rb  key  train  train  format  prefix  train  file  print  Put  object  s3  put  object  Bucket  bucket  Key  key  train  Body  data  train  ServerSideEncryption  aws  kms  SSEKMSKeyId  kms  key  id  print  Done  uploading  the  training  dataset  data  validation  open  validation  file  rb  key  validation  validation  format  prefix  validation  file  print  Put  object  s3  put  object  Bucket  bucket  Key  key ,amazon
 validation  Body  data  validation  ServerSideEncryption  aws  kms  SSEKMSKeyId  kms  key  id  print  Done  uploading  the  validation  dataset  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  boto3  Session  region  name  xgboost  time  from  time  import  gmtime  strftime  import  time  job  name  DEMO  xgboost  single  regression  strftime  gmtime  print  Training  job  job  name  create  training  params  AlgorithmSpecification  TrainingImage  container  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  bucket  path  prefix  output  ResourceConfig  InstanceCount  InstanceType  ml  m4  4xlarge  VolumeSizeInGB  TrainingJobName  job  name  HyperParameters  max  depth  eta  gamma  min  child  weight  subsample  silent  objective  reg  linear  num  round  StoppingCondition  MaxRuntimeInSeconds  86400  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  bucket  path  prefix  train  S3DataDistributionT,amazon
ype  FullyReplicated  ContentType  csv  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  bucket  path  prefix  validation  S3DataDistributionType  FullyReplicated  ContentType  csv  CompressionType  None  client  boto3  client  sagemaker  client  create  training  job  create  training  params  try  wait  for  the  job  to  finish  and  report  the  ending  status  client  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  training  info  client  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  except  print  Training  failed  to  start  if  exception  is  raised  that  means  it  has  failed  message  client  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  time  import  boto3  from  time  import  gmtime  strftime  model  name  job  nam,amazon
e  model  print  model  name  info  client  describe  training  job  TrainingJobName  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  data  primary  container  Image  container  ModelDataUrl  model  data  create  model  response  client  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  endpoint  config  name  DEMO  XGBoostEndpointConfig  strftime  gmtime  print  endpoint  config  name  create  endpoint  config  response  client  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialVariantWeight  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  import  time  endpoint  name  DEMO  XGBoostEndpoint  strftime  gmtime  print  endpoint  name  create  endpoint  response  cli,amazon
ent  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  print  create  endpoint  response  EndpointArn  print  EndpointArn  format  create  endpoint  response  EndpointArn  get  the  status  of  the  endpoint  response  client  describe  endpoint  EndpointName  endpoint  name  status  response  EndpointStatus  print  EndpointStatus  format  status  wait  until  the  status  has  changed  client  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  print  the  status  of  the  endpoint  endpoint  response  client  describe  endpoint  EndpointName  endpoint  name  status  endpoint  response  EndpointStatus  print  Endpoint  creation  ended  with  EndpointStatus  format  status  if  status  InService  raise  Exception  Endpoint  creation  failed  runtime  client  boto3  client  runtime  sagemaker  import  sys  import  math  def  do  predict  data  endpoint  name  content  type  payload  join  data  response  runtime  client  invoke  endpoint  EndpointNa,amazon
me  endpoint  name  ContentType  content  type  Body  payload  result  response  Body  read  result  result  decode  utf  result  result  split  return  result  def  batch  predict  data  batch  size  endpoint  name  content  type  items  len  data  arrs  for  offset  in  range  items  batch  size  if  offset  batch  size  items  results  do  predict  data  offset  offset  batch  size  endpoint  name  content  type  arrs  extend  results  else  arrs  extend  do  predict  data  offset  items  endpoint  name  content  type  sys  stdout  write  return  arrs  time  import  json  import  numpy  as  np  with  open  test  csv  as  lines  readlines  remove  the  labels  labels  line  split  for  line  in  lines  features  line  split  for  line  in  lines  features  str  join  row  for  row  in  features  preds  batch  predict  features  str  100  endpoint  name  text  csv  print  Median  Absolute  Percent  Error  MdAPE  np  median  np  abs  np  asarray  labels  dtype  float  np  asarray  preds  dtype  float  np  asa,amazon
rray  labels  dtype  float  client  delete  endpoint  EndpointName  endpoint  name  ,amazon
Lab  11  MNIST  and  Deep  learning  CNN  import  tensorflow  as  tf  import  matplotlib  pyplot  as  plt  from  tensorflow  examples  tutorials  mnist  import  input  data  tf  set  random  seed  777  reproducibility  mnist  input  data  read  data  sets  MNIST  data  one  hot  True  Check  out  https  www  tensorflow  org  get  started  mnist  beginners  for  more  information  about  the  mnist  dataset  class  Model  def  init  self  sess  name  self  sess  sess  self  name  name  self  build  net  def  build  net  self  with  tf  variable  scope  self  name  dropout  keep  prob  rate  on  training  but  should  be  for  testing  self  keep  prob  tf  placeholder  tf  float32  input  place  holders  self  tf  placeholder  tf  float32  None  784  img  28x28x1  black  white  img  tf  reshape  self  28  28  self  tf  placeholder  tf  float32  None  10  L1  ImgIn  shape  28  28  W1  tf  Variable  tf  random  normal  32  stddev  01  Conv  28  28  32  Pool  14  14  32  L1  tf  nn  conv2d  img  W1  strides  padd,ibm
ing  SAME  L1  tf  nn  relu  L1  L1  tf  nn  max  pool  L1  ksize  strides  padding  SAME  L1  tf  nn  dropout  L1  keep  prob  self  keep  prob  Tensor  Conv2D  shape  28  28  32  dtype  float32  Tensor  Relu  shape  28  28  32  dtype  float32  Tensor  MaxPool  shape  14  14  32  dtype  float32  Tensor  dropout  mul  shape  14  14  32  dtype  float32  L2  ImgIn  shape  14  14  32  W2  tf  Variable  tf  random  normal  32  64  stddev  01  Conv  14  14  64  Pool  64  L2  tf  nn  conv2d  L1  W2  strides  padding  SAME  L2  tf  nn  relu  L2  L2  tf  nn  max  pool  L2  ksize  strides  padding  SAME  L2  tf  nn  dropout  L2  keep  prob  self  keep  prob  Tensor  Conv2D  shape  14  14  64  dtype  float32  Tensor  Relu  shape  14  14  64  dtype  float32  Tensor  MaxPool  shape  64  dtype  float32  Tensor  dropout  mul  shape  64  dtype  float32  L3  ImgIn  shape  64  W3  tf  Variable  tf  random  normal  64  128  stddev  01  Conv  128  Pool  128  Reshape  128  Flatten  them  for  FC  L3  tf  nn  conv2d  L2  W3  stri,ibm
des  padding  SAME  L3  tf  nn  relu  L3  L3  tf  nn  max  pool  L3  ksize  strides  padding  SAME  L3  tf  nn  dropout  L3  keep  prob  self  keep  prob  L3  flat  tf  reshape  L3  128  Tensor  Conv2D  shape  128  dtype  float32  Tensor  Relu  shape  128  dtype  float32  Tensor  MaxPool  shape  128  dtype  float32  Tensor  dropout  mul  shape  128  dtype  float32  Tensor  Reshape  shape  2048  dtype  float32  L4  FC  4x4x128  inputs  625  outputs  W4  tf  get  variable  W4  shape  128  625  initializer  tf  contrib  layers  xavier  initializer  b4  tf  Variable  tf  random  normal  625  L4  tf  nn  relu  tf  matmul  L3  flat  W4  b4  L4  tf  nn  dropout  L4  keep  prob  self  keep  prob  Tensor  Relu  shape  625  dtype  float32  Tensor  dropout  mul  shape  625  dtype  float32  L5  Final  FC  625  inputs  10  outputs  W5  tf  get  variable  W5  shape  625  10  initializer  tf  contrib  layers  xavier  initializer  b5  tf  Variable  tf  random  normal  10  self  logits  tf  matmul  L4  W5  b5  Tensor  add  sh,ibm
ape  10  dtype  float32  define  cost  loss  optimizer  self  cost  tf  reduce  mean  tf  nn  softmax  cross  entropy  with  logits  logits  self  logits  labels  self  self  optimizer  tf  train  AdamOptimizer  learning  rate  learning  rate  minimize  self  cost  correct  prediction  tf  equal  tf  argmax  self  logits  tf  argmax  self  self  accuracy  tf  reduce  mean  tf  cast  correct  prediction  tf  float32  def  predict  self  test  keep  prop  return  self  sess  run  self  logits  feed  dict  self  test  self  keep  prob  keep  prop  def  get  accuracy  self  test  test  keep  prop  return  self  sess  run  self  accuracy  feed  dict  self  test  self  test  self  keep  prob  keep  prop  def  train  self  data  data  keep  prop  return  self  sess  run  self  cost  self  optimizer  feed  dict  self  data  self  data  self  keep  prob  keep  prop  hyper  parameters  learning  rate  001  training  epochs  15  batch  size  100  initialize  sess  tf  Session  m1  Model  sess  m1  sess  run  tf  global ,ibm
 variables  initializer  print  Learning  Started  train  my  model  for  epoch  in  range  training  epochs  avg  cost  total  batch  int  mnist  train  num  examples  batch  size  for  in  range  total  batch  batch  xs  batch  ys  mnist  train  next  batch  batch  size  m1  train  batch  xs  batch  ys  avg  cost  total  batch  print  Epoch  04d  epoch  cost  9f  format  avg  cost  print  Learning  Finished  Test  model  and  check  accuracy  print  Accuracy  m1  get  accuracy  mnist  test  images  mnist  test  labels  ,ibm
import  numpy  as  np  For  matrix  operations  and  numerical  processing  import  pandas  as  pd  For  munging  tabular  data  import  matplotlib  pyplot  as  plt  For  charts  and  visualizations  from  IPython  display  import  Image  For  displaying  images  in  the  notebook  from  IPython  display  import  display  For  displaying  outputs  in  the  notebook  from  time  import  gmtime  strftime  For  labeling  SageMaker  models  endpoints  etc  import  sys  For  writing  outputs  to  notebook  import  math  For  ceiling  function  import  json  For  parsing  hosting  outputs  import  os  For  manipulating  filepath  names  import  sagemaker  Amazon  SageMaker  Python  SDK  provides  many  helper  functions  from  sagemaker  predictor  import  csv  serializer  Converts  strings  for  HTTP  POST  requests  on  inference  import  seaborn  as  sns  bucket  videogame  sales  prefix  xgboost  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  r,amazon
ole  s3  boto3  client  s3  s3  download  file  bucket  Video  Games  csv  video  games  csv  data  pd  read  csv  video  games  csv  sep  data  corr  style  format  background  gradient  cmap  plt  get  cmap  coolwarm  How  many  missing  values  do  we  have  nulls  data  isnull  sum  len  data  100  nulls  nulls  drop  nulls  nulls  index  sort  values  ascending  False  null  data  pd  DataFrame  Null  nulls  null  data  head  16  Too  many  old  game  platforms  with  missing  data  only  keep  modern  platforms  data  data  data  Platform  PS3  data  Platform  PS4  data  Platform  X360  data  Platform  XOne  data  Platform  Wii  data  Platform  WiiU  data  Platform  PC  nulls  data  isnull  sum  len  data  100  nulls  nulls  drop  nulls  nulls  index  sort  values  ascending  False  null  data  pd  DataFrame  Null  nulls  null  data  head  16  Drop  observations  with  null  value  for  critic  score  data  data  dropna  subset  Critic  Score  nulls  data  isnull  sum  len  data  100  nulls  nulls  drop,amazon
  nulls  nulls  index  sort  values  ascending  False  null  data  pd  DataFrame  Null  nulls  null  data  head  16  Fill  in  missing  values  below  of  observations  with  mode  or  median  data  Publisher  data  Publisher  fillna  data  Publisher  mode  data  Developer  data  Developer  fillna  data  Developer  mode  data  Rating  data  Rating  fillna  data  Rating  mode  data  Year  of  Release  data  Year  of  Release  fillna  data  Year  of  Release  median  Get  rid  of  the  TBDs  in  User  score  data  User  Score  data  User  Score  replace  tbd  None  data  User  Score  data  User  Score  fillna  data  User  Score  median  data  User  Count  data  User  Count  fillna  data  User  Count  median  Lets  look  at  poor  mans  standard  deviation  of  the  categorical  fields  Publisher  and  Devloper  pubs  data  groupby  Publisher  Publisher  nunique  print  unqiue  publishers  for  games  format  len  pubs  format  len  data  devs  data  groupby  Developer  Developer  nunique  print  unqiue  Develop,amazon
ers  for  games  format  len  devs  format  len  data  can  get  dummies  to  work  workaround  to  assign  numeric  values  to  categorical  data  Convert  Publisher  and  Developer  and  re  check  correlations  def  getNameIndex  column  columnnames  list  for  index  row  in  data  iterrows  if  row  column  not  in  columnnames  columnnames  append  row  column  unique  names  len  columnnames  index  None  unique  names  for  in  range  unique  names  index  name  index  dict  zip  columnnames  index  return  name  index  Replace  the  variable  data  with  numeric  data  def  encodeString  column  kv  frame  len  len  data  for  in  range  frame  len  target  data  iloc  data  columns  get  loc  column  newValue  kv  target  data  iloc  data  columns  get  loc  column  newValue  print  Generating  Key  value  pairs  Get  unique  numeric  values  name  index  getNameIndex  Name  platform  index  getNameIndex  Platform  genre  index  getNameIndex  Genre  publisher  index  getNameIndex  Publisher  develop,amazon
er  index  getNameIndex  Developer  rating  index  getNameIndex  Rating  print  encoding  dataset  encodeString  Name  name  index  encodeString  Platform  platform  index  encodeString  Genre  genre  index  encodeString  Publisher  publisher  index  encodeString  Developer  developer  index  encodeString  Rating  rating  index  data  corr  style  format  background  gradient  cmap  plt  get  cmap  coolwarm  this  is  tough  dataset  remove  game  name  and  all  sales  except  global  data  data  drop  Name  NA  Sales  EU  Sales  JP  Sales  Other  Sales  axis  leaving  Publisher  and  developer  seems  to  have  as  much  correlation  as  critic  score  data  corr  style  format  background  gradient  cmap  plt  get  cmap  coolwarm  Create  train  val  test  sets  train  data  validation  data  test  data  np  split  data  sample  frac  random  state  1729  int  len  data  int  len  data  move  Global  Sales  to  the  first  column  for  Sagemaker  processing  pd  concat  train  data  Global  Sales  train  d,amazon
ata  drop  Global  Sales  axis  axis  to  csv  train  csv  index  False  header  False  train  check  pd  read  csv  train  csv  sep  train  check  head  pd  concat  validation  data  Global  Sales  validation  data  drop  Global  Sales  axis  axis  to  csv  validation  csv  index  False  header  False  Move  train  and  validation  sets  to  S3  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  train  csv  upload  file  train  csv  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  validation  csv  upload  file  validation  csv  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  boto3  Session  region  name  xgboost  s3  input  train  sagemaker  s3  input  s3  data  s3  train  format  bucket  prefix  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  s3  validation  format  bucket  prefix  content  type  csv  sess  sagemaker  Session  xgb  sagemaker  estimator  Estim,amazon
ator  container  role  train  instance  count  train  instance  type  ml  m4  xlarge  output  path  s3  output  format  bucket  prefix  sagemaker  session  sess  xgb  set  hyperparameters  max  depth  eta  gamma  min  child  weight  subsample  silent  objective  reg  linear  eval  metric  rmse  num  round  100  xgb  fit  train  s3  input  train  validation  s3  input  validation  xgb  predictor  xgb  deploy  initial  instance  count  instance  type  ml  m4  xlarge  xgb  predictor  content  type  text  csv  xgb  predictor  serializer  csv  serializerdef  predict  data  rows  500  split  array  np  array  split  data  int  data  shape  float  rows  predictions  for  array  in  split  array  predictions  join  predictions  xgb  predictor  predict  array  decode  utf  return  np  fromstring  predictions  sep  predictions  predict  test  data  drop  Global  Sales  axis  as  matrix  Set  negative  predictions  zero  actuals  test  data  Global  Sales  as  matrix  variations  with  open  results  csv  as  datafile  ,amazon
for  in  range  len  predictions  variations  append  abs  actuals  predictions  what  of  the  actual  value  is  the  variance  avg  datafile  write  format  actuals  predictions  variations  print  format  actuals  predictions  variations  matplotlib  inline  plt  plot  predictions  plt  plot  actuals  test  data  head  test  data  to  csv  tmp  test  data  no  predictions  csv  Add  predictions  column  to  the  test  data  set  test  data  Predictions  pd  Series  predictions  index  test  data  index  test  data  to  csv  test  data  with  predictions  csv  ,amazon
library  ggplot2  library  randomForest  data  iris  head  iris  qplot  iris  Sepal  Length  geom  histogram  rf  randomForest  Species  data  iris  importance  TRUE  proximity  TRUE  table  iris  Species  rf  predicted  ,amazon
conda  install  scipy  matplotlib  inline  import  os  re  tarfile  import  boto3  import  matplotlib  pyplot  as  plt  import  mxnet  as  mx  import  numpy  as  np  np  set  printoptions  precision  suppress  True  some  helpful  utility  functions  are  defined  in  the  Python  module  generate  example  data  located  in  the  same  directory  as  this  notebook  from  generate  example  data  import  generate  griffiths  data  match  estimated  topics  plot  lda  plot  lda  topics  accessing  the  SageMaker  Python  SDK  import  sagemaker  from  sagemaker  amazon  common  import  numpy  to  record  serializer  from  sagemaker  predictor  import  csv  serializer  json  deserializerfrom  sagemaker  import  get  execution  role  role  get  execution  role  bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  lda  science  print  Training  input  output  will  be  stored  in  format  bucket  prefix  print  nIAM  Role  format  role  print  Generating  example  data  num  documents  6000  known  alph,amazon
a  known  beta  documents  topic  mixtures  generate  griffiths  data  num  documents  num  documents  num  topics  10  num  topics  vocabulary  size  known  beta  shape  separate  the  generated  data  into  training  and  tests  subsets  num  documents  training  int  num  documents  num  documents  test  num  documents  num  documents  training  documents  training  documents  num  documents  training  documents  test  documents  num  documents  training  topic  mixtures  training  topic  mixtures  num  documents  training  topic  mixtures  test  topic  mixtures  num  documents  training  print  documents  training  shape  format  documents  training  shape  print  documents  test  shape  format  documents  test  shape  print  First  training  document  format  documents  training  print  nVocabulary  size  format  vocabulary  size  print  Length  of  first  document  format  documents  training  sum  average  document  length  documents  sum  axis  mean  print  Observed  average  document  length  format ,amazon
 average  document  length  print  First  topic  format  known  beta  print  nTopic  word  probability  matrix  beta  shape  num  topics  vocabulary  size  format  known  beta  shape  print  nSum  of  elements  of  first  topic  format  known  beta  sum  print  Topic  format  known  beta  print  Topic  format  known  beta  matplotlib  inline  fig  plot  lda  documents  training  nrows  ncols  cmap  gray  with  colorbar  True  fig  suptitle  Document  Word  Counts  fig  set  dpi  160  matplotlib  inline  fig  plot  lda  known  beta  nrows  ncols  10  fig  suptitle  Known  beta  Topic  Word  Probability  Distributions  fig  set  dpi  160  fig  set  figheight  print  First  training  document  format  documents  training  print  nVocabulary  size  format  vocabulary  size  print  Length  of  first  document  format  documents  training  sum  print  First  training  document  topic  mixture  format  topic  mixtures  training  print  nNumber  of  topics  format  num  topics  print  sum  theta  format  topic  mixtu,amazon
res  training  sum  matplotlib  inline  fig  ax1  ax2  plt  subplots  ax1  matshow  documents  reshape  cmap  gray  ax1  set  title  Document  fontsize  20  ax1  set  xticks  ax1  set  yticks  cax2  ax2  matshow  topic  mixtures  reshape  cmap  Reds  vmin  vmax  cbar  fig  colorbar  cax2  orientation  horizontal  ax2  set  title  theta  Topic  Mixture  fontsize  20  ax2  set  xticks  ax2  set  yticks  fig  set  dpi  100  matplotlib  inline  pot  fig  plot  lda  known  beta  nrows  ncols  10  fig  suptitle  Known  beta  Topic  Word  Probability  Distributions  fig  set  dpi  160  fig  set  figheight  matplotlib  inline  fig  plot  lda  topics  documents  training  topic  mixtures  topic  mixtures  fig  suptitle  theta  Documents  with  Known  Topic  Mixtures  fig  set  dpi  160  convert  documents  training  to  Protobuf  RecordIO  format  recordio  protobuf  serializer  numpy  to  record  serializer  fbuffer  recordio  protobuf  serializer  documents  training  upload  to  S3  in  bucket  prefix  train  fname,amazon
  lda  data  s3  object  os  path  join  prefix  train  fname  boto3  Session  resource  s3  Bucket  bucket  Object  s3  object  upload  fileobj  fbuffer  s3  train  data  s3  format  bucket  s3  object  print  Uploaded  data  to  S3  format  s3  train  data  containers  us  west  266724342769  dkr  ecr  us  west  amazonaws  com  lda  latest  us  east  766337827248  dkr  ecr  us  east  amazonaws  com  lda  latest  us  east  999911452149  dkr  ecr  us  east  amazonaws  com  lda  latest  eu  west  999678624901  dkr  ecr  eu  west  amazonaws  com  lda  latest  region  name  boto3  Session  region  name  container  containers  region  name  print  Using  SageMaker  LDA  container  format  container  region  name  session  sagemaker  Session  specify  general  training  job  information  lda  sagemaker  estimator  Estimator  container  role  output  path  s3  output  format  bucket  prefix  train  instance  count  train  instance  type  ml  c4  2xlarge  sagemaker  session  session  set  algorithm  specific  hyperp,amazon
arameters  lda  set  hyperparameters  num  topics  num  topics  feature  dim  vocabulary  size  mini  batch  size  num  documents  training  alpha0  run  the  training  job  on  input  data  stored  in  S3  lda  fit  train  s3  train  data  print  Training  job  name  format  lda  latest  training  job  job  name  download  and  extract  the  model  file  from  S3  job  name  lda  latest  training  job  job  name  model  fname  model  tar  gz  model  object  os  path  join  prefix  output  job  name  output  model  fname  boto3  Session  resource  s3  Bucket  bucket  Object  model  object  download  file  fname  with  tarfile  open  fname  as  tar  tar  extractall  print  Downloaded  and  extracted  model  tarball  format  model  object  obtain  the  model  file  model  list  fname  for  fname  in  os  listdir  if  fname  startswith  model  model  fname  model  list  print  Found  model  file  format  model  fname  get  the  model  from  the  model  file  and  store  in  Numpy  arrays  alpha  beta  mx  ndarra,amazon
y  load  model  fname  learned  alpha  permuted  alpha  asnumpy  learned  beta  permuted  beta  asnumpy  print  nLearned  alpha  shape  format  learned  alpha  permuted  shape  print  Learned  beta  shape  format  learned  beta  permuted  shape  permutation  learned  beta  match  estimated  topics  known  beta  learned  beta  permuted  learned  alpha  learned  alpha  permuted  permutation  fig  plot  lda  np  vstack  known  beta  learned  beta  10  fig  set  dpi  160  fig  suptitle  Known  vs  Found  Topic  Word  Probability  Distributions  fig  set  figheight  beta  error  np  linalg  norm  known  beta  learned  beta  alpha  error  np  linalg  norm  known  alpha  learned  alpha  print  L1  error  beta  format  beta  error  print  L1  error  alpha  format  alpha  error  lda  inference  lda  deploy  initial  instance  count  instance  type  ml  m4  xlarge  LDA  inference  may  work  better  at  scale  on  ml  c4  instances  print  Endpoint  name  format  lda  inference  endpoint  lda  inference  content  type ,amazon
 text  csv  lda  inference  serializer  csv  serializer  lda  inference  deserializer  json  deserializerresults  lda  inference  predict  documents  test  12  print  results  inferred  topic  mixtures  permuted  np  array  prediction  topic  mixture  for  prediction  in  results  predictions  print  Inferred  topic  mixtures  permuted  format  inferred  topic  mixtures  permuted  inferred  topic  mixtures  inferred  topic  mixtures  permuted  permutation  print  Inferred  topic  mixtures  format  inferred  topic  mixtures  matplotlib  inline  create  array  of  bar  plots  width  np  arange  10  nrows  ncols  fig  ax  plt  subplots  nrows  ncols  sharey  True  for  in  range  nrows  for  in  range  ncols  index  ncols  ax  bar  topic  mixtures  test  index  width  color  C0  ax  bar  width  inferred  topic  mixtures  index  width  color  C1  ax  set  xticks  range  num  topics  ax  set  yticks  np  linspace  ax  grid  which  major  axis  ax  set  ylim  ax  set  xticklabels  if  nrows  ax  set  xticklabels  r,amazon
ange  num  topics  fontsize  if  ax  set  yticklabels  fontsize  fig  suptitle  Known  vs  Inferred  Topic  Mixtures  ax  super  fig  add  subplot  111  frameon  False  ax  super  tick  params  labelcolor  none  top  off  bottom  off  left  off  right  off  ax  super  grid  False  ax  super  set  xlabel  Topic  Index  ax  super  set  ylabel  Topic  Probability  fig  set  dpi  160  time  create  payload  containing  all  of  the  test  documents  and  run  inference  again  TRY  THIS  try  switching  between  the  test  data  set  and  subset  of  the  training  data  set  It  is  likely  that  LDA  inference  will  perform  better  against  the  training  set  than  the  holdout  test  set  payload  documents  documents  test  Example  known  topic  mixtures  topic  mixtures  test  Example  payload  documents  documents  training  600  Example  known  topic  mixtures  topic  mixtures  training  600  Example  print  Invoking  endpoint  results  lda  inference  predict  payload  documents  inferred  topic  mixt,amazon
ures  permuted  np  array  prediction  topic  mixture  for  prediction  in  results  predictions  inferred  topic  mixtures  inferred  topic  mixtures  permuted  permutation  print  known  topics  mixtures  shape  format  known  topic  mixtures  shape  print  inferred  topics  mixtures  test  shape  format  inferred  topic  mixtures  shape  matplotlib  inline  l1  errors  np  linalg  norm  inferred  topic  mixtures  known  topic  mixtures  axis  plot  the  error  freqency  fig  ax  frequency  plt  subplots  bins  np  linspace  40  weights  np  ones  like  l1  errors  len  l1  errors  freq  bins  ax  frequency  hist  l1  errors  bins  50  weights  weights  color  C0  ax  frequency  set  xlabel  L1  Error  ax  frequency  set  ylabel  Frequency  color  C0  plot  the  cumulative  error  shift  bins  bins  bins  shift  ax  cumulative  ax  frequency  twinx  cumulative  np  cumsum  freq  sum  freq  ax  cumulative  plot  cumulative  marker  color  C1  ax  cumulative  set  ylabel  Cumulative  Frequency  color  C1  ali,amazon
gn  grids  and  show  freq  ticks  np  linspace  freq  max  freq  ticklabels  np  round  100  freq  ticks  100  ax  frequency  set  yticks  freq  ticks  ax  frequency  set  yticklabels  freq  ticklabels  ax  cumulative  set  yticks  np  linspace  ax  cumulative  grid  which  major  axis  ax  cumulative  set  ylim  fig  suptitle  Topic  Mixutre  L1  Errors  fig  set  dpi  110  good  idx  l1  errors  05  good  documents  payload  documents  good  idx  good  topic  mixtures  inferred  topic  mixtures  good  idx  poor  idx  l1  errors  poor  documents  payload  documents  poor  idx  poor  topic  mixtures  inferred  topic  mixtures  poor  idx  matplotlib  inline  fig  plot  lda  topics  good  documents  topic  mixtures  good  topic  mixtures  fig  suptitle  Documents  With  Accurate  Inferred  Topic  Mixtures  fig  set  dpi  120  matplotlib  inline  fig  plot  lda  topics  poor  documents  topic  mixtures  poor  topic  mixtures  fig  suptitle  Documents  With  Inaccurate  Inferred  Topic  Mixtures  fig  set  dpi  ,amazon
120  sagemaker  Session  delete  endpoint  lda  inference  endpoint  ,amazon
mkdir  data  wget  data  aclImdb  v1  tar  gz  http  ai  stanford  edu  amaas  data  sentiment  aclImdb  v1  tar  gz  tar  zxf  data  aclImdb  v1  tar  gz  dataimport  os  import  glob  def  read  imdb  data  data  dir  data  aclImdb  data  labels  for  data  type  in  train  test  data  data  type  labels  data  type  for  sentiment  in  pos  neg  data  data  type  sentiment  labels  data  type  sentiment  path  os  path  join  data  dir  data  type  sentiment  txt  files  glob  glob  path  for  in  files  with  open  as  review  data  data  type  sentiment  append  review  read  Here  we  represent  positive  review  by  and  negative  review  by  labels  data  type  sentiment  append  if  sentiment  pos  else  assert  len  data  data  type  sentiment  len  labels  data  type  sentiment  data  size  does  not  match  labels  size  format  data  type  sentiment  return  data  labelsdata  labels  read  imdb  data  print  IMDB  reviews  train  pos  neg  test  pos  neg  format  len  data  train  pos  len  data ,amazon
 train  neg  len  data  test  pos  len  data  test  neg  from  sklearn  utils  import  shuffle  def  prepare  imdb  data  data  labels  Prepare  training  and  test  sets  from  IMDb  movie  reviews  Combine  positive  and  negative  reviews  and  labels  data  train  data  train  pos  data  train  neg  data  test  data  test  pos  data  test  neg  labels  train  labels  train  pos  labels  train  neg  labels  test  labels  test  pos  labels  test  neg  Shuffle  reviews  and  corresponding  labels  within  training  and  test  sets  data  train  labels  train  shuffle  data  train  labels  train  data  test  labels  test  shuffle  data  test  labels  test  Return  unified  training  data  test  data  training  labels  test  labets  return  data  train  data  test  labels  train  labels  testtrain  test  train  test  prepare  imdb  data  data  labels  print  IMDb  reviews  combined  train  test  format  len  train  len  test  train  100  import  nltk  nltk  download  stopwords  from  nltk  corpus  import  stop,amazon
words  from  nltk  stem  porter  import  stemmer  PorterStemmer  import  re  from  bs4  import  BeautifulSoup  def  review  to  words  review  text  BeautifulSoup  review  html  parser  get  text  Remove  HTML  tags  text  re  sub  zA  Z0  text  lower  Convert  to  lower  case  words  text  split  Split  string  into  words  words  for  in  words  if  not  in  stopwords  words  english  Remove  stopwords  words  PorterStemmer  stem  for  in  words  stem  return  wordsimport  pickle  cache  dir  os  path  join  cache  sentiment  analysis  where  to  store  cache  files  os  makedirs  cache  dir  exist  ok  True  ensure  cache  directory  exists  def  preprocess  data  data  train  data  test  labels  train  labels  test  cache  dir  cache  dir  cache  file  preprocessed  data  pkl  Convert  each  review  to  words  read  from  cache  if  available  If  cache  file  is  not  None  try  to  read  from  it  first  cache  data  None  if  cache  file  is  not  None  try  with  open  os  path  join  cache  dir  cach,amazon
e  file  rb  as  cache  data  pickle  load  print  Read  preprocessed  data  from  cache  file  cache  file  except  pass  unable  to  read  from  cache  but  that  okay  If  cache  is  missing  then  do  the  heavy  lifting  if  cache  data  is  None  Preprocess  training  and  test  data  to  obtain  words  for  each  review  words  train  list  map  review  to  words  data  train  words  test  list  map  review  to  words  data  test  words  train  review  to  words  review  for  review  in  data  train  words  test  review  to  words  review  for  review  in  data  test  Write  to  cache  file  for  future  runs  if  cache  file  is  not  None  cache  data  dict  words  train  words  train  words  test  words  test  labels  train  labels  train  labels  test  labels  test  with  open  os  path  join  cache  dir  cache  file  wb  as  pickle  dump  cache  data  print  Wrote  preprocessed  data  to  cache  file  cache  file  else  Unpack  data  loaded  from  cache  file  words  train  words  test  labels  tr,amazon
ain  labels  test  cache  data  words  train  cache  data  words  test  cache  data  labels  train  cache  data  labels  test  return  words  train  words  test  labels  train  labels  test  Preprocess  data  train  test  train  test  preprocess  data  train  test  train  test  import  numpy  as  np  from  sklearn  feature  extraction  text  import  CountVectorizer  from  sklearn  externals  import  joblib  joblib  is  an  enhanced  version  of  pickle  that  is  more  efficient  for  storing  NumPy  arrays  def  extract  BoW  features  words  train  words  test  vocabulary  size  5000  cache  dir  cache  dir  cache  file  bow  features  pkl  Extract  Bag  of  Words  for  given  set  of  documents  already  preprocessed  into  words  If  cache  file  is  not  None  try  to  read  from  it  first  cache  data  None  if  cache  file  is  not  None  try  with  open  os  path  join  cache  dir  cache  file  rb  as  cache  data  joblib  load  print  Read  features  from  cache  file  cache  file  except  pass  una,amazon
ble  to  read  from  cache  but  that  okay  If  cache  is  missing  then  do  the  heavy  lifting  if  cache  data  is  None  Fit  vectorizer  to  training  documents  and  use  it  to  transform  them  NOTE  Training  documents  have  already  been  preprocessed  and  tokenized  into  words  pass  in  dummy  functions  to  skip  those  steps  preprocessor  lambda  vectorizer  CountVectorizer  max  features  vocabulary  size  preprocessor  lambda  tokenizer  lambda  already  preprocessed  features  train  vectorizer  fit  transform  words  train  toarray  Apply  the  same  vectorizer  to  transform  the  test  documents  ignore  unknown  words  features  test  vectorizer  transform  words  test  toarray  NOTE  Remember  to  convert  the  features  using  toarray  for  compact  representation  Write  to  cache  file  for  future  runs  store  vocabulary  as  well  if  cache  file  is  not  None  vocabulary  vectorizer  vocabulary  cache  data  dict  features  train  features  train  features  test  features  ,amazon
test  vocabulary  vocabulary  with  open  os  path  join  cache  dir  cache  file  wb  as  joblib  dump  cache  data  print  Wrote  features  to  cache  file  cache  file  else  Unpack  data  loaded  from  cache  file  features  train  features  test  vocabulary  cache  data  features  train  cache  data  features  test  cache  data  vocabulary  Return  both  the  extracted  features  as  well  as  the  vocabulary  return  features  train  features  test  vocabulary  Extract  Bag  of  Words  features  for  both  training  and  test  datasets  train  test  vocabulary  extract  BoW  features  train  test  import  pandas  as  pd  TODO  Split  the  train  and  train  arrays  into  the  DataFrames  val  train  and  val  train  Make  sure  that  val  and  val  contain  10  000  entires  while  train  and  train  contain  the  remaining  15  000  entries  val  pd  DataFrame  None  train  pd  DataFrame  None  val  pd  DataFrame  None  train  pd  DataFrame  None  Solution  Earlier  we  shuffled  the  training  dataset,amazon
  so  to  make  things  simple  we  can  just  assign  the  first  10  000  reviews  to  the  validation  set  and  use  the  remaining  reviews  for  training  val  pd  DataFrame  train  10000  train  pd  DataFrame  train  10000  val  pd  DataFrame  train  10000  train  pd  DataFrame  train  10000  First  we  make  sure  that  the  local  directory  in  which  we  like  to  store  the  training  and  validation  csv  files  exists  data  dir  data  xgboost  if  not  os  path  exists  data  dir  os  makedirs  data  dir  First  save  the  test  data  to  test  csv  in  the  data  dir  directory  Note  that  we  do  not  save  the  associated  ground  truth  labels  instead  we  will  use  them  later  to  compare  with  our  model  output  pd  DataFrame  test  to  csv  os  path  join  data  dir  test  csv  header  False  index  False  TODO  Save  the  training  and  validation  data  to  train  csv  and  validation  csv  in  the  data  dir  directory  Make  sure  that  the  files  you  create  are  in  the  co,amazon
rrect  format  Solution  pd  concat  val  val  axis  to  csv  os  path  join  data  dir  validation  csv  header  False  index  False  pd  concat  train  train  axis  to  csv  os  path  join  data  dir  train  csv  header  False  index  False  To  save  bit  of  memory  we  can  set  text  train  val  train  and  val  to  None  test  train  val  train  val  Noneimport  sagemaker  session  sagemaker  Session  Store  the  current  SageMaker  session  S3  prefix  which  folder  will  we  use  prefix  sentiment  xgboost  TODO  Upload  the  test  csv  train  csv  and  validation  csv  files  which  are  contained  in  data  dir  to  S3  using  sess  upload  data  test  location  None  val  location  None  train  location  None  Solution  test  location  session  upload  data  os  path  join  data  dir  test  csv  key  prefix  prefix  val  location  session  upload  data  os  path  join  data  dir  validation  csv  key  prefix  prefix  train  location  session  upload  data  os  path  join  data  dir  train  csv  k,amazon
ey  prefix  prefix  from  sagemaker  import  get  execution  role  Our  current  execution  role  is  require  when  creating  the  model  as  the  training  and  inference  code  will  need  to  access  the  model  artifacts  role  get  execution  role  We  need  to  retrieve  the  location  of  the  container  which  is  provided  by  Amazon  for  using  XGBoost  As  matter  of  convenience  the  training  and  inference  code  both  use  the  same  container  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  session  boto  region  name  xgboost  TODO  Create  SageMaker  estimator  using  the  container  location  determined  in  the  previous  cell  It  is  recommended  that  you  use  single  training  instance  of  type  ml  m4  xlarge  It  is  also  recommended  that  you  use  s3  output  format  session  default  bucket  prefix  as  the  output  path  xgb  None  Solution  xgb  sagemaker  estimator  Estimator  container  The  location  of  the  container  ,amazon
we  wish  to  use  role  What  is  our  current  IAM  Role  train  instance  count  How  many  compute  instances  train  instance  type  ml  m4  xlarge  What  kind  of  compute  instances  output  path  s3  output  format  session  default  bucket  prefix  sagemaker  session  session  TODO  Set  the  XGBoost  hyperparameters  in  the  xgb  object  Don  forget  that  in  this  case  we  have  binary  label  so  we  should  be  using  the  binary  logistic  objective  Solution  xgb  set  hyperparameters  max  depth  eta  gamma  min  child  weight  subsample  silent  objective  binary  logistic  early  stopping  rounds  10  num  round  500  s3  input  train  sagemaker  s3  input  s3  data  train  location  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  val  location  content  type  csv  xgb  fit  train  s3  input  train  validation  s3  input  validation  TODO  Create  transformer  object  from  the  trained  model  Using  an  instance  count  of  and  an  instance  type  of  ml  m4 ,amazon
 xlarge  should  be  more  than  enough  xgb  transformer  xgb  transformer  instance  count  instance  type  ml  m4  xlarge  TODO  Start  the  transform  job  Make  sure  to  specify  the  content  type  and  the  split  type  of  the  test  data  xgb  transformer  transform  test  location  content  type  text  csv  split  type  Line  xgb  transformer  wait  aws  s3  cp  recursive  xgb  transformer  output  path  data  dirpredictions  pd  read  csv  os  path  join  data  dir  test  csv  out  header  None  predictions  round  num  for  num  in  predictions  squeeze  values  from  sklearn  metrics  import  accuracy  score  accuracy  score  test  predictions  First  we  will  remove  all  of  the  files  contained  in  the  data  dir  directory  rm  data  dir  And  then  we  delete  the  directory  itself  rmdir  data  dir  Similarly  we  will  remove  the  files  in  the  cache  dir  directory  and  the  directory  itself  rm  cache  dir  rmdir  cache  dir  ,amazon
az  login  az  account  list  output  table  az  account  set  subscription  subscription  id  subscription  name  location  West  Europe  resourceGroup  group1  appserviceplanName  appserviceplan1  appserviceplanSKU  S2  webappName  testwebappm2  whos  az  group  create  name  resourceGroup  location  location  az  appservice  plan  create  name  appserviceplanName  resource  group  resourceGroup  sku  appserviceplanSKU  az  appservice  web  create  name  webappName  resource  group  resourceGroup  plan  appserviceplanName  az  appservice  web  source  control  config  repo  url  https  github  com  prashanthmadi  sample  express  app  name  webappName  resource  group  resourceGroup  ,microsoft
import  pandas  as  pd  import  numpy  as  np  from  collections  import  Counter  from  keras  preprocessing  text  import  Tokenizer  from  sklearn  preprocessing  import  LabelEncoder  from  sklearn  model  selection  import  train  test  split  from  sklearn  externals  import  joblib  data  pd  read  csv  data  stack  overflow  data  csv  data  head  def  tokenizeText  data  num  words  Bag  of  words  tokenizer  Args  training  features  and  number  of  words  Returns  bag  of  words  matrix  tokenize  Tokenizer  num  words  num  words  tokenize  fit  on  texts  data  joblib  dump  tokenize  encoders  tokenize  pkl  tokenize  texts  to  matrix  data  return  def  encodeLabels  labels  encoder  LabelEncoder  encoder  fit  labels  encoder  transform  labels  num  classes  np  max  print  num  classes  format  num  classes  joblib  dump  encoder  encoders  encoder  pkl  return  yX  pd  DataFrame  tokenizeText  data  post  500  sample  pd  concat  pd  DataFrame  encodeLabels  data  tags  axis  train  test ,amazon
 train  test  split  sample  test  size  15  print  test  shape  test  holdout  train  test  split  test  test  size  test  head  train  to  csv  data  post  train  csv  header  None  index  False  test  to  csv  data  post  test  csv  header  None  index  False  holdout  to  csv  data  post  holdout  csv  header  None  index  False  load  ext  autoreload  autoreload  import  os  import  sagemaker  import  boto3  re  from  sagemaker  import  get  execution  role  from  processing  import  myEncoder  myTokenizer  from  sageBot  import  train  saveAndDeploy  serve  sagemaker  session  sagemaker  Session  role  get  execution  role  classifier  train  data  predictor  saveAndDeploy  classifier  encoder  myEncoder  hosted  serve  predictor  encoder  df  pd  read  csv  data  post  holdout  csv  sample  df  ix  100  500  valueshosted  predict  sample  df  ix  100  500  encoder  classes  11  hosted  delete  ,amazon
pip  install  datasets  https  research  googleblog  com  2016  08  improving  inception  and  image  html  import  numpy  as  np  import  os  import  tensorflow  as  tf  import  urllib2  from  datasets  import  imagenet  from  nets  import  inception  from  preprocessing  import  inception  preprocessing  slim  tf  contrib  slim  batch  size  image  size  inception  inception  v3  default  image  size  checkpoints  dir  root  code  model  checkpoints  filename  inception  resnet  v2  2016  08  30  ckpt  model  name  InceptionResnetV2  sess  tf  InteractiveSession  graph  tf  Graph  graph  as  default  def  classify  from  url  url  image  string  urllib2  urlopen  url  read  image  tf  image  decode  jpeg  image  string  channels  processed  image  inception  preprocessing  preprocess  image  image  image  size  image  size  is  training  False  processed  images  tf  expand  dims  processed  image  Create  the  model  use  the  default  arg  scope  to  configure  the  batch  norm  parameters  with  slim  ar,microsoft
g  scope  inception  inception  resnet  v2  arg  scope  logits  inception  inception  resnet  v2  processed  images  num  classes  1001  is  training  False  probabilities  tf  nn  softmax  logits  init  fn  slim  assign  from  checkpoint  fn  os  path  join  checkpoints  dir  checkpoints  filename  slim  get  model  variables  model  name  init  fn  sess  np  image  probabilities  sess  run  image  probabilities  probabilities  probabilities  sorted  inds  for  in  sorted  enumerate  probabilities  key  lambda  plt  figure  plt  imshow  np  image  astype  np  uint8  plt  axis  off  plt  show  names  imagenet  create  readable  names  for  imagenet  labels  for  in  range  index  sorted  inds  print  Probability  2f  probabilities  index  names  index  ,microsoft
S3  bucket  and  prefix  bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  seq2seq  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  from  time  import  gmtime  strftime  import  time  import  numpy  as  np  import  os  import  json  For  plotting  attention  matrix  later  on  import  matplotlib  matplotlib  inline  import  matplotlib  pyplot  as  plt  bash  wget  http  data  statmt  org  wmt17  translation  task  preprocessed  de  en  corpus  tc  de  gz  wget  http  data  statmt  org  wmt17  translation  task  preprocessed  de  en  corpus  tc  en  gz  wait  gunzip  corpus  tc  de  gz  gunzip  corpus  tc  en  gz  wait  mkdir  validation  curl  http  data  statmt  org  wmt17  translation  task  preprocessed  de  en  dev  tgz  tar  xvzf  validation  head  10000  corpus  tc  en  corpus  tc  en  small  head  10000  corpus  tc  de  corpus  tc  de  small  bash  python3  create  vocab  proto  py  time  bash  python3  create  vocab  proto  py  train  ,amazon
source  corpus  tc  en  small  train  target  corpus  tc  de  small  val  source  validation  newstest2014  tc  en  val  target  validation  newstest2014  tc  dedef  upload  to  s3  bucket  prefix  channel  file  s3  boto3  resource  s3  data  open  file  rb  key  prefix  channel  file  s3  Bucket  bucket  put  object  Key  key  Body  data  upload  to  s3  bucket  prefix  train  train  rec  upload  to  s3  bucket  prefix  validation  val  rec  upload  to  s3  bucket  prefix  vocab  vocab  src  json  upload  to  s3  bucket  prefix  vocab  vocab  trg  json  region  name  boto3  Session  region  namecontainers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  seq2seq  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  seq2seq  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  seq2seq  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  seq2seq  latest  container  containers  region  name  print  Using  SageMaker  Seq2Seq  container  format  container ,amazon
 region  name  job  name  DEMO  seq2seq  en  de  strftime  gmtime  print  Training  job  job  name  create  training  params  AlgorithmSpecification  TrainingImage  container  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  s3  format  bucket  prefix  ResourceConfig  Seq2Seq  does  not  support  multiple  machines  Currently  it  only  supports  single  machine  multiple  GPUs  InstanceCount  InstanceType  ml  p2  xlarge  We  suggest  one  of  ml  p2  16xlarge  ml  p2  8xlarge  ml  p2  xlarge  VolumeSizeInGB  50  TrainingJobName  job  name  HyperParameters  Please  refer  to  the  documentation  for  complete  list  of  parameters  max  seq  len  source  60  max  seq  len  target  60  optimized  metric  bleu  batch  size  64  Please  use  larger  batch  size  256  or  512  if  using  ml  p2  8xlarge  or  ml  p2  16xlarge  checkpoint  frequency  num  batches  1000  rnn  num  hidden  512  num  layers  encoder  num  layers  decoder  num  embed  source  512  num  embed  target  512  checkp,amazon
oint  threshold  max  num  batches  2100  Training  will  stop  after  2100  iterations  batches  This  is  just  for  demo  purposes  Remove  the  above  parameter  if  you  want  better  model  StoppingCondition  MaxRuntimeInSeconds  48  3600  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  S3DataDistributionType  FullyReplicated  ChannelName  vocab  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  vocab  format  bucket  prefix  S3DataDistributionType  FullyReplicated  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  prefix  S3DataDistributionType  FullyReplicated  sagemaker  client  boto3  Session  client  service  name  sagemaker  sagemaker  client  create  training  job  create  training  params  status  sagemaker  client  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  status  status  sagemaker  client  describe  training  ,amazon
job  TrainingJobName  job  name  TrainingJobStatus  print  status  if  the  job  failed  determine  why  if  status  Failed  message  sage  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  use  pretrained  model  False  use  pretrained  model  True  model  name  DEMO  pretrained  en  de  model  curl  https  s3  us  west  amazonaws  com  gsaur  seq2seq  data  seq2seq  eng  german  full  nb  translation  eng  german  p2  16x  2017  11  24  22  25  53  output  model  tar  gz  model  tar  gz  curl  https  s3  us  west  amazonaws  com  gsaur  seq2seq  data  seq2seq  eng  german  full  nb  translation  eng  german  p2  16x  2017  11  24  22  25  53  output  vocab  src  json  vocab  src  json  curl  https  s3  us  west  amazonaws  com  gsaur  seq2seq  data  seq2seq  eng  german  full  nb  translation  eng  german  p2  16x  2017  11  24  22  25  53  output  vocab  trg  json  vocab  trg  j,amazon
son  upload  to  s3  bucket  prefix  pretrained  model  model  tar  gz  model  data  s3  pretrained  model  model  tar  gz  format  bucket  prefix  time  sage  boto3  client  sagemaker  if  not  use  pretrained  model  info  sage  describe  training  job  TrainingJobName  job  name  model  name  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  name  print  model  data  primary  container  Image  container  ModelDataUrl  model  data  create  model  response  sage  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  endpoint  config  name  DEMO  Seq2SeqEndpointConfig  strftime  gmtime  print  endpoint  config  name  create  endpoint  config  response  sage  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  print  Endp,amazon
oint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  import  time  endpoint  name  DEMO  Seq2SeqEndpoint  strftime  gmtime  print  endpoint  name  create  endpoint  response  sage  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  print  create  endpoint  response  EndpointArn  resp  sage  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  wait  until  the  status  has  changed  sage  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  print  the  status  of  the  endpoint  endpoint  response  sage  describe  endpoint  EndpointName  endpoint  name  status  endpoint  response  EndpointStatus  print  Endpoint  creation  ended  with  EndpointStatus  format  status  if  status  InService  raise  Exception  Endpoint  creation  failed  runtime  boto3  client  service  name  runtime  sagemaker  sentences  you  are  so  good  can  you  drive  car  want  to  watch  movie  payload,amazon
  instances  for  sent  in  sentences  payload  instances  append  data  sent  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  json  Body  json  dumps  payload  response  response  Body  read  decode  utf  response  json  loads  response  print  response  sentence  can  you  drive  car  payload  instances  data  sentence  configuration  attention  matrix  true  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  json  Body  json  dumps  payload  response  response  Body  read  decode  utf  response  json  loads  response  predictions  source  sentence  target  response  target  attention  matrix  np  array  response  matrix  print  Source  nTarget  source  target  Define  function  for  plotting  the  attentioan  matrix  def  plot  matrix  attention  matrix  target  source  source  tokens  source  split  target  tokens  target  split  assert  attention  matrix  shape  len  target  tokens  plt  imshow  attention  matrix  tran,amazon
spose  interpolation  nearest  cmap  Greys  plt  xlabel  target  plt  ylabel  source  plt  gca  set  xticks  for  in  range  len  target  tokens  plt  gca  set  yticks  for  in  range  len  source  tokens  plt  gca  set  xticklabels  target  tokens  plt  gca  set  yticklabels  source  tokens  plt  tight  layout  plot  matrix  attention  matrix  target  source  import  io  import  tempfile  from  record  pb2  import  Record  from  create  vocab  proto  import  vocab  from  json  reverse  vocab  write  recordio  list  to  record  bytes  read  next  source  vocab  from  json  vocab  src  json  target  vocab  from  json  vocab  trg  json  source  rev  reverse  vocab  source  target  rev  reverse  vocab  target  sentences  this  is  so  cool  am  having  dinner  am  sitting  in  an  aeroplane  come  let  us  go  for  long  drive  Convert  strings  to  integers  using  source  vocab  mapping  Out  of  vocabulary  strings  are  mapped  to  the  mapping  for  unk  sentences  source  get  token  for  token  in  senten,amazon
ce  split  for  sentence  in  sentences  io  BytesIO  for  sentence  in  sentences  record  list  to  record  bytes  sentence  write  recordio  record  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  recordio  protobuf  Body  getvalue  response  response  Body  read  def  parse  proto  response  received  bytes  output  file  tempfile  NamedTemporaryFile  output  file  write  received  bytes  output  file  flush  target  sentences  with  open  output  file  name  rb  as  datum  next  record  True  while  next  record  next  record  read  next  datum  if  next  record  rec  Record  rec  ParseFromString  next  record  target  list  rec  features  target  int32  tensor  values  target  sentences  append  target  else  break  return  target  sentencestargets  parse  proto  response  response  resp  join  target  rev  get  token  unk  for  token  in  sentence  for  sentence  in  targets  print  resp  sage  delete  endpoint  EndpointName  endpoint  name  ,amazon
import  sagemaker  sess  sagemaker  Session  bucket  sess  default  bucket  prefix  sagemaker  DEMO  batch  transform  role  sagemaker  get  execution  role  import  boto3  import  sagemaker  import  sagemaker  amazon  common  as  smac  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  from  sagemaker  transformer  import  Transformer  from  sagemaker  predictor  import  csv  serializer  json  deserializer  import  matplotlib  pyplot  as  plt  import  pandas  as  pd  import  numpy  as  np  import  scipy  sparse  import  os  import  json  mkdir  tmp  reviews  aws  s3  cp  s3  amazon  reviews  pds  tsv  amazon  reviews  us  Digital  Video  Download  v1  00  tsv  gz  tmp  reviews  df  pd  read  csv  tmp  reviews  amazon  reviews  us  Digital  Video  Download  v1  00  tsv  gz  delimiter  error  bad  lines  False  df  head  df  df  customer  id  product  id  star  rating  product  title  customers  df  customer  id  value  counts  products  df  product  id  value  counts  quantiles  25  75  85  ,amazon
95  96  97  98  99  995  999  9999  print  customers  customers  quantile  quantiles  print  products  products  quantile  quantiles  customers  customers  customers  35  products  products  products  20  reduced  df  df  merge  pd  DataFrame  customer  id  customers  index  merge  pd  DataFrame  product  id  products  index  customers  reduced  df  customer  id  value  counts  products  reduced  df  product  id  value  counts  test  products  products  sample  frac  train  products  products  products  index  isin  test  products  index  customer  index  pd  DataFrame  customer  id  customers  index  user  np  arange  customers  shape  train  product  index  pd  DataFrame  product  id  train  products  index  item  np  arange  train  products  shape  test  product  index  pd  DataFrame  product  id  test  products  index  item  np  arange  test  products  shape  train  df  reduced  df  merge  customer  index  merge  train  product  index  test  df  reduced  df  merge  customer  index  merge  test  product  i,amazon
ndex  train  sparse  scipy  sparse  csr  matrix  np  where  train  df  star  rating  values  train  df  item  values  train  df  user  values  shape  train  df  item  nunique  customers  count  test  sparse  scipy  sparse  csr  matrix  np  where  test  df  star  rating  values  test  df  item  values  test  df  user  values  shape  test  df  item  nunique  customers  count  np  savetxt  tmp  reviews  train  csv  train  sparse  todense  delimiter  fmt  np  savetxt  tmp  reviews  test  csv  test  sparse  todense  delimiter  fmt  train  s3  sess  upload  data  tmp  reviews  train  csv  bucket  bucket  key  prefix  pca  train  format  prefix  test  s3  sess  upload  data  tmp  reviews  test  csv  bucket  bucket  key  prefix  pca  test  format  prefix  train  inputs  sagemaker  s3  input  train  s3  content  type  text  csv  label  size  container  get  image  uri  boto3  Session  region  name  pca  latest  pca  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  m4,amazon
  xlarge  output  path  s3  pca  output  format  bucket  prefix  sagemaker  session  sess  pca  set  hyperparameters  feature  dim  customers  count  num  components  100  subtract  mean  True  algorithm  mode  randomized  mini  batch  size  500  pca  fit  train  train  inputs  pca  transformer  pca  transformer  instance  count  instance  type  ml  m4  xlarge  strategy  MultiRecord  assemble  with  Line  output  path  s3  pca  transform  train  format  bucket  prefix  pca  transformer  transform  train  s3  content  type  text  csv  split  type  Line  pca  transformer  wait  aws  s3  cp  recursive  pca  transformer  output  path  head  train  csv  outpca  predictor  pca  deploy  initial  instance  count  instance  type  ml  m4  xlarge  pca  predictor  content  type  text  csv  pca  predictor  serializer  csv  serializer  pca  predictor  deserializer  json  deserializercomponents  for  array  in  np  array  split  np  array  train  sparse  todense  500  result  pca  predictor  predict  array  components  proj,amazon
ection  for  in  result  projections  components  np  array  components  components  sess  delete  endpoint  pca  predictor  endpoint  pca  model  sess  create  model  from  job  pca  current  job  name  name  test  format  pca  current  job  name  pca  test  transformer  Transformer  pca  model  ml  m4  xlarge  output  path  s3  pca  transform  test  format  bucket  prefix  sagemaker  session  sess  strategy  MultiRecord  assemble  with  Line  pca  test  transformer  transform  test  s3  content  type  text  csv  split  type  Line  pca  test  transformer  wait  aws  s3  cp  recursive  pca  test  transformer  output  path  head  10000  test  csv  out  cat  Dockerfile  cat  dbscan  cat  plumber  sh  The  name  of  our  algorithm  algorithm  name  dbscan  set  stop  if  anything  fails  account  aws  sts  get  caller  identity  query  Account  output  text  Get  the  region  defined  in  the  current  configuration  default  to  us  west  if  none  defined  region  aws  configure  get  region  region  region  u,amazon
s  west  fullname  account  dkr  ecr  region  amazonaws  com  algorithm  name  latest  If  the  repository  doesn  exist  in  ECR  create  it  aws  ecr  describe  repositories  repository  names  algorithm  name  dev  null  if  ne  then  aws  ecr  create  repository  repository  name  algorithm  name  dev  null  fi  Get  the  login  command  from  ECR  and  execute  it  directly  aws  ecr  get  login  region  region  no  include  email  Build  the  docker  image  locally  with  the  image  name  and  then  push  it  to  ECR  with  the  full  name  docker  build  algorithm  name  docker  tag  algorithm  name  fullname  docker  push  fullname  region  boto3  Session  region  name  account  boto3  client  sts  get  caller  identity  get  Account  dbscan  sagemaker  estimator  Estimator  dkr  ecr  amazonaws  com  dbscan  latest  format  account  region  role  train  instance  count  train  instance  type  ml  m4  xlarge  output  path  s3  dbscan  output  format  bucket  prefix  sagemaker  session  sess  dbscan  s,amazon
et  hyperparameters  minPts  dbscan  fit  train  pca  transformer  output  path  dbscan  transformer  dbscan  transformer  instance  count  instance  type  ml  m4  xlarge  output  path  s3  dbscan  transform  format  bucket  prefix  strategy  SingleRecord  assemble  with  Line  dbscan  transformer  transform  pca  test  transformer  output  path  content  type  text  csv  split  type  Line  dbscan  transformer  wait  aws  s3  cp  recursive  dbscan  transformer  output  path  dbscan  output  with  open  test  csv  out  out  as  for  line  in  result  json  loads  line  split  dbscan  output  for  in  result  dbscan  clusters  pd  DataFrame  item  np  arange  test  products  shape  cluster  dbscan  output  dbscan  clusters  items  test  df  groupby  item  product  title  first  reset  index  merge  dbscan  clusters  dbscan  clusters  items  sort  values  cluster  item  groupby  cluster  head  ,amazon
bucket  eduthie  sagemaker  prefix  gluon  recommender  import  sagemaker  role  sagemaker  get  execution  role  import  os  import  mxnet  as  mx  from  mxnet  import  gluon  nd  ndarray  from  mxnet  metric  import  MSE  import  pandas  as  pd  import  numpy  as  np  import  sagemaker  from  sagemaker  mxnet  import  MXNet  import  boto3  import  json  import  matplotlib  pyplot  as  pltopt  sgd  lr  02  momentum  wd  MXNet  recommender  py  py  version  py3  role  role  train  instance  count  train  instance  type  ml  p3  2xlarge  output  path  s3  output  format  bucket  prefix  hyperparameters  num  embeddings  512  opt  opt  lr  lr  momentum  momentum  wd  wd  epochs  10  fit  train  s3  train  format  bucket  prefix  minutes  60  60  25  price  361  minutes  60  price  ,amazon
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  byo  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  time  import  json  import  os  import  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  sh  The  name  of  our  algorithm  algorithm  name  rmars  set  stop  if  anything  fails  account  aws  sts  get  caller  identity  query  Account  output  text  Get  the  region  defined  in  the  current  configuration  default  to  us  west  if  none  defined  region  aws  configure  get  region  region  region  us  west  fullname  account  dkr  ecr  region  amazonaws  com  algorithm  name  latest  If  the  repository  doesn  exist  in  ECR  create  it  aws  ecr  describe  repositories  repository  names  algorithm  name  dev  null  if  ne  then  aws  ecr  create  repository  repository  name  algorithm  name  dev  null  fi  Get  the  login  command  from  ECR  and  execute  it  directly  aws  ecr ,amazon
 get  login  region  region  no  include  email  Build  the  docker  image  locally  with  the  image  name  and  then  push  it  to  ECR  with  the  full  name  docker  build  algorithm  name  docker  tag  algorithm  name  fullname  docker  push  fullname  train  file  iris  csv  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  train  file  upload  file  train  file  region  boto3  Session  region  name  account  boto3  client  sts  get  caller  identity  get  Account  job  DEMO  byo  time  strftime  time  gmtime  print  Training  job  job  training  params  RoleArn  role  TrainingJobName  job  AlgorithmSpecification  TrainingImage  dkr  ecr  amazonaws  com  rmars  latest  format  account  region  TrainingInputMode  File  ResourceConfig  InstanceCount  InstanceType  ml  m4  xlarge  VolumeSizeInGB  10  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  S3DataDistributionType  FullyReplicated  Compre,amazon
ssionType  None  RecordWrapperType  None  OutputDataConfig  S3OutputPath  s3  output  format  bucket  prefix  HyperParameters  target  Sepal  Length  degree  StoppingCondition  MaxRuntimeInSeconds  60  60  time  sm  boto3  client  sagemaker  sm  create  training  job  training  params  status  sm  describe  training  job  TrainingJobName  job  TrainingJobStatus  print  status  sm  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  status  sm  describe  training  job  TrainingJobName  job  TrainingJobStatus  print  Training  job  ended  with  status  status  if  status  Failed  message  sm  describe  training  job  TrainingJobName  job  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  hosting  container  Image  dkr  ecr  amazonaws  com  rmars  latest  format  account  region  ModelDataUrl  sm  describe  training  job  TrainingJobName  job  ModelArtifacts  S3ModelArtifacts  create  model  response  sm  cre,amazon
ate  model  ModelName  job  ExecutionRoleArn  role  PrimaryContainer  hosting  container  print  create  model  response  ModelArn  endpoint  config  DEMO  byo  config  time  strftime  time  gmtime  print  endpoint  config  create  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  endpoint  config  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  job  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  endpoint  DEMO  endpoint  time  strftime  time  gmtime  print  endpoint  create  endpoint  response  sm  create  endpoint  EndpointName  endpoint  EndpointConfigName  endpoint  config  print  create  endpoint  response  EndpointArn  resp  sm  describe  endpoint  EndpointName  endpoint  status  resp  EndpointStatus  print  Status  status  try  sm  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  finally  resp  sm  describe  endpoint  EndpointName  endpoint  status  resp  ,amazon
EndpointStatus  print  Arn  resp  EndpointArn  print  Status  status  if  status  InService  raise  Exception  Endpoint  creation  did  not  succeed  iris  pd  read  csv  iris  csv  runtime  boto3  Session  client  runtime  sagemaker  payload  iris  drop  Sepal  Length  axis  to  csv  index  False  response  runtime  invoke  endpoint  EndpointName  endpoint  ContentType  text  csv  Body  payload  result  json  loads  response  Body  read  decode  result  plt  scatter  iris  Sepal  Length  np  fromstring  result  sep  plt  show  sm  delete  endpoint  EndpointName  endpoint  ,amazon
pip  install  python  igraphimport  igraph  print  igraph  version  ,microsoft
Setup  from  sagemaker  import  get  execution  role  import  sagemaker  sagemaker  session  sagemaker  Session  This  role  retrieves  the  SageMaker  compatible  role  used  by  this  Notebook  Instance  role  get  execution  role  import  chainer  from  chainer  datasets  import  get  cifar10  train  test  get  cifar10  import  os  import  shutil  import  numpy  as  np  train  data  element  for  element  in  train  train  labels  element  for  element  in  train  test  data  element  for  element  in  test  test  labels  element  for  element  in  test  try  os  makedirs  tmp  data  distributed  train  cifar  os  makedirs  tmp  data  distributed  test  cifar  np  savez  tmp  data  distributed  train  cifar  train  npz  data  train  data  labels  train  labels  np  savez  tmp  data  distributed  test  cifar  test  npz  data  test  data  labels  test  labels  train  input  sagemaker  session  upload  data  path  os  path  join  tmp  data  distributed  train  cifar  key  prefix  notebook  distributed  chaine,amazon
r  cifar  train  test  input  sagemaker  session  upload  data  path  os  path  join  tmp  data  distributed  test  cifar  key  prefix  notebook  distributed  chainer  cifar  test  finally  shutil  rmtree  tmp  data  print  training  data  at  train  input  print  test  data  at  test  input  pygmentize  src  chainer  cifar  vgg  distributed  py  from  sagemaker  chainer  estimator  import  Chainer  chainer  estimator  Chainer  entry  point  chainer  cifar  vgg  distributed  py  source  dir  src  role  role  sagemaker  session  sagemaker  session  use  mpi  True  train  instance  count  train  instance  type  ml  p2  xlarge  hyperparameters  epochs  30  batch  size  256  chainer  estimator  fit  train  train  input  test  test  input  from  s3  util  import  retrieve  output  from  s3  chainer  training  job  chainer  estimator  latest  training  job  name  desc  sagemaker  session  sagemaker  client  describe  training  job  TrainingJobName  chainer  training  job  output  data  desc  ModelArtifacts  S3Model,amazon
Artifacts  replace  model  tar  gz  output  tar  gz  retrieve  output  from  s3  output  data  output  distributed  cifar  from  IPython  display  import  Image  from  IPython  display  import  display  accuracy  graph  Image  filename  output  distributed  cifar  accuracy  png  width  800  height  800  loss  graph  Image  filename  output  distributed  cifar  loss  png  width  800  height  800  display  accuracy  graph  loss  graph  predictor  chainer  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  skimage  import  io  import  numpy  as  np  def  read  image  filename  img  io  imread  filename  img  np  array  img  transpose  img  np  expand  dims  img  axis  img  img  astype  np  float32  img  255  img  img  reshape  32  32  return  img  def  read  images  filenames  return  np  array  read  image  for  in  filenames  filenames  images  airplane1  png  images  automobile1  png  images  bird1  png  images  cat1  png  images  deer1  png  images  dog1  png  images  frog1  p,amazon
ng  images  horse1  png  images  ship1  png  images  truck1  png  image  data  read  images  filenames  response  predictor  predict  image  data  for  prediction  in  enumerate  response  print  image  prediction  format  prediction  argmax  axis  chainer  estimator  delete  endpoint  ,amazon
import  os  import  sys  Create  directory  of  dataset  efs  dir  mount  point  of  efs  dataset  dir  name  of  directory  to  store  dataset  efs  dir  home  ec2  user  SageMaker  efs  dataset  dir  food101  if  not  os  path  exists  efs  dir  print  Mount  EFS  on  notebook  instance  Food  101  dataset  is  very  large  sys  exit  elif  not  os  path  exists  os  path  join  efs  dir  dataset  dir  print  Creating  dataset  directory  format  os  path  join  efs  dir  dataset  dir  os  mkdir  os  path  join  efs  dir  dataset  dir  Download  dataset  This  takes  20  30  minutes  import  urllib  request  url  http  data  vision  ee  ethz  ch  cvl  food  101  tar  gz  urllib  request  urlretrieve  url  os  path  join  efs  dir  dataset  dir  food  101  tar  gz  import  tarfile  file  name  os  path  join  efs  dir  dataset  dir  food  101  tar  gz  tar  tarfile  open  file  name  gz  tar  extractall  path  os  path  join  efs  dir  dataset  dir  tar  close  import  os  efs  dir  home  ec2  user  SageMake,amazon
r  efs  dataset  dir  food101  food  name  ramen  edge  dir  ramen  edge  image  path  os  path  join  efs  dir  dataset  dir  food  101  images  food  name  edge  path  os  path  join  efs  dir  dataset  dir  food  101  images  edge  dir  from  PIL  import  Image  ImageFilter  if  not  os  path  exists  edge  path  print  Create  directory  to  store  edge  images  os  mkdir  edge  path  count  for  in  os  listdir  image  path  print  extracting  edge  from  images  format  str  count  len  os  listdir  image  path  end  image  Image  open  os  path  join  image  path  image  image  filter  ImageFilter  FIND  EDGES  image  image  filter  ImageFilter  SMOOTH  MORE  image  image  filter  ImageFilter  CONTOUR  image  image  convert  image  image  point  lambda  if  200  else  255  image  save  os  path  join  edge  path  count  1import  sagemaker  from  sagemaker  mxnet  import  MXNet  sagemaker  session  sagemaker  Session  role  sagemaker  get  execution  role  input  img  sagemaker  session  upload  data  p,amazon
ath  edge  path  key  prefix  food101  edge  ramen  output  img  sagemaker  session  upload  data  path  image  path  key  prefix  food101  image  ramen  wget  https  raw  githubusercontent  com  awslabs  amazon  sagemaker  examples  master  sagemaker  python  sdk  mxnet  gluon  cifar10  setup  sh  sh  setup  shmxnet  estimator  MXNet  pix2pix  py  role  role  train  instance  count  train  instance  type  ml  p3  2xlarge  hyperparameters  batch  size  32  epochs  500  learning  rate  0002  beta1  lambda1  100  mxnet  estimator  fit  feature  input  img  label  output  img  import  sagemaker  from  sagemaker  mxnet  import  MXNetModel  model  s3  sagemaker  ap  northeast  373011628954  sagemaker  mxnet  2018  09  17  03  20  12  778  output  model  tar  gz  mxnet  estimator  MXNetModel  model  data  model  role  sagemaker  get  execution  role  entry  point  pix2pix  py  predictor  mxnet  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  skimage  import  io  import  numpy  as ,amazon
 np  from  IPython  display  import  Image  display  import  mxnet  as  mx  from  mxnet  import  ndarray  as  nd  from  PIL  import  Image  ImageFilter  from  matplotlib  pyplot  import  imshow  import  os  import  matplotlib  pyplot  as  plt  from  sagemaker  mxnet  import  MXNetPredictor  predictor  MXNetPredictor  sagemaker  mxnet  2018  09  17  15  50  25  420  sagemaker  session  None  def  read  image  filename  img  arr  mx  image  imread  filename  astype  np  float32  127  img  arr  mx  image  imresize  img  arr  256  256  img  arr  nd  transpose  img  arr  img  arr  img  arr  reshape  img  arr  shape  return  img  arr  file  list  os  listdir  edge  path  index  np  random  choice  len  file  list  count  for  in  os  listdir  test  print  extracting  edge  from  images  format  str  count  len  os  listdir  test  end  image  Image  open  os  path  join  test  image  image  filter  ImageFilter  FIND  EDGES  image  image  filter  ImageFilter  SMOOTH  MORE  image  image  filter  ImageFilter  CONTOUR  ,amazon
image  image  convert  image  image  point  lambda  if  200  else  255  image  save  os  path  join  test  edge  count  Training  data  file  list  os  listdir  home  ec2  user  SageMaker  efs  food101  food  101  images  ramen  index  np  random  choice  len  file  list  edge  image  read  image  os  path  join  home  ec2  user  SageMaker  efs  food101  food  101  images  ramen  edge  file  list  index  asnumpy  result  predictor  predict  edge  image  gt  image  read  image  os  path  join  home  ec2  user  SageMaker  efs  food101  food  101  images  ramen  file  list  index  asnumpy  gt  image  np  squeeze  gt  image  axis  transpose  gt  image  gt  image  127  gt  image  gt  image  astype  int  edge  image  np  squeeze  edge  image  axis  transpose  edge  image  edge  image  127  edge  image  edge  image  astype  int  gen  image  np  array  result  127  gen  image  gen  image  astype  int  transpose  images  gt  image  edge  image  gen  image  labels  Ground  truth  Edges  Generated  image  fig0  plt  fig,amazon
ure  fig0  set  size  inches  10  10  for  in  range  plt  subplot  plt  imshow  images  plt  title  labels  Test  data  file  list  os  listdir  test  edge  index  np  random  choice  len  file  list  edge  image  read  image  os  path  join  test  edge  file  list  index  asnumpy  result  predictor  predict  edge  image  gt  image  read  image  os  path  join  test  file  list  index  asnumpy  gt  image  np  squeeze  gt  image  axis  transpose  gt  image  gt  image  127  gt  image  gt  image  astype  int  edge  image  np  squeeze  edge  image  axis  transpose  edge  image  edge  image  127  edge  image  edge  image  astype  int  gen  image  np  array  result  127  gen  image  gen  image  astype  int  transpose  images  gt  image  edge  image  gen  image  labels  Ground  truth  Edges  Generated  image  fig1  plt  figure  fig1  set  size  inches  10  10  for  in  range  plt  subplot  plt  imshow  images  plt  title  labels  ,amazon
import  sagemaker  from  sagemaker  import  get  execution  role  import  boto3  import  json  sess  sagemaker  Session  role  get  execution  role  print  role  This  is  the  role  that  SageMaker  would  use  to  leverage  AWS  resources  S3  CloudWatch  on  your  behalf  bucket  sess  default  bucket  Replace  with  your  own  bucket  name  if  needed  print  bucket  prefix  fasttext  pretrained  Replace  with  the  prefix  under  which  you  want  to  store  the  data  if  neededregion  name  boto3  Session  region  namecontainer  sagemaker  amazon  amazon  estimator  get  image  uri  region  name  blazingtext  latest  print  Using  SageMaker  BlazingText  container  format  container  region  name  wget  model  bin  https  s3  us  west  amazonaws  com  fasttext  vectors  supervised  models  lid  176  bin  tar  czvf  langid  tar  gz  model  bin  model  location  sess  upload  data  langid  tar  gz  bucket  bucket  key  prefix  prefix  rm  langid  tar  gz  model  binlang  id  sagemaker  Model  model  data,amazon
  model  location  image  container  role  role  sagemaker  session  sess  lang  id  deploy  initial  instance  count  instance  type  ml  m4  xlarge  predictor  sagemaker  RealTimePredictor  endpoint  lang  id  endpoint  name  sagemaker  session  sess  serializer  json  dumps  deserializer  sagemaker  predictor  json  deserializer  sentences  hi  which  language  is  this  mon  nom  est  Pierre  Dem  Jungen  gab  ich  einen  Ball  payload  instances  sentences  predictions  predictor  predict  payload  print  predictions  import  copy  predictions  copy  copy  deepcopy  predictions  Copying  predictions  object  because  we  want  to  change  the  labels  in  place  for  output  in  predictions  copy  output  label  output  label  upper  label  has  length  of  print  predictions  copy  sess  delete  endpoint  predictor  endpoint  ,amazon
def  getSumOfDivisors  number  ,microsoft
from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  bucket  name  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  30  This  is  format  train  set  30  from  sagemaker  import  KMeans  data  location  s3  kmeans  highlevel  example  data  format  bucket  output  location  s3  kmeans  example  output  format  bucket  print  training  data  will  be  uploaded  to  format  data  location  print  training  artifacts  will  be  uploaded  to  f,amazon
ormat  output  location  kmeans  KMeans  role  role  train  instance  count  train  instance  type  ml  c4  8xlarge  output  path  output  location  10  data  location  data  location  time  kmeans  fit  kmeans  record  set  train  set  time  kmeans  predictor  kmeans  deploy  initial  instance  count  instance  type  ml  m4  xlarge  result  kmeans  predictor  predict  train  set  30  31  print  result  time  result  kmeans  predictor  predict  valid  set  100  clusters  label  closest  cluster  float32  tensor  values  for  in  result  for  cluster  in  range  10  print  nCluster  format  int  cluster  digits  img  for  img  in  zip  clusters  valid  set  if  int  cluster  height  len  digits  width  plt  rcParams  figure  figsize  width  height  subplots  plt  subplots  height  width  subplots  numpy  ndarray  flatten  subplots  for  subplot  image  in  zip  subplots  digits  show  digit  image  subplot  subplot  for  subplot  in  subplots  len  digits  subplot  axis  off  plt  show  print  kmeans  predicto,amazon
r  endpoint  import  sagemaker  sagemaker  Session  delete  endpoint  kmeans  predictor  endpoint  ,amazon
import  sagemaker  from  sagemaker  import  get  execution  role  import  boto3  import  json  sess  sagemaker  Session  role  get  execution  role  print  role  This  is  the  role  that  SageMaker  would  use  to  leverage  AWS  resources  S3  CloudWatch  on  your  behalf  bucket  sess  default  bucket  Replace  with  your  own  bucket  name  if  needed  print  bucket  prefix  sagemaker  DEMO  blazingtext  text8  Replace  with  the  prefix  under  which  you  want  to  store  the  data  if  needed  wget  http  mattmahoney  net  dc  text8  zip  text8  gz  Uncompressing  gzip  text8  gz  ftrain  channel  prefix  train  sess  upload  data  path  text8  bucket  bucket  key  prefix  train  channel  s3  train  data  s3  format  bucket  train  channel  s3  output  location  s3  output  format  bucket  prefix  region  name  boto3  Session  region  namecontainer  sagemaker  amazon  amazon  estimator  get  image  uri  region  name  blazingtext  latest  print  Using  SageMaker  BlazingText  container  format  containe,amazon
r  region  name  bt  model  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  c4  2xlarge  train  volume  size  train  max  run  360000  input  mode  File  output  path  s3  output  location  sagemaker  session  sess  bt  model  set  hyperparameters  mode  batch  skipgram  epochs  min  count  sampling  threshold  0001  learning  rate  05  window  size  vector  dim  100  negative  samples  batch  size  11  window  size  Preferred  Used  only  if  mode  is  batch  skipgram  evaluation  True  Perform  similarity  evaluation  on  WS  353  dataset  at  the  end  of  training  subwords  False  Subword  embedding  learning  is  not  supported  by  batch  skipgramtrain  data  sagemaker  session  s3  input  s3  train  data  distribution  FullyReplicated  content  type  text  plain  s3  data  type  S3Prefix  data  channels  train  train  data  bt  model  fit  inputs  data  channels  logs  True  bt  endpoint  bt  model  deploy  initial  instance  count  instance  type  ,amazon
ml  m4  xlarge  words  awesome  blazing  payload  instances  words  response  bt  endpoint  predict  json  dumps  payload  vecs  json  loads  response  print  vecs  s3  boto3  resource  s3  key  bt  model  model  data  bt  model  model  data  find  s3  Bucket  bucket  download  file  key  model  tar  gz  tar  xvzf  model  tar  gz  cat  eval  jsonimport  numpy  as  np  from  sklearn  preprocessing  import  normalize  Read  the  400  most  frequent  word  vectors  The  vectors  in  the  file  are  in  descending  order  of  frequency  num  points  400  first  line  True  index  to  word  with  open  vectors  txt  as  for  line  num  line  in  enumerate  if  first  line  dim  int  line  strip  split  word  vecs  np  zeros  num  points  dim  dtype  float  first  line  False  continue  line  line  strip  word  line  split  vec  word  vecs  line  num  for  index  vec  val  in  enumerate  line  split  vec  index  float  vec  val  index  to  word  append  word  if  line  num  num  points  break  word  vecs  normalize,amazon
  word  vecs  copy  False  return  norm  False  from  sklearn  manifold  import  TSNE  tsne  TSNE  perplexity  40  components  init  pca  iter  10000  two  embeddings  tsne  fit  transform  word  vecs  num  points  labels  index  to  word  num  points  from  matplotlib  import  pylab  matplotlib  inline  def  plot  embeddings  labels  pylab  figure  figsize  20  20  for  label  in  enumerate  labels  embeddings  pylab  scatter  pylab  annotate  label  xy  xytext  textcoords  offset  points  ha  right  va  bottom  pylab  show  plot  two  embeddings  labels  sess  delete  endpoint  bt  endpoint  endpoint  ,amazon
import  os  import  sys  import  tensorflow  as  tf  import  numpy  as  np  from  keras  import  backend  as  Kcustom  weights  dir  os  path  expanduser  custom  weights  saved  model  dir  os  path  expanduser  models  import  glob  import  imghdr  datadir  os  path  expanduser  catsanddogs  cat  files  glob  glob  os  path  join  datadir  PetImages  Cat  jpg  dog  files  glob  glob  os  path  join  datadir  PetImages  Dog  jpg  Limit  the  data  set  to  make  the  notebook  execute  quickly  cat  files  cat  files  64  dog  files  dog  files  64  The  data  set  has  few  images  that  are  not  jpeg  Remove  them  cat  files  for  in  cat  files  if  imghdr  what  jpeg  dog  files  for  in  dog  files  if  imghdr  what  jpeg  if  not  len  cat  files  or  not  len  dog  files  print  Please  download  the  Kaggle  Cats  and  Dogs  dataset  form  https  www  microsoft  com  en  us  download  details  aspx  id  54765  and  extract  the  zip  to  datadir  raise  ValueError  Data  not  found  else  print  ca,microsoft
t  files  print  dog  files  Construct  numpy  array  as  labels  image  paths  cat  files  dog  files  total  files  len  cat  files  len  dog  files  labels  np  zeros  total  files  labels  len  cat  files  Split  images  data  as  training  data  and  test  data  from  sklearn  model  selection  import  train  test  split  onehot  labels  np  array  if  else  for  in  labels  img  train  img  test  label  train  label  test  train  test  split  image  paths  onehot  labels  random  state  42  shuffle  True  print  len  img  train  len  img  test  label  train  shape  label  test  shape  import  azureml  contrib  brainwave  models  utils  as  utils  def  preprocess  images  Convert  images  to  3D  tensors  width  height  channel  channels  are  in  BGR  order  in  images  tf  placeholder  tf  string  image  tensors  utils  preprocess  array  in  images  return  in  images  image  tensorsdef  construct  classifier  in  tensor  from  keras  layers  import  Dropout  Dense  Flatten  set  session  tf  get  def,microsoft
ault  session  FC  SIZE  1024  NUM  CLASSES  Dropout  input  shape  2048  in  tensor  Dense  FC  SIZE  activation  relu  input  dim  2048  Flatten  preds  Dense  NUM  CLASSES  activation  softmax  input  dim  FC  SIZE  name  classifier  output  return  predsdef  construct  model  quantized  starting  weights  directory  None  from  azureml  contrib  brainwave  models  import  Resnet50  QuantizedResnet50  Convert  images  to  3D  tensors  width  height  channel  in  images  image  tensors  preprocess  images  Construct  featurizer  using  quantized  or  unquantized  ResNet50  model  if  not  quantized  featurizer  Resnet50  saved  model  dir  else  featurizer  QuantizedResnet50  saved  model  dir  custom  weights  directory  starting  weights  directory  features  featurizer  import  graph  def  input  tensor  image  tensors  Construct  classifier  preds  construct  classifier  features  Initialize  weights  sess  tf  get  default  session  tf  global  variables  initializer  run  featurizer  restore  weights ,microsoft
 sess  return  in  images  image  tensors  features  preds  featurizerdef  read  files  files  Read  files  to  array  contents  for  path  in  files  with  open  path  rb  as  contents  append  read  return  contentsdef  train  model  preds  in  images  img  train  label  train  is  retrain  False  train  epoch  10  training  model  from  keras  objectives  import  binary  crossentropy  from  tqdm  import  tqdm  learning  rate  001  if  is  retrain  else  01  Specify  the  loss  function  in  labels  tf  placeholder  tf  float32  shape  None  cross  entropy  tf  reduce  mean  binary  crossentropy  in  labels  preds  optimizer  tf  train  GradientDescentOptimizer  learning  rate  minimize  cross  entropy  def  chunks  Yield  successive  sized  chunks  from  and  if  len  len  print  and  are  not  equal  in  chunks  raise  ValueError  Parameter  error  for  in  range  len  yield  chunk  size  16  chunk  num  len  label  train  chunk  size  sess  tf  get  default  session  for  epoch  in  range  train  epoch  ,microsoft
avg  loss  for  img  chunk  label  chunk  in  tqdm  chunks  img  train  label  train  chunk  size  contents  read  files  img  chunk  loss  sess  run  optimizer  cross  entropy  feed  dict  in  images  contents  in  labels  label  chunk  learning  phase  avg  loss  loss  chunk  num  print  Epoch  epoch  loss  3f  format  avg  loss  Reach  desired  performance  if  avg  loss  001  breakdef  test  model  preds  in  images  img  test  label  test  Test  the  model  from  keras  metrics  import  categorical  accuracy  in  labels  tf  placeholder  tf  float32  shape  None  accuracy  tf  reduce  mean  categorical  accuracy  in  labels  preds  contents  read  files  img  test  accuracy  accuracy  eval  feed  dict  in  images  contents  in  labels  label  test  learning  phase  return  accuracy  Launch  the  training  tf  reset  default  graph  sess  tf  Session  graph  tf  get  default  graph  with  sess  as  default  in  images  image  tensors  features  preds  featurizer  construct  model  quantized  False  train ,microsoft
 model  preds  in  images  img  train  label  train  is  retrain  False  train  epoch  10  accuracy  test  model  preds  in  images  img  test  label  test  print  Accuracy  accuracy  featurizer  save  weights  custom  weights  dir  rn50  tf  get  default  session  tf  reset  default  graph  sess  tf  Session  graph  tf  get  default  graph  with  sess  as  default  print  Testing  trained  model  with  quantization  in  images  image  tensors  features  preds  quantized  featurizer  construct  model  quantized  True  starting  weights  directory  custom  weights  dir  accuracy  test  model  preds  in  images  img  test  label  test  print  Accuracy  accuracy  if  accuracy  93  with  sess  as  default  print  Fine  tuning  model  with  quantization  train  model  preds  in  images  img  train  label  train  is  retrain  True  train  epoch  10  accuracy  test  model  preds  in  images  img  test  label  test  print  Accuracy  accuracy  from  azureml  contrib  brainwave  pipeline  import  ModelDefinition  Tenso,microsoft
rflowStage  BrainWaveStage  model  def  path  os  path  join  saved  model  dir  model  def  zip  model  def  ModelDefinition  model  def  pipeline  append  TensorflowStage  sess  in  images  image  tensors  model  def  pipeline  append  BrainWaveStage  sess  quantized  featurizer  model  def  pipeline  append  TensorflowStage  sess  features  preds  model  def  save  model  def  path  print  model  def  path  from  azureml  core  import  Workspace  ws  Workspace  from  config  from  azureml  core  model  import  Model  from  azureml  core  image  import  Image  from  azureml  core  webservice  import  Webservice  from  azureml  contrib  brainwave  import  BrainwaveWebservice  BrainwaveImage  model  name  catsanddogs  resnet50  model  image  name  catsanddogs  resnet50  image  service  name  modelbuild  service  registered  model  Model  register  ws  service  def  path  model  name  image  config  BrainwaveImage  image  configuration  deployment  config  BrainwaveWebservice  deploy  configuration  try  servi,microsoft
ce  Webservice  ws  service  name  service  delete  service  Webservice  deploy  from  model  ws  service  name  registered  model  image  config  deployment  config  except  WebserviceException  service  Webservice  deploy  from  model  ws  service  name  registered  model  image  config  deployment  config  print  service  ipAddress  str  service  port  from  azureml  contrib  brainwave  client  import  PredictionClient  client  PredictionClient  service  ipAddress  service  port  Specify  an  image  to  classify  print  CATS  for  image  file  in  cat  files  results  client  score  image  image  file  result  CORRECT  if  results  results  else  WRONG  print  result  str  results  print  DOGS  for  image  file  in  dog  files  results  client  score  image  image  file  result  CORRECT  if  results  results  else  WRONG  print  result  str  results  service  delete  ,microsoft
import  boto3  re  from  sagemaker  import  get  execution  role  role  get  execution  role  cat  iris  dnn  classifier  pyfrom  iris  dnn  classifier  import  estimator  fn  classifier  estimator  fn  run  config  None  params  None  import  os  from  six  moves  urllib  request  import  urlopen  Data  sets  IRIS  TRAINING  iris  training  csv  IRIS  TRAINING  URL  http  download  tensorflow  org  data  iris  training  csv  IRIS  TEST  iris  test  csv  IRIS  TEST  URL  http  download  tensorflow  org  data  iris  test  csv  if  not  os  path  exists  IRIS  TRAINING  raw  urlopen  IRIS  TRAINING  URL  read  with  open  IRIS  TRAINING  wb  as  write  raw  if  not  os  path  exists  IRIS  TEST  raw  urlopen  IRIS  TEST  URL  read  with  open  IRIS  TEST  wb  as  write  raw  from  iris  dnn  classifier  import  train  input  fn  train  func  train  input  fn  params  None  classifier  train  input  fn  train  func  steps  1000  from  iris  dnn  classifier  import  serving  input  fn  exported  model  classifier,amazon
  export  savedmodel  export  dir  base  export  Servo  serving  input  receiver  fn  serving  input  fn  print  exported  model  import  tarfile  with  tarfile  open  model  tar  gz  mode  gz  as  archive  archive  add  export  recursive  True  import  sagemaker  sagemaker  session  sagemaker  Session  inputs  sagemaker  session  upload  data  path  model  tar  gz  key  prefix  model  from  sagemaker  tensorflow  model  import  TensorFlowModel  sagemaker  model  TensorFlowModel  model  data  s3  sagemaker  session  default  bucket  model  model  tar  gz  role  role  entry  point  iris  dnn  classifier  py  time  predictor  sagemaker  model  deploy  initial  instance  count  instance  type  ml  m4  xlarge  sample  predictor  predict  sample  os  remove  model  tar  gz  import  shutil  shutil  rmtree  export  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
import  eetry  ee  Initialize  print  The  Earth  Engine  package  initialized  successfully  except  ee  EEException  as  print  The  Earth  Engine  package  failed  to  initialize  except  print  Unexpected  error  sys  exc  info  raise  bash  earthengine  authenticate  quiet  bash  earthengine  authenticate  authorization  code  AAAhLqeNJiDeoAjDbcLPwvH6LSSV6fkW  2zZ8jS122JA1SoQbyp5Vrs  bash  rm  config  earthengine  ,google
Export  as  slides  command  jupyter  nbconvert  Jupyter  Slides  ipynb  to  slides  post  serveimport  os  import  pandas  as  pd  import  numpy  as  np  import  seaborn  as  sns  import  matplotlib  pyplot  as  plt  from  sklearn  import  preprocessing  svm  from  itertools  import  combinations  from  sklearn  preprocessing  import  PolynomialFeatures  LabelEncoder  StandardScaler  import  sklearn  feature  selection  from  sklearn  model  selection  import  train  test  split  from  collections  import  defaultdict  from  sklearn  import  metrics  The  code  was  removed  by  DSX  for  sharing  Checking  that  everything  is  correct  pd  set  option  display  max  columns  30  applicants  head  10  After  running  this  cell  we  will  see  that  we  have  no  missing  values  applicants  info  Convert  columns  with  numbers  as  values  but  object  as  datatype  into  numeric  cols  13  Set  error  level  to  coerce  so  any  string  value  will  be  replaced  with  NaN  applicants  cols  applicants  ,ibm
cols  apply  pd  to  numeric  errors  coerce  applicants  head  10  Check  if  we  have  any  NaN  values  applicants  isnull  values  any  Handle  missing  values  using  scikit  learn  Imputer  from  sklearn  preprocessing  import  Imputer  Define  the  values  to  replce  and  the  strategy  of  choosing  the  replacement  value  imp  Imputer  missing  values  NaN  strategy  mean  applicants  cols  imp  fit  transform  applicants  cols  applicants  head  10  Check  if  we  have  any  NaN  values  applicants  isnull  values  any  applicants  info  Describe  columns  with  numerical  values  pd  set  option  precision  applicants  describe  Find  correlations  applicants  corr  method  pearson  Create  Grid  for  pairwise  relationships  gr  sns  PairGrid  applicants  size  hue  15  gr  gr  map  diag  plt  hist  gr  gr  map  offdiag  plt  scatter  gr  gr  add  legend  Set  up  plot  size  fig  ax  plt  subplots  figsize  20  10  Attributes  destribution  sns  boxplot  orient  palette  hls  data  applicants  ,ibm
iloc  13  fliersize  14  Tenure  data  distribution  histogram  sns  distplot  applicants  iloc  hist  True  plt  show  Use  pandas  get  dummies  applicants  encoded  pd  get  dummies  applicants  applicants  encoded  head  10  Create  training  data  for  non  preprocessed  approach  npp  applicants  encoded  iloc  pd  DataFrame  npp  head  10  Create  training  data  for  that  will  undergo  preprocessing  applicants  encoded  iloc  head  Extract  labels  from  sklearn  preprocessing  import  LabelEncoder  Split  last  column  from  original  dataset  as  the  labels  column  applicants  15  Apply  encoder  to  transform  strings  to  numeric  values  and  le  LabelEncoder  fit  enc  le  transform  pd  DataFrame  enc  head  10  Detect  outlier  using  interquartile  method  and  remove  them  def  find  outliers  df  quartile  quartile  np  percentile  df  25  75  iqr  quartile  quartile  lower  bound  quartile  iqr  upper  bound  quartile  iqr  outlier  indices  list  df  index  df  lower  bound  df  upp,ibm
er  bound  outlier  values  list  df  outlier  indices  df  outlier  indices  np  NaN  return  df  Find  outliers  in  first  column  continuous  values  print  find  outliers  Find  outliers  in  first  column  continuous  values  print  find  outliers  Find  outliers  in  first  column  continuous  values  print  find  outliers  Find  outliers  in  first  column  continuous  values  print  find  outliers  10  Find  outliers  in  first  column  continuous  values  print  find  outliers  13  Check  for  null  values  isnull  values  any  Find  outliers  in  first  column  continuous  values  print  find  outliers  14  Define  the  values  to  replce  and  the  strategy  of  choosing  the  replacement  value  suspected  cols  10  13  14  imp  Imputer  missing  values  NaN  strategy  mean  pd  DataFrame  suspected  cols  imp  fit  transform  pd  DataFrame  suspected  cols  pd  DataFrame  head  10  Check  for  null  values  pd  DataFrame  isnull  values  any  Select  best  features  select  sklearn  feature  sel,ibm
ection  SelectKBest  20  selected  features  select  fit  enc  indexes  selected  features  get  support  indices  True  col  names  selected  pd  DataFrame  columns  for  in  indexes  selected  pd  DataFrame  col  names  selected  pd  DataFrame  selected  head  10  train  npp  test  npp  train  npp  test  npp  train  test  split  npp  enc  test  size  random  state  42  print  train  npp  shape  train  npp  shape  print  test  npp  shape  test  npp  shape  train  test  train  test  train  test  split  selected  enc  test  size  random  state  42  print  train  shape  train  shape  print  test  shape  test  shape  Use  StandardScaler  scaler  preprocessing  StandardScaler  fit  train  train  train  scaled  scaler  transform  train  pd  DataFrame  train  scaled  columns  pd  DataFrame  train  columns  head  pd  DataFrame  train  head  from  sklearn  linear  model  import  LogisticRegression  clf  lr  npp  LogisticRegression  clf  lr  npp  fit  train  npp  train  npp  from  sklearn  linear  model  import  Logis,ibm
ticRegression  clf  lr  LogisticRegression  model  clf  lr  fit  train  scaled  train  model  Use  the  scaler  fit  on  trained  data  to  scale  our  test  data  test  scaled  scaler  transform  test  pd  DataFrame  test  scaled  columns  pd  DataFrame  train  columns  head  score  lr  npp  clf  lr  npp  decision  function  test  npp  score  lr  npp  Get  accuracy  score  from  sklearn  metrics  import  accuracy  score  pred  lr  npp  clf  lr  npp  predict  test  npp  acc  lr  npp  accuracy  score  test  npp  pred  lr  npp  print  acc  lr  npp  Get  Precision  vs  Recall  score  from  sklearn  metrics  import  average  precision  score  average  precision  lr  npp  average  precision  score  test  npp  score  lr  npp  print  Average  precision  recall  score  2f  format  average  precision  lr  npp  score  lr  clf  lr  decision  function  test  scaled  score  lry  pred  lr  clf  lr  predict  test  scaled  acc  lr  accuracy  score  test  pred  lr  print  acc  lr  average  precision  lr  average  precision  s,ibm
core  test  score  lr  print  Average  precision  recall  score  2f  format  average  precision  lr  Plot  SVC  ROC  Curve  plt  figure  figsize  15  10  clf  fpr  lr  npp  tpr  lr  npp  thresh  lr  npp  metrics  roc  curve  test  npp  score  lr  npp  auc  lr  npp  metrics  roc  auc  score  test  npp  score  lr  npp  plt  plot  fpr  lr  npp  tpr  lr  npp  label  Logistic  Regression  on  Non  preprocessed  Data  auc  str  auc  lr  npp  fpr  lr  tpr  lr  thresh  lr  metrics  roc  curve  test  score  lr  auc  lr  metrics  roc  auc  score  test  score  lr  plt  plot  fpr  lr  tpr  lr  label  Logistic  Regression  on  Preprocessed  Data  auc  str  auc  lr  plt  legend  loc  plt  xlabel  False  Positives  plt  ylabel  True  Positives  The  code  was  removed  by  DSX  for  sharing  To  work  with  the  Watson  Machine  Learning  REST  API  you  must  generate  Bearer  access  token  import  urllib3  requests  json  headers  urllib3  util  make  headers  basic  auth  format  credentials  username  credentials  pass,ibm
word  url  v3  identity  token  format  credentials  url  response  requests  get  url  headers  headers  ml  token  Bearer  json  loads  response  text  get  token  print  ml  token  Create  an  online  scoring  endpoint  endpoint  instance  credentials  url  v3  wml  instances  credentials  instance  id  header  Content  Type  application  json  Authorization  ml  token  response  get  instance  requests  get  endpoint  instance  headers  header  print  response  get  instance  print  response  get  instance  text  Create  API  client  from  watson  machine  learning  client  import  WatsonMachineLearningAPIClient  client  WatsonMachineLearningAPIClient  credentials  Publish  model  in  Watson  Machine  Learning  repository  on  Cloud  model  props  client  repository  ModelMetaNames  AUTHOR  NAME  Heba  El  Shimy  client  repository  ModelMetaNames  NAME  Credit  Card  Approval  Model  published  model  client  repository  store  model  model  model  meta  props  model  props  training  data  train  scaled,ibm
  training  target  train  Create  model  deployment  published  model  uid  client  repository  get  model  uid  published  model  created  deployment  client  deployments  create  published  model  uid  Deployment  of  Credit  Card  Approval  Model  Get  Scoring  URL  scoring  endpoint  client  deployments  get  scoring  url  created  deployment  print  scoring  endpoint  Get  model  details  and  expected  input  model  details  client  repository  get  details  published  model  uid  print  json  dumps  model  details  indent  ,ibm
import  sagemaker  bucket  sagemaker  Session  default  bucket  prefix  radix  mnist  fashion  tutorial  role  sagemaker  get  execution  role  import  boto3  from  time  import  gmtime  strftime  from  sagemaker  tensorflow  import  TensorFlow  from  sagemaker  tuner  import  IntegerParameter  CategoricalParameter  ContinuousParameter  HyperparameterTuner  cat  cnn  fashion  mnist  py  estimator  TensorFlow  entry  point  cnn  fashion  mnist  py  role  role  input  mode  Pipe  training  steps  20  000  evaluation  steps  100  train  instance  count  train  instance  type  ml  c5  2xlarge  base  job  name  radix  mnist  fashion  Define  which  objective  has  to  be  optimised  objective  metric  name  loss  objective  type  Minimize  metric  definitions  Name  loss  Regex  loss  Define  hyperparameter  ranges  hyperparameter  ranges  learning  rate  ContinuousParameter  0001  001  dropout  rate  ContinuousParameter  nw  depth  IntegerParameter  optimizer  type  CategoricalParameter  sgd  adam  Instantiate  H,amazon
yperparameterTuner  instance  tuner  HyperparameterTuner  estimator  objective  metric  name  hyperparameter  ranges  metric  definitions  max  jobs  16  max  parallel  jobs  objective  type  objective  type  Fit  the  HyperparameterTuner  to  start  the  hyperparameter  optimisation  process  train  data  s3  sagemaker  eu  central  959924085179  radix  mnist  fashion  tutorial  data  mnist  train  tfrecords  eval  data  s3  sagemaker  eu  central  959924085179  radix  mnist  fashion  tutorial  data  mnist  validation  tfrecords  tuner  fit  train  train  data  eval  eval  data  logs  False  Sanity  check  if  the  optimisation  process  has  started  boto3  client  sagemaker  describe  hyper  parameter  tuning  job  HyperParameterTuningJobName  tuner  latest  tuning  job  job  name  HyperParameterTuningJobStatus  ,amazon
import  numpy  as  np  import  matplotlib  pyplot  as  plt  from  scipy  stats  import  norm  from  keras  layers  import  Input  Dense  Lambda  Merge  from  keras  models  import  Model  from  keras  import  backend  as  from  keras  datasets  import  mnist  from  keras  layers  core  import  Reshape  from  keras  datasets  import  mnist  from  keras  models  import  Sequential  from  keras  layers  import  Dense  Dropout  Activation  Flatten  from  keras  layers  import  Convolution2D  MaxPooling2D  from  keras  layers  convolutional  import  Conv2D  MaxPooling2D  ZeroPadding2D  UpSampling2D  from  keras  layers  normalization  import  BatchNormalization  from  keras  callbacks  import  ModelCheckpoint  LearningRateScheduler  import  random  from  keras  optimizers  import  SGD  import  pandas  as  pd  train  train  test  test  mnist  load  data  train  train  astype  float32  255  test  test  astype  float32  255  60  train  train  test  test  noise  factor  train  noisy  train  noise  factor  np  random  ,amazon
normal  loc  scale  size  train  shape  test  noisy  test  noise  factor  np  random  normal  loc  scale  size  test  shape  train  noisy  np  clip  train  noisy  test  noisy  np  clip  test  noisy  train  train  reshape  28  28  test  test  reshape  28  28  train  noisy  train  noisy  reshape  28  28  test  noisy  test  noisy  reshape  28  28  train  noisy  np  concatenate  train  noisy  train  reshape  28  28  train  np  concatenate  train  train  reshape  28  28  np  random  seed  200  sel  random  sample  range  train  shape  100  train  pd  get  dummies  train  sel  train  noisy  train  noisy  sel  batch  size  30  nb  classes  10  10  classes  for  train  img  rows  img  cols  28  28  sqrt  of  784  nb  filters  32  pool  size  kernel  size  input  shape  28  28  learning  rate  008  decay  rate  5e  momentum  9sgd  SGD  lr  learning  rate  momentum  momentum  decay  decay  rate  nesterov  False  input  img  Input  shape  28  28  Conv2D  32  activation  relu  padding  same  input  img  MaxPooling2D  pad,amazon
ding  same  Conv2D  32  activation  relu  padding  same  MaxPooling2D  padding  same  Conv2D  32  activation  relu  padding  same  UpSampling2D  Conv2D  32  activation  relu  padding  same  UpSampling2D  decoded  Conv2D  activation  sigmoid  padding  same  generator  Model  input  img  decoded  generator  compile  loss  mean  squared  error  optimizer  sgd  metrics  accuracy  generator  summary  discriminator  Sequential  discriminator  add  Conv2D  nb  filters  kernel  size  kernel  size  padding  valid  input  shape  input  shape  discriminator  add  Activation  relu  discriminator  add  Conv2D  nb  filters  kernel  size  kernel  size  discriminator  add  Activation  relu  discriminator  add  MaxPooling2D  pool  size  pool  size  discriminator  add  Dropout  25  discriminator  add  Flatten  discriminator  add  Dense  128  discriminator  add  Activation  relu  discriminator  add  Dropout  discriminator  add  Dense  10  discriminator  add  Activation  softmax  discriminator  compile  loss  categorical  crosse,amazon
ntropy  optimizer  sgd  metrics  accuracy  discriminator  summary  def  trainable  net  val  net  trainable  val  for  in  net  layers  trainable  val  trainable  generator  False  gan  input  Input  batch  shape  None  28  28  gan  level2  discriminator  generator  gan  input  GAN  Model  gan  input  gan  level2  GAN  compile  loss  mean  squared  error  optimizer  sgd  metrics  accuracy  nb  epochs  rate  proportion  of  update  of  Discriminator  Generator  for  in  range  nb  epochs  print  Discriminator  epoch  discriminator  fit  train  noisy  np  array  train  epochs  nb  epochs  batch  size  30  verbose  print  Generator  GAN  fit  train  noisy  np  array  train  batch  size  30  epochs  nb  epochs  verbose  train  train  test  test  mnist  load  data  train  train  train  np  concatenate  train  train  sel  test  test  test  np  concatenate  test  test  sel  test  noisy  test  noisy  test  noisy  np  concatenate  test  noisy  test  noisy  sel  print  Accuracy  Train  np  count  nonzero  train  np  ar,amazon
gmax  GAN  predict  train  noisy  axis  print  Accuracy  Test  np  count  nonzero  test  np  argmax  GAN  predict  test  noisy  axis  ,amazon
import  numpy  as  np  import  pandas  as  pd  from  surprise  import  Reader  Dataset  SVD  evaluatemovie  pd  read  csv  movie  csv  rating  pd  read  csv  rating  csv  movie  head  rating  drop  Unnamed  axis  inplace  True  rating  head  combined  data  pd  merge  rating  movie  on  movieId  combined  data  head  user  combined  data  combined  data  userId  user  like  user  user  rating  loc  movieId  title  rating  user  likereader  Reader  svd  SVD  feed  svd  data  Dataset  load  from  df  combined  data  userId  movieId  rating  reader  trainset  data  build  full  trainset  svd  fit  trainset  calculate  estimate  score  user  would  like  movie  movieId  title  copy  user  would  like  estimate  score  user  would  like  movieId  apply  lambda  svd  predict  est  rank  movies  by  estimate  score  user  would  like  user  would  like  sort  values  estimate  score  ascending  False  user  would  like  head  def  movie  recommender  userID  What  did  user  like  in  the  past  user  combined  data,amazon
  combined  data  userId  userID  user  like  user  user  rating  loc  userId  movieId  title  rating  Predict  top  movies  user  would  love  to  watch  reader  Reader  svd  SVD  feed  svd  data  Dataset  load  from  df  combined  data  userId  movieId  rating  reader  trainset  data  build  full  trainset  svd  fit  trainset  calculate  estimate  score  user  would  like  movie  movieId  title  copy  user  would  like  estimate  score  user  would  like  movieId  apply  lambda  svd  predict  userID  est  rank  movies  by  estimate  score  user  would  like  user  would  like  sort  values  estimate  score  ascending  False  return  user  would  like  head  predict  top  movies  user  100  would  love  to  watch  movie  recommender  100  ,amazon
from  sagemaker  import  get  execution  role  Bucket  location  to  save  your  custom  code  in  tar  gz  format  custom  code  upload  location  s3  bucket  name  customcode  tensorflow  iris  Bucket  location  where  results  of  model  training  are  saved  model  artifacts  location  s3  bucket  name  artifacts  IAM  execution  role  that  gives  SageMaker  access  to  resources  in  your  AWS  account  role  get  execution  role  cat  iris  dnn  classifier  py  def  estimator  model  path  hyperparameters  feature  columns  tf  feature  column  numeric  column  INPUT  TENSOR  NAME  shape  return  tf  estimator  DNNClassifier  feature  columns  feature  columns  hidden  units  10  20  10  classes  model  dir  model  path  def  train  input  fn  training  dir  hyperparameters  training  set  tf  contrib  learn  datasets  base  load  csv  with  header  filename  os  path  join  training  dir  iris  training  csv  target  dtype  np  int  features  dtype  np  float32  return  tf  estimator  inputs  numpy  i,amazon
nput  fn  INPUT  TENSOR  NAME  np  array  training  set  data  np  array  training  set  target  num  epochs  None  shuffle  True  def  serving  input  fn  hyperparameters  feature  spec  INPUT  TENSOR  NAME  tf  FixedLenFeature  dtype  tf  float32  shape  return  tf  estimator  export  build  parsing  serving  input  receiver  fn  feature  spec  from  sagemaker  tensorflow  import  TensorFlow  iris  estimator  TensorFlow  entry  point  iris  dnn  classifier  py  role  role  output  path  model  artifacts  location  code  location  custom  code  upload  location  train  instance  count  train  instance  type  ml  c4  xlarge  training  steps  1000  evaluation  steps  100  time  import  boto3  use  the  region  specific  sample  data  bucket  region  boto3  Session  region  name  train  data  location  s3  sagemaker  sample  data  tensorflow  iris  format  region  iris  estimator  fit  train  data  location  time  iris  predictor  iris  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge,amazon
  iris  predictor  predict  expected  label  to  be  1print  iris  predictor  endpoint  import  sagemaker  sagemaker  Session  delete  endpoint  iris  predictor  endpoint  ,amazon
from  sagemaker  import  get  execution  role  Bucket  with  input  data  data  location  s3  your  S3  bucket  name  IAM  execution  role  that  gives  SageMaker  access  to  resources  in  your  AWS  account  We  can  use  the  SageMaker  Python  SDK  to  get  the  role  from  our  notebook  environment  role  get  execution  role  import  sagemaker  as  sage  from  time  import  gmtime  strftime  sess  sage  Session  region  sess  boto  session  region  name  image  831212071815  dkr  ecr  amazonaws  com  donkey  latest  format  region  tree  sage  estimator  Estimator  image  role  ml  c5  2xlarge  output  path  s3  output  format  sess  default  bucket  sagemaker  session  sess  tree  fit  data  location  print  tree  model  data  ,amazon
bash  CFFI  VERSION  pip  list  dev  null  grep  cffi  echo  CFFI  VERSION  if  CFFI  VERSION  cffi  10  then  pip  uninstall  yes  cffi  fi  yum  log  sudo  yum  install  libffi  devel  openssl  devel  pip  log  pip  install  upgrade  snowflake  connector  python  if  CFFI  VERSION  cffi  10  then  echo  configuration  has  changed  restart  notebook  fi  pip  uninstall  log  js  log  yum  log  pip  logimport  boto3  params  SNOWFLAKE  URL  SNOWFLAKE  ACCOUNT  ID  SNOWFLAKE  USER  ID  SNOWFLAKE  PASSWORD  SNOWFLAKE  DATABASE  SNOWFLAKE  SCHEMA  SNOWFLAKE  WAREHOUSE  SNOWFLAKE  BUCKET  SNOWFLAKE  PREFIX  region  us  east  def  get  credentials  params  ssm  boto3  client  ssm  region  response  ssm  get  parameters  Names  params  WithDecryption  True  Build  dict  of  credentials  param  values  Name  Value  for  in  response  Parameters  return  param  values  param  values  get  credentials  params  import  snowflake  connector  Connecting  to  Snowflake  using  the  default  authenticator  ctx  snowflake ,amazon
 connector  connect  user  param  values  SNOWFLAKE  USER  ID  password  param  values  SNOWFLAKE  PASSWORD  account  param  values  SNOWFLAKE  ACCOUNT  ID  warehouse  param  values  SNOWFLAKE  WAREHOUSE  database  param  values  SNOWFLAKE  DATABASE  schema  param  values  SNOWFLAKE  SCHEMA  cs  ctx  cursor  allrows  cs  execute  select  main  temp  max  273  15  8000  32  00  as  temp  max  far  main  temp  min  273  15  8000  32  00  as  temp  min  far  cast  time  as  timestamp  time  city  coord  lat  lat  city  coord  lon  lon  from  snowflake  sample  data  weather  weather  14  total  where  city  name  New  York  and  city  country  US  fetchall  import  pandas  as  pd  For  munging  tabular  data  data  pd  DataFrame  allrows  data  columns  temp  max  far  temp  min  far  time  lat  lon  pd  set  option  display  max  columns  500  Make  sure  we  can  see  all  of  the  columns  pd  set  option  display  max  rows  10  Keep  the  output  on  one  page  data  ,amazon
conda  install  anaconda  psycopg2import  os  import  boto3  import  pandas  as  pd  import  json  import  psycopg2  import  sqlalchemy  as  sa  region  boto3  Session  region  name  bucket  your  s3  bucket  name  here  put  your  s3  bucket  name  here  and  create  s3  bucket  prefix  sagemaker  DEMO  redshift  customize  to  your  bucket  where  you  have  stored  the  data  credfile  redshift  creds  template  json  nogit  Read  credentials  to  dictionary  with  open  credfile  as  fh  creds  json  loads  fh  read  Sample  query  for  testing  query  select  from  public  irisdata  print  Reading  from  Redshift  def  get  conn  creds  conn  psycopg2  connect  dbname  creds  db  name  user  creds  username  password  creds  password  port  creds  port  num  host  creds  host  name  return  conn  def  get  df  creds  query  with  get  conn  creds  as  conn  with  conn  cursor  as  cur  cur  execute  query  result  set  cur  fetchall  colnames  desc  name  for  desc  in  cur  description  df  pd  DataFram,amazon
e  from  records  result  set  columns  colnames  return  df  df  get  df  creds  query  print  Saving  file  localFile  iris  csv  df  to  csv  localFile  index  False  print  Done  print  Writing  to  S3  fObj  open  localFile  rb  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  localFile  upload  fileobj  fObj  print  Done  print  Reading  from  S3  key  unchanged  for  demo  purposes  change  key  to  read  from  output  data  key  os  path  join  prefix  localFile  s3  boto3  resource  s3  outfile  iris2  csv  s3  Bucket  bucket  download  file  key  outfile  df2  pd  read  csv  outfile  print  Done  print  Writing  to  Redshift  connection  str  postgresql  psycopg2  creds  username  creds  password  creds  host  name  creds  port  num  creds  db  name  df2  to  sql  irisdata  v2  connection  str  schema  public  index  False  print  Done  pd  options  display  max  rows  conn  get  conn  creds  query  select  from  irisdata3  df  pd  read  sql  query  query  conn  df  ,amazon
import  os  import  boto3  import  sagemaker  from  sagemaker  mxnet  import  MXNet  from  mxnet  import  gluon  nd  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  gluon  data  vision  MNIST  data  mnist  train  train  True  gluon  data  vision  MNIST  data  mnist  test  train  False  inputs  sagemaker  session  upload  data  path  data  mnist  key  prefix  data  mnist  cat  mnist  cnn  py  batch  size  100  epochs  10  learning  rate  01  momentum  log  interval  100m  MXNet  mnist  cnn  py  role  role  train  instance  count  train  instance  type  ml  c4  xlarge  hyperparameters  batch  size  batch  size  epochs  epochs  learning  rate  learning  rate  momentum  momentum  log  interval  log  interval  fit  inputs  predictor  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  IPython  display  import  HTML  HTML  open  input  html  read  data  nd  array  data  reshape  28  28  asnumpy  tolist  response  predictor  predi,amazon
ct  data  print  response  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
Set  up  an  account  access  key  spark  conf  set  fs  azure  account  key  storage  account  name  blob  core  windows  net  storage  access  key  spark  conf  set  fs  azure  account  key  databrickstore  blob  core  windows  net  S1PtMWvUw5If1Z8FMzXAxC7OMw9G5Go8BGCXJ81qpFVYpZ9dpXOnU4zlg0PbldKkbLIbmbv02WoJsgYLGKIfgg  dbutils  fs  ls  wasbs  your  container  name  your  storage  account  name  blob  core  windows  net  your  directory  name  dbutils  fs  ls  wasbs  dbdemo01  databrickstore  blob  core  windows  net  mount  Blob  storage  container  or  folder  inside  container  dbutils  fs  mount  source  wasbs  your  container  name  your  storage  account  name  blob  core  windows  net  your  directory  name  mount  point  mount  point  path  extra  configs  conf  key  conf  value  note  mount  point  is  DBFS  path  and  the  path  must  be  under  mnt  dbutils  fs  mount  source  wasbs  dbdemo01  databrickstore  blob  core  windows  net  mount  point  mnt  dbdemo01  extra  configs  fs  azure  account,microsoft
  key  databrickstore  blob  core  windows  net  S1PtMWvUw5If1Z8FMzXAxC7OMw9G5Go8BGCXJ81qpFVYpZ9dpXOnU4zlg0PbldKkbLIbmbv02WoJsgYLGKIfgg  Access  files  in  your  container  as  if  they  were  local  files  TEXT  df  spark  read  text  mnt  mount  point  path  JSON  df  spark  read  json  mnt  mount  point  path  df  spark  read  json  mnt  small  radio  json  json  dbdemo01  display  df  df  show  unmount  dbutils  fs  unmount  mount  point  path  dbutils  fs  unmount  mnt  dbdemo01  specificColumnsDf  df  select  firstname  lastname  gender  location  level  specificColumnsDf  show  renamedColumnsDF  specificColumnsDf  withColumnRenamed  level  subscription  type  renamedColumnsDF  show  Apply  some  transformations  to  the  data  then  use  the  Data  Source  API  to  write  the  data  back  to  another  table  in  SQL  DW  note  the  SQL  date  warehouse  connector  uses  Azure  Blob  Storage  as  temporary  storage  to  upload  data  between  Azure  Databricks  and  Azure  SQL  Data  Warehouse  SQL  Dat,microsoft
a  Warehouse  related  settings  dwTable  mytable001  dwDatabase  sqldwdemo001  dwServer  sqldwdemoserver001  dwUser  yoichika  dwPass  ssw0rd  dwJdbcPort  1433  dwJdbcExtraOptions  encrypt  true  trustServerCertificate  true  hostNameInCertificate  database  windows  net  loginTimeout  30  sqlDwUrl  jdbc  sqlserver  dwServer  database  windows  net  dwJdbcPort  database  dwDatabase  user  dwUser  password  dwPass  dwJdbcExtraOptions  sqlDwUrlSmall  jdbc  sqlserver  dwServer  database  windows  net  dwJdbcPort  database  dwDatabase  user  dwUser  password  dwPass  tempDir  wasbs  dbdemo01tmp  databrickstore  blob  core  windows  net  tempDirs  sc  jsc  hadoopConfiguration  set  fs  azure  account  key  your  storage  account  name  blob  core  windows  net  your  storage  account  access  key  acntInfo  fs  azure  account  key  databrickstore  blob  core  windows  net  sc  jsc  hadoopConfiguration  set  acntInfo  S1PtMWvUw5If1Z8FMzXAxC7OMw9G5Go8BGCXJ81qpFVYpZ9dpXOnU4zlg0PbldKkbLIbmbv02WoJsgYLGKIfgg  Loading  ,microsoft
transformed  dataframe  renamedColumnsDF  into  SQLDW  spark  conf  set  spark  sql  parquet  writeLegacyFormat  true  This  snippet  creates  table  called  dwTable  in  the  SQL  database  df  write  format  com  databricks  spark  sqldw  option  url  jdbc  sqlserver  the  rest  of  the  connection  string  option  forward  spark  azure  storage  credentials  true  option  dbtable  my  table  in  dw  copy  option  tempdir  wasbs  your  container  name  your  storage  account  name  blob  core  windows  net  your  directory  name  save  renamedColumnsDF  write  format  com  databricks  spark  sqldw  option  url  sqlDwUrlSmall  option  dbtable  dwTable  option  forward  spark  azure  storage  credentials  true  option  tempdir  tempDir  mode  overwrite  save  ,microsoft
Write  program  that  will  work  out  the  distance  travelled  if  the  user  enters  the  speed  and  the  time  speed  int  input  Enter  speed  time  int  input  Enter  time  print  Distance  is  format  speed  time  Write  program  to  tell  you  the  speed  you  would  have  to  travel  at  in  order  to  go  distance  within  certain  time  entered  distance  int  input  Enter  distance  time  int  input  Enter  time  print  Distance  is  2f  format  distance  time  Write  program  to  work  out  how  many  days  you  have  lived  for  import  datetime  currdate  datetime  date  today  day  int  input  Enter  day  month  int  input  Enter  month  year  int  input  Enter  year  birthdate  datetime  date  year  month  day  print  No  of  days  you  lived  for  format  abs  currdate  birthdate  days  Write  program  to  work  out  how  many  seconds  you  have  lived  for  import  datetime  currdate  datetime  date  today  day  int  input  Enter  day  month  int  input  Enter  month  year  int  input  En,microsoft
ter  year  birthdate  datetime  date  year  month  day  print  No  of  seconds  you  lived  for  format  abs  currdate  birthdate  days  60  60  24  Write  program  that  will  accept  date  of  birth  and  determine  whether  one  is  considered  an  adult  21  years  old  import  datetime  currdate  datetime  date  today  day  int  input  Enter  day  month  int  input  Enter  month  year  int  input  Enter  year  birthdate  datetime  date  year  month  day  no  of  days  abs  currdate  birthdate  days  def  is  legal  no  of  days  legal  age  21  365  if  no  of  days  legal  age  return  True  return  False  if  is  legal  no  of  days  print  Adult  else  print  Not  an  adult  Write  program  that  will  generate  random  playing  card  Hearts  Queen  Spades  when  the  return  key  is  pressed  Note  Rather  than  generate  random  number  from  to  52  Create  two  random  numbers  one  for  the  suit  and  one  for  the  card  from  random  import  randint  suits  Hearts  Spades  Diamond  Clubs  card,microsoft
s  Ace  10  10  11  12  13  output  cards  randint  13  suits  randint  print  output  Make  game  of  rock  paper  scissors  against  the  computer  Algorithm  Tell  user  to  enter  either  rock  paper  or  scissors  Get  the  response  Generate  random  number  from  to  rock  paper  scissors  Compare  user  selection  and  computer  selection  Display  who  wins  import  random  choices  rock  paper  scissors  user  input  Enter  rock  paper  scissors  if  user  not  in  choices  print  Invalid  input  else  user  choices  user  cpu  random  randint  if  user  cpu  print  Tie  elif  user  and  cpu  or  user  and  cpu  or  user  and  cpu  print  Computer  wins  else  print  User  wins  For  the  rock  paper  scissors  game  above  ensure  that  the  user  enters  valid  entry  Add  loop  structure  to  play  several  times  and  keep  running  score  import  random  choices  rock  paper  scissors  while  True  user  input  Enter  rock  paper  scissors  type  END  to  exit  if  user  END  print  Exit  break,microsoft
  elif  user  not  in  choices  print  Invalid  input  else  user  choices  user  cpu  random  randint  if  user  cpu  print  Tie  elif  user  and  cpu  or  user  and  cpu  or  user  and  cpu  print  Computer  wins  else  print  User  wins  Write  program  that  will  give  the  answer  to  logic  gate  questions  Enter  logic  gate  OR  Enter  first  input  Enter  second  input  Result  It  should  work  for  the  logic  gates  OR  XOR  NAND  and  NOR  gates  logic  gates  OR  AND  XOR  NAND  NOR  XNOR  logic  gate  input  Enter  logic  gate  result  None  if  logic  gate  not  in  logic  gates  print  Not  valid  logic  gate  else  valid  first  int  input  Enter  first  input  second  int  input  Enter  second  input  if  not  first  in  valid  and  second  in  valid  print  Please  enter  ones  and  zeroes  only  else  if  logic  gate  OR  if  first  or  second  result  else  result  elif  logic  gate  AND  if  first  and  second  result  else  result  elif  logic  gate  XOR  if  first  second  result  el,microsoft
se  result  elif  logic  gate  NAND  if  first  and  second  result  else  result  elif  logic  gate  NOR  if  first  and  second  result  else  result  elif  logic  gate  XNOR  if  first  second  result  else  result  print  Result  format  result  Write  program  that  will  display  all  the  factors  of  an  input  number  that  are  bigger  than  the  factors  of  the  number  12  are  and  because  they  divide  into  12  exactly  Hint  To  find  out  whether  number  is  factor  of  use  If  mod  there  is  nothing  remaining  when  is  divided  by  def  factor  num  arr  for  in  range  num  if  num  arr  append  return  arr  num  int  input  Enter  number  print  Factors  for  in  factor  num  print  end  print  ,microsoft
import  http  client  import  urllib  parse  import  uuid  import  jsondef  azure  translate  target  text  api  key  YOUR  API  KEY  HERE  requestBody  Text  text  content  json  dumps  requestBody  ensure  ascii  False  encode  utf  host  api  cognitive  microsofttranslator  com  path  translate  api  version  params  to  target  headers  Ocp  Apim  Subscription  Key  api  key  Content  type  application  json  ClientTraceId  str  uuid  uuid4  conn  http  client  HTTPSConnection  host  conn  request  POST  path  params  content  headers  response  conn  getresponse  output  json  loads  response  read  translations  text  return  outputinput  text  Azure  result  azure  translate  en  input  text  Print  translated  text  print  result  ,microsoft
import  os  os  system  aws  s3  cp  s3  sagemaker  workshop  pdx  mnist  utils  py  utils  py  os  system  aws  s3  cp  s3  sagemaker  workshop  pdx  mnist  mnist  py  mnist  py  import  sagemaker  import  utils  import  numpy  as  np  import  matplotlib  pyplot  as  plt  from  tensorflow  contrib  learn  python  learn  datasets  import  mnist  import  tensorflow  as  tf  import  boto3  role  sagemaker  get  execution  role  sagemaker  session  sagemaker  Session  os  system  aws  s3  cp  recursive  s3  sagemaker  workshop  pdx  mnist  data  data  data  sets  mnist  read  data  sets  mnist  data  dtype  tf  uint8  reshape  False  validation  size  5000  utils  convert  to  data  sets  train  train  mnist  data  utils  convert  to  data  sets  validation  validation  mnist  data  utils  convert  to  data  sets  test  test  mnist  data  cat  utils  pybatch  xs  batch  ys  data  sets  train  next  batch  Change  train  to  test  or  select  different  batch  utils  gen  image  batch  xs  show  utils  gen  image,amazon
  batch  xs  show  utils  gen  image  batch  xs  show  inputs  sagemaker  session  upload  data  path  mnist  data  key  prefix  data  mnist  cat  mnist  py  from  sagemaker  tensorflow  import  TensorFlow  mnist  estimator  TensorFlow  entry  point  mnist  py  role  role  training  steps  1000  evaluation  steps  100  train  instance  count  train  instance  type  ml  c4  8xlarge  mnist  estimator  fit  inputs  mnist  predictor  mnist  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  tensorflow  examples  tutorials  mnist  import  input  data  mnist  input  data  read  data  sets  tmp  data  one  hot  True  for  in  range  10  data  mnist  test  images  tolist  tensor  proto  tf  make  tensor  proto  values  np  asarray  data  shape  len  data  dtype  tf  float32  predict  response  mnist  predictor  predict  tensor  proto  image  mnist  test  images  image  np  array  image  dtype  float  plt  imshow  image  reshape  28  28  plt  show  label  np  argmax  mnist  test  labels,amazon
  print  Label  is  format  label  prediction  predict  response  outputs  classes  int64Val  print  Prediction  is  format  prediction  print  sagemaker  Session  delete  endpoint  mnist  predictor  endpoint  ,amazon
http  blog  aloni  org  posts  backprop  with  tensorflow  https  medium  com  karpathy  yes  you  should  understand  backprop  e2f06eab496b  b3rvzhx89  import  tensorflow  as  tf  tf  set  random  seed  777  reproducibility  Check  out  https  www  tensorflow  org  get  started  mnist  beginners  for  more  information  about  the  mnist  dataset  from  tensorflow  examples  tutorials  mnist  import  input  data  mnist  input  data  read  data  sets  MNIST  data  one  hot  True  tf  placeholder  tf  float32  None  784  tf  placeholder  tf  float32  None  10  w1  tf  Variable  tf  truncated  normal  784  30  b1  tf  Variable  tf  truncated  normal  30  w2  tf  Variable  tf  truncated  normal  30  10  b2  tf  Variable  tf  truncated  normal  10  def  sigma  sigmoid  function  return  tf  div  tf  constant  tf  add  tf  constant  tf  exp  def  sigma  prime  derivative  of  the  sigmoid  function  return  sigma  sigma  Forward  prop  l1  tf  add  tf  matmul  w1  b1  a1  sigma  l1  l2  tf  add  tf  matmul  a1  w,ibm
2  b2  pred  sigma  l2  diff  assert  pred  shape  as  list  shape  as  list  diff  pred  Back  prop  chain  rule  l2  diff  sigma  prime  l2  b2  l2  w2  tf  matmul  tf  transpose  a1  l2  a1  tf  matmul  l2  tf  transpose  w2  l1  a1  sigma  prime  l1  b1  l1  w1  tf  matmul  tf  transpose  l1  Updating  network  using  gradients  learning  rate  step  tf  assign  w1  w1  learning  rate  w1  tf  assign  b1  b1  learning  rate  tf  reduce  mean  b1  reduction  indices  tf  assign  w2  w2  learning  rate  w2  tf  assign  b2  b2  learning  rate  tf  reduce  mean  b2  reduction  indices  Running  and  testing  the  training  process  acct  mat  tf  equal  tf  argmax  pred  tf  argmax  acct  res  tf  reduce  sum  tf  cast  acct  mat  tf  float32  sess  tf  InteractiveSession  sess  run  tf  global  variables  initializer  for  in  range  10000  batch  xs  batch  ys  mnist  train  next  batch  10  sess  run  step  feed  dict  batch  xs  batch  ys  if  1000  res  sess  run  acct  res  feed  dict  mnist  test  imag,ibm
es  1000  mnist  test  labels  1000  print  res  Automatic  differentiation  in  TensorFlow  cost  diff  diff  step  tf  train  GradientDescentOptimizer  minimize  cost  ,ibm
az  login  az  account  list  output  table  az  account  set  subscription  subscription  id  subscription  name  location  West  Europe  resourceGroup  group2  documentdbName  testdocdbmc1  appserviceplanName  appserviceplanmc1  appserviceplanSKU  S2  webappName  testwebappmc1  whos  az  group  create  name  resourceGroup  location  location  az  documentdb  create  name  documentdbName  resource  group  resourceGroup  kind  MongoDB  az  documentdb  list  keys  name  documentdbName  resource  group  resourceGroupdocumentdb  primarykey  xxxxxxxxxxxxxxx  az  appservice  plan  create  name  appserviceplanName  resource  group  resourceGroup  sku  appserviceplanSKU  az  appservice  web  create  name  webappName  resource  group  resourceGroup  plan  appserviceplanName  az  appservice  web  config  appsettings  update  name  webappName  resource  group  resourceGroup  settings  MONGODB  URI  mongodb  documentdbName  documentdb  primarykey  documentdbName  documents  azure  com  10250  mean  ssl  true  sslverifyc,microsoft
ertificate  false  az  appservice  web  source  control  config  repo  url  https  github  com  prashanthmadi  mean  name  webappName  resource  group  resourceGroup  ,microsoft
def  mysquare  return  ,ibm
pip  install  upgrade  watson  developer  cloud  CLASSIFY  IMAGE  DEMO  import  json  from  watson  developer  cloud  import  VisualRecognitionV3  visual  recognition  VisualRecognitionV3  2016  05  20  api  key  YOUR  VISUAL  RECOGNITION  API  KEY  classes  visual  recognition  classify  parameters  json  dumps  url  https  upload  wikimedia  org  wikipedia  commons  c9  Moon  jpg  classifier  ids  default  threshold  print  json  dumps  classes  indent  DETECT  FACES  IN  IMAGES  import  json  from  watson  developer  cloud  import  VisualRecognitionV3  visual  recognition  VisualRecognitionV3  2016  05  20  api  key  YOUR  VISUAL  RECOGNITION  API  KEY  classes  visual  recognition  detect  faces  parameters  json  dumps  url  http  pngimg  com  uploads  barack  obama  barack  obama  PNG22  png  classifier  ids  default  threshold  print  json  dumps  classes  indent  ANALYZE  GENERAL  TONE  import  json  from  watson  developer  cloud  import  ToneAnalyzerV3  tone  analyzer  ToneAnalyzerV3  username  YOUR,ibm
  WATSON  SERVICE  USERNAME  password  YOUR  WATSON  SERVICE  PASSWORD  version  2017  09  21  tone  output  tone  analyzer  tone  Team  know  that  times  are  tough  Product  sales  have  been  disappointing  for  the  past  three  quarters  We  have  competitive  product  but  we  need  to  do  better  job  of  selling  it  content  type  text  plain  print  json  dumps  tone  output  indent  ANALYZE  CUSTOMER  ENGAGEMENT  TONE  import  json  from  watson  developer  cloud  import  ToneAnalyzerV3  tone  analyzer  ToneAnalyzerV3  username  YOUR  WATSON  SERVICE  USERNAME  password  YOUR  WATSON  SERVICE  PASSWORD  version  2017  09  21  utterances  text  am  very  happy  user  glenn  text  It  is  good  day  user  glenn  tone  chat  analysis  tone  analyzer  tone  chat  utterances  print  json  dumps  tone  chat  analysis  indent  import  json  from  watson  developer  cloud  import  PersonalityInsightsV3  personality  insights  PersonalityInsightsV3  version  2017  10  13  username  YOUR  WATSON  SERVICE  ,ibm
USERNAME  password  YOUR  WATSON  SERVICE  PASSWORD  profile  personality  insights  profile  As  the  holiday  season  approaches  it  is  time  of  year  that  we  can  easily  get  off  track  from  doing  the  daily  habits  that  keep  us  healthy  freezer  full  of  baking  the  pantry  with  extra  treats  life  gets  busy  more  entertaining  and  socializing  it  is  common  that  the  regular  health  disciplines  aren  as  consistent  When  this  happens  do  you  find  yourself  feeling  guilty  often  making  matters  worse  and  making  promises  to  improve  things  in  January  While  January  will  come  with  its  set  of  good  intentions  what  can  you  focus  on  during  the  next  couple  of  weeks  that  is  positive  and  beneficial  to  your  health  Here  are  two  unexpected  health  benefits  that  you  can  enjoy  and  do  regularly  as  you  get  into  the  spirit  of  the  holiday  season  content  type  text  plain  raw  scores  True  consumption  preferences  True  print  jso,ibm
n  dumps  profile  indent  import  json  from  watson  developer  cloud  import  LanguageTranslatorV2  as  LanguageTranslator  language  translator  LanguageTranslator  username  YOUR  WATSON  SERVICE  USERNAME  password  YOUR  WATSON  SERVICE  PASSWORD  translation  language  translator  translate  text  HEY  AM  TRYING  WATSON  LANGUAGE  TRANSLATOR  HELLO  WORLD  model  id  en  es  print  json  dumps  translation  indent  ensure  ascii  False  language  language  translator  identify  HEY  AL  WATSON  AM  LANGUAGE  CONVERSOR  MUNDO  HOLA  print  json  dumps  language  indent  import  json  from  watson  developer  cloud  import  NaturalLanguageUnderstandingV1  from  watson  developer  cloud  natural  language  understanding  v1  import  Features  EntitiesOptions  natural  language  understanding  NaturalLanguageUnderstandingV1  username  YOUR  WATSON  SERVICE  USERNAME  password  YOUR  WATSON  SERVICE  PASSWORD  version  2017  02  27  response  natural  language  understanding  analyze  url  www  cnn  com  ,ibm
features  Features  entities  EntitiesOptions  sentiment  True  limit  print  json  dumps  response  indent  import  json  from  watson  developer  cloud  import  AssistantV1  assistant  AssistantV1  username  YOUR  WATSON  SERVICE  USERNAME  password  YOUR  WATSON  SERVICE  PASSWORD  version  2017  04  21  response  assistant  message  workspace  id  ENTER  YOUR  WATSON  ASSISTANT  WORKSPACE  ID  input  text  Hello  print  json  dumps  response  indent  import  sys  import  os  import  json  from  watson  developer  cloud  import  DiscoveryV1  discovery  DiscoveryV1  username  YOUR  WATSON  DISCOVERY  SERVICE  USERNAME  password  YOUR  WATSON  DISCOVERY  SERVICE  PASSWORD  version  2017  11  07  my  query  discovery  query  environment  id  system  collection  id  news  en  query  IBM  filter  watson  count  print  json  dumps  my  query  indent  from  future  import  print  function  import  IPython  import  json  from  watson  developer  cloud  import  TextToSpeechV1  from  IPython  display  import  Audio ,ibm
 text  to  speech  TextToSpeechV1  username  ENTER  TEXT  TO  SPEECH  USERNAME  password  ENTER  TEXT  TO  SPEECH  PASSWORD  Audio  text  to  speech  synthesize  Hi  am  Allison  You  can  change  from  my  voice  to  different  other  voices  too  accept  audio  wav  voice  en  US  AllisonVoice  content  import  json  from  watson  developer  cloud  import  NaturalLanguageClassifierV1  natural  language  classifier  NaturalLanguageClassifierV1  username  YOUR  WATSON  SERVICE  USERNAME  password  YOUR  WATSON  SERVICE  PASSWORD  classes  natural  language  classifier  classify  YOUR  NLC  CLASSIFIER  ID  How  hot  will  it  be  today  print  json  dumps  classes  indent  import  json  from  watson  developer  cloud  import  SpeechToTextV1  import  urllib  request  speech  to  text  SpeechToTextV1  username  ENTER  SPEECH  TO  TEXT  USERNAME  password  ENTER  SPEECH  TO  TEXT  PASSWORD  url  https  stream  watsonplatform  net  speech  to  text  api  with  urllib  request  urlopen  http  www  pacdv  com  sound,ibm
s  voices  pick  up  the  phone  wav  as  url  audio  file  url  read  print  json  dumps  speech  to  text  recognize  audio  audio  file  content  type  audio  wav  timestamps  True  word  confidence  True  indent  ,ibm
wget  http  files  grouplens  org  datasets  movielens  ml  100k  zip  unzip  ml  100k  zip  cd  ml  100k  shuf  ua  base  ua  base  shuffled  head  10  ua  base  shuffled  head  10  ua  testimport  sagemaker  import  sagemaker  amazon  common  as  smac  from  sagemaker  import  get  execution  role  from  sagemaker  predictor  import  json  deserializer  import  boto3  csv  io  json  import  numpy  as  np  from  scipy  sparse  import  lil  matrixnbUsers  943  nbMovies  1682  nbFeatures  nbUsers  nbMovies  nbRatingsTrain  90570  nbRatingsTest  9430  For  each  user  build  list  of  rated  movies  We  need  this  to  add  random  negative  samples  moviesByUser  for  userId  in  range  nbUsers  moviesByUser  str  userId  with  open  ua  base  shuffled  as  samples  csv  reader  delimiter  for  userId  movieId  rating  timestamp  in  samples  moviesByUser  str  int  userId  append  int  movieId  def  loadDataset  filename  lines  columns  Features  are  one  hot  encoded  in  sparse  matrix  lil  matrix  lines,amazon
  columns  astype  float32  Labels  are  stored  in  vector  line  with  open  filename  as  samples  csv  reader  delimiter  for  userId  movieId  rating  timestamp  in  samples  line  int  userId  line  int  nbUsers  int  movieId  if  int  rating  append  else  append  line  line  np  array  astype  float32  return  YX  train  train  loadDataset  ua  base  shuffled  nbRatingsTrain  nbFeatures  test  test  loadDataset  ua  test  nbRatingsTest  nbFeatures  print  train  shape  print  train  shape  assert  train  shape  nbRatingsTrain  nbFeatures  assert  train  shape  nbRatingsTrain  zero  labels  np  count  nonzero  train  print  Training  labels  zeros  ones  zero  labels  nbRatingsTrain  zero  labels  print  test  shape  print  test  shape  assert  test  shape  nbRatingsTest  nbFeatures  assert  test  shape  nbRatingsTest  zero  labels  np  count  nonzero  test  print  Test  labels  zeros  ones  zero  labels  nbRatingsTest  zero  labels  bucket  YOUR  S3  BUCKET  prefix  sagemaker  fm  movielens  train  ke,amazon
y  train  protobuf  train  prefix  format  prefix  train3  test  key  test  protobuf  test  prefix  format  prefix  test3  output  prefix  s3  output  format  bucket  prefix  def  writeDatasetToProtobuf  bucket  prefix  key  buf  io  BytesIO  smac  write  spmatrix  to  sparse  tensor  buf  buf  seek  obj  format  prefix  key  boto3  resource  s3  Bucket  bucket  Object  obj  upload  fileobj  buf  return  s3  format  bucket  obj  train  data  writeDatasetToProtobuf  train  train  bucket  train  prefix  train  key  test  data  writeDatasetToProtobuf  test  test  bucket  test  prefix  test  key  print  train  data  print  test  data  print  Output  format  output  prefix  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  factorization  machines  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  factorization  machines  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  factorization  machines  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  ,amazon
com  factorization  machines  latest  fm  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  get  execution  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  output  prefix  sagemaker  session  sagemaker  Session  fm  set  hyperparameters  feature  dim  nbFeatures  predictor  type  binary  classifier  mini  batch  size  1000  num  factors  64  epochs  100  fm  fit  train  train  data  test  test  data  fm  predictor  fm  deploy  instance  type  ml  c4  xlarge  initial  instance  count  def  fm  serializer  data  js  instances  for  row  in  data  js  instances  append  features  row  tolist  print  js  return  json  dumps  js  fm  predictor  content  type  application  json  fm  predictor  serializer  fm  serializer  fm  predictor  deserializer  json  deserializerresult  fm  predictor  predict  test  1000  1010  toarray  print  result  print  test  1000  1010  ,amazon
import  os  import  boto3  from  pyspark  import  SparkContext  SparkConf  from  pyspark  sql  import  SparkSession  import  sagemaker  from  sagemaker  import  get  execution  role  import  sagemaker  pyspark  role  get  execution  role  Configure  Spark  to  use  the  SageMaker  Spark  dependency  jars  jars  sagemaker  pyspark  classpath  jars  classpath  join  sagemaker  pyspark  classpath  jars  See  the  SageMaker  Spark  Github  to  learn  how  to  connect  to  EMR  from  notebook  instance  spark  SparkSession  builder  config  spark  driver  extraClassPath  classpath  master  local  getOrCreate  sparkimport  boto3  region  boto3  Session  region  name  trainingData  spark  read  format  libsvm  option  numFeatures  784  load  s3a  sagemaker  sample  data  spark  mnist  train  format  region  testData  spark  read  format  libsvm  option  numFeatures  784  load  s3a  sagemaker  sample  data  spark  mnist  test  format  region  trainingData  show  from  sagemaker  pyspark  import  IAMRole  from  sagema,amazon
ker  pyspark  algorithms  import  KMeansSageMakerEstimator  from  sagemaker  pyspark  import  RandomNamePolicyFactory  Create  Means  Estimator  kmeans  estimator  KMeansSageMakerEstimator  sagemakerRole  IAMRole  role  trainingInstanceType  ml  m4  xlarge  Instance  type  to  train  means  on  SageMaker  trainingInstanceCount  endpointInstanceType  ml  t2  large  Instance  type  to  serve  model  endpoint  for  inference  endpointInitialInstanceCount  namePolicyFactory  RandomNamePolicyFactory  sparksm  1a  All  the  resources  created  are  prefixed  with  sparksm  Set  parameters  for  Means  kmeans  estimator  setFeatureDim  784  kmeans  estimator  setK  10  Train  initialModel  kmeans  estimator  fit  trainingData  initialModelEndpointName  initialModel  endpointName  print  initialModelEndpointName  Run  inference  on  the  test  data  and  show  some  results  transformedData  initialModel  transform  testData  transformedData  show  from  pyspark  sql  types  import  DoubleType  import  matplotlib  py,amazon
plot  as  plt  import  numpy  as  np  import  string  Helper  function  to  display  digit  def  showDigit  img  caption  xlabel  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axes  get  xaxis  set  ticks  subplot  axes  get  yaxis  set  ticks  plt  title  caption  plt  xlabel  xlabel  subplot  imshow  imgr  cmap  gray  def  displayClusters  data  images  np  array  data  select  features  cache  take  250  clusters  data  select  closest  cluster  cache  take  250  for  cluster  in  range  10  print  nCluster  format  string  ascii  uppercase  cluster  digits  img  for  img  in  zip  clusters  images  if  int  closest  cluster  cluster  height  len  digits  width  plt  rcParams  figure  figsize  width  height  subplots  plt  subplots  height  width  subplots  np  ndarray  flatten  subplots  for  subplot  image  in  zip  subplots  digits  showDigit  image  subplot  subplot  for  subplot  in  subplots  len  digits  subplot  axis  off  plt  show  displayClusters ,amazon
 transformedData  ENDPOINT  NAME  initialModelEndpointName  print  ENDPOINT  NAME  from  sagemaker  pyspark  import  SageMakerModel  from  sagemaker  pyspark  import  EndpointCreationPolicy  from  sagemaker  pyspark  transformation  serializers  import  ProtobufRequestRowSerializer  from  sagemaker  pyspark  transformation  deserializers  import  KMeansProtobufResponseRowDeserializer  attachedModel  SageMakerModel  existingEndpointName  ENDPOINT  NAME  endpointCreationPolicy  EndpointCreationPolicy  DO  NOT  CREATE  endpointInstanceType  None  Required  endpointInitialInstanceCount  None  Required  requestRowSerializer  ProtobufRequestRowSerializer  featuresColumnName  features  Optional  already  default  value  responseRowDeserializer  KMeansProtobufResponseRowDeserializer  Optional  already  default  values  distance  to  cluster  column  name  distance  to  cluster  closest  cluster  column  name  closest  cluster  transformedData2  attachedModel  transform  testData  transformedData2  show  from  sagemak,amazon
er  pyspark  import  S3DataPath  MODEL  S3  PATH  S3DataPath  initialModel  modelPath  bucket  initialModel  modelPath  objectPath  MODEL  ROLE  ARN  initialModel  modelExecutionRoleARN  MODEL  IMAGE  PATH  initialModel  modelImage  print  MODEL  S3  PATH  bucket  MODEL  S3  PATH  objectPath  print  MODEL  ROLE  ARN  print  MODEL  IMAGE  PATH  from  sagemaker  pyspark  import  RandomNamePolicy  retrievedModel  SageMakerModel  modelPath  MODEL  S3  PATH  modelExecutionRoleARN  MODEL  ROLE  ARN  modelImage  MODEL  IMAGE  PATH  endpointInstanceType  ml  t2  medium  endpointInitialInstanceCount  requestRowSerializer  ProtobufRequestRowSerializer  responseRowDeserializer  KMeansProtobufResponseRowDeserializer  namePolicy  RandomNamePolicy  sparksm  1b  endpointCreationPolicy  EndpointCreationPolicy  CREATE  ON  TRANSFORM  transformedData3  retrievedModel  transform  testData  transformedData3  show  TRAINING  JOB  NAME  YOUR  TRAINING  JOB  NAME  MODEL  ROLE  ARN  initialModel  modelExecutionRoleARN  MODEL  IMAGE ,amazon
 PATH  initialModel  modelImagemodelFromJob  SageMakerModel  fromTrainingJob  trainingJobName  TRAINING  JOB  NAME  modelExecutionRoleARN  MODEL  ROLE  ARN  modelImage  MODEL  IMAGE  PATH  endpointInstanceType  ml  t2  medium  endpointInitialInstanceCount  requestRowSerializer  ProtobufRequestRowSerializer  responseRowDeserializer  KMeansProtobufResponseRowDeserializer  namePolicy  RandomNamePolicy  sparksm  1c  endpointCreationPolicy  EndpointCreationPolicy  CREATE  ON  TRANSFORM  transformedData4  modelFromJob  transform  testData  transformedData4  show  Delete  the  resources  from  sagemaker  pyspark  import  SageMakerResourceCleanup  def  cleanUp  model  resource  cleanup  SageMakerResourceCleanup  model  sagemakerClient  resource  cleanup  deleteResources  model  getCreatedResources  Don  forget  to  include  any  models  or  pipeline  models  that  you  created  in  the  notebook  models  initialModel  attachedModel  retrievedModel  modelFromJob  Delete  regular  SageMakerModels  for  in  models  clea,amazon
nUp  ,amazon
import  numpy  as  np  import  pandas  as  pd  from  sklearn  import  preprocessing  from  sklearn  model  selection  import  train  test  split  from  sklearn  neighbors  import  KNeighborsClassifier  from  sklearn  metrics  import  accuracy  score  import  matplotlib  pyplot  as  plt  import  seaborn  as  sns  from  mlxtend  plotting  import  plot  decision  regionsiris  pd  read  csv  Iris  csv  iris  head  iris  info  iris  Species  value  counts  iris  drop  Id  axis  inplace  True  iris  head  sns  pairplot  iris  hue  Species  plt  show  sns  violinplot  Species  SepalLengthCm  data  iris  inner  quartile  plt  show  sns  violinplot  Species  SepalWidthCm  data  iris  inner  quartile  plt  show  sns  violinplot  Species  PetalLengthCm  data  iris  inner  quartile  plt  show  sns  violinplot  Species  PetalWidthCm  data  iris  inner  quartile  plt  show  le  preprocessing  LabelEncoder  iris  Species  le  fit  transform  iris  Species  iris  head  iris  drop  Species  axis  iris  Species  train  test  t,amazon
rain  test  train  test  split  test  size  range  np  arange  26  score  for  in  range  knn  KNeighborsClassifier  neighbors  knn  fit  train  train  pred  knn  predict  test  score  append  accuracy  score  test  pred  optim  min  range  score  np  max  score  sns  pointplot  range  score  plt  xlabel  Value  of  for  KNN  plt  ylabel  Accuracy  Score  plt  title  Accuracy  Scores  for  Values  of  of  Nearest  Neighbors  plt  show  print  The  optimum  value  of  is  optim  knn  KNeighborsClassifier  neighbors  optim  knn  fit  train  train  pred  knn  predict  test  accuracy  score  pred  test  plot  data  np  array  loc  SepalLengthCm  SepalWidthCm  plot  target  np  array  knn  plot  KNeighborsClassifier  neighbors  optim  knn  plot  fit  plot  data  plot  target  plot  decision  regions  plot  data  plot  target  clf  knn  plot  res  legend  plt  title  Plot  of  Decision  Boundaries  plt  xlabel  SepalLengthCm  plt  ylabel  SepalWidthCm  plt  show  ,amazon
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  kmeans  byom  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  numpy  as  np  import  sklearn  cluster  import  pickle  import  gzip  import  urllib  request  import  json  import  mxnet  as  mx  import  boto3  import  time  import  io  import  osurllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  gzip  open  mnist  pkl  gz  rb  train  set  valid  set  test  set  pickle  load  encoding  latin1  close  kmeans  sklearn  cluster  KMeans  clusters  10  fit  train  set  centroids  mx  ndarray  array  kmeans  cluster  centers  mx  ndarray  save  model  algo  centroids  tar  czvf  model  tar  gz  model  algo  1boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  model  tar  gz  upload  file  model  tar  gz  kmeans  model  DEMO  kmeans  byom  time  strftime  time  gmtime  sm  boto3  client  sagemaker  con,amazon
tainers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  kmeans  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  kmeans  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  kmeans  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  kmeans  latest  create  model  response  sm  create  model  ModelName  kmeans  model  ExecutionRoleArn  role  PrimaryContainer  Image  containers  boto3  Session  region  name  ModelDataUrl  s3  model  tar  gz  format  bucket  prefix  print  create  model  response  ModelArn  kmeans  endpoint  config  DEMO  kmeans  byom  endpoint  config  time  strftime  time  gmtime  print  kmeans  endpoint  config  create  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  kmeans  endpoint  config  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  kmeans  model  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  tim,amazon
e  kmeans  endpoint  DEMO  kmeans  byom  endpoint  time  strftime  time  gmtime  print  kmeans  endpoint  create  endpoint  response  sm  create  endpoint  EndpointName  kmeans  endpoint  EndpointConfigName  kmeans  endpoint  config  print  create  endpoint  response  EndpointArn  resp  sm  describe  endpoint  EndpointName  kmeans  endpoint  status  resp  EndpointStatus  print  Status  status  sm  get  waiter  endpoint  in  service  wait  EndpointName  kmeans  endpoint  resp  sm  describe  endpoint  EndpointName  kmeans  endpoint  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Status  status  if  status  InService  raise  Exception  Endpoint  creation  did  not  succeed  def  np2csv  arr  csv  io  BytesIO  np  savetxt  csv  arr  delimiter  fmt  return  csv  getvalue  decode  rstrip  runtime  boto3  Session  client  runtime  sagemaker  payload  np2csv  train  set  100  response  runtime  invoke  endpoint  EndpointName  kmeans  endpoint  ContentType  text  csv  Body  payload  result  json  ,amazon
loads  response  Body  read  decode  scored  labels  np  array  closest  cluster  for  in  result  predictions  scored  labels  kmeans  labels  100  Remove  endpoint  to  avoid  stray  charges  sm  delete  endpoint  EndpointName  kmeans  endpoint  ,amazon
time  import  boto3  import  re  from  sagemaker  import  get  execution  role  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  role  get  execution  role  bucket  deeplens  worksite  safety  customize  to  your  bucket  training  image  get  image  uri  boto3  Session  region  name  image  classification  print  training  image  The  algorithm  supports  multiple  network  depth  number  of  layers  They  are  18  34  50  101  152  and  200  For  this  training  we  will  use  101  layers  num  layers  50  we  need  to  specify  the  input  image  shape  for  the  training  data  image  shape  224  224  we  also  need  to  specify  the  number  of  training  samples  in  the  training  set  for  our  dataset  this  is  5308  An  easy  way  to  get  this  is  to  wc  dataset  train  lst  num  training  samples  5308  specify  the  number  of  output  classes  num  classes  batch  size  for  training  mini  batch  size  128  number  of  epochs  epochs  learning  rate  learning  rate  01  t,amazon
op  Since  we  are  using  transfer  learning  we  set  use  pretrained  model  to  so  that  weights  can  be  initialized  with  pre  trained  weights  use  pretrained  model  augmentation  type  crop  color  transform  time  import  time  import  boto3  from  time  import  gmtime  strftime  s3  boto3  client  s3  create  unique  job  name  job  name  prefix  deeplens  worksite  safety  224  50  CCT  timestamp  time  strftime  time  gmtime  job  name  job  name  prefix  timestamp  training  params  specify  the  training  docker  image  AlgorithmSpecification  TrainingImage  training  image  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  s3  output  format  bucket  job  name  prefix  ResourceConfig  InstanceCount  InstanceType  ml  p3  16xlarge  VolumeSizeInGB  50  TrainingJobName  job  name  HyperParameters  image  shape  image  shape  num  layers  str  num  layers  num  training  samples  str  num  training  samples  num  classes  str  num  classes  mini  batch  size  str  mini  b,amazon
atch  size  epochs  str  epochs  learning  rate  str  learning  rate  use  pretrained  model  str  use  pretrained  model  augmentation  type  augmentation  type  StoppingCondition  MaxRuntimeInSeconds  360000  Training  data  should  be  inside  subdirectory  called  train  Validation  data  should  be  inside  subdirectory  called  validation  The  algorithm  currently  only  supports  fullyreplicated  model  where  data  is  copied  onto  each  machine  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  recordio  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  recordio  CompressionType  None  print  Training  job  name  format  job  name  print  nInput  Data  Location  format  training  params  InputDataConfig  DataSource ,amazon
 S3DataSource  time  create  the  Amazon  SageMaker  training  job  sagemaker  boto3  client  service  name  sagemaker  sagemaker  create  training  job  training  params  confirm  that  the  training  job  has  started  status  sagemaker  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  Training  job  current  status  format  status  try  wait  for  the  job  to  finish  and  report  the  ending  status  sagemaker  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  except  print  Training  failed  to  start  if  exception  is  raised  that  means  it  has  failed  message  sagemaker  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  time  import  boto3  from  time  import  gmtime  strft,amazon
ime  sage  boto3  Session  client  service  name  sagemaker  model  name  deeplens  WorkSiteSafety  print  model  name  info  sage  describe  training  job  TrainingJobName  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  data  hosting  image  get  image  uri  boto3  Session  region  name  image  classification  primary  container  Image  hosting  image  ModelDataUrl  model  data  create  model  response  sage  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  timestamp  time  strftime  time  gmtime  endpoint  config  name  job  name  prefix  epc  timestamp  endpoint  config  response  sage  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  t2  medium  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  configuration  name  format  endpoint  config  name  pri,amazon
nt  Endpoint  configuration  arn  format  endpoint  config  response  EndpointConfigArn  time  import  time  timestamp  time  strftime  time  gmtime  endpoint  name  job  name  prefix  ep  timestamp  print  Endpoint  name  format  endpoint  name  endpoint  params  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  endpoint  response  sagemaker  create  endpoint  endpoint  params  print  EndpointArn  format  endpoint  response  EndpointArn  get  the  status  of  the  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  response  EndpointStatus  print  EndpointStatus  format  status  wait  until  the  status  has  changed  sagemaker  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  print  the  status  of  the  endpoint  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  endpoint  response  EndpointStatus  print  Endpoint  creation  ended  with  EndpointStatus  format  status  if  status  InSer,amazon
vice  raise  Exception  Endpoint  creation  failed  import  boto3  runtime  boto3  Session  client  service  name  runtime  sagemaker  aws  s3  cp  recursive  s3  devdays  worksite  safety  test  tmp  testimport  json  import  numpy  as  np  from  IPython  display  import  Image  file  name  tmp  test  sample  image1  jpg  display  Image  file  name  width  200  with  open  file  name  rb  as  payload  read  payload  bytearray  payload  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  image  Body  payload  result  response  Body  read  result  will  be  in  json  format  and  convert  it  to  ndarray  result  json  loads  result  the  result  will  output  the  probabilities  for  all  classes  find  the  class  with  maximum  probability  and  print  the  class  index  index  np  argmax  result  inference  categories  compliant  not  compliant  unknown  nosubject  print  Result  label  inference  categories  index  probability  str  result  index  sage  delete  end,amazon
point  EndpointName  endpoint  name  ,amazon
matplotlib  inline  import  tensorflow  as  tf  import  os  import  matplotlib  pyplot  as  plt  import  matplotlib  patheffects  as  PathEffects  import  matplotlib  import  numpy  as  np  from  sklearn  decomposition  import  PCA  from  sklearn  manifold  import  TSNE  from  tensorflow  examples  tutorials  mnist  import  input  data  mnist  input  data  read  data  sets  MNIST  data  one  hot  True  current  dir  os  getcwd  init  op  tf  global  variables  initializer  def  weight  variable  shape  name  return  tf  Variable  tf  truncated  normal  shape  shape  stddev  name  def  bias  variable  shape  name  return  tf  Variable  tf  constant  shape  shape  name  tf  placeholder  tf  float32  shape  None  784  weight  variable  784  300  bias  variable  300  layer1  tf  nn  relu  tf  matmul  weight  variable  300  100  bias  variable  100  layer2  tf  nn  relu  tf  matmul  layer1  bias  variable  100  20  bias  variable  20  code  layer  tf  nn  relu  tf  matmul  layer2  weight  variable  20  100  bias  ,microsoft
variable  100  layer1  tf  nn  relu  tf  matmul  code  layer  weight  variable  100  300  bias  variable  300  layer2  tf  nn  relu  tf  matmul  layer1  weight  variable  300  784  bias  variable  784  output  layer  tf  nn  relu  tf  matmul  layer2  loss  function  loss  tf  reduce  mean  tf  pow  output  layer  optimizer  tf  train  RMSPropOptimizer  01  minimize  loss  init  op  tf  global  variables  initializer  sess  tf  InteractiveSession  sess  run  init  op  for  in  range  20000  batch  mnist  train  next  batch  50  if  100  print  step  loss  loss  eval  feed  dict  batch  optimizer  run  feed  dict  batch  print  final  loss  loss  eval  feed  dict  mnist  test  images  import  numpy  as  np  import  matplotlib  pyplot  as  plt  matplotlib  inline  trainimg  mnist  train  images  trainlabel  mnist  train  labels  output  nd  output  layer  eval  feed  dict  mnist  train  images  for  in  curr  img  np  reshape  trainimg  28  28  ae  img  np  reshape  output  nd  28  28  curr  label  np  argmax  t,microsoft
rainlabel  plt  matshow  curr  img  cmap  plt  get  cmap  gray  plt  matshow  ae  img  cmap  plt  get  cmap  gray  ,microsoft
bucket  your  own  s3  bucket  name  here  prefix  sagemaker  videogames  xgboost  import  sagemaker  role  sagemaker  get  execution  role  import  numpy  as  np  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  from  IPython  display  import  Image  from  IPython  display  import  display  from  sklearn  datasets  import  dump  svmlight  file  from  time  import  gmtime  strftime  import  sys  import  math  import  json  import  boto3raw  data  filename  Video  Games  Sales  as  at  22  Dec  2016  csv  data  bucket  sagemaker  workshop  pdx  s3  boto3  resource  s3  s3  Bucket  data  bucket  download  file  raw  data  filename  raw  data  csv  data  pd  read  csv  raw  data  csv  pd  set  option  display  max  rows  20  datadata  data  Global  Sales  data  shape  data  plt  bar  not  hit  hit  data  value  counts  plt  show  viz  data  filter  User  Score  Critic  Score  Global  Sales  axis  viz  User  Score  pd  Series  viz  User  Score  apply  pd  to  numeric  errors  coerce  viz  User  Score ,amazon
 viz  User  Score  mask  np  isnan  viz  User  Score  viz  Critic  Score  10  viz  plot  kind  scatter  logx  True  logy  True  Critic  Score  Global  Sales  viz  plot  kind  scatter  logx  True  logy  True  User  Score  Global  Sales  plt  show  data  data  drop  Name  Year  of  Release  NA  Sales  EU  Sales  JP  Sales  Other  Sales  Global  Sales  Critic  Count  User  Count  Developer  axis  data  isnull  sum  data  data  dropna  data  User  Score  data  User  Score  apply  pd  to  numeric  errors  coerce  data  User  Score  data  User  Score  mask  np  isnan  data  User  Score  data  Critic  Score  10  data  data  apply  lambda  yes  if  True  else  no  model  data  pd  get  dummies  data  train  data  validation  data  test  data  np  split  model  data  sample  frac  random  state  1729  int  len  model  data  int  len  model  data  dump  svmlight  file  train  data  drop  no  yes  axis  train  data  yes  train  libsvm  dump  svmlight  file  validation  data  drop  no  yes  axis  validation  data  yes  v,amazon
alidation  libsvm  dump  svmlight  file  test  data  drop  no  yes  axis  test  data  yes  test  libsvm  boto3  Session  resource  s3  Bucket  bucket  Object  prefix  train  train  libsvm  upload  file  train  libsvm  boto3  Session  resource  s3  Bucket  bucket  Object  prefix  validation  validation  libsvm  upload  file  validation  libsvm  job  name  videogames  xgboost  strftime  gmtime  print  Training  job  job  name  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  xgboost  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  xgboost  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  xgboost  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  xgboost  latest  create  training  params  RoleArn  role  TrainingJobName  job  name  AlgorithmSpecification  TrainingImage  containers  boto3  Session  region  name  TrainingInputMode  File  ResourceConfig  InstanceCount  InstanceType  ml  c4  xlarge  VolumeSizeInGB  10  InputDataConfig  Ch,amazon
annelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  S3DataDistributionType  FullyReplicated  ContentType  libsvm  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  prefix  S3DataDistributionType  FullyReplicated  ContentType  libsvm  CompressionType  None  OutputDataConfig  S3OutputPath  s3  xgboost  video  games  output  format  bucket  prefix  HyperParameters  max  depth  eta  eval  metric  auc  scale  pos  weight  subsample  objective  binary  logistic  num  round  100  StoppingCondition  MaxRuntimeInSeconds  60  60  time  sm  boto3  client  sagemaker  sm  create  training  job  create  training  params  status  sm  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  status  try  sm  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  finally  status  sm  describe  training  job  TrainingJobName  job  name  ,amazon
TrainingJobStatus  print  Training  job  ended  with  status  status  if  status  Failed  message  sm  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  create  model  response  sm  create  model  ModelName  job  name  ExecutionRoleArn  role  PrimaryContainer  Image  containers  boto3  Session  region  name  ModelDataUrl  sm  describe  training  job  TrainingJobName  job  name  ModelArtifacts  S3ModelArtifacts  print  create  model  response  ModelArn  xgboost  endpoint  config  videogames  xgboost  endpoint  config  strftime  gmtime  print  xgboost  endpoint  config  create  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  xgboost  endpoint  config  ProductionVariants  InstanceType  ml  t2  medium  InitialInstanceCount  ModelName  job  name  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  ti,amazon
me  xgboost  endpoint  EXAMPLE  videogames  xgb  endpoint  strftime  gmtime  print  xgboost  endpoint  create  endpoint  response  sm  create  endpoint  EndpointName  xgboost  endpoint  EndpointConfigName  xgboost  endpoint  config  print  create  endpoint  response  EndpointArn  resp  sm  describe  endpoint  EndpointName  xgboost  endpoint  status  resp  EndpointStatus  print  Status  status  try  sm  get  waiter  endpoint  in  service  wait  EndpointName  xgboost  endpoint  finally  resp  sm  describe  endpoint  EndpointName  xgboost  endpoint  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Status  status  if  status  InService  message  sm  describe  endpoint  EndpointName  xgboost  endpoint  FailureReason  print  Endpoint  creation  failed  with  the  following  error  format  message  raise  Exception  Endpoint  creation  did  not  succeed  runtime  boto3  client  runtime  sagemaker  def  do  predict  data  endpoint  name  content  type  payload  join  data  response  runtime  invoke,amazon
  endpoint  EndpointName  endpoint  name  ContentType  content  type  Body  payload  result  response  Body  read  result  result  decode  utf  result  result  split  preds  float  num  for  num  in  result  preds  round  num  for  num  in  preds  return  preds  def  batch  predict  data  batch  size  endpoint  name  content  type  items  len  data  arrs  for  offset  in  range  items  batch  size  if  offset  batch  size  items  results  do  predict  data  offset  offset  batch  size  endpoint  name  content  type  arrs  extend  results  else  arrs  extend  do  predict  data  offset  items  endpoint  name  content  type  sys  stdout  write  return  arrs  time  import  json  with  open  test  libsvm  as  payload  read  strip  labels  int  line  split  for  line  in  payload  split  test  data  line  for  line  in  payload  split  preds  batch  predict  test  data  100  xgboost  endpoint  text  libsvm  print  nerror  rate  sum  for  in  range  len  preds  if  preds  labels  float  len  preds  pd  crosstab  ind,amazon
ex  np  array  labels  columns  np  array  preds  sm  delete  endpoint  EndpointName  xgboost  endpoint  ,amazon
import  os  from  pyspark  import  SparkContext  SparkConf  from  pyspark  sql  import  SparkSession  import  sagemaker  from  sagemaker  import  get  execution  role  import  sagemaker  pyspark  role  get  execution  role  Configure  Spark  to  use  the  SageMaker  Spark  dependency  jars  jars  sagemaker  pyspark  classpath  jars  classpath  join  sagemaker  pyspark  classpath  jars  See  the  SageMaker  Spark  Github  repo  under  sagemaker  pyspark  sdk  to  learn  how  to  connect  to  remote  EMR  cluster  running  Spark  from  Notebook  Instance  spark  SparkSession  builder  config  spark  driver  extraClassPath  classpath  master  local  getOrCreate  import  boto3  region  boto3  Session  region  name  trainingData  spark  read  format  libsvm  option  numFeatures  784  option  vectorType  dense  load  s3a  sagemaker  sample  data  spark  mnist  train  format  region  testData  spark  read  format  libsvm  option  numFeatures  784  option  vectorType  dense  load  s3a  sagemaker  sample  data  spark ,amazon
 mnist  test  format  region  trainingData  show  import  random  from  sagemaker  pyspark  import  IAMRole  S3DataPath  from  sagemaker  pyspark  algorithms  import  XGBoostSageMakerEstimator  xgboost  estimator  XGBoostSageMakerEstimator  sagemakerRole  IAMRole  role  trainingInstanceType  ml  m4  xlarge  trainingInstanceCount  endpointInstanceType  ml  m4  xlarge  endpointInitialInstanceCount  xgboost  estimator  setEta  xgboost  estimator  setGamma  xgboost  estimator  setMinChildWeight  xgboost  estimator  setSilent  xgboost  estimator  setObjective  multi  softmax  xgboost  estimator  setNumClasses  10  xgboost  estimator  setNumRound  10  train  model  xgboost  estimator  fit  trainingData  transformedData  model  transform  trainingData  transformedData  show  from  pyspark  sql  types  import  DoubleType  import  matplotlib  pyplot  as  plt  import  numpy  as  np  helper  function  to  display  digit  def  show  digit  img  caption  xlabel  subplot  None  if  subplot  None  subplot  plt  subplots  im,amazon
gr  img  reshape  28  28  subplot  axes  get  xaxis  set  ticks  subplot  axes  get  yaxis  set  ticks  plt  title  caption  plt  xlabel  xlabel  subplot  imshow  imgr  cmap  gray  images  np  array  transformedData  select  features  cache  take  250  clusters  transformedData  select  prediction  cache  take  250  for  cluster  in  range  10  print  nCluster  format  int  cluster  digits  img  for  img  in  zip  clusters  images  if  int  prediction  cluster  height  len  digits  width  plt  rcParams  figure  figsize  width  height  subplots  plt  subplots  height  width  subplots  np  ndarray  flatten  subplots  for  subplot  image  in  zip  subplots  digits  show  digit  image  subplot  subplot  for  subplot  in  subplots  len  digits  subplot  axis  off  plt  show  Delete  the  endpoint  from  sagemaker  pyspark  import  SageMakerResourceCleanup  resource  cleanup  SageMakerResourceCleanup  model  sagemakerClient  resource  cleanup  deleteResources  model  getCreatedResources  ,amazon
bucket  eduthie  sagemaker  prefix  gluon  recommender  import  sagemaker  role  sagemaker  get  execution  role  import  os  import  mxnet  as  mx  from  mxnet  import  gluon  nd  ndarray  from  mxnet  metric  import  MSE  import  pandas  as  pd  import  numpy  as  np  import  sagemaker  from  sagemaker  mxnet  import  MXNet  import  boto3  import  json  import  matplotlib  pyplot  as  pltopt  sgd  lr  02  momentum  wd  MXNet  recommender  py  py  version  py3  role  role  train  instance  count  train  instance  type  ml  p3  8xlarge  output  path  s3  output  format  bucket  prefix  hyperparameters  num  embeddings  512  opt  opt  lr  lr  momentum  momentum  wd  wd  epochs  10  fit  train  s3  train  format  bucket  prefix  minutes  60  60  25  price  361  minutes  60  price  ,amazon
IMP  Rename  the  dataframe  from  df  data  xxx  to  cars  for  easy  usage  in  rest  of  the  notebook  cars  df  data  xxximport  brunel  brunel  data  cars  mpg  horsepower  color  origin  width  800  height  300  brunel  data  cars  horsepower  weight  color  origin  tooltip  name  width  800  height  300  brunel  data  cars  origin  year  chord  size  count  color  origin  width  500  height  400def  identify  search  for  in  search  if  lower  in  lower  return  return  None  cars  Type  cars  name  map  lambda  identify  Ford  Buick  cars  head  brunel  data  cars  engine  mpg  color  Type  style  size  50  fill  eee  engine  mpg  color  Type  style  text  font  size  14  font  weight  bold  fill  darker  width  800  height  300  ,ibm
time  import  sys  sys  path  append  home  ec2  user  anaconda3  lib  python3  site  packages  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  cnidus  ml  iad  customize  to  your  bucket  Set  your  target  containers  us  west  107995894928  dkr  ecr  us  west  amazonaws  com  object  detection  containers  us  east  366895301435  dkr  ecr  us  west  amazonaws  com  object  detection  training  image  containers  boto3  Session  region  name  import  requests  import  os  data  dir  data  tools  dir  object  detection  URLList  src  http  www  robots  ox  ac  uk  vgg  data  pets  data  images  tar  gz  dst  data  dir  src  http  www  robots  ox  ac  uk  vgg  data  pets  data  annotations  tar  gz  dst  data  dir  src  https  raw  githubusercontent  com  tensorflow  models  master  research  object  detection  data  pet  label  map  pbtxt  dst  tools  dir  src  https  raw  githubusercontent  com  tensorflow  models  master  research  object  dete,amazon
ction  dataset  tools  create  pet  tf  record  py  dst  src  https  raw  githubusercontent  com  tensorflow  models  master  research  object  detection  utils  dataset  util  py  dst  str  tools  dir  utils  src  https  raw  githubusercontent  com  tensorflow  models  master  research  object  detection  utils  label  map  util  py  dst  str  tools  dir  utils  src  https  raw  githubusercontent  com  tensorflow  models  master  research  object  detection  protos  string  int  label  map  proto  dst  str  tools  dir  protos  Download  each  file  for  URL  in  URLList  Create  the  dst  directory  if  it  doesnt  exist  if  not  os  path  exists  URL  dst  os  makedirs  URL  dst  fname  URL  dst  URL  src  split  print  Downloading  str  URL  src  to  fname  requests  get  URL  src  stream  True  with  open  fname  wb  as  write  content  print  Finished  downloading  training  dataset  files  tfr  dir  tfrecord  import  tarfile  fileList  data  images  tar  gz  data  annotations  tar  gz  for  file  in  f,amazon
ileList  print  Extracting  file  tar  tarfile  open  file  tar  extractall  data  dir  tar  close  print  Finished  Create  the  TFRecord  output  directory  if  it  doesnt  exist  if  not  os  path  exists  tfr  dir  os  makedirs  tfr  dir  print  Done  bash  From  tensorflow  models  research  python3  create  pet  tf  record  py  label  map  path  object  detection  pet  label  map  pbtxt  data  dir  data  output  dir  tfrecord  sh  The  name  of  our  algorithm  algorithm  name  decision  trees  sample  cd  container  chmod  decision  trees  train  chmod  decision  trees  serve  account  aws  sts  get  caller  identity  query  Account  output  text  Get  the  region  defined  in  the  current  configuration  default  to  us  west  if  none  defined  region  aws  configure  get  region  region  region  us  west  fullname  account  dkr  ecr  region  amazonaws  com  algorithm  name  latest  If  the  repository  doesn  exist  in  ECR  create  it  aws  ecr  describe  repositories  repository  names  algorithm,amazon
  name  dev  null  if  ne  then  aws  ecr  create  repository  repository  name  algorithm  name  dev  null  fi  Get  the  login  command  from  ECR  and  execute  it  directly  aws  ecr  get  login  region  region  no  include  email  Build  the  docker  image  locally  with  the  image  name  and  then  push  it  to  ECR  with  the  full  name  On  SageMaker  Notebook  Instance  the  docker  daemon  may  need  to  be  restarted  in  order  to  detect  your  network  configuration  correctly  This  is  known  issue  if  home  ec2  user  SageMaker  then  sudo  service  docker  restart  fi  docker  build  algorithm  name  docker  tag  algorithm  name  fullname  docker  push  fullname  For  this  training  we  will  run  it  for  10  minutes  so  as  to  have  demo  of  it  max  run  time  600  time  import  time  import  boto3  from  time  import  gmtime  strftime  s3  boto3  client  s3  create  unique  job  name  job  name  prefix  object  detection  notebook  timestamp  time  strftime  time  gmtime  job  nam,amazon
e  job  name  prefix  timestamp  training  params  specify  the  training  docker  image  AlgorithmSpecification  TrainingImage  training  image  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  s3  output  format  bucket  job  name  prefix  ResourceConfig  InstanceCount  InstanceType  ml  p3  2xlarge  VolumeSizeInGB  50  TrainingJobName  job  name  HyperParameters  max  run  time  str  max  run  time  after  this  time  training  job  will  terminate  itself  StoppingCondition  MaxRuntimeInSeconds  20  60  20  minutes  After  this  sagemaker  will  stop  training  Training  data  should  be  inside  subdirectory  called  train  Validation  data  should  be  inside  subdirectory  called  validation  The  algorithm  currently  only  supports  fullyreplicated  model  where  data  is  copied  onto  each  machine  InputDataConfig  ChannelName  training  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  pet  detection  data  tf  record  format  bucket  S3DataDistributionType  FullyR,amazon
eplicated  ContentType  application  recordio  CompressionType  None  print  Training  job  name  format  job  name  print  nInput  Data  Location  format  training  params  InputDataConfig  DataSource  S3DataSource  create  the  Amazon  SageMaker  training  job  sagemaker  boto3  client  service  name  sagemaker  sagemaker  create  training  job  training  params  confirm  that  the  training  job  has  started  status  sagemaker  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  Training  job  current  status  format  status  try  wait  for  the  job  to  finish  and  report  the  ending  status  sagemaker  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  except  print  Training  failed  to  start  if  exception  is  raised  that  means  it  has  failed  message  sagemake,amazon
r  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  ,amazon
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  fm  mnist  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  30  This  is  format  train  set  30  import  io  import  numpy  as  np  import  sagemaker  amazon  common  as  smac  vectors  np  array  tolist  for  in  train  set  astype  float32  labels  np  where  np  array  tolist  for  in ,amazon
 train  set  astype  float32  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  vectors  labels  buf  seek  import  boto3  import  os  key  recordio  pb  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  buf  s3  train  data  s3  train  format  bucket  prefix  key  print  uploaded  training  data  location  format  s3  train  data  output  location  s3  output  format  bucket  prefix  print  training  artifacts  will  be  uploaded  to  format  output  location  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  factorization  machines  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  factorization  machines  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  factorization  machines  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  factorization  machines  latest  import  boto3  import  sagemaker  sess  sagemaker  Session  fm  sagemaker  estimator  Estimator  containers ,amazon
 boto3  Session  region  name  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  output  location  sagemaker  session  sess  fm  set  hyperparameters  feature  dim  784  predictor  type  binary  classifier  mini  batch  size  200  num  factors  10  fm  fit  train  s3  train  data  fm  predictor  fm  deploy  initial  instance  count  instance  type  ml  m4  xlarge  import  json  from  sagemaker  predictor  import  json  deserializer  def  fm  serializer  data  js  instances  for  row  in  data  js  instances  append  features  row  tolist  return  json  dumps  js  fm  predictor  content  type  application  json  fm  predictor  serializer  fm  serializer  fm  predictor  deserializer  json  deserializerresult  fm  predictor  predict  train  set  30  31  print  result  import  numpy  as  np  predictions  for  array  in  np  array  split  test  set  100  result  fm  predictor  predict  array  predictions  predicted  label  for  in  result  predictions  predictions  np  array  predi,amazon
ctions  import  pandas  as  pd  pd  crosstab  np  where  test  set  predictions  rownames  actuals  colnames  predictions  import  sagemaker  sagemaker  Session  delete  endpoint  fm  predictor  endpoint  ,amazon
from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  shanxingprojects  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  30  This  is  format  train  set  30  Bucket  location  to  save  your  custom  code  in  tar  gz  format  custom  code  upload  location  s3  shanxingprojects  mxnet  code  output  Bucket  location  where  results  of  model  training  are  saved  model  artifacts  location  s3  shanxingprojects  model  result  IAM  execu,amazon
tion  role  that  gives  SageMaker  access  to  resources  in  your  AWS  account  We  can  use  the  SageMaker  Python  SDK  to  get  the  role  from  our  notebook  environment  role  get  execution  role  print  training  data  will  be  uploaded  to  format  custom  code  upload  location  print  training  artifacts  will  be  uploaded  to  format  model  artifacts  location  cat  mnist  py  time  from  sagemaker  mxnet  import  MXNet  mnist  estimator  MXNet  entry  point  mnist  py  role  role  output  path  model  artifacts  location  code  location  custom  code  upload  location  train  instance  count  train  instance  type  ml  m4  xlarge  hyperparameters  learning  rate  time  import  boto3  region  boto3  Session  region  name  train  data  location  s3  sagemaker  sample  data  mxnet  mnist  train  format  region  test  data  location  s3  sagemaker  sample  data  mxnet  mnist  test  format  region  mnist  estimator  fit  train  train  data  location  test  test  data  location  time  predictor ,amazon
 mnist  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  IPython  display  import  HTML  HTML  open  input  html  read  response  predictor  predict  data  print  Raw  prediction  result  print  response  labeled  predictions  list  zip  range  10  response  print  Labeled  predictions  print  labeled  predictions  labeled  predictions  sort  key  lambda  label  and  prob  label  and  prob  print  Most  likely  answer  format  labeled  predictions  print  predictor  endpoint  import  sagemaker  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
bucket  cyrusmv  sagemaker  demos  prefix  visa  kaggle  original  csv  protocol  s3  datafile  data  original  csv  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  print  role  bucket  prefix  aws  s3  ls  cyrusmv  sagemaker  demos  visa  kaggle  import  os  os  getcwd  Downloading  the  file  to  local  folder  import  boto3  client  boto3  client  s3  with  open  datafile  wb  as  client  download  fileobj  bucket  prefix  loading  data  into  pandas  for  inspection  import  pandas  as  pd  df  pd  read  csv  datafile  df  head  df  Class  count  df  df  Class  Class  count  Converting  Data  Into  Numpy  import  numpy  as  np  raw  data  df  as  matrix  print  raw  data  shape  Shuffling  the  data  np  random  seed  123  np  random  shuffle  raw  data  raw  data  for  in  range  len  if  append  import  matplotlib  pyplot  as  plt  import  seaborn  as  sns  sns  distplot  kde  True  rug  True  hist  False  plt  show  import  io  im,amazon
port  numpy  as  np  import  sagemaker  amazon  common  as  smac  vectors  np  array  tolist  for  in  train  set  astype  float32  labels  np  where  np  array  tolist  for  in  train  set  astype  float32  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  vectors  labels  buf  seek  import  boto3  import  os  key  recordio  pb  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  buf  s3  train  data  s3  train  format  bucket  prefix  key  print  uploaded  training  data  location  format  s3  train  data  output  location  s3  output  format  bucket  prefix  print  training  artifacts  will  be  uploaded  to  format  output  location  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  linear  learner  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  eu  west  438346466558  dkr  ecr  eu  west  ama,amazon
zonaws  com  linear  learner  latest  import  boto3  import  sagemaker  sess  sagemaker  Session  linear  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  output  location  sagemaker  session  sess  linear  set  hyperparameters  feature  dim  784  predictor  type  binary  classifier  mini  batch  size  200  linear  fit  train  s3  train  data  linear  predictor  linear  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  sagemaker  predictor  import  csv  serializer  json  deserializer  linear  predictor  content  type  text  csv  linear  predictor  serializer  csv  serializer  linear  predictor  deserializer  json  deserializerresult  linear  predictor  predict  train  set  30  31  print  result  import  numpy  as  np  predictions  for  array  in  np  array  split  test  set  100  result  linear  predictor  predict  array  predictions  predicted  label  for  in  result  predictions,amazon
  predictions  np  array  predictions  import  pandas  as  pd  pd  crosstab  np  where  test  set  predictions  rownames  actuals  colnames  predictions  import  sagemaker  sagemaker  Session  delete  endpoint  linear  predictor  endpoint  ,amazon
from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  sagemaker  XXXXXXXXXXXXXXXX  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  30  This  is  format  train  set  30  from  sagemaker  import  KMeans  data  location  s3  kmeans  highlevel  example  data  format  bucket  output  location  s3  kmeans  example  output  format  bucket  print  training  data  will  be  uploaded  to  format  data  location  print  training  artifacts  will  be  ,amazon
uploaded  to  format  output  location  kmeans  KMeans  role  role  train  instance  count  train  instance  type  ml  c4  8xlarge  output  path  output  location  10  data  location  data  location  time  kmeans  fit  kmeans  record  set  train  set  time  kmeans  predictor  kmeans  deploy  initial  instance  count  instance  type  ml  m4  xlarge  result  kmeans  predictor  predict  valid  set  31  32  print  result  time  result  kmeans  predictor  predict  valid  set  100  clusters  label  closest  cluster  float32  tensor  values  for  in  result  for  cluster  in  range  10  print  nCluster  format  int  cluster  digits  img  for  img  in  zip  clusters  valid  set  if  int  cluster  height  len  digits  width  plt  rcParams  figure  figsize  width  height  subplots  plt  subplots  height  width  subplots  numpy  ndarray  flatten  subplots  for  subplot  image  in  zip  subplots  digits  show  digit  image  subplot  subplot  for  subplot  in  subplots  len  digits  subplot  axis  off  plt  show  ,amazon
Basic  set  up  to  define  IAM  Role  import  boto3  import  re  import  os  import  numpy  as  np  import  pandas  as  pd  from  sagemaker  import  get  execution  role  role  get  execution  role  Create  SageMaker  Session  that  will  be  used  to  perform  all  SageMaker  operations  import  sagemaker  as  sage  from  time  import  gmtime  strftime  sess  sage  Session  account  sess  boto  session  client  sts  get  caller  identity  Account  region  sess  boto  session  region  name  Note  make  sure  to  include  the  Docker  image  tag  eg  latest  since  there  seem  to  be  some  issues  with  deploying  model  if  you  don  include  the  tag  image  dkr  ecr  amazonaws  com  npng  sagemaker  repo  latest  format  account  region  sagemaker  ml  sage  estimator  Estimator  image  role  ml  c4  2xlarge  output  path  s3  output  format  sess  default  bucket  sagemaker  session  sess  AutoML  only  requires  training  data  during  the  training  process  Additional  data  can  be  ingested  later ,amazon
 for  prediction  Specify  location  of  the  data  with  dictionary  where  the  value  is  the  path  to  the  s3  bucket  containing  the  training  data  data  location  training  s3  h2o  sagemaker  npng  higgs  train  10k  csv  sagemaker  ml  fit  data  location  Deploying  an  actual  predictor  so  that  we  can  make  predictions  on  test  data  here  from  sagemaker  predictor  import  csv  serializer  predictor  sagemaker  ml  deploy  ml  m4  xlarge  serializer  csv  serializer  import  io  s3  boto3  client  s3  obj  s3  get  object  Bucket  h2o  sagemaker  npng  Key  higgs  test  5k  csv  df  pd  read  csv  io  BytesIO  obj  Body  read  np  array  df  columns  reshape  29  test  vals  df  values  valid  np  append  test  vals  axis  preds  predictor  predict  valid  decode  utf  preds  list  preds  split  full  preds  one  row  for  item  in  preds  list  if  in  item  rmloc  item  find  item  item  rmloc  one  row  append  item  full  preds  append  one  row  one  row  else  one  row  append  i,amazon
tem  full  preds  only  run  for  cleanup  deletes  the  endpoint  for  the  predictor  sess  delete  endpoint  predictor  endpoint  ,amazon
time  import  sagemaker  from  sagemaker  import  get  execution  role  role  get  execution  role  print  role  sess  sagemaker  Session  bucket  your  s3  bucket  name  here  custom  bucket  name  bucket  sess  default  bucket  prefix  DEMO  ObjectDetection  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  training  image  get  image  uri  sess  boto  region  name  object  detection  repo  version  latest  print  training  image  time  Download  the  dataset  wget  tmp  http  host  robots  ox  ac  uk  pascal  VOC  voc2012  VOCtrainval  11  May  2012  tar  wget  tmp  http  host  robots  ox  ac  uk  pascal  VOC  voc2007  VOCtrainval  06  Nov  2007  tar  wget  tmp  http  host  robots  ox  ac  uk  pascal  VOC  voc2007  VOCtest  06  Nov  2007  tar  Extract  the  data  tar  xf  tmp  VOCtrainval  11  May  2012  tar  rm  tmp  VOCtrainval  11  May  2012  tar  tar  xf  tmp  VOCtrainval  06  Nov  2007  tar  rm  tmp  VOCtrainval  06  Nov  2007  tar  tar  xf  tmp  VOCtest  06  Nov  2007  tar  rm  tmp,amazon
  VOCtest  06  Nov  2007  tar  python  tools  prepare  dataset  py  dataset  pascal  year  2007  2012  set  trainval  target  VOCdevkit  train  lst  rm  rf  VOCdevkit  VOC2012  python  tools  prepare  dataset  py  dataset  pascal  year  2007  set  test  target  VOCdevkit  val  lst  no  shuffle  rm  rf  VOCdevkit  VOC2007  head  VOCdevkit  train  lst  example  lst  open  example  lst  lst  content  read  print  lst  content  python  tools  im2rec  py  pack  label  num  thread  your  lst  file  name  your  image  folder  time  Upload  the  RecordIO  files  to  train  and  validation  channels  train  channel  prefix  train  validation  channel  prefix  validation  sess  upload  data  path  VOCdevkit  train  rec  bucket  bucket  key  prefix  train  channel  sess  upload  data  path  VOCdevkit  val  rec  bucket  bucket  key  prefix  validation  channel  s3  train  data  s3  format  bucket  train  channel  s3  validation  data  s3  format  bucket  validation  channel  s3  output  location  s3  output  format  buck,amazon
et  prefix  od  model  sagemaker  estimator  Estimator  training  image  role  train  instance  count  train  instance  type  ml  p3  2xlarge  train  volume  size  50  train  max  run  360000  input  mode  File  output  path  s3  output  location  sagemaker  session  sess  od  model  set  hyperparameters  base  network  resnet  50  use  pretrained  model  num  classes  20  mini  batch  size  32  epochs  learning  rate  001  lr  scheduler  step  lr  scheduler  factor  optimizer  sgd  momentum  weight  decay  0005  overlap  threshold  nms  threshold  45  image  shape  300  label  width  350  num  training  samples  16551  train  data  sagemaker  session  s3  input  s3  train  data  distribution  FullyReplicated  content  type  application  recordio  s3  data  type  S3Prefix  validation  data  sagemaker  session  s3  input  s3  validation  data  distribution  FullyReplicated  content  type  application  recordio  s3  data  type  S3Prefix  data  channels  train  train  data  validation  validation  data  od  mode,amazon
l  fit  inputs  data  channels  logs  True  object  detector  od  model  deploy  initial  instance  count  instance  type  ml  m4  xlarge  wget  test  jpg  https  images  pexels  com  photos  980382  pexels  photo  980382  jpeg  file  name  test  jpg  with  open  file  name  rb  as  image  image  read  bytearray  ne  open  txt  wb  ne  write  import  json  object  detector  content  type  image  jpeg  results  object  detector  predict  detections  json  loads  results  print  detections  def  visualize  detection  img  file  dets  classes  thresh  visualize  detections  in  one  image  Parameters  img  numpy  array  image  in  bgr  format  dets  numpy  array  ssd  detections  numpy  array  id  score  x1  y1  x2  y2  each  row  is  one  object  classes  tuple  or  list  of  str  class  names  thresh  float  score  threshold  import  random  import  matplotlib  pyplot  as  plt  import  matplotlib  image  as  mpimg  img  mpimg  imread  img  file  plt  imshow  img  height  img  shape  width  img  shape  colors  ,amazon
dict  for  det  in  dets  klass  score  x0  y0  x1  y1  det  if  score  thresh  continue  cls  id  int  klass  if  cls  id  not  in  colors  colors  cls  id  random  random  random  random  random  random  xmin  int  x0  width  ymin  int  y0  height  xmax  int  x1  width  ymax  int  y1  height  rect  plt  Rectangle  xmin  ymin  xmax  xmin  ymax  ymin  fill  False  edgecolor  colors  cls  id  linewidth  plt  gca  add  patch  rect  class  name  str  cls  id  if  classes  and  len  classes  cls  id  class  name  classes  cls  id  plt  gca  text  xmin  ymin  3f  format  class  name  score  bbox  dict  facecolor  colors  cls  id  alpha  fontsize  12  color  white  plt  show  object  categories  aeroplane  bicycle  bird  boat  bottle  bus  car  cat  chair  cow  diningtable  dog  horse  motorbike  person  pottedplant  sheep  sofa  train  tvmonitor  Setting  threshold  20  will  only  plot  detection  results  that  have  confidence  score  greater  than  20  threshold  20  Visualize  the  detections  visualize  dete,amazon
ction  file  name  detections  prediction  object  categories  threshold  sagemaker  Session  delete  endpoint  object  detector  endpoint  ,amazon
Lab  XOR  import  tensorflow  as  tf  import  numpy  as  np  tf  set  random  seed  777  for  reproducibility  learning  rate  01  data  data  data  np  array  data  dtype  np  float32  data  np  array  data  dtype  np  float32  tf  placeholder  tf  float32  None  tf  placeholder  tf  float32  None  tf  Variable  tf  random  normal  name  weight  tf  Variable  tf  random  normal  name  bias  Hypothesis  using  sigmoid  tf  div  tf  exp  tf  matmul  hypothesis  tf  sigmoid  tf  matmul  cost  loss  function  cost  tf  reduce  mean  tf  log  hypothesis  tf  log  hypothesis  train  tf  train  GradientDescentOptimizer  learning  rate  learning  rate  minimize  cost  Accuracy  computation  True  if  hypothesis  else  False  predicted  tf  cast  hypothesis  dtype  tf  float32  accuracy  tf  reduce  mean  tf  cast  tf  equal  predicted  dtype  tf  float32  Launch  graph  sess  tf  Session  Initialize  TensorFlow  variables  sess  run  tf  global  variables  initializer  for  step  in  range  10001  sess  run  train  ,ibm
feed  dict  data  data  if  step  1000  print  step  sess  run  cost  feed  dict  data  data  print  sess  run  Accuracy  report  sess  run  hypothesis  predicted  accuracy  feed  dict  data  data  print  nHypothesis  nCorrect  nAccuracy  Hypothesis  Correct  Accuracy  ,ibm
import  requests  import  time  import  matplotlib  import  matplotlib  pyplot  as  plt  from  matplotlib  patches  import  Polygon  from  PIL  import  Image  matplotlib  inline  matplotlib  rcParams  figure  figsize  12  def  azure  handwritten  text  image  api  key  YOUR  API  KEY  HERE  region  westeurope  with  open  image  rb  as  image  file  content  image  file  read  headers  dict  headers  Content  Type  application  octet  stream  headers  Ocp  Apim  Subscription  Key  api  key  params  handwriting  true  response  requests  request  post  https  api  cognitive  microsoft  com  vision  v1  RecognizeText  format  region  data  content  headers  headers  params  params  analysis  while  not  recognitionResult  in  analysis  response  final  requests  get  response  headers  Operation  Location  headers  headers  analysis  response  final  json  time  sleep  polygons  line  boundingBox  line  text  for  line  in  analysis  recognitionResult  lines  return  polygonsimg  handwritingsample  png  result ,microsoft
 azure  handwritten  text  img  handwriting  True  def  plot  overlay  img  polygons  remove  long  boxes  False  remove  short  boxes  False  color  fontsize  28  alpha  boxcolor  Function  that  overlays  text  labels  on  original  image  remove  long  boxes  bool  will  ignore  labels  with  bounding  boxes  that  are  wider  than  half  the  image  width  Depending  on  the  image  and  the  API  service  being  used  this  may  be  necessary  to  prevent  overlapping  redundant  boxes  from  being  displayed  remove  short  boxes  bool  will  ignore  labels  with  bounding  boxes  that  are  thinner  than  quarter  of  the  image  width  Depending  on  the  image  and  the  API  service  being  used  this  may  be  necessary  to  prevent  redundant  small  boxes  from  being  displayed  The  fontsize  variable  should  be  manually  adjusted  to  fit  the  image  text  size  image  Image  open  img  ax  plt  imshow  image  alpha  alpha  for  polygon  in  polygons  vertices  polygon  polygon  for  in  ra,microsoft
nge  len  polygon  text  polygon  if  remove  long  boxes  if  vertices  vertices  image  size  continue  if  remove  short  boxes  if  vertices  vertices  image  size  continue  patch  Polygon  vertices  closed  True  fill  False  linewidth  color  boxcolor  ax  axes  add  patch  patch  plt  text  vertices  vertices  vertices  vertices  text  fontsize  fontsize  color  color  va  center  ha  center  plt  axis  off  returnplt  imshow  Image  open  img  plt  axis  off  plot  overlay  img  result  fontsize  24  Print  just  the  text  for  item  in  result  print  item  ,microsoft
import  mxnet  as  mx  import  numpy  as  np  from  glob  import  glob  import  boto3  download  train  files  both  images  and  labels  from  s3  directory  prefer  directory  per  Sagemaker  conventions  s3  boto3  client  s3  bucket  jakechenawspublic  images  prefix  sample  data  mnist  train  images  response  s3  list  objects  v2  Bucket  bucket  Prefix  images  prefix  keys  Key  for  in  response  Contents  for  key  in  keys  fname  key  replace  images  prefix  lstrip  create  local  file  name  by  removing  prefix  from  key  name  s3  download  file  bucket  key  fname  download  train  file  labels  prefix  sample  data  mnist  train  labels  response  s3  list  objects  v2  Bucket  bucket  Prefix  labels  prefix  keys  Key  for  in  response  Contents  for  key  in  keys  fname  key  replace  labels  prefix  lstrip  create  local  file  name  by  removing  prefix  from  key  name  s3  download  file  bucket  key  fname  download  train  file  load  downloaded  files  into  numpy  arrays  fna,amazon
mes  glob  images  csv  arrays  np  array  np  loadtxt  delimiter  for  in  fnames  fnames  glob  labels  csv  arrays  np  array  np  loadtxt  delimiter  for  in  fnames  reshape  into  requisite  shape  for  NN  train  arrays  reshape  28  28  train  arrays  reshape  wrap  mxnet  iterator  around  records  batch  size  100  train  iter  mx  io  NDArrayIter  train  1000  train  1000  batch  size  shuffle  True  val  iter  mx  io  NDArrayIter  train  1000  train  1000  batch  size  define  network  data  mx  sym  var  data  first  conv  layer  conv1  mx  sym  Convolution  data  data  kernel  num  filter  20  tanh1  mx  sym  Activation  data  conv1  act  type  tanh  pool1  mx  sym  Pooling  data  tanh1  pool  type  max  kernel  stride  second  conv  layer  conv2  mx  sym  Convolution  data  pool1  kernel  num  filter  50  tanh2  mx  sym  Activation  data  conv2  act  type  tanh  pool2  mx  sym  Pooling  data  tanh2  pool  type  max  kernel  stride  first  fullc  layer  flatten  mx  sym  flatten  data  pool2  fc,amazon
1  mx  symbol  FullyConnected  data  flatten  num  hidden  500  tanh3  mx  sym  Activation  data  fc1  act  type  tanh  second  fullc  fc2  mx  sym  FullyConnected  data  tanh3  num  hidden  10  softmax  loss  lenet  mx  sym  SoftmaxOutput  data  fc2  name  softmax  create  trainable  module  lenet  model  mx  mod  Module  symbol  lenet  context  mx  cpu  change  to  mx  gpu  if  using  ml  p2  xlarge  train  with  the  same  lenet  model  fit  train  iter  eval  data  val  iter  optimizer  sgd  optimizer  params  learning  rate  eval  metric  acc  batch  end  callback  mx  callback  Speedometer  batch  size  100  num  epoch  load  test  images  s3  boto3  client  s3  bucket  jakechenawspublic  images  prefix  sample  data  mnist  test  images  response  s3  list  objects  v2  Bucket  bucket  Prefix  images  prefix  keys  Key  for  in  response  Contents  for  key  in  keys  fname  key  replace  images  prefix  lstrip  create  local  file  name  by  removing  prefix  from  key  name  s3  download  file  bucke,amazon
t  key  fname  download  train  file  load  downloaded  files  in  this  case  file  into  np  array  fnames  glob  images  sm  csv  arrays  np  array  np  loadtxt  delimiter  for  in  fnames  reshape  into  requisite  shape  for  NN  test  mx  nd  array  arrays  reshape  28  28  test  shape  wrap  mxnet  iterator  around  records  test  iter  mx  io  NDArrayIter  test  None  predict  function  for  lenet  prob  lenet  model  predict  test  iter  print  prob  ,amazon
import  tensorflow  as  tf  import  numpy  as  np  import  matplotlib  pyplot  as  plt  import  matplotlib  gridspec  as  gridspec  from  tensorflow  examples  tutorials  mnist  import  input  data  mnist  input  data  read  data  sets  MNIST  data  one  hot  True  matplotlib  inline  def  weight  variable  shape  name  return  tf  Variable  tf  truncated  normal  shape  shape  stddev  name  def  bias  variable  shape  name  return  tf  Variable  tf  zeros  shape  shape  name  batch  size  256  dim  128  tf  placeholder  tf  float32  shape  None  784  tf  placeholder  tf  float32  shape  None  128  weights  d1  weight  variable  784  128  d1  d2  weight  variable  128  d2  g1  weight  variable  128  256  g1  g2  weight  variable  256  784  g2  biases  d1  bias  variable  128  d1  d2  bias  variable  d2  g1  bias  variable  256  g1  g2  bias  variable  784  g2  var  weights  d1  weights  d2  biases  d1  biases  d2  var  weights  g1  weights  g2  biases  g1  biases  g2  def  generator  g1  tf  nn  relu  tf  add,microsoft
  tf  matmul  weights  g1  biases  g1  g2  tf  nn  sigmoid  tf  add  tf  matmul  g1  weights  g2  biases  g2  return  g2  def  discriminator  d1  tf  nn  relu  tf  add  tf  matmul  weights  d1  biases  d1  d2  tf  nn  sigmoid  tf  add  tf  matmul  d1  weights  d2  biases  d2  return  d2  def  sample  return  np  random  uniform  size  sample  generator  real  discriminator  fake  discriminator  sample  loss  tf  reduce  mean  tf  log  real  tf  log  fake  loss  tf  reduce  mean  tf  log  fake  discriminator  optimizer  tf  train  AdamOptimizer  0005  minimize  loss  var  list  var  generator  optimizer  tf  train  AdamOptimizer  0001  minimize  loss  var  list  var  def  plot  samples  fig  plt  figure  figsize  gs  gridspec  GridSpec  gs  update  wspace  05  hspace  05  for  sample  in  enumerate  samples  ax  plt  subplot  gs  plt  axis  off  ax  set  xticklabels  ax  set  yticklabels  ax  set  aspect  equal  plt  imshow  sample  reshape  28  28  cmap  gray  plt  show  sess  tf  InteractiveSession  init  op,microsoft
  tf  global  variables  initializer  sess  run  init  op  for  step  in  range  20001  batch  mnist  train  next  batch  batch  size  loss  train  sess  run  optimizer  loss  feed  dict  batch  sample  batch  size  dim  loss  train  sess  run  optimizer  loss  feed  dict  sample  batch  size  dim  if  step  1000  if  step  100  print  step  discriminator  loss  5f  step  loss  train  print  generator  loss  5f  loss  train  if  step  1000  sample  plot  sample  eval  feed  dict  sample  16  dim  plot  sample  plot  else  if  step  1000  print  step  discriminator  loss  5f  step  loss  train  print  generator  loss  5f  loss  train  if  step  2000  sample  plot  sample  eval  feed  dict  sample  16  dim  plot  sample  plot  ,microsoft
time  import  boto3  from  sagemaker  import  get  execution  role  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  role  get  execution  role  bucket  yourbucket  prefixpath  customize  to  your  bucket  training  image  get  image  uri  boto3  Session  region  name  image  classification  import  os  import  urllib  request  def  download  url  filename  url  split  if  not  os  path  exists  filename  urllib  request  urlretrieve  url  filename  Caltech  256  image  files  download  http  www  vision  caltech  edu  Image  Datasets  Caltech256  256  ObjectCategories  tar  tar  xf  256  ObjectCategories  tar  rm  rf  tmpdata  images  dir  tmpdata  images  images  tmp  dir  tmpdata  images  tmp  train  dir  tmpdata  images  train  val  dir  tmpdata  images  val  train  images  per  label  60  mkdir  images  dir  This  extracts  the  caltech  dataset  to  images  directory  tar  xf  256  ObjectCategories  tar  mv  256  ObjectCategories  images  dir  Tool  for  creating  lst  file  download,amazon
  https  raw  githubusercontent  com  apache  incubator  mxnet  master  tools  im2rec  py  bash  images  dir  images  tmp  dir  train  dir  val  dir  train  images  per  label  images  dir  images  tmp  dir  train  dir  val  dir  train  images  per  label  ls  images  dir  head  echo  copying  data  from  images  dir  to  images  tmp  dir  mkdir  images  tmp  dir  cp  images  dir  images  tmp  dir  mkdir  train  dir  for  in  images  tmp  dir  do  basename  mkdir  train  dir  for  in  ls  jpg  shuf  head  train  images  per  label  do  mv  train  dir  done  done  echo  moving  rest  of  the  data  from  to  validation  directory  val  dir  mv  images  tmp  dir  val  dir  python  im2rec  py  list  recursive  trainlst  train  dir  python  im2rec  py  list  recursive  vallst  val  dir  head  trainlst  lst  Four  channels  train  validation  train  lst  and  validation  lst  s3train  s3  train  format  bucket  s3validation  s3  validation  format  bucket  s3train  lst  s3  train  lst  format  bucket  s3validation,amazon
  lst  s3  validation  lst  format  bucket  echo  train  dir  upload  the  image  files  to  train  and  validation  channels  aws  s3  cp  train  dir  s3train  recursive  quiet  aws  s3  cp  val  dir  s3validation  recursive  quiet  upload  the  lst  files  to  train  lst  and  validation  lst  channels  aws  s3  cp  trainlst  lst  s3train  lst  quiet  aws  s3  cp  vallst  lst  s3validation  lst  quiet  The  algorithm  supports  multiple  network  depth  number  of  layers  They  are  18  34  50  101  152  and  200  For  this  training  we  will  use  18  layers  num  layers  18  we  need  to  specify  the  input  image  shape  for  the  training  data  image  shape  224  224  we  also  need  to  specify  the  number  of  training  samples  in  the  training  set  num  training  samples  15240  specify  the  number  of  output  classes  num  classes  257  batch  size  for  training  mini  batch  size  128  number  of  epochs  epochs  learning  rate  learning  rate  01  report  top  accuracy  top  resize  ima,amazon
ge  before  training  resize  256  period  to  store  model  parameters  in  number  of  epochs  in  this  case  we  will  save  parameters  from  epoch  and  checkpoint  frequency  Since  we  are  using  transfer  learning  we  set  use  pretrained  model  to  so  that  weights  can  be  initialized  with  pre  trained  weights  use  pretrained  model  time  import  time  import  boto3  from  time  import  gmtime  strftime  s3  boto3  client  s3  create  unique  job  name  job  name  prefix  sagemaker  imageclassification  notebook  timestamp  time  strftime  time  gmtime  job  name  job  name  prefix  timestamp  training  params  specify  the  training  docker  image  AlgorithmSpecification  TrainingImage  training  image  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  s3  output  format  bucket  job  name  prefix  ResourceConfig  InstanceCount  InstanceType  ml  p2  xlarge  VolumeSizeInGB  50  TrainingJobName  job  name  HyperParameters  image  shape  image  shape  num  layers  str,amazon
  num  layers  num  training  samples  str  num  training  samples  num  classes  str  num  classes  mini  batch  size  str  mini  batch  size  epochs  str  epochs  learning  rate  str  learning  rate  top  str  top  resize  str  resize  checkpoint  frequency  str  checkpoint  frequency  use  pretrained  model  str  use  pretrained  model  StoppingCondition  MaxRuntimeInSeconds  360000  Training  data  should  be  inside  subdirectory  called  train  Validation  data  should  be  inside  subdirectory  called  validation  The  algorithm  currently  only  supports  fullyreplicated  model  where  data  is  copied  onto  each  machine  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  image  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  ap,amazon
plication  image  CompressionType  None  ChannelName  train  lst  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  lst  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  image  CompressionType  None  ChannelName  validation  lst  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  lst  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  image  CompressionType  None  print  Training  job  name  format  job  name  print  nInput  Data  Location  format  training  params  InputDataConfig  DataSource  S3DataSource  create  the  Amazon  SageMaker  training  job  sagemaker  boto3  client  service  name  sagemaker  sagemaker  create  training  job  training  params  confirm  that  the  training  job  has  started  status  sagemaker  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  Training  job  current  status  format  status  try  wait  for  the  job  to  finish  and  report  the  ending,amazon
  status  sagemaker  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  except  print  Training  failed  to  start  if  exception  is  raised  that  means  it  has  failed  message  sagemaker  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  print  training  info  time  import  boto3  from  time  import  gmtime  strftime  sage  boto3  Session  client  service  name  sagemaker  timestamp  time  strftime  time  gmtime  model  name  image  classification  model  timestamp  print  model  name  info  sage  describe  training  job  TrainingJobName  jo,amazon
b  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  data  hosting  image  get  image  uri  boto3  Session  region  name  image  classification  primary  container  Image  hosting  image  ModelDataUrl  model  data  create  model  response  sage  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  timestamp  time  strftime  time  gmtime  endpoint  config  name  job  name  prefix  epc  timestamp  endpoint  config  response  sage  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  p2  xlarge  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  configuration  name  format  endpoint  config  name  print  Endpoint  configuration  arn  format  endpoint  config  response  EndpointConfigArn  time  import  time  timestamp  time  strftime  time  gmtime  endpoint  name  job  name ,amazon
 prefix  ep  timestamp  print  Endpoint  name  format  endpoint  name  endpoint  params  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  endpoint  response  sagemaker  create  endpoint  endpoint  params  print  EndpointArn  format  endpoint  response  EndpointArn  get  the  status  of  the  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  response  EndpointStatus  print  EndpointStatus  format  status  try  sagemaker  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  finally  resp  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Create  endpoint  ended  with  status  status  if  status  InService  message  sagemaker  describe  endpoint  EndpointName  endpoint  name  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Endpoint  creation  did  not  succeed  import  boto3  runtime  boto3  Sessio,amazon
n  client  service  name  runtime  sagemaker  wget  tmp  test  jpg  http  www  vision  caltech  edu  Image  Datasets  Caltech256  images  008  bathtub  008  0007  jpg  file  name  tmp  test  jpg  test  image  from  IPython  display  import  Image  Image  file  name  import  json  import  numpy  as  np  with  open  file  name  rb  as  payload  read  payload  bytearray  payload  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  image  Body  payload  result  response  Body  read  result  will  be  in  json  format  and  convert  it  to  ndarray  result  json  loads  result  the  result  will  output  the  probabilities  for  all  classes  find  the  class  with  maximum  probability  and  print  the  class  index  index  np  argmax  result  object  categories  ak47  american  flag  backpack  baseball  bat  baseball  glove  basketball  hoop  bat  bathtub  bear  beer  mug  billiards  binoculars  birdbath  blimp  bonsai  101  boom  box  bowling  ball  bowling  pin  boxing ,amazon
 glove  brain  101  breadmaker  buddha  101  bulldozer  butterfly  cactus  cake  calculator  camel  cannon  canoe  car  tire  cartman  cd  centipede  cereal  box  chandelier  101  chess  board  chimp  chopsticks  cockroach  coffee  mug  coffin  coin  comet  computer  keyboard  computer  monitor  computer  mouse  conch  cormorant  covered  wagon  cowboy  hat  crab  101  desk  globe  diamond  ring  dice  dog  dolphin  101  doorknob  drinking  straw  duck  dumb  bell  eiffel  tower  electric  guitar  101  elephant  101  elk  ewer  101  eyeglasses  fern  fighter  jet  fire  extinguisher  fire  hydrant  fire  truck  fireworks  flashlight  floppy  disk  football  helmet  french  horn  fried  egg  frisbee  frog  frying  pan  galaxy  gas  pump  giraffe  goat  golden  gate  bridge  goldfish  golf  ball  goose  gorilla  grand  piano  101  grapes  grasshopper  guitar  pick  hamburger  hammock  harmonica  harp  harpsichord  hawksbill  101  head  phones  helicopter  101  hibiscus  homer  simpson  horse  horseshoe  crab  h,amazon
ot  air  balloon  hot  dog  hot  tub  hourglass  house  fly  human  skeleton  hummingbird  ibis  101  ice  cream  cone  iguana  ipod  iris  jesus  christ  joy  stick  kangaroo  101  kayak  ketch  101  killer  whale  knife  ladder  laptop  101  lathe  leopards  101  license  plate  lightbulb  light  house  lightning  llama  101  mailbox  mandolin  mars  mattress  megaphone  menorah  101  microscope  microwave  minaret  minotaur  motorbikes  101  mountain  bike  mushroom  mussels  necktie  octopus  ostrich  owl  palm  pilot  palm  tree  paperclip  paper  shredder  pci  card  penguin  people  pez  dispenser  photocopier  picnic  table  playing  card  porcupine  pram  praying  mantis  pyramid  raccoon  radio  telescope  rainbow  refrigerator  revolver  101  rifle  rotary  phone  roulette  wheel  saddle  saturn  school  bus  scorpion  101  screwdriver  segway  self  propelled  lawn  mower  sextant  sheet  music  skateboard  skunk  skyscraper  smokestack  snail  snake  sneaker  snowmobile  soccer  ball  socks  soda,amazon
  can  spaghetti  speed  boat  spider  spoon  stained  glass  starfish  101  steering  wheel  stirrups  sunflower  101  superman  sushi  swan  swiss  army  knife  sword  syringe  tambourine  teapot  teddy  bear  teepee  telephone  box  tennis  ball  tennis  court  tennis  racket  theodolite  toaster  tomato  tombstone  top  hat  touring  bike  tower  pisa  traffic  light  treadmill  triceratops  tricycle  trilobite  101  tripod  shirt  tuning  fork  tweezer  umbrella  101  unicorn  vcr  video  projector  washing  machine  watch  101  waterfall  watermelon  welding  mask  wheelbarrow  windmill  wine  bottle  xylophone  yarmulke  yo  yo  zebra  airplanes  101  car  side  101  faces  easy  101  greyhound  tennis  shoes  toad  clutter  print  Result  label  object  categories  index  probability  str  result  index  sage  delete  endpoint  EndpointName  endpoint  name  ,amazon
import  os  import  boto3  import  sagemaker  from  sagemaker  mxnet  import  MXNet  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  Download  the  training  data  We  re  downloading  the  Stanford  Sentiment  dataset  https  nlp  stanford  edu  sentiment  index  html  mkdir  data  curl  https  raw  githubusercontent  com  saurabh3949  Text  Classification  Datasets  master  stsa  binary  phrases  train  data  train  curl  https  raw  githubusercontent  com  saurabh3949  Text  Classification  Datasets  master  stsa  binary  test  data  testinputs  sagemaker  session  upload  data  path  data  key  prefix  data  sentiment  analysis  cat  sentiment  analysis  py  role  get  execution  role  MXNet  sentiment  analysis  py  role  role  train  instance  count  train  instance  type  ml  c5  4xlarge  hyperparameters  batch  size  epochs  learning  rate  01  embedding  size  50  log  interval  1000  fit  inputs  predictor  deploy  initial  instance  count  instance  type  ml  ,amazon
c5  4xlarge  data  this  was  an  awesome  movie  come  on  you  call  this  movie  best  one  ve  seen  in  ages  just  could  not  watch  it  till  the  end  the  movie  was  so  enthralling  response  predictor  predict  data  print  response  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
importing  necessary  libraries  import  pandas  as  pd  import  numpy  as  np  from  bs4  import  BeautifulSoup  import  requests  getting  data  from  internet  wikipedia  link  https  en  wikipedia  org  wiki  List  of  postal  codes  of  Canada  raw  wikipedia  page  requests  get  wikipedia  link  text  using  beautiful  soup  to  parse  the  HTML  XML  codes  soup  BeautifulSoup  raw  wikipedia  page  xml  print  soup  prettify  extracting  the  raw  table  inside  that  webpage  table  soup  find  table  Postcode  Borough  Neighbourhood  print  table  extracting  clean  form  of  the  table  for  tr  cell  in  table  find  all  tr  counter  Postcode  var  Borough  var  Neighbourhood  var  for  td  cell  in  tr  cell  find  all  td  if  counter  Postcode  var  td  cell  text  if  counter  Borough  var  td  cell  text  tag  Borough  td  cell  find  if  counter  Neighbourhood  var  str  td  cell  text  strip  tag  Neighbourhood  td  cell  find  counter  if  Postcode  var  Not  assigned  or  Borough  var  ,ibm
Not  assigned  or  Neighbourhood  var  Not  assigned  continue  try  if  tag  Borough  is  None  or  tag  Neighbourhood  is  None  continue  except  pass  if  Postcode  var  or  Borough  var  or  Neighbourhood  var  continue  Postcode  append  Postcode  var  Borough  append  Borough  var  Neighbourhood  append  Neighbourhood  var  unique  set  Postcode  print  num  of  unique  Postal  codes  len  unique  Postcode  Borough  Neighbourhood  for  postcode  unique  element  in  unique  var  var  var  for  postcode  idx  postcode  element  in  enumerate  Postcode  if  postcode  unique  element  postcode  element  var  postcode  element  var  Borough  postcode  idx  if  var  var  Neighbourhood  postcode  idx  else  var  var  Neighbourhood  postcode  idx  Postcode  append  var  Borough  append  var  Neighbourhood  append  var  toronto  dict  Postcode  Postcode  Borough  Borough  Neighbourhood  Neighbourhood  df  toronto  pd  DataFrame  from  dict  toronto  dict  df  toronto  to  csv  toronto  part1  csv  df  toronto ,ibm
 head  14  df  toronto  shape  ,ibm
import  os  import  boto3  import  sagemaker  from  sagemaker  mxnet  import  MXNet  from  mxnet  import  gluon  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  gluon  data  vision  MNIST  data  train  train  True  gluon  data  vision  MNIST  data  test  train  False  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  mnist  cat  mnist  py  MXNet  mnist  py  role  role  train  instance  count  train  instance  type  ml  c4  xlarge  hyperparameters  batch  size  100  epochs  10  learning  rate  momentum  log  interval  100  fit  inputs  predictor  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  IPython  display  import  HTML  HTML  open  input  html  read  response  predictor  predict  data  print  int  response  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
from  future  import  print  function  import  keras  from  keras  datasets  import  mnist  from  keras  models  import  Sequential  from  keras  layers  import  Dense  Dropout  Flatten  from  keras  layers  import  Conv2D  MaxPooling2D  from  keras  import  backend  as  import  cv2  import  numpy  as  np  import  os  import  matplotlib  pyplot  as  pltbatch  size  128  num  classes  10  epochs  12  save  dir  os  path  abspath  models  model  name  sample  mnist  model  h5  data  preparation  input  image  dimensions  img  rows  img  cols  28  28  the  data  split  between  train  and  test  sets  train  train  test  test  mnist  load  data  if  image  data  format  channels  first  train  train  reshape  train  shape  img  rows  img  cols  test  test  reshape  test  shape  img  rows  img  cols  input  shape  img  rows  img  cols  else  train  train  reshape  train  shape  img  rows  img  cols  test  test  reshape  test  shape  img  rows  img  cols  input  shape  img  rows  img  cols  train  train  astype  f,microsoft
loat32  test  test  astype  float32  train  255  test  255  print  train  shape  train  shape  print  train  shape  train  samples  print  test  shape  test  samples  convert  class  vectors  to  binary  class  matrices  train  keras  utils  to  categorical  train  num  classes  test  keras  utils  to  categorical  test  num  classes  modeling  model  Sequential  model  add  Conv2D  32  kernel  size  activation  relu  input  shape  input  shape  model  add  Conv2D  64  activation  relu  model  add  MaxPooling2D  pool  size  model  add  Dropout  25  model  add  Flatten  model  add  Dense  128  activation  relu  model  add  Dropout  model  add  Dense  num  classes  activation  softmax  print  model  summary  training  model  compile  loss  keras  losses  categorical  crossentropy  optimizer  keras  optimizers  Adadelta  metrics  accuracy  model  fit  train  train  batch  size  batch  size  epochs  epochs  verbose  validation  data  test  test  score  model  evaluate  test  test  verbose  print  Test  loss  scor,microsoft
e  print  Test  accuracy  score  test  sample  4002  plt  imshow  train  test  sample  reshape  28  28  cmap  gray  plt  axis  off  print  train  test  sample  np  argmax  train  test  sample  train  test  sample  np  expand  dims  axis  print  shape  28  28  pred  model  predict  print  pred  np  argmax  pred  drawing  False  true  if  mouse  is  pressed  ix  iy  mouse  callback  function  def  draw  circle  event  flags  param  global  ix  iy  drawing  mode  if  event  cv2  EVENT  LBUTTONDOWN  drawing  True  ix  iy  elif  event  cv2  EVENT  MOUSEMOVE  if  drawing  True  cv2  circle  img  255  255  255  elif  event  cv2  EVENT  LBUTTONUP  drawing  False  cv2  circle  img  255  255  255  draw  number  and  to  finish  img  np  full  240  240  np  uint8  img  np  zeros  240  240  np  uint8  cv2  namedWindow  image  cv2  setMouseCallback  image  draw  circle  while  cv2  imshow  image  img  cv2  waitKey  0xFF  if  ord  break  cv2  destroyAllWindows  gray  image  cv2  cvtColor  img  cv2  COLOR  BGR2GRAY  resize ,microsoft
 http  opencv  python  readthedocs  io  en  latest  doc  10  imageTransformation  imageTransformation  html  dimg  cv2  resize  gray  image  28  28  interpolation  cv2  INTER  AREA  testx  dimg  astype  np  float32  plt  imshow  dimg  cmap  gray  dimension  https  stackoverflow  com  questions  49057149  expected  conv2d  input  to  have  shape  28  28  but  got  array  with  shape  np  expand  dims  testx  axis  print  batch  shape  28  28  255  np  expand  dims  axis  print  batch  shape  28  28  pred  model  predict  print  pred  print  np  argmax  pred  ,microsoft
numpy  for  matrix  math  and  generating  data  import  numpy  as  np  np  random  seed  create  the  data  xs  np  linspace  100  create  two  sets  of  data  y1  is  the  first  line  y2  the  second  y1  for  in  xs  y2  for  in  xs  print  Some  points  from  the  first  line  print  for  in  zip  xs  y1  print  Some  points  from  the  second  line  print  for  in  zip  xs  y2  plot  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  10  plt  style  use  seaborn  darkgrid  plt  scatter  xs  y1  plt  scatter  xs  y2  plt  scatter  color  marker  55  plt  scatter  black  10  marker  alpha  plt  title  Visualizing  the  solution  plt  show  import  tensorflow  import  tensorflow  as  tf  make  the  matrices  in  numpy  np  matrix  astype  float64  np  matrix  astype  float64  convert  to  tensors  tf  convert  to  tensor  tf  convert  to  tensor  the  inverse  inv  tf  matrix  inverse  the  solution  tf  matmul  inv  Let  start  session  initialize  the  variables  init  tf  global  v,ibm
ariables  initializer  with  tf  Session  as  sess  sess  run  init  print  sess  run  pandas  for  DataFrame  tabular  representation  import  pandas  as  pd  df  pd  DataFrame  index  Bangalore  Poughkeepsie  Rome  columns  Resources  df  df  df  df  Resources  25  31  44  df  import  tensorflow  import  tensorflow  as  tf  make  the  matrices  in  numpy  np  matrix  astype  float64  np  matrix  25  31  44  astype  float64  convert  to  tensors  tf  convert  to  tensor  tf  convert  to  tensor  the  inverse  inv  tf  matrix  inverse  the  solution  tf  matmul  inv  Let  start  session  initialize  the  variables  init  tf  global  variables  initializer  with  tf  Session  as  sess  sess  run  init  print  sess  run  print  inv  sess  run  inv  print  sess  run  xs  np  linspace  np  pi  np  pi  200  y1  np  sin  15  np  random  randn  for  in  xs  y2  np  sin  xs  plt  scatter  xs  y1  alpha  65  plt  plot  xs  y2  linewidth  alpha  plt  title  Sine  curve  segment  with  noise  plt  show  xsx  tf  placeho,ibm
lder  tf  float64  name  data  tf  placeholder  tf  float64  name  data  tf  Variable  tf  random  normal  shape  dtype  tf  float64  name  values  dtype  tf  float64  linear  model  of  the  form  wx  predicted  tf  multiply  err  tf  squared  difference  predicted  optimizer  tf  train  GradientDescentOptimizer  01  train  optimizer  minimize  err  init  tf  global  variables  initializer  with  tf  Session  as  sess  sess  run  init  for  in  range  2500  data  xs  len  xs  data  y1  len  xs  run  the  training  step  sess  run  train  feed  dict  data  data  check  the  weights  every  100  iterations  if  500  print  sess  run  periodically  assign  to  normal  python  variable  for  use  in  the  graph  sess  run  vmodel  values  for  in  xs  plt  title  Linear  model  plt  plot  xs  model  values  linewidth  orange  label  model  plt  plot  xs  y2  linewidth  75  alpha  label  sine  plt  legend  plt  show  ,ibm
from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  machinelearningsagemaker  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  30  This  is  format  train  set  30  from  sagemaker  import  get  execution  role  Bucket  location  to  save  your  custom  code  in  tar  gz  format  output  location  s3  machinelearningsagemaker  mxnet  code  output  Bucket  location  where  results  of  model  training  are  saved  model  location  s3  machi,amazon
nelearningsagemaker  model  result  IAM  execution  role  that  gives  SageMaker  access  to  resources  in  your  AWS  account  We  can  use  the  SageMaker  Python  SDK  to  get  the  role  from  our  notebook  environment  role  get  execution  role  print  training  data  will  be  uploaded  to  format  output  location  print  training  artifacts  will  be  uploaded  to  format  model  location  cat  mnist  py  from  sagemaker  mxnet  import  MXNet  mnist  estimator  MXNet  entry  point  mnist  py  role  role  output  path  model  location  code  location  output  location  train  instance  count  train  instance  type  ml  m4  xlarge  hyperparameters  learning  rate  time  import  boto3  import  tensorflow  as  tf  region  boto3  Session  region  name  train  data  location  s3  sagemaker  sample  data  mxnet  mnist  train  format  region  test  data  location  s3  sagemaker  sample  data  mxnet  mnist  test  format  region  mnist  estimator  fit  train  train  data  location  test  test  data  location,amazon
  time  predictor  mnist  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  IPython  display  import  HTML  HTML  open  input  html  read  response  predictor  predict  data  print  Raw  prediction  result  print  response  labeled  predictions  list  zip  range  10  response  print  Labeled  predictions  print  labeled  predictions  labeled  predictions  sort  key  lambda  label  and  prob  label  and  prob  print  Most  likely  answer  format  labeled  predictions  ,amazon
import  numpy  as  np  import  os  import  glob  import  shutildata  path  home  etanchik  Documents  research  pytorch  examples  imagenet  data  traindir  os  path  join  data  path  train  os  path  exists  traindir  subdirs  glob  glob  os  path  join  traindir  len  subdirs  src  home  etanchik  Documents  research  pytorch  examples  imagenet  data  train  n01440764  n01440764  10026  JPEG  dst  chunk  train  os  makedirs  dst  exist  ok  True  shutil  copy2  src  dst  subdirs  copy  subdirs  tree  without  files  shutil  copytree  data  path  chunk  ignore  shutil  ignore  patterns  JPEG  224  224  1000  1024  1024  os  path  dirname  src  ,amazon
import  pandas  import  urllib  request  data  filename  nyc  taxi  csv  data  source  https  raw  githubusercontent  com  numenta  NAB  master  data  realKnownCause  nyc  taxi  csv  urllib  request  urlretrieve  data  source  data  filename  taxi  data  pandas  read  csv  data  filename  delimiter  taxi  data  plot  title  Taxi  Ridership  in  NYC  sh  conda  install  boto3def  convert  and  upload  training  data  ndarray  bucket  prefix  filename  data  pbr  import  boto3  import  os  from  sagemaker  amazon  common  import  numpy  to  record  serializer  convert  Numpy  array  to  Protobuf  RecordIO  format  serializer  numpy  to  record  serializer  buffer  serializer  ndarray  upload  to  S3  s3  object  os  path  join  prefix  train  filename  boto3  Session  resource  s3  Bucket  bucket  Object  s3  object  upload  fileobj  buffer  s3  path  s3  format  bucket  s3  object  return  s3  path  bucket  gk  sagemaker  test  01  use  your  own  bucket  here  prefix  sagemaker01  randomcutforest01  s3  train,amazon
  data  convert  and  upload  training  data  taxi  data  value  as  matrix  reshape  bucket  prefix  import  boto3  import  sagemaker  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  randomcutforest  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  randomcutforest  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  randomcutforest  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  randomcutforest  latest  region  name  boto3  Session  region  name  container  containers  region  name  session  sagemaker  Session  rcf  sagemaker  estimator  Estimator  container  sagemaker  get  execution  role  output  path  s3  output  format  bucket  prefix  train  instance  count  train  instance  type  ml  c5  xlarge  sagemaker  session  session  rcf  set  hyperparameters  num  samples  per  tree  200  num  trees  50  feature  dim  feature  size  s3  train  input  sagemaker  session  s3  input  s3  train  data  distribution  ShardedByS3Key  cont,amazon
ent  type  application  recordio  protobuf  rcf  fit  train  s3  train  input  from  sagemaker  predictor  import  csv  serializer  json  deserializer  rcf  inference  rcf  deploy  initial  instance  count  instance  type  ml  c5  xlarge  rcf  inference  content  type  text  csv  rcf  inference  serializer  csv  serializer  rcf  inference  deserializer  json  deserializer  results  rcf  inference  predict  taxi  data  value  as  matrix  reshape  scores  datum  score  for  datum  in  results  scores  taxi  data  score  pandas  Series  scores  index  taxi  data  index  score  mean  taxi  data  score  mean  score  std  taxi  data  score  std  score  cutoff  score  mean  score  std  anomalies  taxi  data  taxi  data  score  score  cutoff  import  matplotlib  pyplot  as  plt  fig  ax1  plt  subplots  ax2  ax1  twinx  ax1  plot  taxi  data  value  alpha  ax1  set  ylabel  Taxi  Ridership  color  C0  ax1  tick  params  colors  C0  ax2  plot  taxi  data  score  color  C1  ax2  plot  anomalies  index  anomalies  score,amazon
  ko  ax2  set  ylabel  Anomaly  Score  color  C1  ax2  tick  params  colors  C1  fig  suptitle  Taxi  Ridership  in  NYC  plt  show  ,amazon
importing  old  libraries  also  used  in  the  two  first  parts  import  pandas  as  pd  import  numpy  as  np  from  bs4  import  BeautifulSoup  import  requests  from  bs4  import  BeautifulSoup  as  bs  importing  new  libraries  import  numpy  as  np  library  to  handle  data  in  vectorized  manner  import  pandas  as  pd  library  for  data  analsysis  pd  set  option  display  max  columns  None  pd  set  option  display  max  rows  None  conda  install  conda  forge  geopy  yes  uncomment  this  line  if  you  haven  completed  the  Foursquare  API  lab  from  geopy  geocoders  import  Nominatim  convert  an  address  into  latitude  and  longitude  values  import  requests  library  to  handle  requests  conda  install  conda  forge  folium  yes  uncomment  this  line  if  you  haven  completed  the  Foursquare  API  lab  import  folium  map  rendering  library  print  Libraries  imported  df  toronto  pd  read  csv  toronto  base  csv  df  toronto  head  for  the  city  Toronto  latitude  and  lo,ibm
ngtitude  are  manually  extracted  via  google  search  toronto  latitude  43  6532  toronto  longitude  79  3832  map  toronto  folium  Map  location  toronto  latitude  toronto  longitude  zoom  start  10  add  markers  to  map  for  lat  lng  borough  neighborhood  in  zip  df  toronto  Latitude  df  toronto  Longitude  df  toronto  Borough  df  toronto  Neighbourhood  label  format  neighborhood  borough  label  folium  Popup  label  parse  html  True  folium  CircleMarker  lat  lng  radius  popup  label  color  blue  fill  True  fill  color  3186cc  fill  opacity  add  to  map  toronto  map  toronto  hiddel  cell  CLIENT  ID  0MJA3NYYG3U2ZY1LTZN2OYEHS3Y3WVSON2GBSO3IL4EDYVIR  your  Foursquare  ID  CLIENT  SECRET  WGWSAF2TKVUQPE3PD0N3EOITFVBY5EYP1VCZI3BMUG0ROUS5  your  Foursquare  Secret  VERSION  20180605  Foursquare  API  version  scarborough  data  df  toronto  df  toronto  Borough  Scarborough  reset  index  drop  True  scarborough  data  head  address  scar  Scarborough  Toronto  latitude  scar  43  ,ibm
773077  longitude  scar  79  257774  print  The  geograpical  coordinate  of  Scarborough  are  format  latitude  scar  longitude  scar  map  scarb  folium  Map  location  latitude  scar  longitude  scar  zoom  start  12  add  markers  to  map  for  lat  lng  label  in  zip  scarborough  data  Latitude  scarborough  data  Longitude  scarborough  data  Neighbourhood  label  folium  Popup  label  parse  html  True  folium  CircleMarker  lat  lng  radius  popup  label  color  blue  fill  True  fill  color  3186cc  fill  opacity  add  to  map  scarb  map  scarbneighborhood  latitude  scarborough  data  loc  Latitude  neighbourhood  latitude  value  neighborhood  longitude  scarborough  data  loc  Longitude  neighbourhood  longitude  value  neighborhood  name  scarborough  data  loc  Neighbourhood  neighbourhood  name  print  Latitude  and  longitude  values  of  are  format  neighborhood  name  neighborhood  latitude  neighborhood  longitude  LIMIT  100  radius  500  url  https  api  foursquare  com  v2  venues  ,ibm
explore  client  id  client  secret  ll  radius  limit  format  CLIENT  ID  CLIENT  SECRET  latitude  scar  longitude  scar  VERSION  radius  LIMIT  results  requests  get  url  json  resultsdef  get  category  type  row  try  categories  list  row  categories  except  categories  list  row  venue  categories  if  len  categories  list  return  None  else  return  categories  list  name  import  json  library  to  handle  JSON  files  from  pandas  io  json  import  json  normalize  tranform  JSON  file  into  pandas  dataframe  venues  results  response  groups  items  nearby  venues  json  normalize  venues  flatten  JSON  filter  columns  filtered  columns  venue  name  venue  categories  venue  location  lat  venue  location  lng  nearby  venues  nearby  venues  loc  filtered  columns  filter  the  category  for  each  row  nearby  venues  venue  categories  nearby  venues  apply  get  category  type  axis  clean  columns  nearby  venues  columns  col  split  for  col  in  nearby  venues  columns  nearby ,ibm
 venues  head  10  print  venues  were  returned  by  Foursquare  format  nearby  venues  shape  def  getNearbyVenues  names  latitudes  longitudes  radius  500  venues  list  for  name  lat  lng  in  zip  names  latitudes  longitudes  print  name  create  the  API  request  URL  url  https  api  foursquare  com  v2  venues  explore  client  id  client  secret  ll  radius  limit  format  CLIENT  ID  CLIENT  SECRET  VERSION  lat  lng  radius  LIMIT  make  the  GET  request  results  requests  get  url  json  response  groups  items  return  only  relevant  information  for  each  nearby  venue  venues  list  append  name  lat  lng  venue  name  venue  location  lat  venue  location  lng  venue  categories  name  for  in  results  nearby  venues  pd  DataFrame  item  for  venue  list  in  venues  list  for  item  in  venue  list  nearby  venues  columns  Neighborhood  Neighborhood  Latitude  Neighborhood  Longitude  Venue  Venue  Latitude  Venue  Longitude  Venue  Category  return  nearby  venues  scarborough  ,ibm
venues  getNearbyVenues  names  scarborough  data  Neighbourhood  latitudes  scarborough  data  Latitude  longitudes  scarborough  data  Longitude  scarborough  venues  head  10  scarborough  venues  tail  10  scarborough  venues  groupby  Neighborhood  count  print  There  are  uniques  categories  format  len  scarborough  venues  Venue  Category  unique  one  hot  encoding  scarb  onehot  pd  get  dummies  scarborough  venues  Venue  Category  prefix  prefix  sep  add  neighborhood  column  back  to  dataframe  scarb  onehot  Neighborhood  scarborough  venues  Neighborhood  move  neighborhood  column  to  the  first  column  fixed  columns  scarb  onehot  columns  list  scarb  onehot  columns  scarb  onehot  scarb  onehot  fixed  columns  scarb  onehot  head  scarb  onehot  shapescarb  grouped  scarb  onehot  groupby  Neighborhood  mean  reset  index  scarb  grouped  head  def  return  most  common  venues  row  num  top  venues  row  categories  row  iloc  row  categories  sorted  row  categories  sort  v,ibm
alues  ascending  False  return  row  categories  sorted  index  values  num  top  venues  num  top  venues  10  indicators  st  nd  rd  create  columns  according  to  number  of  top  venues  columns  Neighborhood  for  ind  in  np  arange  num  top  venues  try  columns  append  Most  Common  Venue  format  ind  indicators  ind  except  columns  append  th  Most  Common  Venue  format  ind  create  new  dataframe  neighborhoods  venues  sorted  pd  DataFrame  columns  columns  neighborhoods  venues  sorted  Neighborhood  scarb  grouped  Neighborhood  for  ind  in  np  arange  scarb  grouped  shape  neighborhoods  venues  sorted  iloc  ind  return  most  common  venues  scarb  grouped  iloc  ind  num  top  venues  neighborhoods  venues  sorted  import  means  from  clustering  stage  from  sklearn  cluster  import  KMeans  scarb  data  scarborough  data  drop  16  set  number  of  clusters  kclusters  scarb  grouped  clustering  scarb  grouped  drop  Neighborhood  run  means  clustering  kmeans  KMeans  clu,ibm
sters  kclusters  random  state  fit  scarb  grouped  clustering  check  cluster  labels  generated  for  each  row  in  the  dataframe  kmeans  labels  10  len  kmeans  labels  16  scarborough  data  shapescarb  merged  scarb  data  add  clustering  labels  scarb  merged  Cluster  Labels  kmeans  labels  merge  toronto  grouped  with  toronto  data  to  add  latitude  longitude  for  each  neighborhood  scarb  merged  scarb  merged  join  neighborhoods  venues  sorted  set  index  Neighborhood  on  Neighbourhood  scarb  merged  Matplotlib  and  associated  plotting  modules  import  matplotlib  cm  as  cm  import  matplotlib  colors  as  colors  create  map  map  clusters  folium  Map  location  latitude  scar  longitude  scar  zoom  start  11  set  color  scheme  for  the  clusters  np  arange  kclusters  ys  for  in  range  kclusters  colors  array  cm  rainbow  np  linspace  len  ys  rainbow  colors  rgb2hex  for  in  colors  array  add  markers  to  the  map  markers  colors  for  lat  lon  poi  cluster ,ibm
 in  zip  scarb  merged  Latitude  scarb  merged  Longitude  scarb  merged  Neighbourhood  scarb  merged  Cluster  Labels  label  folium  Popup  str  poi  Cluster  str  cluster  parse  html  True  folium  CircleMarker  lat  lon  radius  popup  label  color  rainbow  cluster  fill  True  fill  color  rainbow  cluster  fill  opacity  add  to  map  clusters  map  clustersscarb  merged  loc  scarb  merged  Cluster  Labels  scarb  merged  columns  list  range  scarb  merged  shape  scarb  merged  loc  scarb  merged  Cluster  Labels  scarb  merged  columns  list  range  scarb  merged  shape  scarb  merged  loc  scarb  merged  Cluster  Labels  scarb  merged  columns  list  range  scarb  merged  shape  scarb  merged  loc  scarb  merged  Cluster  Labels  scarb  merged  columns  list  range  scarb  merged  shape  scarb  merged  loc  scarb  merged  Cluster  Labels  scarb  merged  columns  list  range  scarb  merged  shape  ,ibm
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  linear  mnist  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  30  This  is  format  train  set  30  import  io  import  numpy  as  np  import  sagemaker  amazon  common  as  smac  vectors  np  array  tolist  for  in  train  set  astype  float32  labels  np  where  np  array  tolist  for ,amazon
 in  train  set  astype  float32  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  vectors  labels  buf  seek  import  boto3  import  os  key  recordio  pb  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  buf  s3  train  data  s3  train  format  bucket  prefix  key  print  uploaded  training  data  location  format  s3  train  data  output  location  s3  output  format  bucket  prefix  print  training  artifacts  will  be  uploaded  to  format  output  location  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  boto3  Session  region  name  linear  learner  import  boto3  import  sagemaker  sess  sagemaker  Session  linear  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  output  location  sagemaker  session  sess  linear  set  hyperparameters  feature  dim  784  predictor  type  binary  classifier  mini  batch  size  ,amazon
200  linear  fit  train  s3  train  data  linear  predictor  linear  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  sagemaker  predictor  import  csv  serializer  json  deserializer  linear  predictor  content  type  text  csv  linear  predictor  serializer  csv  serializer  linear  predictor  deserializer  json  deserializerresult  linear  predictor  predict  train  set  30  31  print  result  import  numpy  as  np  predictions  for  array  in  np  array  split  test  set  100  result  linear  predictor  predict  array  predictions  predicted  label  for  in  result  predictions  predictions  np  array  predictions  import  pandas  as  pd  pd  crosstab  np  where  test  set  predictions  rownames  actuals  colnames  predictions  import  sagemaker  sagemaker  Session  delete  endpoint  linear  predictor  endpoint  ,amazon
import  os  import  tarfile  from  six  moves  import  urllibDOWNLOAD  ROOT  https  raw  githubusercontent  com  ageron  handson  ml  master  HOUSING  PATH  datasets  housing  HOUSING  URL  DOWNLOAD  ROOT  HOUSING  PATH  housing  tgz  def  fetch  housing  data  housing  url  HOUSING  URL  housing  path  HOUSING  PATH  if  not  os  path  isdir  housing  path  os  makedirs  housing  path  tgz  path  os  path  join  housing  path  housing  tgz  urllib  request  urlretrieve  housing  url  tgz  path  housing  tgz  tarfile  open  tgz  path  housing  tgz  extractall  path  housing  path  housing  tgz  close  fetch  housing  data  import  pandas  as  pddef  load  housing  data  housing  path  HOUSING  PATH  csv  path  os  path  join  housing  path  housing  csv  return  pd  read  csv  csv  path  housing  load  housing  data  housing  head  housing  info  housing  ocean  proximity  value  counts  housing  describe  matplotlib  inline  import  matplotlib  pyplot  as  plt  housing  hist  bins  50  figsize  20  15  plt  ,ibm
show  import  numpy  as  npdef  split  train  test  data  test  ratio  shuffled  indices  np  random  permutation  len  data  test  set  size  int  len  data  test  ratio  test  indices  shuffled  indices  test  set  size  train  indices  shuffled  indices  test  set  size  return  data  iloc  train  indices  data  iloc  test  indices  train  set  test  set  split  train  test  housing  print  len  train  set  Training  len  test  set  Test  import  hashlibdef  test  set  check  identifier  test  ratio  hash  return  hash  np  int64  identifier  digest  256  test  ratiodef  split  train  test  by  id  data  test  ratio  id  column  hash  hashlib  md5  ids  data  id  column  in  test  set  ids  apply  lambda  id  test  set  check  id  test  ratio  hash  return  data  loc  in  test  set  data  loc  in  test  set  housing  with  id  housing  reset  index  train  set  test  set  split  train  test  by  id  housing  with  id  index  print  len  train  set  Training  len  test  set  Test  from  sklearn  cross  vali,ibm
dation  import  train  test  split  train  set  test  set  train  test  split  housing  test  size  random  state  42  print  len  train  set  Training  len  test  set  Test  import  sklearnprint  sklearn  version  housing  income  cat  np  ceil  housing  median  income  housing  income  cat  where  housing  income  cat  inplace  True  print  len  housing  from  sklearn  cross  validation  import  StratifiedShuffleSplit  num  int  len  housing  split  StratifiedShuffleSplit  test  size  random  state  42  for  train  index  test  index  in  split  split  housing  housing  income  cat  strat  train  set  housing  loc  train  index  strat  test  set  housing  loc  test  index  housing  income  cat  value  counts  len  housing  housing  income  cat  hist  plt  show  for  set  in  strat  train  set  strat  test  set  set  drop  income  cat  axis  inplace  True  import  sklearn  sklearn  info  cross  validation  StratifiedShuffleSplit  sklearn  info  housing  load  housing  data  housing  head  train  set  head  h,ibm
ousing  train  set  copy  housing  plot  kind  scatter  longitude  latitude  alpha  housing  population  100  label  population  figsize  10  median  house  value  cmap  plt  get  cmap  jet  colorbar  True  corr  matrix  housing  corr  temp  corr  matrix  median  house  value  copy  np  info  temp  from  pandas  import  Series  np  info  Series  sort  from  pandas  tools  plotting  import  scatter  matrix  attributes  median  house  value  median  income  total  rooms  housing  median  age  scatter  matrix  housing  attributes  figsize  12  housing  plot  kind  scatter  median  income  median  house  value  alpha  housing  rooms  per  household  housing  total  rooms  housing  households  housing  bedrooms  per  room  housing  total  bedrooms  housing  total  rooms  housing  population  per  household  housing  population  housing  total  rooms  corr  matrix  housing  corr  corr  matrix  median  house  value  housing  train  set  drop  median  house  value  axis  housing  labels  train  set  median  house  va,ibm
lue  copy  from  sklearn  preprocessing  import  Imputer  imputer  Imputer  strategy  median  housing  num  housing  drop  ocean  proximity  axis  imputer  fit  housing  num  imputer  statistics  housing  num  median  valuesX  imputer  transform  housing  num  housing  tr  pd  DataFrame  columns  housing  num  columns  housing  cat  housing  ocean  proximity  housing  cat  head  10  housing  cat  encoded  housing  categories  housing  cat  factorize  housing  cat  encoded  10  housing  categoriesfrom  sklearn  preprocessing  import  OneHotEncoder  encoder  OneHotEncoder  housing  cat  1hot  encoder  fit  transform  housing  cat  encoded  reshape  housing  cat  1hot  housing  cat  1hot  toarray  np  info  np  reshape  from  sklearn  base  import  BaseEstimator  TransformerMixin  rooms  ix  bedrooms  ix  population  ix  household  ix  class  CombinedAttributesAdder  BaseEstimator  TransformerMixin  def  init  self  add  bedrooms  per  room  True  self  add  bedrooms  per  room  add  bedrooms  per  room  def  fi,ibm
t  self  None  return  self  def  transform  self  None  rooms  per  household  rooms  ix  household  ix  population  per  household  population  ix  household  ix  if  self  add  bedrooms  per  room  bedrooms  per  room  bedrooms  ix  rooms  ix  return  np  rooms  per  household  population  per  household  bedrooms  per  room  else  return  np  rooms  per  household  population  per  household  attr  adder  CombinedAttributesAdder  add  bedrooms  per  room  False  housing  extra  attribs  attr  adder  transform  housing  values  from  sklearn  pipeline  import  Pipeline  from  sklearn  preprocessing  import  StandardScaler  num  pipeline  Pipeline  imputer  Imputer  strategy  median  attribs  adder  CombinedAttributesAdder  std  scaler  StandardScaler  housing  num  tr  num  pipeline  fit  transform  housing  num  class  DataFrameSelector  BaseEstimator  TransformerMixin  def  init  self  attribute  names  self  attribute  names  attribute  names  def  fit  self  None  return  self  def  transform  self  re,ibm
turn  self  attribute  names  valuesfrom  sklearn  preprocessing  import  LabelBinarizer  num  attribs  list  housing  num  cat  attribs  ocean  proximity  num  pipeline  Pipeline  selector  DataFrameSelector  num  attribs  imputer  Imputer  strategy  median  attribs  adder  CombinedAttributesAdder  std  scaler  StandardScaler  cat  pipeline  Pipeline  selector  DataFrameSelector  cat  attribs  label  binarizer  LabelBinarizer  from  sklearn  pipeline  import  FeatureUnion  full  pipeline  FeatureUnion  transformer  list  num  pipeline  num  pipeline  cat  pipeline  cat  pipeline  housing  prepared  full  pipeline  fit  transform  housing  housing  preparedhousing  prepared  shapefrom  sklearn  linear  model  import  LinearRegression  lin  reg  LinearRegression  lin  reg  fit  housing  prepared  housing  labels  some  data  housing  iloc  some  datasome  data  housing  iloc  some  labels  housing  labels  iloc  some  data  prepared  full  pipeline  transform  some  data  print  Vorhersagen  lin  reg  predict ,ibm
 some  data  prepared  print  Labels  list  some  labels  num  attribs  list  housing  num  num  attribsfrom  sklearn  metrics  import  mean  squared  error  housing  predictions  lin  reg  predict  housing  prepared  lin  mse  mean  squared  error  housing  labels  housing  predictions  lin  rmse  np  sqrt  lin  mse  lin  rmsefrom  sklearn  tree  import  DecisionTreeRegressor  tree  reg  DecisionTreeRegressor  tree  reg  fit  housing  prepared  housing  labels  housing  predictions  tree  reg  predict  housing  prepared  tree  mse  mean  squared  error  housing  labels  housing  predictions  tree  rmse  np  sqrt  tree  mse  tree  rmsefrom  sklearn  cross  validation  import  cross  val  score  scores  cross  val  score  tree  reg  housing  prepared  housing  labels  scoring  mean  squared  error  cv  10  tree  rmse  scores  np  sqrt  scores  def  display  scores  scores  print  Scores  scores  print  Mittelwert  scores  mean  print  Standardabweichung  scores  std  display  scores  tree  rmse  scores  lin  s,ibm
cores  cross  val  score  lin  reg  housing  prepared  housing  labels  scoring  mean  squared  error  cv  10  lin  rmse  scores  np  sqrt  lin  scores  display  scores  lin  rmse  scores  from  sklearn  ensemble  import  RandomForestRegressor  forest  reg  RandomForestRegressor  forest  reg  fit  housing  prepared  housing  labels  forest  scores  cross  val  score  forest  reg  housing  prepared  housing  labels  scoring  mean  squared  error  cv  10  forest  rmse  scores  np  sqrt  forest  scores  display  scores  forest  rmse  scores  from  sklearn  grid  search  import  GridSearchCV  param  grid  estimators  10  30  max  features  bootstrap  False  estimators  10  max  features  forest  reg  RandomForestRegressor  grid  search  GridSearchCV  forest  reg  param  grid  cv  scoring  mean  squared  error  grid  search  fit  housing  prepared  housing  labels  grid  search  best  params  grid  search  best  estimator  cvres  grid  search  grid  scores  for  in  cvres  print  np  sqrt  mean  validation  score ,ibm
 parameters  for  mean  score  params  in  zip  cvres  mean  cvres  params  print  np  sqrt  mean  score  params  np  info  grid  search  spam  eggs  spam  feature  importances  grid  search  best  estimator  feature  importances  feature  importancesextra  attribs  rooms  per  hhold  pop  per  hhold  bedrooms  per  room  cat  one  hot  attribs  list  encoder  active  features  attributes  num  attribs  extra  attribs  cat  one  hot  attribs  sorted  zip  feature  importances  attributes  reverse  True  np  info  encoder  final  model  grid  search  best  estimator  test  test  set  drop  median  house  value  axis  test  test  set  median  house  value  copy  test  prepared  full  pipeline  transform  test  final  predictions  final  model  predict  test  prepared  final  mse  mean  squared  error  test  final  predictions  final  rmse  np  sqrt  final  mse  final  rmse  ,ibm
import  os  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  your  s3  bucket  name  here  enter  your  s3  bucket  where  you  will  copy  data  and  model  artifacts  prefix  sagemaker  DEMO  breast  cancer  prediction  place  to  upload  training  files  within  the  bucketimport  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  import  io  import  time  import  json  import  sagemaker  amazon  common  as  smacdata  pd  read  csv  https  archive  ics  uci  edu  ml  machine  learning  databases  breast  cancer  wisconsin  wdbc  data  header  None  specify  columns  extracted  from  wbdc  names  data  columns  id  diagnosis  radius  mean  texture  mean  perimeter  mean  area  mean  smoothness  mean  compactness  mean  concavity  mean  concave  points  mean  symmetry  mean  fractal  dimension  mean  radius  se  texture  se  perimeter  se  area  se  smoothness  se  compactness  se  concavity  se  concave  points  se  symmet,amazon
ry  se  fractal  dimension  se  radius  worst  texture  worst  perimeter  worst  area  worst  smoothness  worst  compactness  worst  concavity  worst  concave  points  worst  symmetry  worst  fractal  dimension  worst  save  the  data  data  to  csv  data  csv  sep  index  False  print  the  shape  of  the  data  file  print  data  shape  show  the  top  few  rows  display  data  head  describe  the  data  object  display  data  describe  we  will  also  summarize  the  categorical  field  diganosis  display  data  diagnosis  value  counts  rand  split  np  random  rand  len  data  train  list  rand  split  val  list  rand  split  rand  split  test  list  rand  split  data  train  data  train  list  data  val  data  val  list  data  test  data  test  list  train  data  train  iloc  as  matrix  train  data  train  iloc  as  matrix  val  data  val  iloc  as  matrix  val  data  val  iloc  as  matrix  test  data  test  iloc  as  matrix  test  data  test  iloc  as  matrix  train  file  linear  train  data  io  Byt,amazon
esIO  smac  write  numpy  to  dense  tensor  train  astype  float32  train  astype  float32  seek  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  train  file  upload  fileobj  validation  file  linear  validation  data  io  BytesIO  smac  write  numpy  to  dense  tensor  val  astype  float32  val  astype  float32  seek  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  validation  file  upload  fileobj  See  Algorithms  Provided  by  Amazon  SageMaker  Common  Parameters  in  the  SageMaker  documentation  for  an  explanation  of  these  values  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  boto3  Session  region  name  linear  learner  linear  job  DEMO  linear  time  strftime  time  gmtime  print  Job  name  is  linear  job  linear  training  params  RoleArn  role  TrainingJobName  linear  job  AlgorithmSpecification  TrainingImage  container  TrainingInputMode  File  ResourceCon,amazon
fig  InstanceCount  InstanceType  ml  c4  2xlarge  VolumeSizeInGB  10  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  S3DataDistributionType  ShardedByS3Key  CompressionType  None  RecordWrapperType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  prefix  S3DataDistributionType  FullyReplicated  CompressionType  None  RecordWrapperType  None  OutputDataConfig  S3OutputPath  s3  format  bucket  prefix  HyperParameters  feature  dim  30  mini  batch  size  100  predictor  type  regressor  epochs  10  num  models  32  loss  absolute  loss  StoppingCondition  MaxRuntimeInSeconds  60  60  time  region  boto3  Session  region  name  sm  boto3  client  sagemaker  sm  create  training  job  linear  training  params  status  sm  describe  training  job  TrainingJobName  linear  job  TrainingJobStatus  print  status  sm  get  waiter  training  job  completed  or  stopped ,amazon
 wait  TrainingJobName  linear  job  if  status  Failed  message  sm  describe  training  job  TrainingJobName  linear  job  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  linear  hosting  container  Image  container  ModelDataUrl  sm  describe  training  job  TrainingJobName  linear  job  ModelArtifacts  S3ModelArtifacts  create  model  response  sm  create  model  ModelName  linear  job  ExecutionRoleArn  role  PrimaryContainer  linear  hosting  container  print  create  model  response  ModelArn  linear  endpoint  config  DEMO  linear  endpoint  config  time  strftime  time  gmtime  print  linear  endpoint  config  create  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  linear  endpoint  config  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  linear  job  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  ,amazon
time  linear  endpoint  DEMO  linear  endpoint  time  strftime  time  gmtime  print  linear  endpoint  create  endpoint  response  sm  create  endpoint  EndpointName  linear  endpoint  EndpointConfigName  linear  endpoint  config  print  create  endpoint  response  EndpointArn  resp  sm  describe  endpoint  EndpointName  linear  endpoint  status  resp  EndpointStatus  print  Status  status  sm  get  waiter  endpoint  in  service  wait  EndpointName  linear  endpoint  resp  sm  describe  endpoint  EndpointName  linear  endpoint  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Status  status  if  status  InService  raise  Exception  Endpoint  creation  did  not  succeed  def  np2csv  arr  csv  io  BytesIO  np  savetxt  csv  arr  delimiter  fmt  return  csv  getvalue  decode  rstrip  runtime  boto3  client  runtime  sagemaker  payload  np2csv  test  response  runtime  invoke  endpoint  EndpointName  linear  endpoint  ContentType  text  csv  Body  payload  result  json  loads  response  Body  ,amazon
read  decode  test  pred  np  array  score  for  in  result  predictions  test  mae  linear  np  mean  np  abs  test  test  pred  test  mae  baseline  np  mean  np  abs  test  np  median  train  training  median  as  baseline  predictor  print  Test  MAE  Baseline  round  test  mae  baseline  print  Test  MAE  Linear  round  test  mae  linear  test  pred  class  test  pred  test  pred  baseline  np  repeat  np  median  train  len  test  prediction  accuracy  np  mean  test  test  pred  class  100  baseline  accuracy  np  mean  test  test  pred  baseline  100  print  Prediction  Accuracy  round  prediction  accuracy  print  Baseline  Accuracy  round  baseline  accuracy  sm  delete  endpoint  EndpointName  linear  endpoint  ,amazon
import  os  import  sagemaker  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  DEMO  abalone  cat  abalone  py  from  sagemaker  tensorflow  import  TensorFlow  abalone  estimator  TensorFlow  entry  point  abalone  py  role  role  framework  version  training  steps  100  evaluation  steps  100  hyperparameters  learning  rate  001  train  instance  count  train  instance  type  ml  c4  xlarge  abalone  estimator  fit  inputs  abalone  predictor  abalone  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  import  tensorflow  as  tf  import  numpy  as  np  prediction  set  tf  contrib  learn  datasets  base  load  csv  without  header  filename  os  path  join  data  abalone  predict  csv  target  dtype  np  int  features  dtype  np  float32  data  prediction  set  data  tensor  proto  tf  make  tensor  proto  values  np  asarray  data  shape  le,amazon
n  data  dtype  tf  float32  abalone  predictor  predict  tensor  proto  sagemaker  Session  delete  endpoint  abalone  predictor  endpoint  ,amazon
matplotlib  inline  azureml  history  offimport  pickle  import  sys  import  os  import  numpy  as  np  import  matplotlib  pyplot  as  plt  from  mpl  toolkits  mplot3d  import  Axes3D  from  sklearn  import  decomposition  from  sklearn  import  datasets  from  sklearn  import  preprocessing  from  sklearn  datasets  import  load  iris  from  sklearn  linear  model  import  LogisticRegression  from  sklearn  metrics  import  precision  recall  curve  from  azureml  logging  import  get  azureml  logger  from  azureml  dataprep  import  packagelogger  get  azureml  logger  print  Python  version  format  sys  version  print  azureml  history  on  load  Iris  dataset  from  DataPrep  package  iris  package  run  iris  dprep  dataflow  idx  spark  False  load  features  and  labels  iris  Sepal  Length  Sepal  Width  Petal  Length  Petal  Width  values  iris  Species  values  tag  this  cell  to  measure  duration  logger  log  Cell  Load  Data  logger  log  Rows  iris  shape  print  Iris  dataset  shape  for,microsoft
mat  iris  shape  logger  log  Cell  Training  change  regularization  rate  and  you  will  likely  get  different  accuracy  reg  01  print  Regularization  rate  is  format  reg  logger  log  Regularization  Rate  reg  train  logistic  regression  model  clf  LogisticRegression  reg  fit  print  clf  Log  curves  for  label  value  Iris  versicolor  scores  clf  predict  proba  precision  recall  thresholds  precision  recall  curve  scores  pos  label  Iris  versicolor  logger  log  Precision  precision  logger  log  Recall  recall  logger  log  Thresholds  thresholds  accuracy  clf  score  logger  log  Accuracy  accuracy  print  Accuracy  is  format  accuracy  logger  log  Cell  Scoring  predict  new  sample  new  25  print  New  sample  format  new  pred  clf  predict  new  logger  log  Prediction  pred  tolist  print  Predicted  class  is  format  pred  azureml  history  off  Plot  Iris  data  in  3D  centers  fig  plt  figure  figsize  plt  clf  ax  Axes3D  fig  rect  95  elev  48  azim  134  plt  cla,microsoft
  decompose  feature  columns  into  components  for  3D  plotting  pca  decomposition  PCA  components  pca  fit  pca  transform  le  preprocessing  LabelEncoder  le  fit  le  transform  for  name  label  in  Setosa  Versicolour  Virginica  ax  text3D  label  mean  label  mean  label  mean  name  horizontalalignment  center  bbox  dict  alpha  edgecolor  facecolor  Reorder  the  labels  to  have  colors  matching  the  cluster  results  np  choose  astype  np  float  ax  scatter  cmap  plt  cm  spectral  ax  xaxis  set  ticklabels  ax  yaxis  set  ticklabels  ax  zaxis  set  ticklabels  plt  show  ,microsoft
import  numpy  as  np  import  matplotlib  pyplot  as  plt  import  seaborn  as  sns  The  code  was  removed  by  Watson  Studio  for  sharing  The  code  was  removed  by  Watson  Studio  for  sharing  data  df  createOrReplaceTempView  PATIENTS  data  df  createOrReplaceTempView  GENE  patientsSpark  spark  sql  SELECT  FROM  PATIENTS  AS  WHERE  EXISTS  SELECT  FROM  GENE  AS  WHERE  PATIENT  ID  PATIENT  ID  patientsSpark  show  patientsPandas  patientsSpark  toPandas  patientsPandas  head  patientsPandas  sort  values  by  PATIENT  ID  inplace  True  patientsPandas  head  patientsPandas  describe  patientsPandas  shapegenePandas  data  df  toPandas  genePandas  head  genePandasMatrix  genePandas  pivot  index  PATIENT  ID  columns  GENE  VELUE  genePandasMatrix  columns  name  None  remove  categories  genePandasMatrix  genePandasMatrix  reset  index  genePandasMatrix  sort  values  by  PATIENT  ID  inplace  True  genePandasMatrix  head  genePandasMatrix  describe  genePandasMatrix  shapepatientsPandas ,ibm
 drop  columns  PATIENT  ID  inplace  True  genePandasMatrix  drop  columns  PATIENT  ID  inplace  True  patientsPandas  shapegenePandasMatrix  shapepatientNumpy  patientsPandas  valuesgeneNumpy  genePandasMatrix  valuespatientNumpy  TcorrMatrix  np  dot  patientNumpy  geneNumpy  corrMatrix  shapefig  ax  plt  subplots  figsize  30  15  ax  sns  heatmap  corrMatrix  cmap  RdBu  ax  ax  ,ibm
Export  as  slides  command  jupyter  nbconvert  Jupyter  Slides  ipynb  to  slides  post  serveimport  os  import  pandas  as  pd  import  numpy  as  np  import  seaborn  as  sns  import  matplotlib  pyplot  as  plt  from  sklearn  import  preprocessing  svm  from  itertools  import  combinations  from  sklearn  preprocessing  import  PolynomialFeatures  LabelEncoder  StandardScaler  import  sklearn  feature  selection  from  sklearn  model  selection  import  train  test  split  from  collections  import  defaultdict  from  sklearn  import  metrics  The  code  was  removed  by  DSX  for  sharing  Checking  that  everything  is  correct  pd  set  option  display  max  columns  30  credit  status  head  10  After  running  this  cell  we  will  see  that  we  have  no  missing  values  credit  status  info  Check  if  we  have  any  NaN  values  credit  status  isnull  values  any  Describe  columns  with  numerical  values  pd  set  option  precision  credit  status  describe  Find  correlations  credit  sta,ibm
tus  corr  method  pearson  Create  Grid  for  pairwise  relationships  gr  sns  PairGrid  credit  status  size  hue  class  gr  gr  map  diag  plt  hist  gr  gr  map  offdiag  plt  scatter  gr  gr  add  legend  Set  up  plot  size  fig  ax  plt  subplots  figsize  20  10  Attributes  destribution  sns  boxplot  orient  palette  hls  data  credit  status  credit  amount  fliersize  14  Tenure  data  distribution  histogram  sns  distplot  credit  status  credit  amount  hist  True  plt  show  Use  pandas  get  dummies  credit  status  encoded  pd  get  dummies  credit  status  credit  status  encoded  head  10  Create  training  data  for  that  will  undergo  preprocessing  credit  status  encoded  iloc  head  Extract  labels  from  sklearn  preprocessing  import  LabelEncoder  Split  last  column  from  original  dataset  as  the  labels  column  credit  status  class  Apply  encoder  to  transform  strings  to  numeric  values  and  le  LabelEncoder  fit  enc  le  transform  pd  DataFrame  enc  head  10  D,ibm
etect  outlier  using  interquartile  method  and  remove  them  def  find  outliers  df  quartile  quartile  np  percentile  df  25  75  iqr  quartile  quartile  lower  bound  quartile  iqr  upper  bound  quartile  iqr  outlier  indices  list  df  index  df  lower  bound  df  upper  bound  outlier  values  list  df  outlier  indices  df  outlier  indices  np  NaN  return  df  Find  outliers  in  first  column  continuous  values  print  find  outliers  duration  Find  outliers  in  first  column  continuous  values  print  find  outliers  credit  amount  Find  outliers  in  first  column  continuous  values  print  find  outliers  age  Check  for  null  values  isnull  values  any  Define  the  values  to  replce  and  the  strategy  of  choosing  the  replacement  value  from  sklearn  preprocessing  import  Imputer  suspected  cols  duration  credit  amount  age  imp  Imputer  missing  values  NaN  strategy  mean  pd  DataFrame  suspected  cols  imp  fit  transform  pd  DataFrame  suspected  cols  pd  Data,ibm
Frame  head  10  Check  for  null  values  pd  DataFrame  isnull  values  any  train  test  train  test  train  test  split  enc  test  size  random  state  42  print  train  shape  train  shape  print  test  shape  test  shape  print  train  columns  np  set  printoptions  precision  list  train  iloc  10  values  Use  StandardScaler  scaler  preprocessing  StandardScaler  fit  train  train  train  scaled  scaler  transform  train  pd  DataFrame  train  scaled  columns  train  columns  head  pd  DataFrame  train  head  from  sklearn  linear  model  import  LogisticRegression  clf  lr  LogisticRegression  model  clf  lr  fit  train  scaled  train  model  Use  the  scaler  fit  on  trained  data  to  scale  our  test  data  test  scaled  scaler  transform  test  pd  DataFrame  test  scaled  columns  train  columns  head  score  lr  clf  lr  decision  function  test  scaled  score  lry  pred  lr  clf  lr  predict  test  scaled  acc  lr  accuracy  score  test  pred  lr  print  acc  lr  average  precision  lr  av,ibm
erage  precision  score  test  score  lr  print  Average  precision  recall  score  2f  format  average  precision  lr  Plot  SVC  ROC  Curve  plt  figure  figsize  15  10  clf  fpr  lr  tpr  lr  thresh  lr  metrics  roc  curve  test  score  lr  auc  lr  metrics  roc  auc  score  test  score  lr  plt  plot  fpr  lr  tpr  lr  label  Logistic  Regression  on  Preprocessed  Data  auc  str  auc  lr  plt  legend  loc  plt  xlabel  False  Positives  plt  ylabel  True  Positives  The  code  was  removed  by  DSX  for  sharing  To  work  with  the  Watson  Machine  Learning  REST  API  you  must  generate  Bearer  access  token  import  urllib3  requests  json  headers  urllib3  util  make  headers  basic  auth  format  credentials  username  credentials  password  url  v3  identity  token  format  credentials  url  response  requests  get  url  headers  headers  ml  token  Bearer  json  loads  response  text  get  token  print  ml  token  Create  an  online  scoring  endpoint  endpoint  instance  credentials  url  v,ibm
3  wml  instances  credentials  instance  id  header  Content  Type  application  json  Authorization  ml  token  response  get  instance  requests  get  endpoint  instance  headers  header  print  response  get  instance  print  response  get  instance  text  Create  API  client  from  watson  machine  learning  client  import  WatsonMachineLearningAPIClient  client  WatsonMachineLearningAPIClient  credentials  Publish  model  in  Watson  Machine  Learning  repository  on  Cloud  model  props  client  repository  ModelMetaNames  AUTHOR  NAME  Heba  El  Shimy  client  repository  ModelMetaNames  NAME  Loan  Approval  Model  published  model  client  repository  store  model  model  model  meta  props  model  props  training  data  train  scaled  training  target  train  Create  model  deployment  published  model  uid  client  repository  get  model  uid  published  model  created  deployment  client  deployments  create  published  model  uid  Deployment  of  Loan  Approval  Model  Get  Scoring  URL  scoring,ibm
  endpoint  client  deployments  get  scoring  url  created  deployment  print  scoring  endpoint  Get  model  details  and  expected  input  model  details  client  repository  get  details  published  model  uid  print  json  dumps  model  details  indent  ,ibm
import  tensorflow  as  tf  import  numpy  as  np  tf  set  random  seed  777  for  reproducibility  import  sys  import  types  import  pandas  as  pd  from  botocore  client  import  Config  import  ibm  boto3  def  iter  self  return  The  following  code  accesses  file  in  your  IBM  Cloud  Object  Storage  It  includes  your  credentials  You  might  want  to  remove  those  credentials  before  you  share  your  notebook  client  ace6f71a1b9946cbb684ca0d9e6c34c0  ibm  boto3  client  service  name  s3  ibm  api  key  id  xBNCWwO4mooXZRHuqZ71nRmyIlQt8ca3ZWN8pOo56X69Dx  masking  use  your  own  ibm  auth  endpoint  https  iam  ng  bluemix  net  oidc  token  config  Config  signature  version  oauth  endpoint  url  https  s3  api  us  geo  objectstorage  service  networklayer  com  body  client  ace6f71a1b9946cbb684ca0d9e6c34c0  get  object  Bucket  tensorflowlabwithwatsonstudio  donotdelete  pr  neiwaip4a29fcg  Key  data  03  diabetes  csv  Body  add  missing  iter  method  so  pandas  accepts  body  as ,ibm
 file  like  object  if  not  hasattr  body  iter  body  iter  types  MethodType  iter  body  df  data  pd  read  csv  body  df  data  head  xy  df  data  values  xy  np  loadtxt  data  03  diabetes  csv  delimiter  dtype  np  float32  data  xy  data  xy  print  data  shape  data  shape  placeholders  for  tensor  that  will  be  always  fed  tf  placeholder  tf  float32  shape  None  tf  placeholder  tf  float32  shape  None  tf  Variable  tf  random  normal  name  weight  tf  Variable  tf  random  normal  name  bias  Hypothesis  using  sigmoid  tf  div  tf  exp  tf  matmul  hypothesis  tf  sigmoid  tf  matmul  cost  loss  function  cost  tf  reduce  mean  tf  log  hypothesis  tf  log  hypothesis  train  tf  train  GradientDescentOptimizer  learning  rate  01  minimize  cost  Accuracy  computation  True  if  hypothesis  else  False  predicted  tf  cast  hypothesis  dtype  tf  float32  accuracy  tf  reduce  mean  tf  cast  tf  equal  predicted  dtype  tf  float32  Launch  graph  sess  tf  Session  sess  run  ,ibm
tf  global  variables  initializer  for  step  in  range  10001  cost  val  sess  run  cost  train  feed  dict  data  data  if  step  500  print  step  cost  val  Accuracy  report  sess  run  hypothesis  predicted  accuracy  feed  dict  data  data  print  nHypothesis  nCorrect  nAccuracy  ,ibm
With  your  cursor  in  this  cell  insert  the  code  to  read  the  dataset  into  DataFrame  as  instructed  in  step  of  the  setup  instructions  above  New  version  of  imported  DataFrame  indexed  by  the  DATE  column  Make  sure  variable  name  on  the  right  of  the  assigment  statement  matches  the  value  inserted  into  the  code  cell  above  data  df  data  set  index  DATE  Required  imports  from  math  import  sqrt  from  numpy  import  concatenate  from  matplotlib  import  pyplot  import  pandas  as  pd  from  datetime  import  datetime  from  sklearn  preprocessing  import  MinMaxScaler  from  sklearn  metrics  import  mean  squared  error  from  keras  models  import  Sequential  from  keras  layers  import  Dense  from  keras  layers  import  Dropout  from  keras  layers  import  LSTM  import  plotly  offline  as  py  import  plotly  graph  objs  as  go  import  numpy  as  np  py  init  notebook  mode  connected  True  matplotlib  inline  Plot  the  data  read  in  cop  trace  go,ibm
  Scatter  data  index  data  WCOILWTICO  name  Price  py  iplot  cop  trace  Create  scaled  version  of  the  data  with  oil  prices  normalized  between  and  values  data  WCOILWTICO  values  reshape  values  values  astype  float32  scaler  MinMaxScaler  feature  range  scaled  scaler  fit  transform  values  Split  the  data  between  training  and  testing  The  first  70  of  the  data  is  used  for  training  while  the  remaining  30  is  used  for  validation  train  size  int  len  scaled  test  size  len  scaled  train  size  train  test  scaled  train  size  scaled  train  size  len  scaled  print  len  train  len  test  Generates  the  and  data  from  the  downloaded  dataset  The  last  values  in  the  input  data  are  left  off  and  the  values  are  generated  by  shifting  the  values  by  where  is  the  value  of  the  prev  periods  paramater  See  the  example  below  prev  periods  is  set  to  Original  weeks  05  15  25  35  45  New  weeks  05  15  25  25  35  45  def  gen  dat,ibm
asets  dataset  prev  periods  dataX  dataY  for  in  range  len  dataset  prev  periods  dataset  prev  periods  dataX  append  dataY  append  dataset  prev  periods  print  len  dataY  return  np  array  dataX  np  array  dataY  Generate  testing  and  validation  data  We  ll  use  sliding  window  size  of  week  to  predict  the  next  week  price  prev  periods  trainX  trainY  gen  datasets  train  prev  periods  testX  testY  gen  datasets  test  prev  periods  Reshape  into  numpy  arraya  of  shape  prev  periods  where  is  the  number  of  training  or  testing  values  trainX  np  reshape  trainX  trainX  shape  trainX  shape  testX  np  reshape  testX  testX  shape  testX  shape  Build  RNN  this  should  take  few  minutes  model  Sequential  model  add  LSTM  100  input  shape  trainX  shape  trainX  shape  model  add  Dropout  model  add  Dense  model  compile  loss  mean  squared  error  optimizer  adam  metrics  mae  history  model  fit  trainX  trainY  epochs  50  batch  size  32  validati,ibm
on  data  testX  testY  verbose  shuffle  False  Check  out  MSE  RMSE  MAE  for  training  and  testing  data  training  error  model  evaluate  trainX  trainY  verbose  print  Training  error  5f  MSE  5f  RMSE  5f  MAE  training  error  sqrt  training  error  training  error  testing  error  model  evaluate  testX  testY  verbose  print  Testing  error  5f  MSE  5f  RMSE  5f  MAE  testing  error  sqrt  testing  error  testing  error  Plot  validation  loss  vs  epoch  number  pyplot  plot  history  history  loss  label  training  loss  pyplot  plot  history  history  val  loss  label  test  loss  pyplot  legend  pyplot  show  Plot  prediction  vs  actual  using  scaled  values  yhat  test  model  predict  testX  print  yhat  test  shape  pyplot  plot  yhat  test  color  red  label  prediction  pyplot  plot  testY  color  blue  label  actual  pyplot  legend  pyplot  show  Convert  scaled  prices  back  to  original  scale  USD  yhat  test  inverse  scaler  inverse  transform  yhat  test  reshape  testY  inv,ibm
erse  scaler  inverse  transform  testY  reshape  Add  dates  back  dates  data  tail  len  testX  index  testY  reshape  testY  inverse  reshape  len  testY  inverse  yhat  test  reshape  yhat  test  inverse  reshape  len  yhat  test  inverse  Calculate  MSE  RMSE  based  on  original  USD  prices  mse  mean  squared  error  testY  inverse  yhat  test  inverse  rmse  sqrt  mse  print  Test  MSE  USD  3f  Test  RMSE  USD  3f  mse  rmse  Plot  actual  vs  predicted  using  actual  dates  and  USD  actual  go  Scatter  dates  testY  reshape  line  dict  color  rgb  255  width  name  Actual  Price  predicted  go  Scatter  dates  yhat  test  reshape  line  dict  color  rgb  255  width  name  Predicted  Price  py  iplot  predicted  actual  Grab  last  week  of  normalized  data  and  reshape  into  shape  expected  by  model  scaled  last  prices  scaled  len  scaled  prev  periods  len  scaled  scaled  last  prices  np  reshape  scaled  last  prices  prev  periods  print  scaled  last  prices  Predict  the  price,ibm
  for  the  week  of  2018  using  the  model  Note  this  will  be  on  scale  of  print  new  scaled  last  prices  shape  next  price  prediction  model  predict  scaled  last  prices  Transform  scaled  predicion  back  to  USD  price  next  price  inverse  scaler  inverse  transform  next  price  prediction  reshape  print  next  price  inverse  ,ibm
import  tensorflow  as  tf  tf  set  random  seed  777  for  reproducibility  data  data  Evaluation  our  model  using  this  test  dataset  test  test  tf  placeholder  float  None  tf  placeholder  float  None  tf  Variable  tf  random  normal  tf  Variable  tf  random  normal  tf  nn  softmax  computes  softmax  activations  softmax  exp  logits  reduce  sum  exp  logits  dim  hypothesis  tf  nn  softmax  tf  matmul  Cross  entropy  cost  loss  cost  tf  reduce  mean  tf  reduce  sum  tf  log  hypothesis  axis  Try  to  change  learning  rate  to  small  numbersl  rate  01  optimizer  tf  train  GradientDescentOptimizer  learning  rate  rate  minimize  cost  Correct  prediction  Test  model  prediction  tf  arg  max  hypothesis  is  correct  tf  equal  prediction  tf  argmax  accuracy  tf  reduce  mean  tf  cast  is  correct  tf  float32  Launch  graph  sess  tf  Session  Initialize  TensorFlow  variables  sess  run  tf  global  variables  initializer  for  step  in  range  1001  cost  val  val  sess  run,ibm
  cost  optimizer  feed  dict  data  data  if  step  100  print  step  cost  val  predict  print  Prediction  sess  run  prediction  feed  dict  test  Calculate  the  accuracy  print  Accuracy  sess  run  accuracy  feed  dict  test  test  when  lr  73203  30548954  22985029  66033536  39069986  29670858  99386835  34510708  09743214  80419564  23  1494  06951046  29449689  0999819  95319986  63627958  48935604  90760708  65020132  50593793  27  2798  44451016  85699677  03748143  48429942  98872018  57314301  52989244  16229868  74406147  668  12396193  61504567  47498202  22003263  2470119  9268558  96035379  41933775  43156195  77111  9524312  13037777  08607888  78651619  26245379  42393875  07170963  14037919  12054014  inf  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  Prediction  Accuracy  When  lr  1e  10  73203  80269563  67861295  21728313  3051686  3032113  50825703  75722361  7008909  10820389  73203  80269563  67861295  21728313  3051686  3032113  5,ibm
0825703  75722361  7008909  10820389  73203  80269563  67861295  21728313  3051686  3032113  50825703  75722361  7008909  10820389  198  73203  80269563  67861295  21728313  3051686  3032113  50825703  75722361  7008909  10820389  199  73203  80269563  67861295  21728313  3051686  3032113  50825703  75722361  7008909  10820389  200  73203  80269563  67861295  21728313  3051686  3032113  50825703  75722361  7008909  10820389  Prediction  Accuracy  When  lr  73203  72881663  71536207  18015325  57753736  12988332  60729778  48373488  51433605  02127004  318  66219079  74796319  14612854  81948912  03000021  68936598  23214608  33772916  94628811  0218  64342022  74127686  12067163  81161296  00900121  72049117  2086665  35079569  909742  199  672261  15377033  28146935  13632679  37484586  18958236  33544877  35609841  43973011  25604188  200  670909  15885413  28058422  14229572  37609792  19073224  33304682  35536593  44033223  2561723  Prediction  Accuracy  ,ibm
import  sys  sys  path  append  home  ec2  user  anaconda3  lib  python3  site  packages  import  boto3  import  re  from  sagemaker  import  get  execution  role  assumed  role  get  execution  role  print  str  assumed  role  Bucket  location  temp  s3  s3  cnidus  ml  pdx  criteo  temp  dataset  s3  s3  sagemaker  us  west  369233609183  datasets  criteo  16  tb  criteo  20180605  141913  aws  s3  ls  s3  sagemaker  us  west  369233609183  datasets  criteo  16  tb  criteo  20180605  141913  wc  bash  mkdir  tmp  criteo  cd  tmp  criteo  curl  http  azuremlsampleexperiments  blob  core  windows  net  criteo  day  gz  Upload  to  S3  TODO  bash  git  clone  mvs  script  mode  pipe  ps  server  https  github  com  mvsusp  sagemaker  tensorflow  containers  git  pwdimport  os  print  os  getcwd  os  chdir  sagemaker  tensorflow  containers  test  integration  benchmarks  criteo  print  os  getcwd  from  sagemaker  tensorflow  import  TensorFlow  Change  this  to  your  criteo  small  clicks  or  large  clicks ,amazon
 datasets  https  github  com  GoogleCloudPlatform  cloudml  samples  tree  master  criteo  tft  criteo  dataset  CRITEO  DATASET  dataset  s3  hyperparameters  sets  the  number  of  parameter  servers  in  the  cluster  sagemaker  num  parameter  servers  10  s3  channel  CRITEO  DATASET  batch  size  30000  dataset  kaggle  model  type  linear  l2  regularization  100  see  https  www  tensorflow  org  performance  performance  guide  optimizing  for  cpu  Best  value  for  this  model  is  10  default  value  in  the  container  is  sets  the  value  to  the  number  of  logical  cores  inter  op  parallelism  threads  10  environment  variables  that  will  be  written  to  the  container  before  training  starts  sagemaker  env  vars  True  uses  HTTPS  uses  HTTP  otherwise  Default  false  see  https  docs  aws  amazon  com  sdk  for  cpp  v1  developer  guide  client  config  html  S3  USE  HTTPS  True  True  verifies  SSL  Default  false  S3  VERIFY  SSL  True  Sets  the  time  in  milliseconds  th,amazon
at  thread  should  wait  after  completing  the  execution  of  parallel  region  before  sleeping  Default  see  https  github  com  tensorflow  tensorflow  blob  faff6f2a60a01dba57cf3a3ab832279dbe174798  tensorflow  docs  src  performance  performance  guide  md  tuning  mkl  for  the  best  performance  KMP  BLOCKTIME  25  tf  TensorFlow  entry  point  task  py  source  dir  trainer  train  instance  count  10  train  instance  type  ml  c5  9xlarge  pass  in  your  own  SageMaker  role  role  assumed  role  hyperparameters  hyperparameters  This  points  to  the  prototype  images  Change  the  region  to  us  west  or  us  east  or  TF  version  to  if  needed  tf  train  image  lambda  520713654638  dkr  ecr  us  west  amazonaws  com  sagemaker  tensorflow  cpu  py2  script  mode  preview  publicly  accessible  placeholder  data  Change  the  region  if  needed  tf  fit  training  s3  sagemaker  sample  data  us  west  spark  mnist  train  run  tensorboard  locally  True  ,amazon
import  mxnet  as  mx  import  symbol  alexnet  as  alexnet  import  recotools  Define  some  constants  max  user  int  1e6  title  vocab  int  1e5  ngram  dimensions  int  1e8  def  dssm  recommender  input  variables  title  mx  symbol  Variable  title  words  image  mx  symbol  Variable  image  queries  mx  symbol  Variable  query  ngrams  user  mx  symbol  Variable  user  id  label  mx  symbol  Variable  label  Process  content  stack  image  alexnet  features  image  256  title  recotools  SparseBagOfWordProjection  data  title  vocab  size  title  vocab  output  dim  title  mx  symbol  FullyConnected  data  title  num  hidden  content  mx  symbol  Concat  image  title  content  mx  symbol  Dropout  content  content  mx  symbol  FullyConnected  data  content  num  hidden  Process  user  stack  user  mx  symbol  Embedding  data  user  input  dim  max  user  output  dim  user  mx  symbol  FullyConnected  data  user  num  hidden  queries  recotools  SparseBagOfWordProjection  data  queries  vocab  size  ng,amazon
ram  dimensions  output  dim  queries  mx  symbol  FullyConnected  data  queries  num  hidden  user  mx  symbol  Concat  user  queries  user  mx  symbol  Dropout  user  user  mx  symbol  FullyConnected  data  user  num  hidden  loss  layer  pred  recotools  CosineLoss  user  content  label  label  return  pred  net1  dssm  recommender  256  mx  viz  plot  network  net1  ,amazon
bucket  my  bucket  name  prefix  sagemaker  closest  avg  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  import  io  import  os  import  time  import  json  import  sagemaker  amazon  common  as  smac  import  sagemaker  from  sagemaker  predictor  import  csv  serializer  json  deserializerv  np  random  random  1000  25  labels  John  Petrucci  Mike  Mangini  Jordan  Rudess  James  LaBrie  John  Myung  avg  350  450  780  280  100  label  series  feature  series  for  in  range  1000  label  series  append  labels  feature  series  append  avg  df  pd  DataFrame  label  label  series  feature  feature  series  df  groupby  label  agg  feature  count  len  mean  np  mean  dftrain  data  rate  evaluation  data  rate  train  data  df  int  len  df  train  data  rate  validation  data  df  int  len  df  train  data  rate  int  len  df  train  data  rate  ,amazon
evaluation  data  rate  test  data  df  int  len  df  train  data  rate  evaluation  data  rate  print  len  train  data  len  validation  data  len  test  data  import  pickle  buf  io  BytesIO  buf  write  pickle  dumps  train  data  buf  seek  key  closest  avg  train  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  buf  s3  train  data  s3  train  format  bucket  prefix  key  print  uploaded  training  data  location  format  s3  train  data  import  pickle  buf  io  BytesIO  buf  write  pickle  dumps  validation  data  buf  seek  key  closest  avg  validation  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  key  upload  fileobj  buf  s3  validation  data  s3  validation  format  bucket  prefix  key  print  uploaded  training  data  location  format  s3  train  data  container  my  account  id  dkr  ecr  us  east  amazonaws  com  sagemaker  closest  avg  latest  sess  sagemaker  Session  linear  sagemaker  estim,amazon
ator  Estimator  container  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  s3  output  format  bucket  prefix  sagemaker  session  sess  linear  set  hyperparameters  feature  dim  59  mini  batch  size  100  predictor  type  regressor  epochs  10  num  models  32  loss  absolute  loss  linear  fit  training  s3  train  data  validation  s3  validation  data  linear  predictor  linear  deploy  initial  instance  count  instance  type  ml  c4  xlarge  import  pickle  pickle  serializer  lambda  data  pickle  dumps  data  pickle  deserializer  lambda  body  mime  pickle  loads  body  read  linear  predictor  content  type  binary  octet  stream  linear  predictor  serializer  pickle  serializer  linear  predictor  deserializer  pickle  deserializerresult  linear  predictor  predict  test  data  feature  test  data  predicted  result  accuracy  len  test  data  test  data  label  test  data  predicted  len  test  data  sagemaker  Session  delete  endpoint  linear  predictor  e,amazon
ndpoint  ,amazon
S3  bucket  and  prefix  bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  data  distribution  types  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  boto3  import  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  from  IPython  display  import  display  import  io  import  time  import  copy  import  json  import  sys  import  sagemaker  amazon  common  as  smac  import  osdef  get  gdelt  filename  s3  boto3  resource  s3  s3  Bucket  gdelt  open  data  download  file  events  filename  gdelt  csv  df  pd  read  csv  gdelt  csv  sep  header  pd  read  csv  https  www  gdeltproject  org  data  lookups  CSV  header  historical  txt  sep  df  columns  header  columns  return  dfdata  get  gdelt  1979  csv  datadata  data  EventCode  NumArticles  AvgTone  Actor1Geo  Lat  Actor1Geo  Long  Actor2Geo  Lat  Actor2Geo  Long  data  EventCode  data  EventCode  astype  object  for  column  in  data  ,amazon
select  dtypes  include  object  columns  display  pd  crosstab  index  data  column  columns  observations  normalize  columns  display  data  describe  hist  data  hist  bins  30  sharey  True  figsize  10  10  plt  show  events  pd  crosstab  index  data  EventCode  columns  count  sort  values  by  count  ascending  False  index  20  def  write  to  s3  bucket  prefix  channel  file  prefix  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  astype  float32  astype  float32  buf  seek  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  channel  file  prefix  data  upload  fileobj  buf  def  transform  gdelt  df  events  None  df  df  AvgTone  EventCode  NumArticles  Actor1Geo  Lat  Actor1Geo  Long  Actor2Geo  Lat  Actor2Geo  Long  df  EventCode  df  EventCode  astype  object  if  events  is  not  None  df  df  np  in1d  df  EventCode  events  return  pd  get  dummies  df  df  Actor1Geo  Lat  df  Actor1Geo  Long  True  df  Actor2Geo  Lat  df  Actor2Geo  Long  True  def,amazon
  prepare  gdelt  bucket  prefix  file  prefix  events  None  random  state  1729  df  get  gdelt  file  prefix  csv  model  data  transform  gdelt  df  events  train  data  validation  data  np  split  model  data  sample  frac  random  state  random  state  as  matrix  int  len  model  data  write  to  s3  bucket  prefix  train  file  prefix  train  data  train  data  write  to  s3  bucket  prefix  validation  file  prefix  validation  data  validation  data  for  year  in  range  1979  1984  prepare  gdelt  bucket  prefix  str  year  events  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  boto3  Session  region  name  linear  learner  common  training  params  RoleArn  role  AlgorithmSpecification  TrainingImage  container  TrainingInputMode  File  ResourceConfig  InstanceCount  InstanceType  ml  c4  2xlarge  VolumeSizeInGB  10  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  Comp,amazon
ressionType  None  RecordWrapperType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  prefix  S3DataDistributionType  FullyReplicated  CompressionType  None  RecordWrapperType  None  OutputDataConfig  S3OutputPath  s3  format  bucket  prefix  HyperParameters  feature  dim  25  mini  batch  size  500  predictor  type  regressor  epochs  num  models  32  loss  absolute  loss  StoppingCondition  MaxRuntimeInSeconds  60  60  sharded  job  DEMO  linear  sharded  time  strftime  time  gmtime  print  Job  name  is  sharded  job  sharded  training  params  copy  deepcopy  common  training  params  sharded  training  params  TrainingJobName  sharded  job  sharded  training  params  InputDataConfig  DataSource  S3DataSource  S3DataDistributionType  ShardedByS3Key  replicated  job  DEMO  linear  replicated  time  strftime  time  gmtime  print  Job  name  is  replicated  job  replicated  training  params  copy  deepcopy  common  training  params  repli,amazon
cated  training  params  TrainingJobName  replicated  job  replicated  training  params  InputDataConfig  DataSource  S3DataSource  S3DataDistributionType  FullyReplicated  time  region  boto3  Session  region  name  sm  boto3  Session  client  sagemaker  sm  create  training  job  sharded  training  params  sm  create  training  job  replicated  training  params  status  sm  describe  training  job  TrainingJobName  replicated  job  TrainingJobStatus  print  status  sm  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  replicated  job  status  sm  describe  training  job  TrainingJobName  replicated  job  TrainingJobStatus  print  Training  job  ended  with  status  status  if  status  Failed  message  sm  describe  training  job  TrainingJobName  replicated  job  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  print  Sharded  sm  describe  training  job  TrainingJobName  sharded  job  TrainingJobStatus  p,amazon
rint  Replicated  sm  describe  training  job  TrainingJobName  replicated  job  TrainingJobStatus  def  get  train  timing  job  client  boto3  client  logs  streams  client  describe  log  streams  logGroupName  aws  sagemaker  TrainingJobs  logStreamNamePrefix  job  streams  logStreamName  for  in  streams  logStreams  times  for  stream  in  streams  events  client  get  log  events  logGroupName  aws  sagemaker  TrainingJobs  logStreamName  stream  events  times  timestamp  for  in  events  return  max  times  min  times  60000  print  Sharded  get  train  timing  sharded  job  minutes  print  Replicated  get  train  timing  replicated  job  minutes  sharded  model  response  sm  create  model  ModelName  sharded  job  ExecutionRoleArn  role  PrimaryContainer  Image  container  ModelDataUrl  sm  describe  training  job  TrainingJobName  sharded  job  ModelArtifacts  S3ModelArtifacts  print  sharded  model  response  ModelArn  replicated  model  response  sm  create  model  ModelName  replicated  job  Exe,amazon
cutionRoleArn  role  PrimaryContainer  Image  container  ModelDataUrl  sm  describe  training  job  TrainingJobName  replicated  job  ModelArtifacts  S3ModelArtifacts  print  replicated  model  response  ModelArn  sharded  endpoint  config  DEMO  sharded  endpoint  config  time  strftime  time  gmtime  print  sharded  endpoint  config  sharded  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  sharded  endpoint  config  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  sharded  job  VariantName  AllTraffic  print  Endpoint  Config  Arn  sharded  endpoint  config  response  EndpointConfigArn  replicated  endpoint  config  DEMO  replicated  endpoint  config  time  strftime  time  gmtime  print  replicated  endpoint  config  replicated  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  replicated  endpoint  config  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  replicated  job  VariantName,amazon
  AllTraffic  print  Endpoint  Config  Arn  replicated  endpoint  config  response  EndpointConfigArn  time  sharded  endpoint  DEMO  sharded  endpoint  time  strftime  time  gmtime  print  sharded  endpoint  sharded  endpoint  response  sm  create  endpoint  EndpointName  sharded  endpoint  EndpointConfigName  sharded  endpoint  config  print  sharded  endpoint  response  EndpointArn  replicated  endpoint  DEMO  replicated  endpoint  time  strftime  time  gmtime  print  replicated  endpoint  replicated  endpoint  response  sm  create  endpoint  EndpointName  replicated  endpoint  EndpointConfigName  replicated  endpoint  config  print  replicated  endpoint  response  EndpointArn  resp  sm  describe  endpoint  EndpointName  replicated  endpoint  status  resp  EndpointStatus  print  Status  status  sm  get  waiter  endpoint  in  service  wait  EndpointName  replicated  endpoint  resp  sm  describe  endpoint  EndpointName  replicated  endpoint  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print ,amazon
 Status  status  if  status  InService  raise  Exception  Endpoint  creation  did  not  succeed  print  Sharded  sm  describe  endpoint  EndpointName  sharded  endpoint  EndpointStatus  print  Replicated  sm  describe  endpoint  EndpointName  replicated  endpoint  EndpointStatus  test  data  transform  gdelt  get  gdelt  1984  csv  events  as  matrix  test  test  data  test  test  data  def  np2csv  arr  csv  io  BytesIO  np  savetxt  csv  arr  delimiter  fmt  return  csv  getvalue  decode  rstrip  def  predict  batches  data  endpoint  rows  1024  1024  sys  getsizeof  np2csv  data  split  array  np  array  split  data  int  data  shape  float  rows  predictions  runtime  boto3  Session  client  runtime  sagemaker  for  array  in  split  array  payload  np2csv  array  response  runtime  invoke  endpoint  EndpointName  endpoint  ContentType  text  csv  Body  payload  result  json  loads  response  Body  read  decode  predictions  score  for  in  result  predictions  return  np  array  predictions  sharded  pr,amazon
edictions  predict  batches  test  sharded  endpoint  replicated  predictions  predict  batches  test  replicated  endpoint  print  Sharded  MSE  np  mean  test  sharded  predictions  print  Replicated  MSE  np  mean  test  replicated  predictions  sm  delete  endpoint  EndpointName  sharded  endpoint  sm  delete  endpoint  EndpointName  replicated  endpoint  ,amazon
import  tensorflow  as  tf  import  numpy  as  np  tf  set  random  seed  777  for  reproducibility  learning  rate  data  data  data  np  array  data  dtype  np  float32  data  np  array  data  dtype  np  float32  tf  placeholder  tf  float32  None  tf  placeholder  tf  float32  None  W1  tf  Variable  tf  random  normal  10  name  weight1  b1  tf  Variable  tf  random  normal  10  name  bias1  layer1  tf  sigmoid  tf  matmul  W1  b1  W2  tf  Variable  tf  random  normal  10  10  name  weight2  b2  tf  Variable  tf  random  normal  10  name  bias2  layer2  tf  sigmoid  tf  matmul  layer1  W2  b2  W3  tf  Variable  tf  random  normal  10  10  name  weight3  b3  tf  Variable  tf  random  normal  10  name  bias3  layer3  tf  sigmoid  tf  matmul  layer2  W3  b3  W4  tf  Variable  tf  random  normal  10  name  weight4  b4  tf  Variable  tf  random  normal  name  bias4  hypothesis  tf  sigmoid  tf  matmul  layer3  W4  b4  cost  loss  function  cost  tf  reduce  mean  tf  log  hypothesis  tf  log  hypothesis  train,ibm
  tf  train  GradientDescentOptimizer  learning  rate  learning  rate  minimize  cost  Accuracy  computation  True  if  hypothesis  else  False  predicted  tf  cast  hypothesis  dtype  tf  float32  accuracy  tf  reduce  mean  tf  cast  tf  equal  predicted  dtype  tf  float32  Launch  graph  sess  tf  Session  Initialize  TensorFlow  variables  sess  run  tf  global  variables  initializer  for  step  in  range  10001  sess  run  train  feed  dict  data  data  if  step  1000  print  step  sess  run  cost  feed  dict  data  data  print  w1  sess  run  W1  print  w2  sess  run  W2  print  w3  sess  run  W3  print  w4  sess  run  W4  Accuracy  report  sess  run  hypothesis  predicted  accuracy  feed  dict  data  data  print  nHypothesis  nCorrect  nAccuracy  Hypothesis  01338218  98166394  98809403  01135799  Correct  Accuracy  ,ibm
Setup  from  sagemaker  import  get  execution  role  import  sagemaker  sagemaker  session  sagemaker  Session  This  role  retrieves  the  SageMaker  compatible  role  used  by  this  Notebook  Instance  role  get  execution  role  import  dataset  file  paths  dataset  download  dataset  stsa  binary  new  file  paths  dataset  get  stsa  dataset  file  paths  train  test  vocab  dataset  get  stsa  dataset  file  paths  with  open  file  paths  as  for  in  range  20  line  readline  print  line  import  os  import  shutil  import  numpy  as  np  train  data  element  for  element  in  train  train  labels  element  for  element  in  train  test  data  element  for  element  in  test  test  labels  element  for  element  in  test  try  os  makedirs  tmp  data  train  sentiment  os  makedirs  tmp  data  test  sentiment  os  makedirs  tmp  data  vocab  np  savez  tmp  data  train  sentiment  train  npz  data  train  data  labels  train  labels  np  savez  tmp  data  test  sentiment  test  npz  data  test  d,amazon
ata  labels  test  labels  np  save  tmp  data  vocab  vocab  npy  vocab  train  input  sagemaker  session  upload  data  path  os  path  join  tmp  data  train  sentiment  key  prefix  notebook  chainer  sentiment  train  test  input  sagemaker  session  upload  data  path  os  path  join  tmp  data  test  sentiment  key  prefix  notebook  chainer  sentiment  test  vocab  input  sagemaker  session  upload  data  path  os  path  join  tmp  data  vocab  key  prefix  notebook  chainer  sentiment  vocab  finally  shutil  rmtree  tmp  data  pygmentize  src  sentiment  analysis  py  from  sagemaker  chainer  estimator  import  Chainer  chainer  estimator  Chainer  entry  point  sentiment  analysis  py  source  dir  src  role  role  sagemaker  session  sagemaker  session  train  instance  count  train  instance  type  ml  p2  xlarge  hyperparameters  epochs  10  batch  size  64  chainer  estimator  fit  train  train  input  test  test  input  vocab  vocab  input  from  s3  util  import  retrieve  output  from  s3  ,amazon
chainer  training  job  chainer  estimator  latest  training  job  name  desc  sagemaker  session  sagemaker  client  describe  training  job  TrainingJobName  chainer  training  job  output  data  desc  ModelArtifacts  S3ModelArtifacts  replace  model  tar  gz  output  tar  gz  retrieve  output  from  s3  output  data  output  sentiment  from  IPython  display  import  Image  from  IPython  display  import  display  accuracy  graph  Image  filename  output  sentiment  accuracy  png  width  800  height  800  loss  graph  Image  filename  output  sentiment  loss  png  width  800  height  800  display  accuracy  graph  loss  graph  predictor  chainer  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  sentences  It  is  fun  and  easy  to  train  Chainer  models  on  Amazon  SageMaker  It  used  to  be  slow  difficult  and  laborious  to  train  and  deploy  model  to  production  But  now  it  is  super  fast  to  deploy  to  production  And  love  it  when  my  model  generalizes  p,amazon
redictions  predictor  predict  sentences  for  prediction  in  predictions  sentence  prediction  score  prediction  print  sentence  nprediction  nscore  format  sentence  prediction  score  with  open  file  paths  as  sentences  readlines  2000  sentences  sentence  strip  for  sentence  in  sentences  predictions  predictor  predict  sentences  predictions  predictor  predict  sentences  for  prediction  in  predictions  sentence  prediction  score  prediction  print  sentence  nprediction  nscore  format  sentence  prediction  score  chainer  estimator  delete  endpoint  ,amazon
Use  Curl  to  get  document  from  GitHub  and  open  it  curl  https  raw  githubusercontent  com  MicrosoftLearning  AI  Introduction  master  files  Moon  txt  Moon  txt  doc1  open  Moon  txt  Read  the  document  and  print  its  contents  doc1Txt  doc1  read  print  doc1Txt  from  string  import  punctuation  remove  numeric  digits  txt  join  for  in  doc1Txt  if  not  isdigit  remove  punctuation  and  make  lower  case  txt  join  for  in  txt  if  not  in  punctuation  lower  print  the  normalized  text  print  txt  import  nltk  import  pandas  as  pd  from  nltk  probability  import  FreqDist  nltk  download  punkt  Tokenize  the  text  into  individual  words  words  nltk  tokenize  word  tokenize  txt  Get  the  frequency  distribution  of  the  words  into  data  frame  fdist  FreqDist  words  count  frame  pd  DataFrame  fdist  index  count  frame  columns  Count  print  count  frame  matplotlib  inline  import  matplotlib  pyplot  as  plt  Sort  the  data  frame  by  frequency  counts  cou,microsoft
nt  frame  sort  values  Count  ascending  False  Display  the  top  60  words  as  bar  plot  fig  plt  figure  figsize  16  ax  fig  gca  counts  Count  60  plot  kind  bar  ax  ax  ax  set  title  Frequency  of  the  most  common  words  ax  set  ylabel  Frequency  of  word  ax  set  xlabel  Word  plt  show  Get  standard  stop  words  from  NLTK  nltk  download  stopwords  from  nltk  corpus  import  stopwords  Filter  out  the  stop  words  txt  join  word  for  word  in  txt  split  if  word  not  in  stopwords  words  english  Get  the  frequency  distribution  of  the  remaining  words  words  nltk  tokenize  word  tokenize  txt  fdist  FreqDist  words  count  frame  pd  DataFrame  fdist  index  count  frame  columns  Count  Plot  the  frequency  of  the  top  60  words  counts  count  frame  sort  values  Count  ascending  False  fig  plt  figure  figsize  16  ax  fig  gca  counts  Count  60  plot  kind  bar  ax  ax  ax  set  title  Frequency  of  the  most  common  words  ax  set  ylabel  Frequency ,microsoft
 of  word  ax  set  xlabel  Word  plt  show  remind  ourselves  of  the  first  document  print  doc1Txt  print  Get  second  document  normalize  it  and  remove  stop  words  curl  https  raw  githubusercontent  com  MicrosoftLearning  AI  Introduction  master  files  Gettysburg  txt  Gettysburg  txt  doc2  open  Gettysburg  txt  doc2Txt  doc2  read  print  doc2Txt  from  string  import  punctuation  txt2  join  for  in  doc2Txt  if  not  isdigit  txt2  join  for  in  txt2  if  not  in  punctuation  lower  txt2  join  word  for  word  in  txt2  split  if  word  not  in  stopwords  words  english  and  third  print  curl  https  raw  githubusercontent  com  MicrosoftLearning  AI  Introduction  master  files  Cognitive  txt  Cognitive  txt  doc3  open  Cognitive  txt  doc3Txt  doc3  read  print  doc3Txt  from  string  import  punctuation  txt3  join  for  in  doc3Txt  if  not  isdigit  txt3  join  for  in  txt3  if  not  in  punctuation  lower  txt3  join  word  for  word  in  txt3  split  if  word  not  in  ,microsoft
stopwords  words  english  install  textblob  library  and  define  functions  for  TF  IDF  pip  install  textblob  import  math  from  textblob  import  TextBlob  as  tb  def  tf  word  doc  return  doc  words  count  word  len  doc  words  def  contains  word  docs  return  sum  for  doc  in  docs  if  word  in  doc  words  def  idf  word  docs  return  math  log  len  docs  contains  word  docs  def  tfidf  word  doc  docs  return  tf  word  doc  idf  word  docs  Create  collection  of  documents  as  textblobs  doc1  tb  txt  doc2  tb  txt2  doc3  tb  txt3  docs  doc1  doc2  doc3  Use  TF  IDF  to  get  the  three  most  important  words  from  each  document  print  for  doc  in  enumerate  docs  print  Top  words  in  document  format  scores  word  tfidf  word  doc  docs  for  word  in  doc  words  sorted  words  sorted  scores  items  key  lambda  reverse  True  for  word  score  in  sorted  words  print  tWord  TF  IDF  format  word  round  score  Load  and  print  text  curl  https  raw  githubuser,microsoft
content  com  MicrosoftLearning  AI  Introduction  master  files  KennedyInaugural  txt  KennedyInaugural  txt  doc4  open  KennedyInaugural  txt  kenTxt  doc4  read  print  kenTxt  Normalize  and  remove  stop  words  from  string  import  punctuation  kenTxt  join  for  in  kenTxt  if  not  isdigit  kenTxt  join  for  in  kenTxt  if  not  in  punctuation  lower  kenTxt  join  word  for  word  in  kenTxt  split  if  word  not  in  stopwords  words  english  Get  Frequency  distribution  words  nltk  tokenize  word  tokenize  kenTxt  fdist  FreqDist  words  count  frame  pd  DataFrame  fdist  index  count  frame  columns  Count  Plot  frequency  counts  count  frame  sort  values  Count  ascending  False  fig  plt  figure  figsize  16  ax  fig  gca  counts  Count  60  plot  kind  bar  ax  ax  ax  set  title  Frequency  of  the  most  common  words  ax  set  ylabel  Frequency  of  word  ax  set  xlabel  Word  plt  show  from  nltk  stem  porter  import  PorterStemmer  Get  the  word  stems  ps  PorterStemmer  ,microsoft
stems  ps  stem  word  for  word  in  words  Get  Frequency  distribution  fdist  FreqDist  stems  count  frame  pd  DataFrame  fdist  index  count  frame  columns  Count  Plot  frequency  counts  count  frame  sort  values  Count  ascending  False  fig  plt  figure  figsize  16  ax  fig  gca  counts  Count  60  plot  kind  bar  ax  ax  ax  set  title  Frequency  of  the  most  common  words  ax  set  ylabel  Frequency  of  word  ax  set  xlabel  Word  plt  show  import  http  client  urllib  request  urllib  parse  urllib  error  base64  json  myText  input  Please  enter  some  text  headers  Request  headers  Content  Type  application  json  Ocp  Apim  Subscription  Key  YOUR  KEY  HERE  params  urllib  parse  urlencode  body  language  en  analyzerIds  4fa79af1  f22c  408d  98bb  b7d7aeef7f04  22a6b758  420f  4745  8a3c  46835a67c0d2  text  myText  try  conn  http  client  HTTPSConnection  westus  api  cognitive  microsoft  com  conn  request  POST  linguistics  v1  analyze  params  str  body  headers  r,microsoft
esponse  conn  getresponse  data  response  read  parsed  json  loads  data  for  analyzer  in  parsed  print  Analyzer  analyzer  analyzerId  print  analyzer  result  print  conn  close  except  Exception  as  print  Errno  format  errno  strerror  textAnalyticsURI  REGION  api  cognitive  microsoft  com  textKey  YOUR  KEY  HERE  import  http  client  urllib  request  urllib  parse  urllib  error  base64  json  urllib  Define  the  request  headers  headers  Content  Type  application  json  Ocp  Apim  Subscription  Key  textKey  Accept  application  json  Define  the  parameters  params  urllib  parse  urlencode  Define  the  request  body  body  documents  language  en  id  text  doc2Txt  language  en  id  text  doc3Txt  try  Execute  the  REST  API  call  and  get  the  response  conn  http  client  HTTPSConnection  textAnalyticsURI  conn  request  POST  text  analytics  v2  keyPhrases  params  str  body  headers  response  conn  getresponse  data  response  read  decode  UTF  data  contains  the  JSON  ,microsoft
response  which  includes  collection  of  documents  parsed  json  loads  data  for  document  in  parsed  documents  print  Document  document  id  key  phrases  for  phrase  in  document  keyPhrases  print  phrase  print  conn  close  except  Exception  as  print  Error  print  body  documents  language  en  id  text  Wow  cognitive  services  are  fantastic  language  en  id  text  hate  it  when  computers  don  understand  me  try  conn  http  client  HTTPSConnection  textAnalyticsURI  conn  request  POST  text  analytics  v2  sentiment  params  str  body  headers  response  conn  getresponse  data  response  read  decode  UTF  parsed  json  loads  data  Get  the  numeric  score  for  each  document  for  document  in  parsed  documents  sentiment  negative  if  it  more  than  consider  the  sentiment  to  be  positive  if  document  score  sentiment  positive  print  Document  document  id  sentiment  conn  close  except  Exception  as  print  Errno  format  errno  strerror  speechKey  YOUR  KEY  HERE,microsoft
  pip  install  SpeechRecognition  pip  install  pyaudioimport  speech  recognition  as  sr  Read  the  audio  file  sr  Recognizer  with  sr  Microphone  as  source  print  Say  something  audio  listen  source  transcribe  speech  using  the  Bing  Speech  API  try  transcription  recognize  bing  audio  key  speechKey  print  Here  what  heard  print  transcription  except  sr  UnknownValueError  print  The  audio  was  unclear  except  sr  RequestError  as  print  print  Something  went  wrong  format  import  IPython  import  http  client  urllib  parse  json  from  xml  etree  import  ElementTree  Get  the  input  text  myText  input  What  would  you  like  me  to  say  The  Speech  API  requires  an  access  token  valid  for  10  mins  apiKey  speechKey  params  headers  Ocp  Apim  Subscription  Key  apiKey  AccessTokenHost  api  cognitive  microsoft  com  path  sts  v1  issueToken  Use  the  API  key  to  request  an  access  token  conn  http  client  HTTPSConnection  AccessTokenHost  conn  request,microsoft
  POST  path  params  headers  response  conn  getresponse  data  response  read  conn  close  accesstoken  data  decode  UTF  Now  that  we  have  token  we  can  set  up  the  request  body  ElementTree  Element  speak  version  body  set  http  www  w3  org  XML  1998  namespace  lang  en  us  voice  ElementTree  SubElement  body  voice  voice  set  http  www  w3  org  XML  1998  namespace  lang  en  US  voice  set  http  www  w3  org  XML  1998  namespace  gender  Male  voice  set  name  Microsoft  Server  Speech  Text  to  Speech  Voice  en  US  JessaRUS  voice  text  myText  headers  Content  type  application  ssml  xml  Microsoft  OutputFormat  riff  16khz  16bit  mono  pcm  Authorization  Bearer  accesstoken  Search  AppId  07D3234E49CE426DAA29772419F436CA  Search  ClientID  1ECFAE91408841A480F00935DC390960  User  Agent  TTSForPython  Connect  to  server  to  synthesize  wav  from  the  text  conn  http  client  HTTPSConnection  speech  platform  bing  com  conn  request  POST  synthesize  ElementTre,microsoft
e  tostring  body  headers  response  conn  getresponse  data  response  read  conn  close  Play  the  wav  IPython  display  Audio  data  autoplay  True  transTextKey  YOUR  KEY  HERE  import  requests  http  client  urllib  request  urllib  parse  urllib  error  base64  json  urllib  from  xml  etree  import  ElementTree  textToTranslate  input  Please  enter  some  text  fromLangCode  input  What  language  is  this  toLangCode  input  To  what  language  would  you  like  it  translated  try  Connect  to  server  to  get  the  Access  Token  apiKey  transTextKey  params  headers  Ocp  Apim  Subscription  Key  apiKey  AccessTokenHost  api  cognitive  microsoft  com  path  sts  v1  issueToken  conn  http  client  HTTPSConnection  AccessTokenHost  conn  request  POST  path  params  headers  response  conn  getresponse  data  response  read  conn  close  accesstoken  Bearer  data  decode  UTF  Define  the  request  headers  headers  Authorization  accesstoken  Define  the  parameters  params  urllib  parse  u,microsoft
rlencode  text  textToTranslate  to  toLangCode  from  fromLangCode  Execute  the  REST  API  call  and  get  the  response  conn  http  client  HTTPSConnection  api  microsofttranslator  com  conn  request  GET  V2  Http  svc  Translate  params  body  headers  response  conn  getresponse  data  response  read  translation  ElementTree  fromstring  data  decode  utf  print  translation  text  conn  close  except  Exception  as  print  Errno  format  errno  strerror  matplotlib  inline  from  matplotlib  pyplot  import  imshow  from  PIL  import  Image  import  requests  from  io  import  BytesIO  import  json  Set  up  API  configuration  endpointUrl  https  eastus  api  cognitive  microsoft  com  luis  v2  apps  7306b6e8  0656  41c1  8dd9  2af977be639d  subscription  key  f181418faa3b4ec6aa753c518d9699a4  verbose  true  timezoneOffset  prompt  for  command  command  input  Please  enter  command  Call  the  LUIS  service  and  get  the  JSON  response  endpoint  endpointUrl  command  replace  response  reque,microsoft
sts  get  endpoint  data  json  loads  response  content  decode  UTF  Identify  the  top  scoring  intent  intent  data  topScoringIntent  intent  if  intent  Light  On  img  url  https  raw  githubusercontent  com  MicrosoftLearning  AI  Introduction  master  files  LightOn  jpg  elif  intent  Light  Off  img  url  https  raw  githubusercontent  com  MicrosoftLearning  AI  Introduction  master  files  LightOff  jpg  else  img  url  https  raw  githubusercontent  com  MicrosoftLearning  AI  Introduction  master  files  Dunno  jpg  Get  the  appropriate  image  and  show  it  response  requests  get  img  url  img  Image  open  BytesIO  response  content  imshow  img  ,microsoft
import  copy  import  numpy  as  np  import  matplotlib  pyplot  as  plt  from  matplotlib  patches  import  Wedge  def  main  fig  ax  plt  subplots  fractal  10  ax  ax  axis  equal  ax  set  facecolor  fig  set  size  inches  20  20  plt  show  def  fractal  ax  centers  np  array  radius  for  in  range  new  centers  angle  90  for  center  in  centers  dual  half  circle  center  center  radius  radius  angle  angle  ax  ax  delta  np  array  delta  radius  new  cs  center  delta  center  delta  new  centers  center  delta  center  delta  print  center  radius  delta  new  cs  radius  delta  radius  radius  centers  copy  copy  new  centers  def  dual  half  circle  center  radius  angle  ax  None  colors  kwargs  if  ax  is  None  ax  plt  gca  kwargs  update  transform  ax  transAxes  clip  on  False  theta1  theta2  angle  angle  180  w1  Wedge  center  radius  theta1  theta2  fc  colors  kwargs  w2  Wedge  center  radius  theta2  theta1  fc  colors  kwargs  for  wedge  in  w1  w2  ax  add  artist  w,google
edge  return  w1  w2  main  ,google
livedoor  py  import  def  import  MeCab  import  os  from  os  import  listdir  walk  from  os  path  import  join  isfile  mecab  MeCab  Tagger  mecabrc  labels  dokujo  tsushin  it  life  hack  kaden  channel  livedoor  homme  movie  enter  peachy  smax  sports  watch  topic  news  def  tokenize  text  node  mecab  parseToNode  text  strip  tokens  while  node  tokens  append  node  surface  node  node  next  return  join  tokens  def  is  post  directory  filename  if  isfile  join  directory  filename  if  filename  LICENSE  txt  return  True  return  False  def  read  content  directory  filename  print  join  directory  filename  body  strip  for  in  enumerate  open  join  directory  filename  encoding  utf  if  text  join  body  return  tokenize  text  def  read  posts  directory  if  os  path  exists  join  directory  LICENSE  txt  files  for  in  listdir  directory  if  is  post  directory  for  in  files  yield  read  content  directory  def  read  corpus  input  directory  output  file  for  dirp,google
ath  in  walk  input  directory  label  name  os  path  basename  dirpath  with  open  output  file  encoding  utf  as  file  for  post  in  read  posts  dirpath  file  write  label  format  labels  index  label  name  post  corpora  ldcc  text  mkdir  data  rm  data  ldcc  txt  read  corpus  corpora  ldcc  text  data  ldcc  txt  rand  split  py  import  numpy  as  np  import  os  from  os  path  import  join  def  append  newline  filename  line  with  open  filename  encoding  utf  as  file  file  write  line  def  split  random  filename  dirname  os  path  dirname  filename  comps  os  path  basename  filename  split  train  name  train  format  comps  comps  test  name  test  format  comps  comps  for  in  open  filename  encoding  utf  choice  np  random  choice  True  False  if  choice  write  train  append  newline  join  dirname  train  name  else  write  test  append  newline  join  dirname  test  name  ldcc  text  ldcc  train  txt  ldcc  test  txt  rm  data  ldcc  train  test  txt  split  random  d,google
ata  ldcc  txt  env  PATH  workspace  bin  PATH  fasttext  supervised  input  data  ldcc  train  txt  output  data  ldcc  fasttext  supervised  dim  10  lr  wordNgrams  minCount  bucket  10000000  epoch  100  thread  fasttext  test  data  ldcc  fasttext  supervised  bin  data  ldcc  test  txt  ,google
import  os  import  random  import  time  import  json  import  boto3  dir  name  data  dogscats  train  dogs  dir  name  data  dogscats  test1  endpoint  name  london  summit  demo  endpoint  file  name  dir  name  random  choice  os  listdir  dir  name  change  dir  name  to  whatever  print  file  name  file  name  data  dogscats  test1  9969  jpg  test  image  from  IPython  display  import  Image  Image  file  name  time  runtime  boto3  Session  client  runtime  sagemaker  print  Filename  is  file  name  with  open  file  name  rb  as  payload  read  payload  bytearray  payload  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  image  Body  payload  result  response  Body  read  print  json  loads  result  ,amazon
Check  core  SDK  version  number  import  azureml  core  print  SDK  version  azureml  core  VERSION  wget  https  s3  us  west  amazonaws  com  coreml  models  TinyYOLO  mlmodelimport  onnxmltools  import  coremltools  Load  CoreML  model  coreml  model  coremltools  utils  load  spec  TinyYOLO  mlmodel  Convert  from  CoreML  into  ONNX  onnx  model  onnxmltools  convert  coreml  coreml  model  TinyYOLOv2  Save  ONNX  model  onnxmltools  utils  save  model  onnx  model  tinyyolov2  onnx  import  os  print  os  path  getsize  tinyyolov2  onnx  from  azureml  core  import  Workspace  ws  Workspace  from  config  print  ws  name  ws  location  ws  resource  group  sep  from  azureml  core  model  import  Model  model  Model  register  model  path  tinyyolov2  onnx  model  name  tinyyolov2  tags  onnx  demo  description  TinyYOLO  workspace  ws  models  ws  models  for  in  models  print  Name  name  tVersion  version  tDescription  description  tags  writefile  score  py  import  json  import  time  import  s,microsoft
ys  import  os  from  azureml  core  model  import  Model  import  numpy  as  np  we  re  going  to  use  numpy  to  process  input  and  output  data  import  onnxruntime  to  inference  ONNX  models  we  use  the  ONNX  Runtime  def  init  global  session  model  Model  get  model  path  model  name  tinyyolov2  session  onnxruntime  InferenceSession  model  def  preprocess  input  data  json  convert  the  JSON  data  into  the  tensor  input  return  np  array  json  loads  input  data  json  data  astype  float32  def  postprocess  result  return  np  array  result  tolist  def  run  input  data  json  try  start  time  time  start  timer  input  data  preprocess  input  data  json  input  name  session  get  inputs  name  get  the  id  of  the  first  input  of  the  model  result  session  run  input  name  input  data  end  time  time  stop  timer  return  result  postprocess  result  time  end  start  except  Exception  as  result  str  return  error  result  from  azureml  core  conda  dependencies ,microsoft
 import  CondaDependencies  myenv  CondaDependencies  create  pip  packages  numpy  onnxruntime  with  open  myenv  yml  as  write  myenv  serialize  to  string  from  azureml  core  image  import  ContainerImage  image  config  ContainerImage  image  configuration  execution  script  score  py  runtime  python  conda  file  myenv  yml  description  TinyYOLO  ONNX  Demo  tags  demo  onnx  image  ContainerImage  create  name  onnxyolo  models  model  image  config  image  config  workspace  ws  image  wait  for  creation  show  output  True  print  image  image  build  log  uri  from  azureml  core  webservice  import  AciWebservice  aciconfig  AciWebservice  deploy  configuration  cpu  cores  memory  gb  tags  demo  onnx  description  web  service  for  TinyYOLO  ONNX  model  from  azureml  core  webservice  import  Webservice  from  random  import  randint  aci  service  name  onnx  tinyyolo  str  randint  100  print  Service  aci  service  name  aci  service  Webservice  deploy  from  image  deployment  con,microsoft
fig  aciconfig  image  image  name  aci  service  name  workspace  ws  aci  service  wait  for  deployment  True  print  aci  service  state  if  aci  service  state  Healthy  run  this  command  for  debugging  print  aci  service  get  logs  aci  service  delete  print  aci  service  scoring  uri  aci  service  delete  ,microsoft
import  sys  import  types  import  pandas  as  pd  from  botocore  client  import  Config  import  ibm  boto3  def  iter  self  return  hidden  cell  The  following  code  accesses  file  in  your  IBM  Cloud  Object  Storage  It  includes  your  credentials  You  might  want  to  remove  those  credentials  before  you  share  your  notebook  client  70ab70feb7fb4d8f9a47a408afd9f30f  ibm  boto3  client  service  name  s3  ibm  api  key  id  ZATI1oq  TlsWqN  oEz3Wo1IPXWwOkCx0PXV3gj0d5eui  ibm  auth  endpoint  https  iam  ng  bluemix  net  oidc  token  config  Config  signature  version  oauth  endpoint  url  https  s3  api  us  geo  objectstorage  service  networklayer  com  body  client  70ab70feb7fb4d8f9a47a408afd9f30f  get  object  Bucket  watstudworkshop  donotdelete  pr  basx79wonvxlys  Key  GoSales  Tx  csv  Body  add  missing  iter  method  so  pandas  accepts  body  as  file  like  object  if  not  hasattr  body  iter  body  iter  types  MethodType  iter  body  df  data  pd  read  csv  body  df  data,ibm
  head  if  not  df  data  in  globals  keys  print  nERROR  df  data  variable  is  not  defined  please  check  teh  cell  above  else  Created  convenience  shortcut  variable  df  df  data  1df  dtypesimport  numpy  as  np  df  count  df  describe  include  all  df  IS  TENT  value  counts  df  GENDER  value  counts  df  MARITAL  STATUS  value  counts  to  frame  df  PROFESSION  value  counts  pf  lambda  0f  format  100  df  PROFESSION  count  df  PROFESSION  value  counts  map  pf  df  MARITAL  STATUS  value  counts  map  pf  df  GENDER  value  counts  map  pf  df  IS  TENT  value  counts  map  pf  tent  gender  pd  crosstab  df  IS  TENT  df  GENDER  tent  genderx  tent  prof  pd  crosstab  df  IS  TENT  df  PROFESSION  tent  profx  age  tent  pd  crosstab  df  AGE  df  IS  TENT  age  tent  Rename  the  columns  of  the  CrossTab  for  use  by  Brunel  import  brunel  age  tent  columns  AGE  SUM  TENT  brunel  data  age  tent  bar  AGE  SUM  TENT  ,ibm
from  datetime  import  datetime  now  datetime  now  mm  str  now  month  dd  str  now  day  yyyy  str  now  year  hour  str  now  hour  mi  str  now  minute  ss  str  now  second  print  mm  dd  yyyy  hour  mi  ss  ,ibm
import  tensorflow  as  tf  tf  set  random  seed  777  for  reprducibiltyx  train  train  Try  to  find  value  for  and  to  compute  data  data  We  know  that  should  be  and  should  be  But  let  TensorFlow  figure  it  out  tf  Variable  tf  random  normal  name  weight  tf  Variable  tf  random  normal  name  bias  Out  hypothesis  XW  hypothesis  train  cost  loss  function  cost  tf  reduce  mean  tf  square  hypothesis  train  optimizer  tf  train  GradientDescentOptimizer  learning  rate  01  train  optimizer  minimize  cost  Launch  the  graph  in  session  sess  tf  Session  Initializes  global  variables  in  the  graph  sess  run  tf  global  variables  initializer  for  step  in  range  2001  sess  run  train  if  step  200  print  step  sess  run  cost  sess  run  sess  run  ,ibm
S3  bucket  and  prefix  bucket  sagemaker  walebadr  prefix  sagemaker  DEMO  data  distribution  types  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  boto3  import  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  from  IPython  display  import  display  import  io  import  time  import  copy  import  json  import  sys  import  sagemaker  amazon  common  as  smac  import  osdef  get  gdelt  filename  s3  boto3  resource  s3  s3  Bucket  gdelt  open  data  download  file  events  filename  gdelt  csv  df  pd  read  csv  gdelt  csv  sep  header  pd  read  csv  https  www  gdeltproject  org  data  lookups  CSV  header  historical  txt  sep  df  columns  header  columns  return  dfdata  get  gdelt  1979  csv  datadata  data  EventCode  NumArticles  AvgTone  Actor1Geo  Lat  Actor1Geo  Long  Actor2Geo  Lat  Actor2Geo  Long  data  EventCode  data  EventCode  astype  object  for  column  in  data  select  d,amazon
types  include  object  columns  display  pd  crosstab  index  data  column  columns  observations  normalize  columns  display  data  describe  hist  data  hist  bins  30  sharey  True  figsize  10  10  plt  show  events  pd  crosstab  index  data  EventCode  columns  count  sort  values  by  count  ascending  False  index  20  def  write  to  s3  bucket  prefix  channel  file  prefix  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  astype  float32  astype  float32  buf  seek  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  channel  file  prefix  data  upload  fileobj  buf  def  transform  gdelt  df  events  None  df  df  AvgTone  EventCode  NumArticles  Actor1Geo  Lat  Actor1Geo  Long  Actor2Geo  Lat  Actor2Geo  Long  df  EventCode  df  EventCode  astype  object  if  events  is  not  None  df  df  np  in1d  df  EventCode  events  return  pd  get  dummies  df  df  Actor1Geo  Lat  df  Actor1Geo  Long  True  df  Actor2Geo  Lat  df  Actor2Geo  Long  True  def  prepare,amazon
  gdelt  bucket  prefix  file  prefix  events  None  random  state  1729  df  get  gdelt  file  prefix  csv  model  data  transform  gdelt  df  events  train  data  validation  data  np  split  model  data  sample  frac  random  state  random  state  as  matrix  int  len  model  data  write  to  s3  bucket  prefix  train  file  prefix  train  data  train  data  write  to  s3  bucket  prefix  validation  file  prefix  validation  data  validation  data  for  year  in  range  1979  1984  prepare  gdelt  bucket  prefix  str  year  events  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  linear  learner  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  linear  learner  latest  ap  northeast  351501993468  dkr  ecr  ap  northeast  amazonaws  com  linear  learner  latest  ap  northeast  835164637446  dkr  e,amazon
cr  ap  northeast  amazonaws  com  linear  learner  latest  container  containers  boto3  Session  region  name  common  training  params  RoleArn  role  AlgorithmSpecification  TrainingImage  container  TrainingInputMode  File  ResourceConfig  InstanceCount  InstanceType  ml  c4  2xlarge  VolumeSizeInGB  10  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  CompressionType  None  RecordWrapperType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  prefix  S3DataDistributionType  FullyReplicated  CompressionType  None  RecordWrapperType  None  OutputDataConfig  S3OutputPath  s3  format  bucket  prefix  HyperParameters  feature  dim  25  mini  batch  size  500  predictor  type  regressor  epochs  num  models  32  loss  absolute  loss  StoppingCondition  MaxRuntimeInSeconds  60  60  sharded  job  DEMO  linear  sharded  time  strftime  time  gmtime  print  Job  name  is,amazon
  sharded  job  sharded  training  params  copy  deepcopy  common  training  params  sharded  training  params  TrainingJobName  sharded  job  sharded  training  params  InputDataConfig  DataSource  S3DataSource  S3DataDistributionType  ShardedByS3Key  replicated  job  DEMO  linear  replicated  time  strftime  time  gmtime  print  Job  name  is  replicated  job  replicated  training  params  copy  deepcopy  common  training  params  replicated  training  params  TrainingJobName  replicated  job  replicated  training  params  InputDataConfig  DataSource  S3DataSource  S3DataDistributionType  FullyReplicated  time  region  boto3  Session  region  name  sm  boto3  Session  client  sagemaker  sm  create  training  job  sharded  training  params  sm  create  training  job  replicated  training  params  status  sm  describe  training  job  TrainingJobName  replicated  job  TrainingJobStatus  print  status  sm  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  replicated  job  status  sm  de,amazon
scribe  training  job  TrainingJobName  replicated  job  TrainingJobStatus  print  Training  job  ended  with  status  status  if  status  Failed  message  sm  describe  training  job  TrainingJobName  replicated  job  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  print  Sharded  sm  describe  training  job  TrainingJobName  sharded  job  TrainingJobStatus  print  Replicated  sm  describe  training  job  TrainingJobName  replicated  job  TrainingJobStatus  def  get  train  timing  job  client  boto3  client  logs  streams  client  describe  log  streams  logGroupName  aws  sagemaker  TrainingJobs  logStreamNamePrefix  job  streams  logStreamName  for  in  streams  logStreams  times  for  stream  in  streams  events  client  get  log  events  logGroupName  aws  sagemaker  TrainingJobs  logStreamName  stream  events  times  timestamp  for  in  events  return  max  times  min  times  60000  print  Sharded  get  train  timing  sharde,amazon
d  job  minutes  print  Replicated  get  train  timing  replicated  job  minutes  sharded  model  response  sm  create  model  ModelName  sharded  job  ExecutionRoleArn  role  PrimaryContainer  Image  container  ModelDataUrl  sm  describe  training  job  TrainingJobName  sharded  job  ModelArtifacts  S3ModelArtifacts  print  sharded  model  response  ModelArn  replicated  model  response  sm  create  model  ModelName  replicated  job  ExecutionRoleArn  role  PrimaryContainer  Image  container  ModelDataUrl  sm  describe  training  job  TrainingJobName  replicated  job  ModelArtifacts  S3ModelArtifacts  print  replicated  model  response  ModelArn  sharded  endpoint  config  DEMO  sharded  endpoint  config  time  strftime  time  gmtime  print  sharded  endpoint  config  sharded  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  sharded  endpoint  config  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  sharded  job  VariantName  AllTraffic  print  E,amazon
ndpoint  Config  Arn  sharded  endpoint  config  response  EndpointConfigArn  replicated  endpoint  config  DEMO  replicated  endpoint  config  time  strftime  time  gmtime  print  replicated  endpoint  config  replicated  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  replicated  endpoint  config  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  replicated  job  VariantName  AllTraffic  print  Endpoint  Config  Arn  replicated  endpoint  config  response  EndpointConfigArn  time  sharded  endpoint  DEMO  sharded  endpoint  time  strftime  time  gmtime  print  sharded  endpoint  sharded  endpoint  response  sm  create  endpoint  EndpointName  sharded  endpoint  EndpointConfigName  sharded  endpoint  config  print  sharded  endpoint  response  EndpointArn  replicated  endpoint  DEMO  replicated  endpoint  time  strftime  time  gmtime  print  replicated  endpoint  replicated  endpoint  response  sm  create  endpoint  EndpointName  replicated  endp,amazon
oint  EndpointConfigName  replicated  endpoint  config  print  replicated  endpoint  response  EndpointArn  resp  sm  describe  endpoint  EndpointName  replicated  endpoint  status  resp  EndpointStatus  print  Status  status  sm  get  waiter  endpoint  in  service  wait  EndpointName  replicated  endpoint  resp  sm  describe  endpoint  EndpointName  replicated  endpoint  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Status  status  if  status  InService  raise  Exception  Endpoint  creation  did  not  succeed  print  Sharded  sm  describe  endpoint  EndpointName  sharded  endpoint  EndpointStatus  print  Replicated  sm  describe  endpoint  EndpointName  replicated  endpoint  EndpointStatus  test  data  transform  gdelt  get  gdelt  1984  csv  events  as  matrix  test  test  data  test  test  data  def  np2csv  arr  csv  io  BytesIO  np  savetxt  csv  arr  delimiter  fmt  return  csv  getvalue  decode  rstrip  def  predict  batches  data  endpoint  rows  1024  1024  sys  getsizeof  np2cs,amazon
v  data  split  array  np  array  split  data  int  data  shape  float  rows  predictions  runtime  boto3  Session  client  runtime  sagemaker  for  array  in  split  array  payload  np2csv  array  response  runtime  invoke  endpoint  EndpointName  endpoint  ContentType  text  csv  Body  payload  result  json  loads  response  Body  read  decode  predictions  score  for  in  result  predictions  return  np  array  predictions  sharded  predictions  predict  batches  test  sharded  endpoint  replicated  predictions  predict  batches  test  replicated  endpoint  print  Sharded  MSE  np  mean  test  sharded  predictions  print  Replicated  MSE  np  mean  test  replicated  predictions  sm  delete  endpoint  EndpointName  sharded  endpoint  sm  delete  endpoint  EndpointName  replicated  endpoint  ,amazon
from  sagemaker  import  get  execution  role  import  os  import  sagemaker  sagemaker  session  sagemaker  Session  role  get  execution  role  IAM  execution  role  that  gives  SageMaker  access  to  resources  in  your  AWS  account  data  folder  s3  wmp  machinelearning  poc  december2017  data  cat  recommender2017  pyfrom  sagemaker  tensorflow  import  TensorFlow  recEstimator  TensorFlow  entry  point  recommender2017  py  role  role  training  steps  50  evaluation  steps  hyperparameters  learning  rate  001  train  instance  count  train  instance  type  ml  c4  xlarge  recEstimator  fit  data  folder  time  recommenderDeploy  recEstimator  deploy  initial  instance  count  instance  type  ml  c4  xlarge  time  recommenderDeploy  predict  recommenderDeploy  accept  git  init  git  remote  add  origin  https  github  com  jswortz  SageMaker  git  git  add  git  commit  initial  push  git  push  origin  master  ,amazon
import  os  import  sagemaker  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  Fill  in  your  bucket  data  directory  bucket  s3  sagemaker  blah  data  Fill  in  your  bucket  output  path  output  path  s3  sagemaker  blah  role  get  execution  role  from  sagemaker  tensorflow  import  TensorFlow  abalone  estimator  TensorFlow  entry  point  abalone  py  role  role  training  steps  100  evaluation  steps  100  hyperparameters  learning  rate  001  train  instance  count  train  instance  type  ml  p2  xlarge  output  path  output  path  abalone  estimator  fit  bucket  abalone  predictor  abalone  estimator  deploy  initial  instance  count  instance  type  ml  t2  medium  import  tensorflow  as  tf  import  numpy  as  np  Update  with  your  own  path  prediction  set  tf  contrib  learn  datasets  base  load  csv  without  header  filename  os  path  join  data  abalone  predict  csv  target  dtype  np  int  features  dtype  np  float32  data  prediction  set  ,amazon
data  tensor  proto  tf  make  tensor  proto  values  np  asarray  data  shape  len  data  dtype  tf  float32  abalone  predictor  predict  tensor  proto  sagemaker  Session  delete  endpoint  abalone  predictor  endpoint  ,amazon
import  tensorflow  as  tfimport  importlibimport  main  from  main  import  main  as  runMain  importlib  reload  main  from  main  import  main  as  runMainmodel  main  CycleGAN  print  LOAD  DATASET  model  input  setup  print  DONE  Build  the  network  print  LOAD  MODEL  model  model  setup  print  DONE  from  tensorflow  python  client  import  device  lib  device  lib  list  local  devices  Loss  function  calculations  print  CALCULATE  LOSS  model  loss  calc  print  DONE  with  tf  model  train  ,amazon
from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  bucket  name  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  30  This  is  format  train  set  30  time  from  sagemaker  amazon  common  import  write  numpy  to  dense  tensor  import  io  import  boto3  data  key  kmeans  lowlevel  example  data  data  location  s3  format  bucket  data  key  print  training  data  will  be  uploaded  to  format  data  location  Convert  the  trainin,amazon
g  data  into  the  format  required  by  the  SageMaker  KMeans  algorithm  buf  io  BytesIO  write  numpy  to  dense  tensor  buf  train  set  train  set  buf  seek  boto3  resource  s3  Bucket  bucket  Object  data  key  upload  fileobj  buf  time  import  boto3  from  time  import  gmtime  strftime  job  name  kmeans  lowlevel  strftime  gmtime  print  Training  job  job  name  images  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  kmeans  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  kmeans  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  kmeans  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  kmeans  latest  image  images  boto3  Session  region  name  output  location  s3  kmeans  example  output  format  bucket  print  training  artifacts  will  be  uploaded  to  format  output  location  create  training  params  AlgorithmSpecification  TrainingImage  image  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPat,amazon
h  output  location  ResourceConfig  InstanceCount  InstanceType  ml  c4  8xlarge  VolumeSizeInGB  50  TrainingJobName  job  name  HyperParameters  10  feature  dim  784  mini  batch  size  500  force  dense  True  StoppingCondition  MaxRuntimeInSeconds  60  60  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  data  location  S3DataDistributionType  FullyReplicated  CompressionType  None  RecordWrapperType  None  sagemaker  boto3  client  sagemaker  sagemaker  create  training  job  create  training  params  status  sagemaker  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  status  try  sagemaker  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  finally  status  sagemaker  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  Training  job  ended  with  status  status  if  status  Failed  message  sagemaker  describe  training  job  TrainingJobName  job  name  FailureReason  ,amazon
print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  time  import  boto3  from  time  import  gmtime  strftime  model  name  job  name  print  model  name  info  sagemaker  describe  training  job  TrainingJobName  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  primary  container  Image  image  ModelDataUrl  model  data  create  model  response  sagemaker  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  endpoint  config  name  KMeansEndpointConfig  strftime  gmtime  print  endpoint  config  name  create  endpoint  config  response  sagemaker  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointC,amazon
onfigArn  time  import  time  endpoint  name  KMeansEndpoint  strftime  gmtime  print  endpoint  name  create  endpoint  response  sagemaker  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  print  create  endpoint  response  EndpointArn  resp  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  try  sagemaker  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  finally  resp  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Create  endpoint  ended  with  status  status  if  status  InService  message  sagemaker  describe  endpoint  EndpointName  endpoint  name  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Endpoint  creation  did  not  succeed  Simple  function  to  create  csv  from  our  numpy  array  def  np2csv  arr  csv  io  BytesIO  numpy  save,amazon
txt  csv  arr  delimiter  fmt  return  csv  getvalue  decode  rstrip  runtime  boto3  Session  client  runtime  sagemaker  import  json  payload  np2csv  train  set  30  31  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  text  csv  Body  payload  result  json  loads  response  Body  read  decode  print  result  time  payload  np2csv  valid  set  100  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  text  csv  Body  payload  result  json  loads  response  Body  read  decode  clusters  closest  cluster  for  in  result  predictions  for  cluster  in  range  10  print  nCluster  format  int  cluster  digits  img  for  img  in  zip  clusters  valid  set  if  int  cluster  height  len  digits  width  plt  rcParams  figure  figsize  width  height  subplots  plt  subplots  height  width  subplots  numpy  ndarray  flatten  subplots  for  subplot  image  in  zip  subplots  digits  show  digit  image  subplot  subplot  for  subplot  in  subplots  len  digi,amazon
ts  subplot  axis  off  plt  show  sagemaker  delete  endpoint  EndpointName  endpoint  name  ,amazon
import  boto3  sagemaker  boto3  client  sagemaker  model  name  spark  regression  model  001  endpoint  config  name  inference  server  001a  config  endpoint  name  inference  server  001a  Create  SageMaker  Model  Update  Role  Arn  role  arn  aws  iam  036983998592  role  service  role  AmazonSageMaker  ExecutionRole  20180518T114698  primary  container  Do  not  change  the  inference  image  link  unless  you  have  your  own  Image  959874710265  dkr  ecr  us  west  amazonaws  com  inference  server  latest  ModelDataUrl  s3  ruchika  wibd  models2  spark  regression  model  model  tgz  ModelDataUrl  s3  ruchika  wibd  west  mleap  models  pipeline1  model  tgz  create  model  response  sagemaker  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  Create  EndPoint  Config  response  sagemaker  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  VariantName  default  varia,amazon
nt  name  ModelName  model  name  InitialInstanceCount  InstanceType  ml  m4  xlarge  print  response  Create  Sagemaker  Endpoint  Wait  for  Response  This  may  take  few  minutes  response  sagemaker  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  print  response  time  Test  the  Endpoint  import  boto3  io  json  client  boto3  client  sagemaker  runtime  input  schema  fields  name  Price  type  double  name  Mileage  type  integer  name  Make  type  string  name  Model  type  string  name  Trim  type  string  name  Type  type  string  name  Cylinder  type  integer  name  Liter  type  double  name  Doors  type  integer  name  Cruise  type  integer  name  Sound  type  integer  name  Leather  type  integer  rows  9041  9062544231  26191  Chevrolet  AVEO  SVM  Sedan  4D  Sedan  response  client  invoke  endpoint  EndpointName  endpoint  name  Body  input  ContentType  application  json  Accept  application  json  res  json  json  loads  response  Body  read  de,amazon
code  utf  print  json  dumps  res  json  indent  ,amazon
import  tensorflow  as  tf  import  numpy  as  np  tf  set  random  seed  777  for  reproducibility  learning  rate  data  data  data  np  array  data  dtype  np  float32  data  np  array  data  dtype  np  float32  tf  placeholder  tf  float32  None  tf  placeholder  tf  float32  None  W1  tf  Variable  tf  random  normal  name  weight1  b1  tf  Variable  tf  random  normal  name  bias1  layer1  tf  sigmoid  tf  matmul  W1  b1  W2  tf  Variable  tf  random  normal  name  weight2  b2  tf  Variable  tf  random  normal  name  bias2  hypothesis  tf  sigmoid  tf  matmul  layer1  W2  b2  cost  loss  function  cost  tf  reduce  mean  tf  log  hypothesis  tf  log  hypothesis  train  tf  train  GradientDescentOptimizer  learning  rate  learning  rate  minimize  cost  Accuracy  computation  True  if  hypothesis  else  False  predicted  tf  cast  hypothesis  dtype  tf  float32  accuracy  tf  reduce  mean  tf  cast  tf  equal  predicted  dtype  tf  float32  Launch  graph  sess  tf  Session  Initialize  TensorFlow  variab,ibm
les  sess  run  tf  global  variables  initializer  for  step  in  range  10001  sess  run  train  feed  dict  data  data  if  step  1000  print  step  sess  run  cost  feed  dict  data  data  print  sess  run  W1  W2  Accuracy  report  sess  run  hypothesis  predicted  accuracy  feed  dict  data  data  print  nHypothesis  nCorrect  nAccuracy  Hypothesis  01338218  98166394  98809403  01135799  Correct  Accuracy  ,ibm
pip  install  eikon  Run  in  cmd  window  Or  in  Visual  Studio  PTVSimport  eikon  as  ekek  set  app  id  D163218EE154B9D1851F8C9  ek  get  news  headlines  TD  TO  date  from  2017  10  19T09  00  00  date  to  2017  10  20T18  00  00  df  ek  get  timeseries  MSFT  start  date  2016  01  01  end  date  2016  01  10  dfdata  grid1  err  ek  get  data  IBM  GOOG  MSFT  TR  PriceClose  TR  Volume  TR  PriceLow  data  grid2  err  ek  get  data  IBM  TR  Employees  TR  GrossProfit  params  Scale  Curn  EUR  sort  dir  asc  fields  ek  TR  Field  tr  revenue  ek  TR  Field  tr  open  None  asc  ek  TR  Field  TR  GrossProfit  Scale  Curn  EUR  asc  data  grid3  err  ek  get  data  IBM  MSFT  fields  data  grid3df  err  ek  get  data  GOOG  MSFT  FB  TR  Revenue  TR  GrossProfit  dfdf  err  ek  get  data  GOOG  MSFT  FB  AMZN  TWTR  TR  Revenue  date  TR  Revenue  TR  GrossProfit  Scale  SDate  EDate  FRQ  FY  Curn  EUR  dfdf  err  ek  get  data  IBM  TR  RevenueActValue  params  Period  FY0  Scale  Curn  USD ,microsoft
 TR  RevenueMeanEstimate  params  Period  FY1  Scale  Curn  USD  TR  RevenueMeanEstimate  params  Period  FY2  Scale  Curn  USD  dfdf1  err  ek  get  data  IBM  TR  RevenueActValue  params  Period  FY0  Scale  Curn  USD  TR  RevenueMeanEstimate  params  Period  FY1  Scale  Curn  USD  TR  RevenueMeanEstimate  params  Period  FY2  Scale  Curn  USD  raw  output  False  debug  True  df1df2  err  ek  get  data  IBM  TR  RevenueHigh  params  Period  FY1  Scale  Curn  USD  TR  RevenueHigh  params  Period  FY2  Scale  Curn  USD  TR  RevenueLow  params  Period  FY1  Scale  Curn  USD  TR  RevenueLow  params  Period  FY2  Scale  Curn  USD  debug  True  df2df3  err  ek  get  data  IBM  TR  RevenueLow  params  Period  FY1  Scale  Curn  USD  TR  RevenueLow  params  Period  FY2  Scale  Curn  USD  debug  True  df3df4  err  ek  get  data  IBM  TR  RevenueMeanEstimate  params  Period  FY1  RollPeriods  False  Scale  Curn  USD  TR  RevenueMeanEstimate  params  Period  FY2  RollPeriods  False  Scale  Curn  USD  debug  True  df4c,microsoft
onstituents  data  err  ek  get  data  instruments  ALLCOCO  fields  OFFCL  CODE  constituents  dataek  get  data  USGDPF  ECI  GN  TXT16  ek  get  timeseries  aCNFRTRRAW  interval  monthly  ,microsoft
import  requests  import  matplotlib  pyplot  as  plt  from  matplotlib  patches  import  Polygon  from  PIL  import  Image  matplotlib  inline  plt  rcParams  figure  figsize  12  def  azure  text  image  Function  that  sends  cell  image  to  Microsoft  Computer  vision  API  for  text  recognition  Receives  and  returns  the  results  as  polygons  api  key  YOUR  API  KEY  HERE  region  westeurope  with  open  image  rb  as  image  file  content  image  file  read  headers  Content  Type  application  octet  stream  Ocp  Apim  Subscription  Key  api  key  response  requests  post  https  api  cognitive  microsoft  com  vision  v2  ocr  format  region  headers  headers  data  content  polygons  for  region  in  response  json  regions  for  line  in  region  lines  for  word  in  line  words  box  word  boundingBox  split  box  int  for  in  box  box2  box  box  box  box  box  box  box  box  box  box  box  box  text  word  text  polygons  append  box2  text  return  polygonsimg  einsteinquote  jpg  resul,microsoft
t  azure  text  img  def  plot  overlay  img  polygons  remove  long  boxes  False  remove  short  boxes  False  color  fontsize  28  alpha  boxcolor  Function  that  overlays  text  labels  on  original  image  remove  long  boxes  bool  will  ignore  labels  with  bounding  boxes  that  are  wider  than  half  the  image  width  Depending  on  the  image  and  the  API  service  being  used  this  may  be  necessary  to  prevent  overlapping  redundant  boxes  from  being  displayed  remove  short  boxes  bool  will  ignore  labels  with  bounding  boxes  that  are  thinner  than  quarter  of  the  image  width  Depending  on  the  image  and  the  API  service  being  used  this  may  be  necessary  to  prevent  redundant  small  boxes  from  being  displayed  The  fontsize  variable  should  be  manually  adjusted  to  fit  the  image  text  size  image  Image  open  img  ax  plt  imshow  image  alpha  alpha  for  polygon  in  polygons  vertices  polygon  polygon  for  in  range  len  polygon  text  polyg,microsoft
on  if  remove  long  boxes  if  vertices  vertices  image  size  continue  if  remove  short  boxes  if  vertices  vertices  image  size  continue  patch  Polygon  vertices  closed  True  fill  False  linewidth  color  boxcolor  ax  axes  add  patch  patch  plt  text  vertices  vertices  vertices  vertices  text  fontsize  fontsize  color  color  va  center  ha  center  plt  axis  off  returnplt  imshow  Image  open  img  plt  axis  off  plot  overlay  img  result  Print  just  the  text  for  item  in  result  print  item  ,microsoft
Write  program  that  will  work  out  the  distance  travelled  if  the  user  enters  the  speed  and  the  time  speed  int  input  Enter  speed  time  int  input  Enter  time  print  Distance  is  format  speed  time  Write  program  to  tell  you  the  speed  you  would  have  to  travel  at  in  order  to  go  distance  within  certain  time  entered  distance  int  input  Enter  distance  time  int  input  Enter  time  print  Distance  is  2f  format  distance  time  Write  program  to  work  out  how  many  days  you  have  lived  for  import  datetime  currdate  datetime  date  today  day  int  input  Enter  day  month  int  input  Enter  month  year  int  input  Enter  year  birthdate  datetime  date  year  month  day  print  No  of  days  you  lived  for  format  abs  currdate  birthdate  days  Write  program  to  work  out  how  many  seconds  you  have  lived  for  import  datetime  currdate  datetime  date  today  day  int  input  Enter  day  month  int  input  Enter  month  year  int  input  En,microsoft
ter  year  birthdate  datetime  date  year  month  day  print  No  of  seconds  you  lived  for  format  abs  currdate  birthdate  days  60  60  24  Write  program  that  will  accept  date  of  birth  and  determine  whether  one  is  considered  an  adult  21  years  old  import  datetime  currdate  datetime  date  today  day  int  input  Enter  day  month  int  input  Enter  month  year  int  input  Enter  year  birthdate  datetime  date  year  month  day  no  of  days  abs  currdate  birthdate  days  def  is  legal  no  of  days  legal  age  21  365  if  no  of  days  legal  age  return  True  return  False  if  is  legal  no  of  days  print  Adult  else  print  Not  an  adult  Write  program  that  will  generate  random  playing  card  Hearts  Queen  Spades  when  the  return  key  is  pressed  Note  Rather  than  generate  random  number  from  to  52  Create  two  random  numbers  one  for  the  suit  and  one  for  the  card  from  random  import  randint  suits  Hearts  Spades  Diamond  Clubs  card,microsoft
s  Ace  10  10  11  12  13  output  cards  randint  13  suits  randint  print  output  Make  game  of  rock  paper  scissors  against  the  computer  Algorithm  Tell  user  to  enter  either  rock  paper  or  scissors  Get  the  response  Generate  random  number  from  to  rock  paper  scissors  Compare  user  selection  and  computer  selection  Display  who  wins  import  random  choices  rock  paper  scissors  user  input  Enter  rock  paper  scissors  if  user  not  in  choices  print  Invalid  input  else  user  choices  user  cpu  random  randint  if  user  cpu  print  Tie  elif  user  and  cpu  or  user  and  cpu  or  user  and  cpu  print  Computer  wins  else  print  User  wins  For  the  rock  paper  scissors  game  above  ensure  that  the  user  enters  valid  entry  Add  loop  structure  to  play  several  times  and  keep  running  score  import  random  choices  rock  paper  scissors  while  True  user  input  Enter  rock  paper  scissors  type  END  to  exit  if  user  END  print  Exit  break,microsoft
  elif  user  not  in  choices  print  Invalid  input  else  user  choices  user  cpu  random  randint  if  user  cpu  print  Tie  elif  user  and  cpu  or  user  and  cpu  or  user  and  cpu  print  Computer  wins  else  print  User  wins  Write  program  that  will  give  the  answer  to  logic  gate  questions  Enter  logic  gate  OR  Enter  first  input  Enter  second  input  Result  It  should  work  for  the  logic  gates  OR  XOR  NAND  and  NOR  gates  logic  gates  OR  AND  XOR  NAND  NOR  XNOR  logic  gate  input  Enter  logic  gate  result  None  if  logic  gate  not  in  logic  gates  print  Not  valid  logic  gate  else  valid  first  int  input  Enter  first  input  second  int  input  Enter  second  input  if  not  first  in  valid  and  second  in  valid  print  Please  enter  ones  and  zeroes  only  else  if  logic  gate  OR  if  first  or  second  result  else  result  elif  logic  gate  AND  if  first  and  second  result  else  result  elif  logic  gate  XOR  if  first  second  result  el,microsoft
se  result  elif  logic  gate  NAND  if  first  and  second  result  else  result  elif  logic  gate  NOR  if  first  and  second  result  else  result  elif  logic  gate  XNOR  if  first  second  result  else  result  print  Result  format  result  Write  program  that  will  display  all  the  factors  of  an  input  number  that  are  bigger  than  the  factors  of  the  number  12  are  and  because  they  divide  into  12  exactly  Hint  To  find  out  whether  number  is  factor  of  use  If  mod  there  is  nothing  remaining  when  is  divided  by  def  factor  num  arr  for  in  range  num  if  num  arr  append  return  arr  num  int  input  Enter  number  print  Factors  for  in  factor  num  print  end  print  ,microsoft
import  random  as  rnd  from  numpy  random  import  import  pandas  as  pd  CTR  ad  ctrs  10  09  08  07  06  05  04  03  02  10  01  adspot  ctrs  05  04  03  02  01  00  01  02  03  10  04  ad  ids  list  range  11  adspot  ids  list  range  11  def  create  log  ad  id  rnd  choice  ad  ids  adspot  id  rnd  choice  adspot  ids  ad  ctr  ad  ctrs  ad  id  adspot  ctr  adspot  ctrs  adspot  id  random  normal  001  real  ctr  15  ad  ctr  adspot  ctr  random  is  clicked  binomial  real  ctr  return  ad  id  adspot  id  real  ctr  is  clicked  def  create  logs  1000  test  data  for  in  list  range  tmp  create  log  test  data  append  tmp  return  pd  DataFrame  test  data  columns  ad  id  adspot  id  real  ctr  is  clicked  train  data  create  logs  50000  train  data  to  csv  train  data  csv  ,amazon
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  linear  time  series  forecast  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  import  io  import  os  import  time  import  json  import  sagemaker  amazon  common  as  smac  import  sagemaker  from  sagemaker  predictor  import  csv  serializer  json  deserializer  wget  http  robjhyndman  com  data  gasoline  csvgas  pd  read  csv  gasoline  csv  header  None  names  thousands  barrels  display  gas  head  plt  plot  gas  plt  show  gas  thousands  barrels  lag1  gas  thousands  barrels  shift  gas  thousands  barrels  lag2  gas  thousands  barrels  shift  gas  thousands  barrels  lag3  gas  thousands  barrels  shift  gas  thousands  barrels  lag4  gas  thousands  barrels  shift  gas  trend  np  arange  len  gas  gas  log  trend  np  log1p  np  arange  len  gas  gas  sq  trend  np  arange ,amazon
 len  gas  weeks  pd  get  dummies  np  array  list  range  52  15  len  gas  prefix  week  gas  pd  concat  gas  weeks  axis  gas  gas  iloc  split  train  int  len  gas  split  test  int  len  gas  train  gas  thousands  barrels  split  train  train  gas  drop  thousands  barrels  axis  iloc  split  train  as  matrix  validation  gas  thousands  barrels  split  train  split  test  validation  gas  drop  thousands  barrels  axis  iloc  split  train  split  test  as  matrix  test  gas  thousands  barrels  split  test  test  gas  drop  thousands  barrels  axis  iloc  split  test  as  matrix  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  np  array  train  astype  float32  np  array  train  astype  float32  buf  seek  key  linear  train  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  buf  s3  train  data  s3  train  format  bucket  prefix  key  print  uploaded  training  data  location  format  s3  train  data  buf  io  BytesIO  smac  write,amazon
  numpy  to  dense  tensor  buf  np  array  validation  astype  float32  np  array  validation  astype  float32  buf  seek  key  linear  validation  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  key  upload  fileobj  buf  s3  validation  data  s3  validation  format  bucket  prefix  key  print  uploaded  validation  data  location  format  s3  validation  data  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  boto3  Session  region  name  linear  learner  sess  sagemaker  Session  linear  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  s3  output  format  bucket  prefix  sagemaker  session  sess  linear  set  hyperparameters  feature  dim  59  mini  batch  size  100  predictor  type  regressor  epochs  10  num  models  32  loss  absolute  loss  linear  fit  train  s3  train  data  validation  s3  validation  data  linear  predictor  linear  dep,amazon
loy  initial  instance  count  instance  type  ml  m4  xlarge  gas  thousands  barrels  lag52  gas  thousands  barrels  shift  52  gas  thousands  barrels  lag104  gas  thousands  barrels  shift  104  gas  thousands  barrels  naive  forecast  gas  thousands  barrels  lag52  gas  thousands  barrels  lag104  naive  gas  split  test  thousands  barrels  naive  forecast  as  matrix  print  Naive  MdAPE  np  median  np  abs  test  naive  test  plt  plot  np  array  test  label  actual  plt  plot  naive  label  naive  plt  legend  plt  show  linear  predictor  content  type  text  csv  linear  predictor  serializer  csv  serializer  linear  predictor  deserializer  json  deserializerresult  linear  predictor  predict  test  one  step  np  array  score  for  in  result  predictions  print  One  step  ahead  MdAPE  np  median  np  abs  test  one  step  test  plt  plot  np  array  test  label  actual  plt  plot  one  step  label  forecast  plt  legend  plt  show  multi  step  lags  test  for  row  in  test  row  lags ,amazon
 result  linear  predictor  predict  row  prediction  result  predictions  score  multi  step  append  prediction  lags  lags  lags  prediction  multi  step  np  array  multi  step  print  Multi  step  ahead  MdAPE  np  median  np  abs  test  multi  step  test  plt  plot  np  array  test  label  actual  plt  plot  multi  step  label  forecast  plt  legend  plt  show  sagemaker  Session  delete  endpoint  linear  predictor  endpoint  ,amazon
time  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  bucket  name  customize  to  your  bucket  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  image  classification  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  image  classification  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  image  classification  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  image  classification  latest  training  image  containers  boto3  Session  region  name  print  training  image  import  os  import  urllib  request  import  boto3  def  download  url  filename  url  split  if  not  os  path  exists  filename  urllib  request  urlretrieve  url  filename  def  upload  to  s3  channel  file  s3  boto3  resource  s3  data  open  file  rb  key  channel  file  s3  Bucket  bucket  put  object  Key  key  Body  data  caltech  256  download  http  data  mxnet  io  data  caltech  256  calte,amazon
ch  256  60  train  rec  download  http  data  mxnet  io  data  caltech  256  caltech  256  60  val  rec  upload  to  s3  validation  caltech  256  60  val  rec  upload  to  s3  train  caltech  256  60  train  rec  The  algorithm  supports  multiple  network  depth  number  of  layers  They  are  18  34  50  101  152  and  200  For  this  training  we  will  use  18  layers  num  layers  18  we  need  to  specify  the  input  image  shape  for  the  training  data  image  shape  224  224  we  also  need  to  specify  the  number  of  training  samples  in  the  training  set  for  caltech  it  is  15420  num  training  samples  15420  specify  the  number  of  output  classes  num  classes  257  batch  size  for  training  mini  batch  size  128  number  of  epochs  epochs  learning  rate  learning  rate  01  top  Since  we  are  using  transfer  learning  we  set  use  pretrained  model  to  so  that  weights  can  be  initialized  with  pre  trained  weights  use  pretrained  model  time  import  time  impo,amazon
rt  boto3  from  time  import  gmtime  strftime  s3  boto3  client  s3  create  unique  job  name  job  name  prefix  DEMO  imageclassification  timestamp  time  strftime  time  gmtime  job  name  job  name  prefix  timestamp  training  params  specify  the  training  docker  image  AlgorithmSpecification  TrainingImage  training  image  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  s3  output  format  bucket  job  name  prefix  ResourceConfig  InstanceCount  InstanceType  ml  p2  xlarge  VolumeSizeInGB  50  TrainingJobName  job  name  HyperParameters  image  shape  image  shape  num  layers  str  num  layers  num  training  samples  str  num  training  samples  num  classes  str  num  classes  mini  batch  size  str  mini  batch  size  epochs  str  epochs  learning  rate  str  learning  rate  use  pretrained  model  str  use  pretrained  model  StoppingCondition  MaxRuntimeInSeconds  360000  Training  data  should  be  inside  subdirectory  called  train  Validation  data  should  b,amazon
e  inside  subdirectory  called  validation  The  algorithm  currently  only  supports  fullyreplicated  model  where  data  is  copied  onto  each  machine  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  recordio  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  recordio  CompressionType  None  print  Training  job  name  format  job  name  print  nInput  Data  Location  format  training  params  InputDataConfig  DataSource  S3DataSource  create  the  Amazon  SageMaker  training  job  sagemaker  boto3  client  service  name  sagemaker  sagemaker  create  training  job  training  params  confirm  that  the  training  job  has  started  status  sagemaker  describe  training  job  TrainingJobName  job  name  TrainingJobStatu,amazon
s  print  Training  job  current  status  format  status  try  wait  for  the  job  to  finish  and  report  the  ending  status  sagemaker  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  except  print  Training  failed  to  start  if  exception  is  raised  that  means  it  has  failed  message  sagemaker  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  time  import  boto3  from  time  import  gmtime  strftime  sage  boto3  Session  client  service  name  sagemaker  model  name  DEMO  image  classification  model  print  model  name  info ,amazon
 sage  describe  training  job  TrainingJobName  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  data  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  image  classification  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  image  classification  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  image  classification  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  image  classification  latest  hosting  image  containers  boto3  Session  region  name  primary  container  Image  hosting  image  ModelDataUrl  model  data  create  model  response  sage  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  timestamp  time  strftime  time  gmtime  endpoint  config  name  job  name  prefix  epc  timestamp  endpoint  config  response  sage  create  endpoint  config  EndpointConfigN,amazon
ame  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  configuration  name  format  endpoint  config  name  print  Endpoint  configuration  arn  format  endpoint  config  response  EndpointConfigArn  time  import  time  timestamp  time  strftime  time  gmtime  endpoint  name  job  name  prefix  ep  timestamp  print  Endpoint  name  format  endpoint  name  endpoint  params  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  endpoint  response  sagemaker  create  endpoint  endpoint  params  print  EndpointArn  format  endpoint  response  EndpointArn  get  the  status  of  the  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  response  EndpointStatus  print  EndpointStatus  format  status  wait  until  the  status  has  changed  sagemaker  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  print  the  status  of  the  e,amazon
ndpoint  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  endpoint  response  EndpointStatus  print  Endpoint  creation  ended  with  EndpointStatus  format  status  if  status  InService  raise  Exception  Endpoint  creation  failed  import  boto3  runtime  boto3  Session  client  service  name  runtime  sagemaker  wget  tmp  test  jpg  http  www  vision  caltech  edu  Image  Datasets  Caltech256  images  008  bathtub  008  0007  jpg  file  name  tmp  test  jpg  test  image  from  IPython  display  import  Image  Image  file  name  import  json  import  numpy  as  np  with  open  file  name  rb  as  payload  read  payload  bytearray  payload  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  image  Body  payload  result  response  Body  read  result  will  be  in  json  format  and  convert  it  to  ndarray  result  json  loads  result  the  result  will  output  the  probabilities  for  all  classes  find  the  class  with  m,amazon
aximum  probability  and  print  the  class  index  index  np  argmax  result  object  categories  ak47  american  flag  backpack  baseball  bat  baseball  glove  basketball  hoop  bat  bathtub  bear  beer  mug  billiards  binoculars  birdbath  blimp  bonsai  101  boom  box  bowling  ball  bowling  pin  boxing  glove  brain  101  breadmaker  buddha  101  bulldozer  butterfly  cactus  cake  calculator  camel  cannon  canoe  car  tire  cartman  cd  centipede  cereal  box  chandelier  101  chess  board  chimp  chopsticks  cockroach  coffee  mug  coffin  coin  comet  computer  keyboard  computer  monitor  computer  mouse  conch  cormorant  covered  wagon  cowboy  hat  crab  101  desk  globe  diamond  ring  dice  dog  dolphin  101  doorknob  drinking  straw  duck  dumb  bell  eiffel  tower  electric  guitar  101  elephant  101  elk  ewer  101  eyeglasses  fern  fighter  jet  fire  extinguisher  fire  hydrant  fire  truck  fireworks  flashlight  floppy  disk  football  helmet  french  horn  fried  egg  frisbee  fro,amazon
g  frying  pan  galaxy  gas  pump  giraffe  goat  golden  gate  bridge  goldfish  golf  ball  goose  gorilla  grand  piano  101  grapes  grasshopper  guitar  pick  hamburger  hammock  harmonica  harp  harpsichord  hawksbill  101  head  phones  helicopter  101  hibiscus  homer  simpson  horse  horseshoe  crab  hot  air  balloon  hot  dog  hot  tub  hourglass  house  fly  human  skeleton  hummingbird  ibis  101  ice  cream  cone  iguana  ipod  iris  jesus  christ  joy  stick  kangaroo  101  kayak  ketch  101  killer  whale  knife  ladder  laptop  101  lathe  leopards  101  license  plate  lightbulb  light  house  lightning  llama  101  mailbox  mandolin  mars  mattress  megaphone  menorah  101  microscope  microwave  minaret  minotaur  motorbikes  101  mountain  bike  mushroom  mussels  necktie  octopus  ostrich  owl  palm  pilot  palm  tree  paperclip  paper  shredder  pci  card  penguin  people  pez  dispenser  photocopier  picnic  table  playing  card  porcupine  pram  praying  mantis  pyramid  raccoon  radi,amazon
o  telescope  rainbow  refrigerator  revolver  101  rifle  rotary  phone  roulette  wheel  saddle  saturn  school  bus  scorpion  101  screwdriver  segway  self  propelled  lawn  mower  sextant  sheet  music  skateboard  skunk  skyscraper  smokestack  snail  snake  sneaker  snowmobile  soccer  ball  socks  soda  can  spaghetti  speed  boat  spider  spoon  stained  glass  starfish  101  steering  wheel  stirrups  sunflower  101  superman  sushi  swan  swiss  army  knife  sword  syringe  tambourine  teapot  teddy  bear  teepee  telephone  box  tennis  ball  tennis  court  tennis  racket  theodolite  toaster  tomato  tombstone  top  hat  touring  bike  tower  pisa  traffic  light  treadmill  triceratops  tricycle  trilobite  101  tripod  shirt  tuning  fork  tweezer  umbrella  101  unicorn  vcr  video  projector  washing  machine  watch  101  waterfall  watermelon  welding  mask  wheelbarrow  windmill  wine  bottle  xylophone  yarmulke  yo  yo  zebra  airplanes  101  car  side  101  faces  easy  101  greyhound  ,amazon
tennis  shoes  toad  clutter  print  Result  label  object  categories  index  probability  str  result  index  sage  delete  endpoint  EndpointName  endpoint  name  ,amazon
import  tensorflow  as  tf  import  numpy  as  np  import  matplotlib  pyplot  as  plt  from  keras  datasets  import  mnist  import  random  import  pandas  as  pd  from  future  import  print  function  train  train  test  test  mnist  load  data  train  train  255  test  test  255  train  train  reshape  len  train  np  prod  train  shape  test  test  reshape  len  test  np  prod  test  shape  60  train  train  test  test  noise  factor  train  noisy  train  noise  factor  np  random  normal  loc  scale  size  train  shape  test  noisy  test  noise  factor  np  random  normal  loc  scale  size  test  shape  def  norm  return  np  min  np  max  np  min  train  noisy  norm  train  noisy  test  noisy  norm  test  noisy  train  noisy  np  concatenate  train  noisy  train  noisy  test  noisy  np  concatenate  test  noisy  test  noisy  train  norm  np  concatenate  train  train  np  random  seed  200  sel  random  sample  range  train  shape  train  noisy  train  noisy  sel  test  noisy  test  noisy  sel  train ,amazon
 train  sel  train  train  train  np  concatenate  train  train  sel  train  np  array  pd  get  dummies  train  astype  np  float32  test  test  test0  np  concatenate  test  test  sel  test  np  array  pd  get  dummies  test0  train  np  array  train  astype  np  float64  train  noisy  train  noisy  astype  np  float64  num  steps  10  batch  size  show  steps  50  learning  rate1  0001  image  dim  784  gen  hidden  dim  80  number  of  hidden  layers  in  Generator  disc  hidden  dim  80  number  of  hidden  layers  in  Discriminator  noise  dim  10  def  mean  mm  tf  nn  moments  axes  return  mm  def  var  var  tf  nn  moments  axes  return  var  tf  reset  default  graph  def  generator  reuse  False  with  tf  variable  scope  Generator  reuse  reuse  tf  layers  dense  units  64  tf  nn  relu  tf  reshape  shape  64  tf  layers  conv2d  transpose  32  strides  tf  nn  batch  normalization  mean  mean  variance  var  offset  None  scale  None  variance  epsilon  1e  tf  layers  conv2d  transpose  str,amazon
ides  tf  nn  relu  tf  reshape  784  return  xdef  discriminator  reuse  False  with  tf  variable  scope  Discriminator  reuse  reuse  tf  reshape  28  28  tf  layers  conv2d  32  tf  nn  relu  tf  layers  average  pooling2d  padding  same  tf  layers  conv2d  64  padding  same  tf  nn  relu  tf  layers  average  pooling2d  tf  contrib  layers  flatten  tf  layers  dense  784  tf  nn  sigmoid  return  xnoise  input  tf  placeholder  tf  float32  shape  None  784  real  image  input  tf  placeholder  tf  float32  shape  None  784  gen  sample  generator  noise  input  disc  real  discriminator  real  image  input  disc  fake  discriminator  gen  sample  reuse  True  disc  concat  tf  concat  disc  real  disc  fake  axis  stacked  gan  discriminator  gen  sample  reuse  True  disc  target  tf  placeholder  tf  float32  shape  None  784  gen  target  tf  placeholder  tf  float32  shape  None  784  gen  loss  tf  reduce  mean  tf  losses  mean  squared  error  real  image  input  gen  sample  disc  loss  tf  re,amazon
duce  mean  tf  losses  mean  squared  error  real  image  input  stacked  gan  optimizer  gen  tf  train  AdamOptimizer  learning  rate  learning  rate1  optimizer  disc  tf  train  AdamOptimizer  learning  rate  learning  rate1  gen  vars  tf  get  collection  tf  GraphKeys  TRAINABLE  VARIABLES  scope  Generator  disc  vars  tf  get  collection  tf  GraphKeys  TRAINABLE  VARIABLES  scope  Discriminator  train  disc  optimizer  disc  minimize  disc  loss  train  gen  optimizer  gen  minimize  gen  loss  init  tf  global  variables  initializer  def  next  batch  num  data  labels  idx  np  arange  len  data  np  random  shuffle  idx  idx  idx  num  data  shuffle  data  for  in  idx  labels  shuffle  labels  for  in  idx  return  np  asarray  data  shuffle  astype  np  float32  np  asarray  labels  shuffle  astype  np  float32  with  tf  Session  as  sess  sess  run  init  for  in  range  num  steps  batch  batch  next  batch  batch  size  train  train  noisy  feed  dict  real  image  input  batch  noise  in,amazon
put  batch  disc  target  batch  gen  target  batch  gl  dl  sess  run  train  gen  train  disc  gen  loss  disc  loss  feed  dict  feed  dict  sess  run  stacked  gan  feed  dict  noise  input  batch  sess  run  gen  sample  feed  dict  noise  input  batch  if  show  steps  or  print  Epoch  Generator  Loss  Discriminator  Loss  gl  dl  plt  figure  figsize  10  10  for  in  range  11  18  ax  plt  subplot  10  plt  imshow  train  noisy  reshape  28  28  plt  gray  ax  get  xaxis  set  visible  False  ax  get  yaxis  set  visible  False  plt  figure  figsize  10  10  for  in  range  11  18  ax  plt  subplot  10  plt  imshow  np  array  reshape  60  28  28  plt  gray  ax  get  xaxis  set  visible  False  ax  get  yaxis  set  visible  False  plt  show  plt  figure  figsize  10  10  for  in  range  11  18  ax  plt  subplot  10  plt  imshow  np  array  reshape  60  28  28  plt  gray  ax  get  xaxis  set  visible  False  ax  get  yaxis  set  visible  False  plt  show  ,amazon
bin  bash  setup  shimport  os  import  numpy  as  np  import  tempfile  import  tensorflow  as  tf  import  sagemaker  import  boto3  from  sagemaker  estimator  import  Estimator  region  boto3  Session  region  name  sagemaker  session  sagemaker  Session  smclient  boto3  client  sagemaker  bucket  sagemaker  Session  default  bucket  s3  bucket  name  must  be  in  the  same  region  as  the  one  specified  above  prefix  sagemaker  DEMO  hpo  keras  cifar10  role  sagemaker  get  execution  role  NUM  CLASSES  10  the  data  set  has  10  categories  of  imagesimport  shlex  import  subprocess  def  get  image  name  ecr  repository  tensorflow  version  tag  return  tensorflow  ecr  repository  tensorflow  version  tag  def  build  image  name  version  cmd  docker  build  build  arg  VERSION  Dockerfile  name  version  subprocess  check  call  shlex  split  cmd  version  tag  can  be  found  at  https  hub  docker  com  tensorflow  tensorflow  tags  latest  cpu  version  is  latest  while  latest  gp,amazon
u  version  is  latest  gpu  tensorflow  version  tag  latest  account  boto3  client  sts  get  caller  identity  Account  ecr  repository  dkr  ecr  amazonaws  com  test  account  region  your  ECR  repository  which  you  should  have  been  created  before  running  the  notebook  image  name  get  image  name  ecr  repository  tensorflow  version  tag  print  building  image  image  name  build  image  image  name  tensorflow  version  tag  def  upload  channel  channel  name  tf  keras  utils  to  categorical  NUM  CLASSES  file  path  tempfile  mkdtemp  np  savez  compressed  os  path  join  file  path  cifar  10  npz  compressed  npz  return  sagemaker  session  upload  data  path  file  path  bucket  bucket  key  prefix  data  DEMO  keras  cifar10  channel  name  def  upload  training  data  The  data  split  between  train  and  test  sets  train  train  test  test  tf  keras  datasets  cifar10  load  data  train  data  location  upload  channel  train  train  train  test  data  location  upload  ch,amazon
annel  test  test  test  return  train  train  data  location  test  test  data  location  channels  upload  training  data  hyperparameters  dict  batch  size  32  data  augmentation  True  learning  rate  0001  width  shift  range  height  shift  range  epochs  hyperparameters  time  output  location  s3  output  format  bucket  prefix  estimator  Estimator  image  name  role  role  output  path  output  location  train  instance  count  train  instance  type  local  hyperparameters  hyperparameters  estimator  fit  channels  def  push  image  name  cmd  aws  ecr  get  login  no  include  email  region  region  login  subprocess  check  output  shlex  split  cmd  strip  subprocess  check  call  shlex  split  login  decode  cmd  docker  push  name  subprocess  check  call  shlex  split  cmd  print  pushing  image  image  name  push  image  image  name  import  json  from  time  import  gmtime  strftime  tuning  job  name  BYO  keras  tuningjob  strftime  gmtime  print  tuning  job  name  tuning  job  config ,amazon
 ParameterRanges  CategoricalParameterRanges  ContinuousParameterRanges  MaxValue  001  MinValue  0001  Name  learning  rate  IntegerParameterRanges  ResourceLimits  MaxNumberOfTrainingJobs  MaxParallelTrainingJobs  Strategy  Bayesian  HyperParameterTuningJobObjective  MetricName  loss  Type  Minimize  training  image  image  name  print  training  artifacts  will  be  uploaded  to  format  output  location  training  job  definition  AlgorithmSpecification  MetricDefinitions  Name  loss  Regex  loss  TrainingImage  training  image  TrainingInputMode  File  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  channels  train  S3DataDistributionType  FullyReplicated  CompressionType  None  RecordWrapperType  None  ChannelName  test  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  channels  test  S3DataDistributionType  FullyReplicated  CompressionType  None  RecordWrapperType  None  OutputDataConfig  S3OutputPath  s3  output  format  bucket  prefix  ResourceConfig ,amazon
 InstanceCount  InstanceType  ml  m4  xlarge  VolumeSizeInGB  50  RoleArn  role  StaticHyperParameters  batch  size  32  data  augmentation  True  height  shift  range  width  shift  range  epochs  StoppingCondition  MaxRuntimeInSeconds  43200  smclient  create  hyper  parameter  tuning  job  HyperParameterTuningJobName  tuning  job  name  HyperParameterTuningJobConfig  tuning  job  config  TrainingJobDefinition  training  job  definition  smclient  describe  hyper  parameter  tuning  job  HyperParameterTuningJobName  tuning  job  name  HyperParameterTuningJobStatus  ,amazon
pip  install  wgetimport  os  import  json  import  time  import  wget  import  keras  import  numpy  import  pickle  import  warnings  import  ibm  boto3  from  keras  datasets  import  mnist  from  ibm  botocore  client  import  Config  from  sklearn  model  selection  import  train  test  splitcos  credentials  PLACE  YOUR  COS  CREDENTIALS  HERE  api  key  cos  credentials  apikey  service  instance  id  cos  credentials  resource  instance  id  auth  endpoint  https  iam  bluemix  net  oidc  token  service  endpoint  https  s3  api  us  geo  objectstorage  softlayer  net  cos  ibm  boto3  resource  s3  ibm  api  key  id  api  key  ibm  service  instance  id  service  instance  id  ibm  auth  endpoint  auth  endpoint  config  Config  signature  version  oauth  endpoint  url  service  endpoint  timestamp  str  time  time  buckets  mnist  data  timestamp  mnist  results  timestamp  for  bucket  in  buckets  if  not  cos  Bucket  bucket  in  cos  buckets  all  print  Creating  bucket  format  bucket  try  co,ibm
s  create  bucket  Bucket  bucket  except  ibm  boto3  exceptions  ibm  botocore  client  ClientError  as  print  Error  format  response  Error  Message  link  https  s3  amazonaws  com  img  datasets  mnist  npz  data  dir  MNIST  KERAS  DATA  if  not  os  path  isdir  data  dir  os  mkdir  data  dir  if  not  os  path  isfile  os  path  join  data  dir  os  path  join  link  split  wget  download  link  out  data  dir  ls  MNIST  KERAS  DATAbucket  name  buckets  bucket  obj  cos  Bucket  bucket  name  for  filename  in  os  listdir  data  dir  with  open  os  path  join  data  dir  filename  rb  as  data  bucket  obj  upload  file  os  path  join  data  dir  filename  filename  print  is  uploaded  format  filename  for  obj  in  cos  Bucket  mnist  data  timestamp  objects  all  print  Object  key  size  1f  kB  format  obj  key  obj  size  1024  print  ,ibm
mkdir  data  wget  data  aclImdb  v1  tar  gz  http  ai  stanford  edu  amaas  data  sentiment  aclImdb  v1  tar  gz  tar  zxf  data  aclImdb  v1  tar  gz  dataimport  os  import  glob  def  read  imdb  data  data  dir  data  aclImdb  data  labels  for  data  type  in  train  test  data  data  type  labels  data  type  for  sentiment  in  pos  neg  data  data  type  sentiment  labels  data  type  sentiment  path  os  path  join  data  dir  data  type  sentiment  txt  files  glob  glob  path  for  in  files  with  open  as  review  data  data  type  sentiment  append  review  read  Here  we  represent  positive  review  by  and  negative  review  by  labels  data  type  sentiment  append  if  sentiment  pos  else  assert  len  data  data  type  sentiment  len  labels  data  type  sentiment  data  size  does  not  match  labels  size  format  data  type  sentiment  return  data  labelsdata  labels  read  imdb  data  print  IMDB  reviews  train  pos  neg  test  pos  neg  format  len  data  train  pos  len  data ,amazon
 train  neg  len  data  test  pos  len  data  test  neg  from  sklearn  utils  import  shuffle  def  prepare  imdb  data  data  labels  Prepare  training  and  test  sets  from  IMDb  movie  reviews  Combine  positive  and  negative  reviews  and  labels  data  train  data  train  pos  data  train  neg  data  test  data  test  pos  data  test  neg  labels  train  labels  train  pos  labels  train  neg  labels  test  labels  test  pos  labels  test  neg  Shuffle  reviews  and  corresponding  labels  within  training  and  test  sets  data  train  labels  train  shuffle  data  train  labels  train  data  test  labels  test  shuffle  data  test  labels  test  Return  unified  training  data  test  data  training  labels  test  labets  return  data  train  data  test  labels  train  labels  testtrain  test  train  test  prepare  imdb  data  data  labels  print  IMDb  reviews  combined  train  test  format  len  train  len  test  train  100  import  nltk  nltk  download  stopwords  from  nltk  corpus  import  stop,amazon
words  from  nltk  stem  porter  import  stemmer  PorterStemmer  import  re  from  bs4  import  BeautifulSoup  def  review  to  words  review  text  BeautifulSoup  review  html  parser  get  text  Remove  HTML  tags  text  re  sub  zA  Z0  text  lower  Convert  to  lower  case  words  text  split  Split  string  into  words  words  for  in  words  if  not  in  stopwords  words  english  Remove  stopwords  words  PorterStemmer  stem  for  in  words  stem  return  wordsimport  pickle  cache  dir  os  path  join  cache  sentiment  analysis  where  to  store  cache  files  os  makedirs  cache  dir  exist  ok  True  ensure  cache  directory  exists  def  preprocess  data  data  train  data  test  labels  train  labels  test  cache  dir  cache  dir  cache  file  preprocessed  data  pkl  Convert  each  review  to  words  read  from  cache  if  available  If  cache  file  is  not  None  try  to  read  from  it  first  cache  data  None  if  cache  file  is  not  None  try  with  open  os  path  join  cache  dir  cach,amazon
e  file  rb  as  cache  data  pickle  load  print  Read  preprocessed  data  from  cache  file  cache  file  except  pass  unable  to  read  from  cache  but  that  okay  If  cache  is  missing  then  do  the  heavy  lifting  if  cache  data  is  None  Preprocess  training  and  test  data  to  obtain  words  for  each  review  words  train  list  map  review  to  words  data  train  words  test  list  map  review  to  words  data  test  words  train  review  to  words  review  for  review  in  data  train  words  test  review  to  words  review  for  review  in  data  test  Write  to  cache  file  for  future  runs  if  cache  file  is  not  None  cache  data  dict  words  train  words  train  words  test  words  test  labels  train  labels  train  labels  test  labels  test  with  open  os  path  join  cache  dir  cache  file  wb  as  pickle  dump  cache  data  print  Wrote  preprocessed  data  to  cache  file  cache  file  else  Unpack  data  loaded  from  cache  file  words  train  words  test  labels  tr,amazon
ain  labels  test  cache  data  words  train  cache  data  words  test  cache  data  labels  train  cache  data  labels  test  return  words  train  words  test  labels  train  labels  test  Preprocess  data  train  test  train  test  preprocess  data  train  test  train  test  import  numpy  as  np  from  sklearn  feature  extraction  text  import  CountVectorizer  from  sklearn  externals  import  joblib  joblib  is  an  enhanced  version  of  pickle  that  is  more  efficient  for  storing  NumPy  arrays  def  extract  BoW  features  words  train  words  test  vocabulary  size  5000  cache  dir  cache  dir  cache  file  bow  features  pkl  Extract  Bag  of  Words  for  given  set  of  documents  already  preprocessed  into  words  If  cache  file  is  not  None  try  to  read  from  it  first  cache  data  None  if  cache  file  is  not  None  try  with  open  os  path  join  cache  dir  cache  file  rb  as  cache  data  joblib  load  print  Read  features  from  cache  file  cache  file  except  pass  una,amazon
ble  to  read  from  cache  but  that  okay  If  cache  is  missing  then  do  the  heavy  lifting  if  cache  data  is  None  Fit  vectorizer  to  training  documents  and  use  it  to  transform  them  NOTE  Training  documents  have  already  been  preprocessed  and  tokenized  into  words  pass  in  dummy  functions  to  skip  those  steps  preprocessor  lambda  vectorizer  CountVectorizer  max  features  vocabulary  size  preprocessor  lambda  tokenizer  lambda  already  preprocessed  features  train  vectorizer  fit  transform  words  train  toarray  Apply  the  same  vectorizer  to  transform  the  test  documents  ignore  unknown  words  features  test  vectorizer  transform  words  test  toarray  NOTE  Remember  to  convert  the  features  using  toarray  for  compact  representation  Write  to  cache  file  for  future  runs  store  vocabulary  as  well  if  cache  file  is  not  None  vocabulary  vectorizer  vocabulary  cache  data  dict  features  train  features  train  features  test  features  ,amazon
test  vocabulary  vocabulary  with  open  os  path  join  cache  dir  cache  file  wb  as  joblib  dump  cache  data  print  Wrote  features  to  cache  file  cache  file  else  Unpack  data  loaded  from  cache  file  features  train  features  test  vocabulary  cache  data  features  train  cache  data  features  test  cache  data  vocabulary  Return  both  the  extracted  features  as  well  as  the  vocabulary  return  features  train  features  test  vocabulary  Extract  Bag  of  Words  features  for  both  training  and  test  datasets  train  test  vocabulary  extract  BoW  features  train  test  import  pandas  as  pd  TODO  Split  the  train  and  train  arrays  into  the  DataFrames  val  train  and  val  train  Make  sure  that  val  and  val  contain  10  000  entires  while  train  and  train  contain  the  remaining  15  000  entries  val  pd  DataFrame  None  train  pd  DataFrame  None  val  pd  DataFrame  None  train  pd  DataFrame  None  Solution  Earlier  we  shuffled  the  training  dataset,amazon
  so  to  make  things  simple  we  can  just  assign  the  first  10  000  reviews  to  the  validation  set  and  use  the  remaining  reviews  for  training  val  pd  DataFrame  train  10000  train  pd  DataFrame  train  10000  val  pd  DataFrame  train  10000  train  pd  DataFrame  train  10000  First  we  make  sure  that  the  local  directory  in  which  we  like  to  store  the  training  and  validation  csv  files  exists  data  dir  data  xgboost  if  not  os  path  exists  data  dir  os  makedirs  data  dir  First  save  the  test  data  to  test  csv  in  the  data  dir  directory  Note  that  we  do  not  save  the  associated  ground  truth  labels  instead  we  will  use  them  later  to  compare  with  our  model  output  pd  DataFrame  test  to  csv  os  path  join  data  dir  test  csv  header  False  index  False  TODO  Save  the  training  and  validation  data  to  train  csv  and  validation  csv  in  the  data  dir  directory  Make  sure  that  the  files  you  create  are  in  the  co,amazon
rrect  format  Solution  pd  concat  val  val  axis  to  csv  os  path  join  data  dir  validation  csv  header  False  index  False  pd  concat  train  train  axis  to  csv  os  path  join  data  dir  train  csv  header  False  index  False  To  save  bit  of  memory  we  can  set  text  train  val  train  and  val  to  None  test  train  val  train  val  Noneimport  sagemaker  session  sagemaker  Session  Store  the  current  SageMaker  session  S3  prefix  which  folder  will  we  use  prefix  sentiment  xgboost  TODO  Upload  the  test  csv  train  csv  and  validation  csv  files  which  are  contained  in  data  dir  to  S3  using  sess  upload  data  test  location  None  val  location  None  train  location  None  Solution  test  location  session  upload  data  os  path  join  data  dir  test  csv  key  prefix  prefix  val  location  session  upload  data  os  path  join  data  dir  validation  csv  key  prefix  prefix  train  location  session  upload  data  os  path  join  data  dir  train  csv  k,amazon
ey  prefix  prefix  from  sagemaker  import  get  execution  role  Our  current  execution  role  is  require  when  creating  the  model  as  the  training  and  inference  code  will  need  to  access  the  model  artifacts  role  get  execution  role  We  need  to  retrieve  the  location  of  the  container  which  is  provided  by  Amazon  for  using  XGBoost  As  matter  of  convenience  the  training  and  inference  code  both  use  the  same  container  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  session  boto  region  name  xgboost  TODO  Create  SageMaker  estimator  using  the  container  location  determined  in  the  previous  cell  It  is  recommended  that  you  use  single  training  instance  of  type  ml  m4  xlarge  It  is  also  recommended  that  you  use  s3  output  format  session  default  bucket  prefix  as  the  output  path  xgb  None  Solution  xgb  sagemaker  estimator  Estimator  container  The  location  of  the  container  ,amazon
we  wish  to  use  role  What  is  our  current  IAM  Role  train  instance  count  How  many  compute  instances  train  instance  type  ml  m4  xlarge  What  kind  of  compute  instances  output  path  s3  output  format  session  default  bucket  prefix  sagemaker  session  session  TODO  Set  the  XGBoost  hyperparameters  in  the  xgb  object  Don  forget  that  in  this  case  we  have  binary  label  so  we  should  be  using  the  binary  logistic  objective  Solution  xgb  set  hyperparameters  max  depth  eta  gamma  min  child  weight  subsample  silent  objective  binary  logistic  early  stopping  rounds  10  num  round  500  s3  input  train  sagemaker  s3  input  s3  data  train  location  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  val  location  content  type  csv  xgb  fit  train  s3  input  train  validation  s3  input  validation  TODO  Create  transformer  object  from  the  trained  model  Using  an  instance  count  of  and  an  instance  type  of  ml  m4 ,amazon
 xlarge  should  be  more  than  enough  xgb  transformer  xgb  transformer  instance  count  instance  type  ml  m4  xlarge  TODO  Start  the  transform  job  Make  sure  to  specify  the  content  type  and  the  split  type  of  the  test  data  xgb  transformer  transform  test  location  content  type  text  csv  split  type  Line  xgb  transformer  wait  aws  s3  cp  recursive  xgb  transformer  output  path  data  dirpredictions  pd  read  csv  os  path  join  data  dir  test  csv  out  header  None  predictions  round  num  for  num  in  predictions  squeeze  values  from  sklearn  metrics  import  accuracy  score  accuracy  score  test  predictions  First  we  will  remove  all  of  the  files  contained  in  the  data  dir  directory  rm  data  dir  And  then  we  delete  the  directory  itself  rmdir  data  dir  Similarly  we  will  remove  the  files  in  the  cache  dir  directory  and  the  directory  itself  rm  cache  dir  rmdir  cache  dir  ,amazon
import  collections  import  hashlib  import  numpy  as  np  import  os  path  import  random  import  re  import  sys  import  tarfile  import  tensorflow  as  tf  from  datetime  import  datetime  from  six  moves  import  urllib  from  tensorflow  python  framework  import  graph  util  from  tensorflow  python  framework  import  tensor  shape  from  tensorflow  python  platform  import  gfile  from  tensorflow  python  util  import  compat  MAX  NUM  IMAGES  PER  CLASS  27  134M  tf  logging  set  verbosity  tf  logging  INFO  project  id  datalab  project  id  bucket  gs  image  classifier  project  id  gsutil  mb  bucket  print  Execution  complete  import  IPython  import  base64  import  pandas  as  pd  from  cStringIO  import  StringIO  from  tensorflow  python  lib  io  import  file  io  as  tf  file  io  def  resize  image  image  str  tensor  Decodes  jpeg  string  resizes  it  and  re  encode  it  to  jpeg  import  tensorflow  as  tf  These  constants  are  set  by  Inception  v3  expectations  ,google
height  299  width  299  channels  image  tf  image  decode  jpeg  image  str  tensor  channels  channels  Note  resize  expects  batch  size  but  tf  map  supresses  that  index  thus  we  have  to  expand  then  squeeze  Resize  returns  float32  in  the  range  uint8  max  image  tf  expand  dims  image  image  tf  image  resize  bilinear  image  height  width  align  corners  False  image  tf  squeeze  image  squeeze  dims  image  tf  cast  image  dtype  tf  uint8  image  tf  image  encode  jpeg  image  quality  100  return  image  def  display  images  image  files  Predict  using  deployed  online  model  images  for  image  file  in  image  files  with  tf  file  io  FileIO  image  file  as  ff  images  append  ff  read  To  resize  run  tf  session  so  we  can  reuse  decode  and  resize  which  is  used  in  prediction  graph  This  makes  sure  we  don  lose  any  quality  in  prediction  while  decreasing  the  size  of  the  images  submitted  to  the  model  over  network  image  str  tensor  t,google
f  placeholder  tf  string  shape  None  image  tf  map  fn  resize  image  image  str  tensor  back  prop  False  feed  dict  collections  defaultdict  list  feed  dict  image  str  tensor  name  images  with  tf  Session  as  sess  images  resized  sess  run  image  feed  dict  feed  dict  html  table  for  image  in  enumerate  images  resized  encoded  image  base64  b64encode  image  image  html  td  img  src  data  image  jpg  base64  td  encoded  image  if  html  html  tr  html  html  image  html  if  html  html  tr  IPython  display  display  IPython  display  Image  data  image  html  html  table  IPython  display  display  IPython  display  HTML  html  train  dataset  gs  candies  ml  dataset  v1  metadata  train  candies560  csv  storage  read  object  train  dataset  variable  text  df  pd  read  csv  StringIO  text  names  image  uri  category  images  categories  df  category  drop  duplicates  values  for  category  in  categories  image  uris  df  loc  df  category  category  images  extend  i,google
mage  uris  image  uri  sample  replace  False  values  display  images  images  print  Execution  complete  def  ensure  dir  exists  dir  name  Makes  sure  the  folder  exists  on  disk  Args  dir  name  Path  string  to  the  folder  we  want  to  create  if  not  os  path  exists  dir  name  os  makedirs  dir  name  def  prepare  file  system  summaries  dir  intermediate  output  graphs  dir  intermediate  store  frequency  Setup  the  directory  we  ll  write  summaries  to  for  TensorBoard  if  tf  gfile  Exists  summaries  dir  tf  gfile  DeleteRecursively  summaries  dir  tf  gfile  MakeDirs  summaries  dir  if  intermediate  store  frequency  ensure  dir  exists  intermediate  output  graphs  dir  return  Prepare  necessary  directories  that  can  be  used  during  training  summaries  dir  tf  files  training  summaries  mobilenet  50  224  intermediate  output  graphs  dir  tmp  intermediate  graph  intermediate  store  frequency  prepare  file  system  summaries  dir  summaries  dir  intermedi,google
ate  output  graphs  dir  intermediate  output  graphs  dir  intermediate  store  frequency  intermediate  store  frequency  print  Execution  complete  def  create  model  info  architecture  Given  the  name  of  model  architecture  returns  information  about  it  There  are  different  base  image  recognition  pretrained  models  that  can  be  retrained  using  transfer  learning  and  this  function  translates  from  the  name  of  model  to  the  attributes  that  are  needed  to  download  and  train  with  it  Args  architecture  Name  of  model  architecture  Returns  Dictionary  of  information  about  the  model  or  None  if  the  name  isn  recognized  Raises  ValueError  If  architecture  name  is  unknown  architecture  architecture  lower  if  architecture  inception  v3  pylint  disable  line  too  long  data  url  http  download  tensorflow  org  models  image  imagenet  inception  2015  12  05  tgz  pylint  enable  line  too  long  bottleneck  tensor  name  pool  reshape  bottleneck  te,google
nsor  size  2048  input  width  299  input  height  299  input  depth  resized  input  tensor  name  Mul  model  file  name  classify  image  graph  def  pb  input  mean  128  input  std  128  elif  architecture  startswith  mobilenet  parts  architecture  split  if  len  parts  and  len  parts  tf  logging  error  Couldn  understand  architecture  name  architecture  return  None  version  string  parts  if  version  string  and  version  string  75  and  version  string  50  and  version  string  25  tf  logging  error  The  Mobilenet  version  should  be  75  50  or  25  but  found  for  architecture  version  string  architecture  return  None  size  string  parts  if  size  string  224  and  size  string  192  and  size  string  160  and  size  string  128  tf  logging  error  The  Mobilenet  input  size  should  be  224  192  160  or  128  but  found  for  architecture  size  string  architecture  return  None  if  len  parts  is  quantized  False  else  if  parts  quantized  tf  logging  error  Couldn ,google
 understand  architecture  suffix  for  parts  architecture  return  None  is  quantized  True  data  url  http  download  tensorflow  org  models  mobilenet  v1  data  url  version  string  size  string  frozen  tgz  bottleneck  tensor  name  MobilenetV1  Predictions  Reshape  bottleneck  tensor  size  1001  input  width  int  size  string  input  height  int  size  string  input  depth  resized  input  tensor  name  input  if  is  quantized  model  base  name  quantized  graph  pb  else  model  base  name  frozen  graph  pb  model  dir  name  mobilenet  v1  version  string  size  string  model  file  name  os  path  join  model  dir  name  model  base  name  input  mean  127  input  std  127  else  tf  logging  error  Couldn  understand  architecture  name  architecture  raise  ValueError  Unknown  architecture  architecture  return  data  url  data  url  bottleneck  tensor  name  bottleneck  tensor  name  bottleneck  tensor  size  bottleneck  tensor  size  input  width  input  width  input  height  input  ,google
height  input  depth  input  depth  resized  input  tensor  name  resized  input  tensor  name  model  file  name  model  file  name  input  mean  input  mean  input  std  input  std  def  maybe  download  and  extract  data  url  model  dir  Download  and  extract  model  tar  file  If  the  pretrained  model  we  re  using  doesn  already  exist  this  function  downloads  it  from  the  TensorFlow  org  website  and  unpacks  it  into  directory  Args  data  url  Web  location  of  the  tar  file  containing  the  pretrained  model  dest  directory  model  dir  if  not  os  path  exists  dest  directory  os  makedirs  dest  directory  filename  data  url  split  filepath  os  path  join  dest  directory  filename  if  not  os  path  exists  filepath  def  progress  count  block  size  total  size  sys  stdout  write  Downloading  1f  filename  float  count  block  size  float  total  size  100  sys  stdout  flush  filepath  urllib  request  urlretrieve  data  url  filepath  progress  print  statinfo  os  s,google
tat  filepath  tf  logging  info  Successfully  downloaded  bytes  format  filename  statinfo  st  size  tarfile  open  filepath  gz  extractall  dest  directory  def  create  model  graph  model  info  model  dir  Creates  graph  from  saved  GraphDef  file  and  returns  Graph  object  Args  model  info  Dictionary  containing  information  about  the  model  architecture  Returns  Graph  holding  the  trained  Inception  network  and  various  tensors  we  ll  be  manipulating  with  tf  Graph  as  default  as  graph  model  path  os  path  join  model  dir  model  info  model  file  name  with  gfile  FastGFile  model  path  rb  as  graph  def  tf  GraphDef  graph  def  ParseFromString  read  bottleneck  tensor  resized  input  tensor  tf  import  graph  def  graph  def  name  return  elements  model  info  bottleneck  tensor  name  model  info  resized  input  tensor  name  return  graph  bottleneck  tensor  resized  input  tensor  Gather  information  about  the  model  architecture  we  ll  be  using  ,google
architecture  mobilenet  50  224  model  info  create  model  info  architecture  architecture  Set  up  the  pre  trained  graph  model  dir  tf  files  models  maybe  download  and  extract  model  info  data  url  model  dir  graph  bottleneck  tensor  resized  image  tensor  create  model  graph  model  info  model  dir  print  Execution  complete  def  create  image  lists  image  dir  testing  percentage  validation  percentage  Builds  list  of  training  images  from  the  file  system  Analyzes  the  sub  folders  in  the  image  directory  splits  them  into  stable  training  testing  and  validation  sets  and  returns  data  structure  describing  the  lists  of  images  for  each  label  and  their  paths  Args  image  dir  String  path  to  folder  containing  subfolders  of  images  testing  percentage  Integer  percentage  of  the  images  to  reserve  for  tests  validation  percentage  Integer  percentage  of  images  reserved  for  validation  Returns  dictionary  containing  an  entry  fo,google
r  each  label  subfolder  with  images  split  into  training  testing  and  validation  sets  within  each  label  if  not  gfile  Exists  image  dir  tf  logging  error  Image  directory  image  dir  not  found  return  None  result  collections  OrderedDict  sub  dirs  gfile  ListDirectory  image  dir  sub  dirs  sorted  item  for  item  in  sub  dirs  if  gfile  IsDirectory  os  path  join  image  dir  item  print  sub  dirs  for  dir  name  in  sub  dirs  extensions  jpg  jpeg  JPG  JPEG  file  list  if  dir  name  image  dir  continue  tf  logging  info  Looking  for  images  in  dir  name  for  extension  in  extensions  file  glob  os  path  join  image  dir  dir  name  extension  file  list  extend  gfile  Glob  file  glob  if  not  file  list  tf  logging  warning  No  files  found  continue  if  len  file  list  20  tf  logging  warning  WARNING  Folder  has  less  than  20  images  which  may  cause  issues  elif  len  file  list  MAX  NUM  IMAGES  PER  CLASS  tf  logging  warning  WARNING  Folde,google
r  has  more  than  images  Some  images  will  never  be  selected  format  dir  name  MAX  NUM  IMAGES  PER  CLASS  label  name  re  sub  z0  dir  name  lower  training  images  testing  images  validation  images  for  file  name  in  file  list  base  name  os  path  basename  file  name  hash  name  re  sub  nohash  file  name  hash  name  hashed  hashlib  sha1  compat  as  bytes  hash  name  hexdigest  percentage  hash  int  hash  name  hashed  16  MAX  NUM  IMAGES  PER  CLASS  100  MAX  NUM  IMAGES  PER  CLASS  Based  on  the  percentage  hash  probability  add  the  image  base  name  in  consistent  manner  in  either  the  validation  set  test  set  or  training  set  YOUR  CODE  HERE  raise  NotImplementedError  result  label  name  dir  dir  name  training  training  images  testing  testing  images  validation  validation  images  return  result  image  dir  gs  candies  ml  dataset  v1  images  testing  percentage  15  validation  percentage  15  image  lists  create  image  lists  image  dir  ,google
image  dir  testing  percentage  testing  percentage  validation  percentage  validation  percentage  print  Execution  complete  def  create  image  lists  image  dir  testing  percentage  validation  percentage  Builds  list  of  training  images  from  the  file  system  Analyzes  the  sub  folders  in  the  image  directory  splits  them  into  stable  training  testing  and  validation  sets  and  returns  data  structure  describing  the  lists  of  images  for  each  label  and  their  paths  Args  image  dir  String  path  to  folder  containing  subfolders  of  images  testing  percentage  Integer  percentage  of  the  images  to  reserve  for  tests  validation  percentage  Integer  percentage  of  images  reserved  for  validation  Returns  dictionary  containing  an  entry  for  each  label  subfolder  with  images  split  into  training  testing  and  validation  sets  within  each  label  if  not  gfile  Exists  image  dir  tf  logging  error  Image  directory  image  dir  not  found  return  No,google
ne  result  collections  OrderedDict  sub  dirs  gfile  ListDirectory  image  dir  sub  dirs  sorted  item  for  item  in  sub  dirs  if  gfile  IsDirectory  os  path  join  image  dir  item  print  sub  dirs  for  dir  name  in  sub  dirs  extensions  jpg  jpeg  JPG  JPEG  file  list  if  dir  name  image  dir  continue  tf  logging  info  Looking  for  images  in  dir  name  for  extension  in  extensions  file  glob  os  path  join  image  dir  dir  name  extension  file  list  extend  gfile  Glob  file  glob  if  not  file  list  tf  logging  warning  No  files  found  continue  if  len  file  list  20  tf  logging  warning  WARNING  Folder  has  less  than  20  images  which  may  cause  issues  elif  len  file  list  MAX  NUM  IMAGES  PER  CLASS  tf  logging  warning  WARNING  Folder  has  more  than  images  Some  images  will  never  be  selected  format  dir  name  MAX  NUM  IMAGES  PER  CLASS  label  name  re  sub  z0  dir  name  lower  training  images  testing  images  validation  images  for  fil,google
e  name  in  file  list  base  name  os  path  basename  file  name  hash  name  re  sub  nohash  file  name  hash  name  hashed  hashlib  sha1  compat  as  bytes  hash  name  hexdigest  percentage  hash  int  hash  name  hashed  16  MAX  NUM  IMAGES  PER  CLASS  100  MAX  NUM  IMAGES  PER  CLASS  if  percentage  hash  validation  percentage  validation  images  append  base  name  elif  percentage  hash  testing  percentage  validation  percentage  testing  images  append  base  name  else  training  images  append  base  name  result  label  name  dir  dir  name  training  training  images  testing  testing  images  validation  validation  images  return  result  image  dir  gs  candies  ml  dataset  v1  images  testing  percentage  15  validation  percentage  15  image  lists  create  image  lists  image  dir  image  dir  testing  percentage  testing  percentage  validation  percentage  validation  percentage  print  Execution  complete  def  get  image  path  image  lists  label  name  index  image  dir  ,google
category  Returns  path  to  an  image  for  label  at  the  given  index  Args  image  lists  Dictionary  of  training  images  for  each  label  label  name  Label  string  we  want  to  get  an  image  for  index  Int  offset  of  the  image  we  want  This  will  be  moduloed  by  the  available  number  of  images  for  the  label  so  it  can  be  arbitrarily  large  image  dir  Root  folder  string  of  the  subfolders  containing  the  training  images  category  Name  string  of  set  to  pull  images  from  training  testing  or  validation  Returns  File  system  path  string  to  an  image  that  meets  the  requested  parameters  if  label  name  not  in  image  lists  tf  logging  fatal  Label  does  not  exist  label  name  label  lists  image  lists  label  name  if  category  not  in  label  lists  tf  logging  fatal  Category  does  not  exist  category  category  list  label  lists  category  if  not  category  list  tf  logging  fatal  Label  has  no  images  in  the  category  label  name,google
  category  mod  index  index  len  category  list  base  name  category  list  mod  index  sub  dir  label  lists  dir  full  path  os  path  join  image  dir  sub  dir  base  name  return  full  path  def  get  bottleneck  path  image  lists  label  name  index  bottleneck  dir  category  architecture  Returns  path  to  bottleneck  file  for  label  at  the  given  index  Args  image  lists  Dictionary  of  training  images  for  each  label  label  name  Label  string  we  want  to  get  an  image  for  index  Integer  offset  of  the  image  we  want  This  will  be  moduloed  by  the  available  number  of  images  for  the  label  so  it  can  be  arbitrarily  large  bottleneck  dir  Folder  string  holding  cached  files  of  bottleneck  values  category  Name  string  of  set  to  pull  images  from  training  testing  or  validation  architecture  The  name  of  the  model  architecture  Returns  File  system  path  string  to  an  image  that  meets  the  requested  parameters  return  get  image  ,google
path  image  lists  label  name  index  bottleneck  dir  category  architecture  txt  def  run  bottleneck  on  image  sess  image  data  image  data  tensor  decoded  image  tensor  resized  input  tensor  bottleneck  tensor  Runs  inference  on  an  image  to  extract  the  bottleneck  summary  layer  Args  sess  Current  active  TensorFlow  Session  image  data  String  of  raw  JPEG  data  image  data  tensor  Input  data  layer  in  the  graph  decoded  image  tensor  Output  of  initial  image  resizing  and  preprocessing  resized  input  tensor  The  input  node  of  the  recognition  graph  bottleneck  tensor  Layer  before  the  final  softmax  Returns  Numpy  array  of  bottleneck  values  First  decode  the  JPEG  image  resize  it  and  rescale  the  pixel  values  resized  input  values  sess  run  decoded  image  tensor  image  data  tensor  image  data  Then  run  it  through  the  recognition  network  bottleneck  values  sess  run  bottleneck  tensor  resized  input  tensor  resized  input  ,google
values  bottleneck  values  np  squeeze  bottleneck  values  return  bottleneck  values  bottleneck  path  bottleneck  values  def  create  bottleneck  file  bottleneck  path  image  lists  label  name  index  image  dir  category  sess  jpeg  data  tensor  decoded  image  tensor  resized  input  tensor  bottleneck  tensor  Create  single  bottleneck  file  tf  logging  info  Creating  bottleneck  at  bottleneck  path  image  path  get  image  path  image  lists  label  name  index  image  dir  category  if  not  gfile  Exists  image  path  tf  logging  fatal  File  does  not  exist  image  path  image  data  gfile  FastGFile  image  path  rb  read  try  bottleneck  values  run  bottleneck  on  image  sess  image  data  jpeg  data  tensor  decoded  image  tensor  resized  input  tensor  bottleneck  tensor  except  Exception  as  raise  RuntimeError  Error  during  processing  file  image  path  str  bottleneck  string  join  str  for  in  bottleneck  values  with  open  bottleneck  path  as  bottleneck  file ,google
 bottleneck  file  write  bottleneck  string  def  get  or  create  bottleneck  sess  image  lists  label  name  index  image  dir  category  bottleneck  dir  jpeg  data  tensor  decoded  image  tensor  resized  input  tensor  bottleneck  tensor  architecture  Retrieves  or  calculates  bottleneck  values  for  an  image  If  cached  version  of  the  bottleneck  data  exists  on  disk  return  that  otherwise  calculate  the  data  and  save  it  to  disk  for  future  use  Args  sess  The  current  active  TensorFlow  Session  image  lists  Dictionary  of  training  images  for  each  label  label  name  Label  string  we  want  to  get  an  image  for  index  Integer  offset  of  the  image  we  want  This  will  be  modulo  ed  by  the  available  number  of  images  for  the  label  so  it  can  be  arbitrarily  large  image  dir  Root  folder  string  of  the  subfolders  containing  the  training  images  category  Name  string  of  which  set  to  pull  images  from  training  testing  or  validation ,google
 bottleneck  dir  Folder  string  holding  cached  files  of  bottleneck  values  jpeg  data  tensor  The  tensor  to  feed  loaded  jpeg  data  into  decoded  image  tensor  The  output  of  decoding  and  resizing  the  image  resized  input  tensor  The  input  node  of  the  recognition  graph  bottleneck  tensor  The  output  tensor  for  the  bottleneck  values  architecture  The  name  of  the  model  architecture  Returns  Numpy  array  of  values  produced  by  the  bottleneck  layer  for  the  image  label  lists  image  lists  label  name  sub  dir  label  lists  dir  sub  dir  path  os  path  join  bottleneck  dir  sub  dir  ensure  dir  exists  sub  dir  path  bottleneck  path  get  bottleneck  path  image  lists  label  name  index  bottleneck  dir  category  architecture  if  not  os  path  exists  bottleneck  path  create  bottleneck  file  bottleneck  path  image  lists  label  name  index  image  dir  category  sess  jpeg  data  tensor  decoded  image  tensor  resized  input  tensor  bottlen,google
eck  tensor  with  open  bottleneck  path  as  bottleneck  file  bottleneck  string  bottleneck  file  read  did  hit  error  False  try  bottleneck  values  float  for  in  bottleneck  string  split  except  ValueError  tf  logging  warning  Invalid  float  found  recreating  bottleneck  did  hit  error  True  if  did  hit  error  create  bottleneck  file  bottleneck  path  image  lists  label  name  index  image  dir  category  sess  jpeg  data  tensor  decoded  image  tensor  resized  input  tensor  bottleneck  tensor  with  open  bottleneck  path  as  bottleneck  file  bottleneck  string  bottleneck  file  read  Allow  exceptions  to  propagate  here  since  they  shouldn  happen  after  fresh  creation  bottleneck  values  float  for  in  bottleneck  string  split  return  bottleneck  values  bottleneck  string  def  add  jpeg  decoding  input  width  input  height  input  depth  input  mean  input  std  Adds  operations  that  perform  JPEG  decoding  and  resizing  to  the  graph  Args  input  width  D,google
esired  width  of  the  image  fed  into  the  recognizer  graph  input  height  Desired  width  of  the  image  fed  into  the  recognizer  graph  input  depth  Desired  channels  of  the  image  fed  into  the  recognizer  graph  input  mean  Pixel  value  that  should  be  zero  in  the  image  for  the  graph  input  std  How  much  to  divide  the  pixel  values  by  before  recognition  Returns  Tensors  for  the  node  to  feed  JPEG  data  into  and  the  output  of  the  preprocessing  steps  jpeg  data  tf  placeholder  tf  string  name  DecodeJPGInput  decoded  image  tf  image  decode  jpeg  jpeg  data  channels  input  depth  decoded  image  as  float  tf  cast  decoded  image  dtype  tf  float32  decoded  image  4d  tf  expand  dims  decoded  image  as  float  resize  shape  tf  stack  input  height  input  width  resize  shape  as  int  tf  cast  resize  shape  dtype  tf  int32  resized  image  tf  image  resize  bilinear  decoded  image  4d  resize  shape  as  int  offset  image  tf  subtract ,google
 resized  image  input  mean  mul  image  tf  multiply  offset  image  input  std  return  jpeg  data  mul  image  def  cache  bottlenecks  sess  image  lists  image  dir  bottleneck  dir  jpeg  data  tensor  decoded  image  tensor  resized  input  tensor  bottleneck  tensor  architecture  Ensures  all  the  training  testing  and  validation  bottlenecks  are  cached  Because  we  re  likely  to  read  the  same  image  multiple  times  it  can  speed  things  up  lot  if  we  calculate  the  bottleneck  layer  values  once  for  each  image  during  preprocessing  and  then  just  read  those  cached  values  repeatedly  during  training  Here  we  go  through  all  the  images  we  ve  found  calculate  those  values  and  save  them  off  Args  sess  The  current  active  TensorFlow  Session  image  lists  Dictionary  of  training  images  for  each  label  image  dir  Root  folder  string  of  the  subfolders  containing  the  training  images  bottleneck  dir  Folder  string  holding  cached  files  of ,google
 bottleneck  values  jpeg  data  tensor  Input  tensor  for  jpeg  data  from  file  decoded  image  tensor  The  output  of  decoding  and  resizing  the  image  resized  input  tensor  The  input  node  of  the  recognition  graph  bottleneck  tensor  The  penultimate  output  layer  of  the  graph  architecture  The  name  of  the  model  architecture  Returns  Nothing  how  many  bottlenecks  ensure  dir  exists  bottleneck  dir  bottleneck  gcs  dir  os  path  join  bucket  bottlenecks  for  category  in  training  testing  validation  bottlenecks  for  label  name  label  lists  in  image  lists  items  category  list  label  lists  category  for  index  unused  base  name  in  enumerate  category  list  bottleneck  string  get  or  create  bottleneck  sess  image  lists  label  name  index  image  dir  category  bottleneck  dir  jpeg  data  tensor  decoded  image  tensor  resized  input  tensor  bottleneck  tensor  architecture  bottlenecks  append  bottleneck  string  label  name  strip  how  many  bo,google
ttlenecks  if  how  many  bottlenecks  100  tf  logging  info  str  how  many  bottlenecks  bottleneck  files  created  Saving  bottlenecks  on  GCS  by  category  for  Cloud  ML  Engine  training  bottleneck  gcs  path  os  path  join  bottleneck  gcs  dir  csv  category  tf  logging  info  Writing  bottlenecks  on  GCS  to  category  bottleneck  gcs  path  with  gfile  FastGFile  bottleneck  gcs  path  as  bottleneck  gcs  file  for  bottleneck  string  in  bottlenecks  bottleneck  gcs  file  write  bottleneck  string  bottleneck  gcs  file  close  bottleneck  dir  tmp  bottleneck  with  tf  Session  graph  graph  as  sess  Set  up  the  image  decoding  sub  graph  jpeg  data  tensor  decoded  image  tensor  add  jpeg  decoding  model  info  input  width  model  info  input  height  model  info  input  depth  model  info  input  mean  model  info  input  std  We  ll  make  sure  we  ve  calculated  the  bottleneck  image  summaries  and  cached  them  on  disk  cache  bottlenecks  sess  image  lists  image,google
  dir  bottleneck  dir  jpeg  data  tensor  decoded  image  tensor  resized  image  tensor  bottleneck  tensor  architecture  print  Execution  complete  def  variable  summaries  var  Attach  lot  of  summaries  to  Tensor  for  TensorBoard  visualization  with  tf  name  scope  summaries  mean  tf  reduce  mean  var  tf  summary  scalar  mean  mean  with  tf  name  scope  stddev  stddev  tf  sqrt  tf  reduce  mean  tf  square  var  mean  tf  summary  scalar  stddev  stddev  tf  summary  scalar  max  tf  reduce  max  var  tf  summary  scalar  min  tf  reduce  min  var  tf  summary  histogram  histogram  var  def  add  final  training  ops  class  count  final  tensor  name  bottleneck  tensor  bottleneck  tensor  size  learning  rate  Adds  new  softmax  and  fully  connected  layer  for  training  We  need  to  retrain  the  top  layer  to  identify  our  new  classes  so  this  function  adds  the  right  operations  to  the  graph  along  with  some  variables  to  hold  the  weights  and  then  sets  up ,google
 all  the  gradients  for  the  backward  pass  The  set  up  for  the  softmax  and  fully  connected  layers  is  based  on  https  www  tensorflow  org  versions  master  tutorials  mnist  beginners  index  html  Args  class  count  Integer  of  how  many  categories  of  things  we  re  trying  to  recognize  final  tensor  name  Name  string  for  the  new  final  node  that  produces  results  bottleneck  tensor  The  output  of  the  main  CNN  graph  bottleneck  tensor  size  How  many  entries  in  the  bottleneck  vector  Returns  The  tensors  for  the  training  and  cross  entropy  results  and  tensors  for  the  bottleneck  input  and  ground  truth  input  with  tf  name  scope  input  bottleneck  input  tf  placeholder  with  default  bottleneck  tensor  shape  None  bottleneck  tensor  size  name  BottleneckInputPlaceholder  ground  truth  input  tf  placeholder  tf  float32  None  class  count  name  GroundTruthInput  Organizing  the  following  ops  as  final  training  ops  so  they  re  ,google
easier  to  see  in  TensorBoard  layer  name  final  training  ops  with  tf  name  scope  layer  name  with  tf  name  scope  weights  Create  the  layer  weights  tensor  variable  with  the  following  characteristics  size  entries  in  the  bottleneck  vector  number  of  classes  initial  values  truncated  normal  distribution  of  001  standard  deviation  name  final  weights  YOUR  CODE  HERE  raise  NotImplementedError  variable  summaries  layer  weights  with  tf  name  scope  biases  Create  the  layer  biases  tensor  variable  with  the  following  characteristics  size  number  of  classes  initial  values  name  final  biases  YOUR  CODE  HERE  raise  NotImplementedError  variable  summaries  layer  biases  with  tf  name  scope  logits  Create  the  logits  operation  for  calculating  the  last  layer  logits  logits  input  weights  biases  YOUR  CODE  HERE  raise  NotImplementedError  tf  summary  histogram  pre  activations  logits  Create  the  final  tensor  representing  the  predic,google
tions  of  the  last  layer  by  applying  softmax  on  the  logits  previously  calculated  Name  it  with  the  value  of  the  parameter  final  tensor  name  YOUR  CODE  HERE  raise  NotImplementedError  tf  summary  histogram  activations  final  tensor  with  tf  name  scope  cross  entropy  Calculate  the  cross  entropy  loss  as  the  difference  between  the  predicted  labels  and  the  ground  truth  Hint  tf  nn  softmax  cross  entropy  with  logits  YOUR  CODE  HERE  raise  NotImplementedError  with  tf  name  scope  total  Average  the  cross  entropy  over  all  the  examples  in  the  current  batch  Hint  tf  reduce  mean  YOUR  CODE  HERE  raise  NotImplementedError  tf  summary  scalar  cross  entropy  cross  entropy  mean  with  tf  name  scope  train  Create  gradient  descent  optimizer  with  the  given  learning  rate  Create  the  training  step  operation  that  minimises  the  mean  cross  entropy  with  gradient  descent  YOUR  CODE  HERE  raise  NotImplementedError  return  trai,google
n  step  cross  entropy  mean  bottleneck  input  ground  truth  input  final  tensor  print  Execution  complete  def  variable  summaries  var  Attach  lot  of  summaries  to  Tensor  for  TensorBoard  visualization  with  tf  name  scope  summaries  mean  tf  reduce  mean  var  tf  summary  scalar  mean  mean  with  tf  name  scope  stddev  stddev  tf  sqrt  tf  reduce  mean  tf  square  var  mean  tf  summary  scalar  stddev  stddev  tf  summary  scalar  max  tf  reduce  max  var  tf  summary  scalar  min  tf  reduce  min  var  tf  summary  histogram  histogram  var  def  add  final  training  ops  class  count  final  tensor  name  bottleneck  tensor  bottleneck  tensor  size  learning  rate  Adds  new  softmax  and  fully  connected  layer  for  training  We  need  to  retrain  the  top  layer  to  identify  our  new  classes  so  this  function  adds  the  right  operations  to  the  graph  along  with  some  variables  to  hold  the  weights  and  then  sets  up  all  the  gradients  for  the  backwar,google
d  pass  The  set  up  for  the  softmax  and  fully  connected  layers  is  based  on  https  www  tensorflow  org  versions  master  tutorials  mnist  beginners  index  html  Args  class  count  Integer  of  how  many  categories  of  things  we  re  trying  to  recognize  final  tensor  name  Name  string  for  the  new  final  node  that  produces  results  bottleneck  tensor  The  output  of  the  main  CNN  graph  bottleneck  tensor  size  How  many  entries  in  the  bottleneck  vector  Returns  The  tensors  for  the  training  and  cross  entropy  results  and  tensors  for  the  bottleneck  input  and  ground  truth  input  with  tf  name  scope  input  bottleneck  input  tf  placeholder  with  default  bottleneck  tensor  shape  None  bottleneck  tensor  size  name  BottleneckInputPlaceholder  ground  truth  input  tf  placeholder  tf  float32  None  class  count  name  GroundTruthInput  Organizing  the  following  ops  as  final  training  ops  so  they  re  easier  to  see  in  TensorBoard  layer,google
  name  final  training  ops  with  tf  name  scope  layer  name  with  tf  name  scope  weights  initial  value  tf  truncated  normal  bottleneck  tensor  size  class  count  stddev  001  layer  weights  tf  Variable  initial  value  name  final  weights  variable  summaries  layer  weights  with  tf  name  scope  biases  layer  biases  tf  Variable  tf  zeros  class  count  name  final  biases  variable  summaries  layer  biases  with  tf  name  scope  logits  logits  tf  matmul  bottleneck  input  layer  weights  layer  biases  tf  summary  histogram  pre  activations  logits  final  tensor  tf  nn  softmax  logits  name  final  tensor  name  tf  summary  histogram  activations  final  tensor  with  tf  name  scope  cross  entropy  cross  entropy  tf  nn  softmax  cross  entropy  with  logits  labels  ground  truth  input  logits  logits  with  tf  name  scope  total  cross  entropy  mean  tf  reduce  mean  cross  entropy  tf  summary  scalar  cross  entropy  cross  entropy  mean  with  tf  name  scope  t,google
rain  optimizer  tf  train  GradientDescentOptimizer  learning  rate  train  step  optimizer  minimize  cross  entropy  mean  return  train  step  cross  entropy  mean  bottleneck  input  ground  truth  input  final  tensor  print  Execution  complete  def  add  evaluation  step  result  tensor  ground  truth  tensor  Inserts  the  operations  we  need  to  evaluate  the  accuracy  of  our  results  Args  result  tensor  The  new  final  node  that  produces  results  ground  truth  tensor  The  node  we  feed  ground  truth  data  into  Returns  Tuple  of  evaluation  step  prediction  with  tf  name  scope  accuracy  with  tf  name  scope  correct  prediction  Create  prediction  tensor  containing  the  predicted  class  Remember  that  the  result  tensor  contains  probability  distribution  of  predictions  for  all  classes  for  all  examples  in  the  batch  and  the  class  with  the  highest  probability  is  considered  to  be  the  predicted  one  YOUR  CODE  HERE  raise  NotImplementedError  Cal,google
culate  all  the  correct  predictions  by  comparing  the  prediction  tnesor  with  the  ground  truth  tensor  YOUR  CODE  HERE  raise  NotImplementedError  with  tf  name  scope  accuracy  Create  an  evaluation  step  averaging  the  correct  prediction  over  all  examples  in  the  batch  YOUR  CODE  HERE  raise  NotImplementedError  tf  summary  scalar  accuracy  evaluation  step  return  evaluation  step  prediction  print  Execution  complete  def  add  evaluation  step  result  tensor  ground  truth  tensor  Inserts  the  operations  we  need  to  evaluate  the  accuracy  of  our  results  Args  result  tensor  The  new  final  node  that  produces  results  ground  truth  tensor  The  node  we  feed  ground  truth  data  into  Returns  Tuple  of  evaluation  step  prediction  with  tf  name  scope  accuracy  with  tf  name  scope  correct  prediction  prediction  tf  argmax  result  tensor  correct  prediction  tf  equal  prediction  tf  argmax  ground  truth  tensor  with  tf  name  scope  accura,google
cy  evaluation  step  tf  reduce  mean  tf  cast  correct  prediction  tf  float32  tf  summary  scalar  accuracy  evaluation  step  return  evaluation  step  prediction  print  Execution  complete  def  save  graph  to  file  sess  graph  graph  file  name  final  tensor  name  output  graph  def  graph  util  convert  variables  to  constants  sess  graph  as  graph  def  final  tensor  name  with  gfile  FastGFile  graph  file  name  wb  as  write  output  graph  def  SerializeToString  return  print  Execution  complete  from  google  datalab  ml  import  TensorBoard  tb  id  TensorBoard  start  tf  files  training  summaries  mobilenet  50  224  def  get  random  cached  bottlenecks  sess  image  lists  how  many  category  bottleneck  dir  image  dir  jpeg  data  tensor  decoded  image  tensor  resized  input  tensor  bottleneck  tensor  architecture  Retrieves  bottleneck  values  for  cached  images  directly  from  disk  It  picks  random  set  of  images  from  the  specified  category  Args  sess  ,google
Current  TensorFlow  Session  image  lists  Dictionary  of  training  images  for  each  label  how  many  If  positive  random  sample  of  this  size  will  be  chosen  If  negative  all  bottlenecks  will  be  retrieved  category  Name  string  of  which  set  to  pull  from  training  testing  or  validation  bottleneck  dir  Folder  string  holding  cached  files  of  bottleneck  values  image  dir  Root  folder  string  of  the  subfolders  containing  the  training  images  jpeg  data  tensor  The  layer  to  feed  jpeg  image  data  into  decoded  image  tensor  The  output  of  decoding  and  resizing  the  image  resized  input  tensor  The  input  node  of  the  recognition  graph  bottleneck  tensor  The  bottleneck  output  layer  of  the  CNN  graph  architecture  The  name  of  the  model  architecture  Returns  List  of  bottleneck  arrays  their  corresponding  ground  truths  and  the  relevant  filenames  class  count  len  image  lists  keys  bottlenecks  ground  truths  filenames  if  how,google
  many  Retrieve  random  sample  of  bottlenecks  for  unused  in  range  how  many  label  index  random  randrange  class  count  label  name  list  image  lists  keys  label  index  image  index  random  randrange  MAX  NUM  IMAGES  PER  CLASS  image  name  get  image  path  image  lists  label  name  image  index  image  dir  category  bottleneck  get  or  create  bottleneck  sess  image  lists  label  name  image  index  image  dir  category  bottleneck  dir  jpeg  data  tensor  decoded  image  tensor  resized  input  tensor  bottleneck  tensor  architecture  ground  truth  np  zeros  class  count  dtype  np  float32  ground  truth  label  index  bottlenecks  append  bottleneck  ground  truths  append  ground  truth  filenames  append  image  name  else  Retrieve  all  bottlenecks  for  label  index  label  name  in  enumerate  image  lists  keys  for  image  index  image  name  in  enumerate  image  lists  label  name  category  image  name  get  image  path  image  lists  label  name  image  index  im,google
age  dir  category  bottleneck  get  or  create  bottleneck  sess  image  lists  label  name  image  index  image  dir  category  bottleneck  dir  jpeg  data  tensor  decoded  image  tensor  resized  input  tensor  bottleneck  tensor  architecture  ground  truth  np  zeros  class  count  dtype  np  float32  ground  truth  label  index  bottlenecks  append  bottleneck  ground  truths  append  ground  truth  filenames  append  image  name  return  bottlenecks  ground  truths  filenames  def  retrain  architecture  mobilenet  50  224  image  dir  gs  candies  ml  dataset  v2  images  output  graph  gs  candies  ml  model  test  retrained  graph  pb  output  labels  gs  candies  ml  model  test  retrained  labels  txt  final  tensor  name  final  result  how  many  training  steps  1000  eval  step  interval  10  learning  rate  01  train  batch  size  100  test  batch  size  validation  batch  size  100  intermediate  store  frequency  intermediate  output  graphs  dir  tmp  intermediate  graph  bottleneck  dir ,google
 tmp  bottleneck  summaries  dir  tf  files  training  summaries  mobilenet  50  224  print  misclassified  test  images  True  Look  at  the  folder  structure  and  create  lists  of  all  the  images  class  count  len  image  lists  keys  with  tf  Session  graph  graph  as  sess  Add  the  new  layer  that  we  ll  be  training  train  step  cross  entropy  bottleneck  input  ground  truth  input  final  tensor  add  final  training  ops  len  image  lists  keys  final  tensor  name  bottleneck  tensor  model  info  bottleneck  tensor  size  learning  rate  Create  the  operations  we  need  to  evaluate  the  accuracy  of  our  new  layer  evaluation  step  prediction  add  evaluation  step  final  tensor  ground  truth  input  Merge  all  the  summaries  and  write  them  out  to  the  summaries  dir  merged  tf  summary  merge  all  train  writer  tf  summary  FileWriter  summaries  dir  train  sess  graph  validation  writer  tf  summary  FileWriter  summaries  dir  validation  Set  up  all  our  wei,google
ghts  to  their  initial  default  values  init  tf  global  variables  initializer  sess  run  init  Run  the  training  for  as  many  cycles  as  requested  on  the  command  line  for  in  range  how  many  training  steps  Get  batch  of  input  bottleneck  values  from  the  cache  stored  on  disk  train  bottlenecks  train  ground  truth  get  random  cached  bottlenecks  sess  image  lists  train  batch  size  training  bottleneck  dir  image  dir  jpeg  data  tensor  decoded  image  tensor  resized  image  tensor  bottleneck  tensor  architecture  Feed  the  bottlenecks  and  ground  truth  into  the  graph  and  run  training  step  Capture  training  summaries  for  TensorBoard  with  the  merged  op  train  summary  sess  run  merged  train  step  feed  dict  bottleneck  input  train  bottlenecks  ground  truth  input  train  ground  truth  train  writer  add  summary  train  summary  Every  so  often  print  out  how  well  the  graph  is  training  is  last  step  how  many  training  steps  if,google
  eval  step  interval  or  is  last  step  train  accuracy  cross  entropy  value  sess  run  evaluation  step  cross  entropy  feed  dict  bottleneck  input  train  bottlenecks  ground  truth  input  train  ground  truth  tf  logging  info  Step  Train  accuracy  1f  datetime  now  train  accuracy  100  tf  logging  info  Step  Cross  entropy  datetime  now  cross  entropy  value  validation  bottlenecks  validation  ground  truth  get  random  cached  bottlenecks  sess  image  lists  validation  batch  size  validation  bottleneck  dir  image  dir  jpeg  data  tensor  decoded  image  tensor  resized  image  tensor  bottleneck  tensor  architecture  Run  validation  step  and  capture  training  summaries  for  TensorBoard  with  the  merged  op  validation  summary  validation  accuracy  sess  run  merged  evaluation  step  feed  dict  bottleneck  input  validation  bottlenecks  ground  truth  input  validation  ground  truth  validation  writer  add  summary  validation  summary  tf  logging  info  Step  ,google
Validation  accuracy  1f  datetime  now  validation  accuracy  100  len  validation  bottlenecks  Store  intermediate  results  intermediate  frequency  intermediate  store  frequency  if  intermediate  frequency  and  intermediate  frequency  and  intermediate  file  name  intermediate  output  graphs  dir  intermediate  str  pb  tf  logging  info  Save  intermediate  result  to  intermediate  file  name  save  graph  to  file  sess  graph  intermediate  file  name  final  tensor  name  We  ve  completed  all  our  training  so  run  final  test  evaluation  on  some  new  images  we  haven  used  before  test  bottlenecks  test  ground  truth  test  filenames  get  random  cached  bottlenecks  sess  image  lists  test  batch  size  testing  bottleneck  dir  image  dir  jpeg  data  tensor  decoded  image  tensor  resized  image  tensor  bottleneck  tensor  architecture  test  accuracy  predictions  sess  run  evaluation  step  prediction  feed  dict  bottleneck  input  test  bottlenecks  ground  truth  input,google
  test  ground  truth  tf  logging  info  Final  test  accuracy  1f  test  accuracy  100  len  test  bottlenecks  if  print  misclassified  test  images  tf  logging  info  MISCLASSIFIED  TEST  IMAGES  for  test  filename  in  enumerate  test  filenames  if  predictions  test  ground  truth  argmax  tf  logging  info  70s  test  filename  list  image  lists  keys  predictions  Write  out  the  trained  graph  and  labels  with  the  weights  stored  as  constants  save  graph  to  file  sess  graph  output  graph  final  tensor  name  with  gfile  FastGFile  output  labels  as  write  join  image  lists  keys  output  graph  path  retrained  graph  pb  bucket  output  labels  path  retrained  labels  txt  bucket  retrain  architecture  architecture  image  dir  image  dir  output  graph  output  graph  path  output  labels  output  labels  path  summaries  dir  summaries  dir  intermediate  output  graphs  dir  intermediate  output  graphs  dir  intermediate  store  frequency  intermediate  store  frequency  ,google
bottleneck  dir  bottleneck  dir  print  Execution  complete  import  datetime  curr  date  format  datetime  datetime  today  train  file  gs  GCP  PROJECT  ID  image  classifier  bottlenecks  training  csv  eval  file  gs  GCP  PROJECT  ID  image  classifier  bottlenecks  validation  csv  train  steps  1000  eval  steps  100  job  name  retrain  mobilenets  candies  curr  date  job  dir  cloudml  training  bucket  job  name  gcloud  template  command  ml  engine  jobs  submit  training  stream  logs  runtime  version  job  dir  module  name  trainer  task  package  path  trainer  region  us  central1  train  files  eval  files  train  steps  eval  steps  gcloud  command  gcloud  template  command  job  name  job  dir  train  file  eval  file  train  steps  eval  steps  print  Submitting  the  Cloud  ML  Engine  job  with  the  command  gcloud  gcloud  command  gcloud  gcloud  command  print  Execution  complete  TensorBoard  stop  tb  id  print  Execution  complete  ,google
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  ntm  synthetic  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  numpy  as  np  from  generate  example  data  import  generate  griffiths  data  plot  topic  data  import  io  import  os  import  time  import  json  import  sys  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  from  IPython  display  import  display  import  scipy  import  sagemaker  import  sagemaker  amazon  common  as  smac  from  sagemaker  predictor  import  csv  serializer  json  deserializer  generate  the  sample  data  num  documents  5000  num  topics  vocabulary  size  25  known  alpha  known  beta  documents  topic  mixtures  generate  griffiths  data  num  documents  num  documents  num  topics  num  topics  vocabulary  size  vocabulary  size  separate  the  generated  data  into  training  and  tests  subsets  num  documents  training  int  num  documents  num  documents  tes,amazon
t  num  documents  num  documents  training  documents  training  documents  num  documents  training  documents  test  documents  num  documents  training  topic  mixtures  training  topic  mixtures  num  documents  training  topic  mixtures  test  topic  mixtures  num  documents  training  data  training  documents  training  np  zeros  num  documents  training  data  test  documents  test  np  zeros  num  documents  test  print  First  training  document  format  documents  print  nVocabulary  size  format  vocabulary  size  np  set  printoptions  precision  suppress  True  print  Known  topic  mixture  of  first  training  document  format  topic  mixtures  training  print  nNumber  of  topics  format  num  topics  matplotlib  inline  fig  plot  topic  data  documents  training  10  nrows  ncols  cmap  gray  with  colorbar  False  fig  suptitle  Example  Documents  fig  set  dpi  160  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  data  training  astype  float32  buf  seek  key  ntm  data  ,amazon
boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  buf  s3  train  data  s3  train  format  bucket  prefix  key  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  ntm  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  ntm  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  ntm  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  ntm  latest  sess  sagemaker  Session  ntm  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  s3  output  format  bucket  prefix  sagemaker  session  sess  ntm  set  hyperparameters  num  topics  num  topics  feature  dim  vocabulary  size  ntm  fit  train  s3  train  data  ntm  predictor  ntm  deploy  initial  instance  count  instance  type  ml  m4  xlarge  ntm  predictor  content  type  text  csv  ntm  predictor  serializer  csv  serializer  ntm  predictor  ,amazon
deserializer  json  deserializerresults  ntm  predictor  predict  documents  training  10  print  results  predictions  np  array  prediction  topic  weights  for  prediction  in  results  predictions  print  predictions  print  topic  mixtures  training  known  topic  mixture  print  predictions  computed  topic  mixturedef  predict  batches  data  rows  1000  split  array  np  array  split  data  int  data  shape  float  rows  predictions  for  array  in  split  array  results  ntm  predictor  predict  array  predictions  topic  weights  for  in  results  predictions  return  np  array  predictions  predictions  predict  batches  documents  training  data  pd  DataFrame  np  concatenate  topic  mixtures  training  predictions  axis  columns  actual  format  for  in  range  predictions  format  for  in  range  display  data  corr  pd  plotting  scatter  matrix  pd  DataFrame  np  concatenate  topic  mixtures  training  predictions  axis  figsize  12  12  plt  show  sagemaker  Session  delete  endpoint  ntm  ,amazon
predictor  endpoint  ,amazon
Basic  Code  is  taken  from  https  github  com  ckmarkoh  GAN  tensorflow  import  tensorflow  as  tf  from  tensorflow  examples  tutorials  mnist  import  input  data  import  numpy  as  np  from  skimage  io  import  imsave  import  os  import  shutilimg  height  28  img  width  28  img  size  img  height  img  width  to  train  True  to  restore  False  output  path  output  max  epoch  500  h1  size  150  h2  size  300  size  100  batch  size  256  ngf  128def  general  conv2d  inputconv  name  conv2d  64  stddev  02  padding  None  do  norm  True  do  relu  True  Defines  general  2D  convolution  layer  with  batch  normalization  with  tf  variable  scope  name  initializer  tf  truncated  normal  intializer  stddev  stddev  tf  get  variable  inputconv  get  shape  initializer  initializer  conv  tf  nn  conv2d  inputconv  filter  strides  padding  padding  biases  tf  get  variable  initializer  tf  constant  initializer  conv  tf  nn  bias  add  conv  biases  Add  batch  norm  layer  if  do  norm,amazon
  dims  conv  get  shape  scale  tf  get  variable  scale  dims  dims  dims  tf  constant  initializer  beta  tf  get  variable  beta  dims  dims  dims  tf  constant  initializer  conv  mean  conv  var  tf  nn  moments  conv  conv  tf  nn  batch  normalization  conv  conv  mean  conv  var  beta  scale  001  Add  ReLU  activation  if  do  relu  conv  tf  nn  relu  conv  return  convdef  build  resnet  block  inputres  dim  name  resnet  out  res  inputres  with  tf  variable  scope  name  out  res  general  conv2d  inputres  dim  02  SAME  c1  out  res  general  conv2d  out  res  dim  02  SAME  c2  do  relu  False  return  tf  nn  relu  out  res  inputres  def  build  generator  resnet  6blocks  inputgen  name  generator  with  tf  variable  scope  name  ks  c1  general  conv2d  inputgen  ngf  02  SAME  c1  c2  general  conv2d  c1  ngf  ks  ks  02  None  c2  c3  general  conv2d  c2  ngf  ks  ks  02  None  c3  def  show  result  batch  res  fname  grid  size  grid  pad  batch  res  batch  res  reshape  batch  r,amazon
es  shape  img  height  img  width  img  img  batch  res  shape  batch  res  shape  grid  img  grid  size  grid  pad  grid  size  grid  img  grid  size  grid  pad  grid  size  img  grid  np  zeros  grid  grid  dtype  np  uint8  for  res  in  enumerate  batch  res  if  grid  size  grid  size  break  img  res  255  img  img  astype  np  uint8  row  grid  size  img  grid  pad  col  grid  size  img  grid  pad  img  grid  row  row  img  col  col  img  img  imsave  fname  img  grid  def  train  mnist  input  data  read  data  sets  MNIST  data  one  hot  True  data  tf  placeholder  tf  float32  batch  size  img  size  name  data  prior  tf  placeholder  tf  float32  batch  size  size  name  prior  keep  prob  tf  placeholder  tf  float32  name  keep  prob  global  step  tf  Variable  name  global  step  trainable  False  generated  params  build  generator  prior  data  generated  params  build  discriminator  data  generated  keep  prob  loss  tf  log  data  tf  log  generated  loss  tf  log  generated  optimizer,amazon
  tf  train  AdamOptimizer  0001  trainer  optimizer  minimize  loss  var  list  params  trainer  optimizer  minimize  loss  var  list  params  init  tf  initialize  all  variables  saver  tf  train  Saver  sess  tf  Session  sess  run  init  if  to  restore  chkpt  fname  tf  train  latest  checkpoint  output  path  saver  restore  sess  chkpt  fname  else  if  os  path  exists  output  path  shutil  rmtree  output  path  os  mkdir  output  path  sample  val  np  random  normal  size  batch  size  size  astype  np  float32  for  in  range  sess  run  global  step  max  epoch  for  in  range  60000  batch  size  print  epoch  iter  value  mnist  train  next  batch  batch  size  value  value  astype  np  float32  value  np  random  normal  size  batch  size  size  astype  np  float32  sess  run  trainer  feed  dict  data  value  prior  value  keep  prob  np  sum  astype  np  float32  if  sess  run  trainer  feed  dict  data  value  prior  value  keep  prob  np  sum  astype  np  float32  gen  val  sess  run  ge,amazon
nerated  feed  dict  prior  sample  val  show  result  gen  val  output  sample  jpg  format  random  sample  val  np  random  normal  size  batch  size  size  astype  np  float32  gen  val  sess  run  generated  feed  dict  prior  random  sample  val  show  result  gen  val  output  random  sample  jpg  format  sess  run  tf  assign  global  step  saver  save  sess  os  path  join  output  path  model  global  step  global  step  def  test  prior  tf  placeholder  tf  float32  batch  size  size  name  prior  generated  build  generator  prior  chkpt  fname  tf  train  latest  checkpoint  output  path  init  tf  initialize  all  variables  sess  tf  Session  saver  tf  train  Saver  sess  run  init  saver  restore  sess  chkpt  fname  test  value  np  random  normal  size  batch  size  size  astype  np  float32  gen  val  sess  run  generated  feed  dict  prior  test  value  show  result  gen  val  output  test  result  jpg  if  name  main  if  to  train  train  else  test  ,amazon
pip2  install  ipyevents  upgrade  strategy  only  if  needed  user  usr  local  bin  jupyter  nbextension  enable  py  user  ipyevents  pip  install  clustergrammer  pip  install  clustergrammer  widget  jupyter  nbextension  enable  py  sys  prefix  widgetsnbextension  jupyter  nbextension  enable  py  sys  prefix  clustergrammer  widget  import  widget  classes  and  instantiate  Network  instance  from  clustergrammer  widget  import  net  Network  clustergrammer  widget  ls  home  nbuser  library  data  load  matrix  file  net  load  file  home  nbuser  library  data  rc  two  cats  txt  cluster  using  default  parameters  net  cluster  enrichrgram  True  make  interactive  widget  net  widget  import  numpy  as  np  import  pandas  as  pd  generate  random  matrix  num  rows  500  num  cols  10  np  random  seed  seed  100  mat  np  random  rand  num  rows  num  cols  make  row  and  col  labels  rows  range  num  rows  cols  range  num  cols  rows  str  for  in  rows  cols  str  for  in  cols  make  d,microsoft
ataframe  df  pd  DataFrame  data  mat  columns  cols  index  rows  net  load  df  df  net  cluster  enrichrgram  False  net  widget  ,microsoft
if  require  ndtv  install  packages  ndtv  library  ndtv  library  ndtv  load  the  package  data  short  stergm  sim  load  an  example  dynamic  network  dataset  render  d3movie  short  stergm  sim  filename  library  output  html  Test  NDTV  html  verbose  FALSE  create  HTML5  animation  render  interactive  widget  in  rmarkdown  or  RStudio  plot  window  render  d3movie  short  stergm  sim  output  mode  htmlWidget  verbose  FALSE  d3  options  list  playControls  TRUE  animateOnLoad  TRUE  slider  TRUE  system  dir  tmp  RtmpcXIViV  intern  TRUE  system  cp  tmp  RtmpcXIViV  html  library  output  html  intern  TRUE  system  dir  library  output  html  intern  TRUE  system  rm  library  output  html  html  intern  TRUE  if  require  IRdisplay  install  packages  IRdisplay  library  IRdisplay  htmlfile  library  output  html  Test  NDTV  html  display  html  file  htmlfile  htmlstring  DOCTYPE  html  html  body  h3  My  First  Heading  h3  My  first  paragraph  body  html  display  html  data  htmls,microsoft
tring  file  tmp  Rtmp55r7DV  fileee77b56a6  html  display  javascript  execute  this  display  markdown  MD  http  commonmark  org  formatted  ,microsoft
import  numpy  as  np  import  matplotlib  pylab  as  plt  Convert  x1  x2  x3  x4  to  bool  bool  bool  Convert  bool  bool  to  def  step  function  return  astype  np  int  np  arange  step  function  plt  plot  plt  ylim  plt  show  Convert  x1  x2  x3  x4  to  All  results  are  between  and  def  sigmoid  return  np  exp  np  arange  sigmoid  plt  plot  plt  ylim  plt  show  def  relu  return  np  maximum  np  arange  relu  plt  plot  plt  ylim  plt  show  def  identify  function  return  xdef  softmax  np  max  exp  np  exp  sum  exp  np  sum  exp  exp  sum  exp  return  softmax  def  softmax  if  ndim  np  max  axis  axis  np  exp  np  sum  np  exp  axis  return  np  max  return  np  exp  np  sum  np  exp  ,amazon
Basic  set  up  to  define  IAM  Role  import  boto3  import  re  import  os  import  numpy  as  np  import  pandas  as  pd  from  sagemaker  import  get  execution  role  role  get  execution  role  Create  SageMaker  Session  that  will  be  used  to  perform  all  SageMaker  operations  import  sagemaker  as  sage  from  time  import  gmtime  strftime  sess  sage  Session  account  sess  boto  session  client  sts  get  caller  identity  Account  region  sess  boto  session  region  name  Note  make  sure  to  include  the  Docker  image  tag  eg  latest  since  there  seem  to  be  some  issues  with  deploying  model  if  you  don  include  the  tag  image  dkr  ecr  amazonaws  com  npng  sagemaker  repo  latest  format  account  region  sagemaker  ml  sage  estimator  Estimator  image  role  ml  c4  2xlarge  output  path  s3  output  format  sess  default  bucket  sagemaker  session  sess  The  current  Docker  image  needs  both  training  and  testing  data  so  they  re  specified  in  two  different,amazon
  channels  It  assumed  that  the  user  has  already  uploaded  the  required  data  into  couple  of  different  directories  and  this  dictionary  just  specifies  where  the  training  and  testing  data  are  respectively  data  location  training  s3  h2o  sagemaker  npng  s3titanic  train  csv  testing  s3  h2o  sagemaker  npng  s3titanictest  csv  sagemaker  ml  fit  data  location  Deploying  an  actual  predictor  so  that  we  can  make  predictions  on  test  data  here  from  sagemaker  predictor  import  csv  serializer  predictor  sagemaker  ml  deploy  ml  m4  xlarge  serializer  csv  serializer  import  io  s3  boto3  client  s3  obj  s3  get  object  Bucket  h2o  sagemaker  npng  Key  titanic  test  csv  df  pd  read  csv  io  BytesIO  obj  Body  read  df  df  drop  PassengerId  axis  np  array  df  columns  reshape  10  test  vals  df  values  valid  np  append  test  vals  axis  preds  predictor  predict  valid  decode  utf  preds  list  preds  split  full  preds  one  row  for  item  in,amazon
  preds  list  if  in  item  rmloc  item  find  item  item  rmloc  one  row  append  item  full  preds  append  one  row  one  row  else  one  row  append  item  full  preds  only  run  for  cleanup  deletes  the  endpoint  for  the  predictor  sess  delete  endpoint  predictor  endpoint  ,amazon
Some  required  imports  import  ibm  boto3  from  ibm  botocore  client  import  Config  import  os  import  json  import  warnings  import  time  Copy  and  paste  your  Cloud  Object  Storage  credentials  here  Start  COS  credentials  cos  credentials  End  COS  credentials  api  key  cos  credentials  apikey  service  instance  id  cos  credentials  resource  instance  id  auth  endpoint  https  iam  bluemix  net  oidc  token  service  endpoint  https  s3  api  us  geo  objectstorage  softlayer  net  cos  ibm  boto3  resource  s3  ibm  api  key  id  api  key  ibm  service  instance  id  service  instance  id  ibm  auth  endpoint  auth  endpoint  config  Config  signature  version  oauth  endpoint  url  service  endpoint  buckets  oilprice  rnn  data  oilprice  rnn  results  for  bucket  in  buckets  if  not  cos  Bucket  bucket  in  cos  buckets  all  print  Creating  bucket  format  bucket  try  cos  create  bucket  Bucket  bucket  except  ibm  boto3  exceptions  ibm  botocore  client  ClientError  as ,ibm
 print  Error  format  response  Error  Message  Now  we  should  have  our  buckets  created  print  list  cos  buckets  all  pip  install  wget  import  wget  os  link  https  raw  githubusercontent  com  djccarew  timeseries  rnn  lab  part1  master  data  WCOILWTICO  csv  data  dir  OILPRICE  RNN  DATA  if  not  os  path  isdir  data  dir  os  mkdir  data  dir  if  not  os  path  isfile  os  path  join  data  dir  os  path  join  link  split  wget  download  link  out  data  dir  data  file  path  os  path  join  data  dir  os  path  join  link  split  ls  OILPRICE  RNN  DATA  bucket  name  buckets  bucket  obj  cos  Bucket  bucket  name  for  filename  in  os  listdir  data  dir  with  open  os  path  join  data  dir  filename  rb  as  data  bucket  obj  upload  file  os  path  join  data  dir  filename  filename  print  is  uploaded  format  filename  for  obj  in  bucket  obj  objects  all  print  Object  key  format  obj  key  print  Object  size  kb  format  obj  size  1024  Required  imports  import,ibm
  urllib3  requests  json  base64  time  os  warnings  filterwarnings  ignore  Copy  and  paste  your  Cloud  Object  Storage  credentials  here  Start  WML  service  credentials  wml  credentials  End  WML  service  credentials  pip  install  upgrade  watson  machine  learning  clientfrom  watson  machine  learning  client  import  WatsonMachineLearningAPIClient  client  WatsonMachineLearningAPIClient  wml  credentials  print  client  version  model  definition  metadata  client  repository  DefinitionMetaNames  NAME  OILPRICE  RNN  client  repository  DefinitionMetaNames  FRAMEWORK  NAME  tensorflow  client  repository  DefinitionMetaNames  FRAMEWORK  VERSION  client  repository  DefinitionMetaNames  RUNTIME  NAME  python  client  repository  DefinitionMetaNames  RUNTIME  VERSION  client  repository  DefinitionMetaNames  EXECUTION  COMMAND  python3  oilprice  rnn  py  model  filename  oilprice  rnnV2  zip  if  os  path  isfile  model  filename  ls  oilprice  rnnV2  zip  else  wget  https  github  com  djcca,ibm
rew  timeseries  rnn  lab  part2  raw  master  model  source  oilprice  rnnV2  zip  ls  oilprice  rnnV2  zip  definition  details  client  repository  store  definition  model  filename  model  definition  metadata  definition  url  client  repository  get  definition  url  definition  details  definition  uid  client  repository  get  definition  uid  definition  details  print  definition  url  TRAINING  DATA  REFERENCE  connection  endpoint  url  service  endpoint  aws  access  key  id  cos  credentials  cos  hmac  keys  access  key  id  aws  secret  access  key  cos  credentials  cos  hmac  keys  secret  access  key  source  bucket  buckets  type  s3  TRAINING  RESULTS  REFERENCE  connection  endpoint  url  service  endpoint  aws  access  key  id  cos  credentials  cos  hmac  keys  access  key  id  aws  secret  access  key  cos  credentials  cos  hmac  keys  secret  access  key  target  bucket  buckets  type  s3  HPO  method  name  random  parameters  client  experiments  HPOMethodParam  objective  val  l,ibm
oss  client  experiments  HPOMethodParam  maximize  or  minimize  minimize  client  experiments  HPOMethodParam  num  optimizer  steps  hyper  parameters  client  experiments  HPOParameter  dropout  rate  min  max  step  client  experiments  HPOParameter  prev  periods  min  max  step  experiment  metadata  client  repository  ExperimentMetaNames  NAME  Oil  Price  RNN  Experiment  client  repository  ExperimentMetaNames  DESCRIPTION  Best  model  for  RNN  oil  price  forecaster  client  repository  ExperimentMetaNames  AUTHOR  EMAIL  yourname  youremail  com  client  repository  ExperimentMetaNames  EVALUATION  METRICS  mae  client  repository  ExperimentMetaNames  TRAINING  DATA  REFERENCE  TRAINING  DATA  REFERENCE  client  repository  ExperimentMetaNames  TRAINING  RESULTS  REFERENCE  TRAINING  RESULTS  REFERENCE  client  repository  ExperimentMetaNames  TRAINING  REFERENCES  name  OILPRICE  RNN  training  definition  url  definition  url  compute  configuration  name  k80x2  hyper  parameters  optimizat,ibm
ion  HPO  experiment  details  client  repository  store  experiment  meta  props  experiment  metadata  experiment  uid  client  repository  get  experiment  uid  experiment  details  print  experiment  uid  experiment  run  details  client  experiments  run  experiment  uid  asynchronous  True  experiment  run  uid  client  experiments  get  run  uid  experiment  run  details  print  experiment  run  uid  Keep  running  this  cell  periodically  until  all  the  training  runs  are  in  the  COMPLETED  state  as  illustrated  above  client  experiments  list  training  runs  experiment  run  uid  experiment  run  details  client  experiments  get  run  details  experiment  run  uid  import  pandas  as  pd  rows  list  for  in  experiment  run  details  entity  training  statuses  if  len  metrics  for  in  metrics  if  phase  test  last  metric  break  for  in  hyper  parameters  if  name  dropout  rate  dropout  rate  double  value  else  prev  periods  int  value  for  in  last  metric  values  if  name  ,ibm
val  loss  val  loss  value  else  val  mae  value  one  row  training  guid  last  metric  phase  val  mae  val  loss  dropout  rate  prev  periods  rows  list  append  one  row  metrics  df  pd  DataFrame  rows  list  columns  GUID  PHASE  MAE  VAL  LOSS  DROPOUT  PREV  PERIODS  metrics  dfbest  run  df  metrics  df  nsmallest  VAL  LOSS  best  run  dfbest  model  guid  best  run  df  GUID  iloc  best  prev  periods  best  run  df  PREV  PERIODS  iloc  print  Best  run  GUID  str  best  model  guid  saved  model  details  client  repository  store  model  best  model  guid  name  Oil  price  RNN  best  model  model  guid  client  repository  get  model  uid  saved  model  details  print  Saved  model  guid  model  guid  deployment  details  client  deployments  create  name  Oil  price  RNN  deployment  model  uid  model  guid  scoring  url  client  deployments  get  scoring  url  deployment  details  print  scoring  url  import  numpy  as  np  prices  df  pd  read  csv  data  file  path  index  col  DATE  ,ibm
last  prices  prices  df  tail  best  prev  periods  WCOILWTICO  values  reshape  astype  float32  last  prices  np  reshape  last  prices  best  prev  periods  scoring  data  values  last  prices  tolist  predictions  client  deployments  score  scoring  url  scoring  data  print  Scoring  result  str  predictions  ,ibm
import  torch  import  torch  utils  import  sklearn  datasets  import  os  import  numpy  as  npdata  sklearn  datasets  load  svmlight  file  part  00000  bda45ecd  9518  4fb7  9339  694ca61fbbca  c000  libsvm  np  concatenate  np  array  axis  class  MNISTDataset  torch  utils  data  Dataset  Simple  MNIST  Dataset  def  init  self  training  dir  files  os  listdir  training  dir  print  files  svms  sklearn  datasets  load  svmlight  file  os  path  join  training  dir  for  in  files  data  np  concatenate  todense  for  in  svms  axis  targets  np  concatenate  for  in  svms  axis  self  data  torch  FloatTensor  data  self  targets  torch  LongTensor  targets  def  getitem  self  index  img  target  self  data  index  reshape  28  28  self  targets  index  return  img  target  def  len  self  return  len  self  data  temp  dataset  MNISTDataset  temp  train  temp  dataset  getitem  shapefrom  torchvision  import  datasets  transformsdataset  datasets  MNIST  download  True  train  True  transform  tra,amazon
nsforms  Compose  transforms  ToTensor  transforms  Normalize  1307  3081  dataset  getitem  shapedataset  getitem  shapetorch  Tensor  torch  LongTensor  torch  FloatTensor  dtypedataset  train  labelsdef  model  fn  model  dir  device  torch  device  cuda  if  torch  cuda  is  available  else  cpu  model  torch  nn  DataParallel  Net  with  open  os  path  join  model  dir  model  pth  rb  as  model  load  state  dict  torch  load  return  model  to  device  import  torch  nn  as  nn  import  torch  nn  functional  as  import  torch  optim  as  optim  import  torch  utils  dataclass  Net  nn  Module  def  init  self  super  Net  self  init  self  conv1  nn  Conv2d  10  kernel  size  self  conv2  nn  Conv2d  10  20  kernel  size  self  conv2  drop  nn  Dropout2d  self  fc1  nn  Linear  320  50  self  fc2  nn  Linear  50  10  def  forward  self  relu  max  pool2d  self  conv1  relu  max  pool2d  self  conv2  drop  self  conv2  view  320  relu  self  fc1  dropout  training  self  training  self  fc2  return  l,amazon
og  softmax  dim  model  fn  mdef  predict  fn  input  data  model  device  torch  device  cuda  if  torch  cuda  is  available  else  cpu  model  to  device  model  eval  with  torch  no  grad  return  model  input  data  to  device  def  get  train  data  loader  batch  size  training  dir  is  distributed  kwargs  dataset  MNISTDataset  training  dir  return  torch  utils  data  DataLoader  dataset  batch  size  batch  size  kwargs  for  batch  idx  data  target  in  enumerate  get  train  data  loader  testing  temp  train  False  print  predict  fn  data  break  ,amazon
pip  install  pysparkfrom  pyspark  import  SparkConf  SparkContext  from  pyspark  sql  import  SQLContext  import  pyspark  sql  import  pandas  as  pd  mtcars  pd  read  csv  https  ibm  box  com  shared  static  f1dhhjnzjwxmy2c1ys2whvrgz05d1pui  csv  mtcars  head  conf  SparkConf  setAppName  Spark  SQL  sc  SparkContext  conf  conf  sqlContext  SQLContext  sc  sdf  sqlContext  createDataFrame  mtcars  sdf  printSchema  sdf  show  10  sdf  select  mpg  show  sdf  filter  sdf  mpg  18  show  sdf  groupby  cyl  agg  wt  AVG  show  sdf  groupby  cyl  agg  wt  SUM  show  car  counts  sdf  groupby  cyl  agg  wt  count  sort  count  wt  ascending  False  show  sdf  withColumn  wtTon  sdf  wt  45  select  car  wt  wtTon  show  sdf  registerTempTable  cars  highgearcars  sqlContext  sql  SELECT  car  gear  FROM  cars  WHERE  gear  highgearcars  show  ,ibm
import  tensorflow  as  tf  import  os  import  time  os  environ  TF  CPP  MIN  LOG  LEVEL  print  tensorflow  Version  is  str  tf  version  import  numpy  as  np  os  environ  KERAS  BACKEND  tensorflow  from  keras  import  backend  as  print  os  environ  KERAS  BACKEND  Fashion  MNIST  Dataset  CNN  model  development  https  github  com  zalandoresearch  fashion  mnist  from  keras  datasets  import  fashion  mnist  from  keras  models  import  Sequential  from  keras  layers  import  Dense  Dropout  Flatten  from  keras  layers  import  Conv2D  MaxPooling2D  from  keras  import  utils  losses  optimizers  import  matplotlib  pyplot  as  plt  no  of  classes  num  classes  10  batch  size  and  training  iterations  epochs  batch  size  128  epochs  24  input  image  dimensions  img  rows  img  cols  28  28  data  for  train  and  testing  train  train  test  test  fashion  mnist  load  data  print  train  shape  train  set  print  test  shape  test  set  Define  the  text  labels  fashion  mnist  labe,microsoft
ls  Top  index  Trouser  index  Jumper  index  Dress  index  Coat  index  Sandal  index  Shirt  index  Trainer  index  Bag  index  Ankle  boot  index  img  index  90  label  index  train  img  index  plt  imshow  train  img  index  print  Label  Index  str  label  index  Fashion  Labels  fashion  mnist  labels  label  index  type  convert  and  scale  the  test  and  training  data  train  train  astype  float32  test  test  astype  float32  train  255  test  255  one  hot  encoding  train  utils  to  categorical  train  num  classes  test  utils  to  categorical  test  num  classes  formatting  issues  for  depth  of  image  greyscale  with  different  kernels  tensorflow  cntk  etc  if  image  data  format  channels  first  train  train  reshape  train  shape  img  rows  img  cols  test  test  reshape  test  shape  img  rows  img  cols  input  shape  img  rows  img  cols  else  train  train  reshape  train  shape  img  rows  img  cols  test  test  reshape  test  shape  img  rows  img  cols  input  shape  im,microsoft
g  rows  img  cols  Define  the  CNN  model  model  Sequential  model  add  Conv2D  64  kernel  size  activation  relu  input  shape  input  shape  model  add  MaxPooling2D  pool  size  model  add  Conv2D  64  kernel  size  activation  relu  model  add  MaxPooling2D  pool  size  model  add  Flatten  model  add  Dense  128  activation  relu  model  add  Dropout  model  add  Dense  num  classes  activation  softmax  model  summary  compile  how  to  measure  loss  model  compile  loss  losses  categorical  crossentropy  optimizer  optimizers  Adam  metrics  accuracy  train  the  model  and  return  loss  and  accuracy  for  each  epoch  history  dictionary  start  time  time  hist  model  fit  train  train  batch  size  batch  size  epochs  epochs  verbose  validation  data  test  test  end  time  time  evaluate  the  model  on  the  test  data  score  model  evaluate  test  test  verbose  print  Test  Loss  score  print  Test  Accuracy  score  print  Time  to  run  end  start  epoch  list  list  range  len  hi,microsoft
st  history  acc  plt  plot  epoch  list  hist  history  acc  epoch  list  hist  history  val  acc  plt  legend  Training  Accuracy  Validation  Accuracy  plt  show  predictions  model  predict  test  Plot  random  sample  of  10  test  images  their  predicted  labels  and  ground  truth  figure  plt  figure  figsize  20  for  index  in  enumerate  np  random  choice  test  shape  size  15  replace  False  ax  figure  add  subplot  xticks  yticks  Display  each  image  ax  imshow  np  squeeze  test  index  predict  index  np  argmax  predictions  index  true  index  np  argmax  test  index  Set  the  title  for  each  image  ax  set  title  format  fashion  mnist  labels  predict  index  fashion  mnist  labels  true  index  color  green  if  predict  index  true  index  else  red  ,microsoft
import  time  import  numpy  as  np  np  random  seed  import  pandas  as  pd  import  json  import  matplotlib  pyplot  as  plt  conda  install  s3fsimport  boto3  import  s3fs  import  sagemaker  from  sagemaker  import  get  execution  rolebucket  sagemaker  ap  northeast  handson  YYYYMMDD  XX  prefix  sagemaker  DEMO  deepar  sagemaker  session  sagemaker  Session  role  get  execution  role  boto3  resource  s3  create  bucket  Bucket  bucket  CreateBucketConfiguration  LocationConstraint  ap  northeast  s3  data  path  data  format  bucket  prefix  s3  output  path  output  format  bucket  prefix  containers  us  east  522234722520  dkr  ecr  us  east  amazonaws  com  forecasting  deepar  latest  us  east  566113047672  dkr  ecr  us  east  amazonaws  com  forecasting  deepar  latest  us  west  156387875391  dkr  ecr  us  west  amazonaws  com  forecasting  deepar  latest  eu  west  224300973850  dkr  ecr  eu  west  amazonaws  com  forecasting  deepar  latest  ap  northeast  633353088612  dkr  ecr  ap  n,amazon
ortheast  amazonaws  com  forecasting  deepar  latest  image  name  containers  boto3  Session  region  name  mkdir  data  cd  data  wget  https  archive  ics  uci  edu  ml  machine  learning  databases  00381  PRSA  data  2010  2014  12  31  csvimport  pandas  as  pd  from  IPython  display  import  display  data  pd  read  csv  data  PRSA  data  2010  2014  12  31  csv  display  data  head  from  datetime  import  datetime  data  time  data  apply  lambda  datetime  year  month  day  hour  axis  data  set  index  time  inplace  True  Categoriacal  variable  is  converted  into  dummy  variables  cbwd  dummy  pd  get  dummies  data  cbwd  data  pd  concat  data  cbwd  dummy  axis  Unnecessary  variables  are  dropped  here  data  drop  columns  No  cbwd  year  month  day  hour  inplace  True  Rows  including  are  dropped  data  dropna  inplace  True  Linear  interploation  data  data  resample  interpolate  method  linear  print  Rows  Columns  format  len  data  index  len  data  columns  display  data  he,amazon
ad  train  data  data  data  index  year  2014  test  data  data  data  index  year  2014  import  matplotlib  pyplot  as  plt  plt  plot  train  data  index  100  train  data  values  100  label  pm2  plt  legend  plt  show  def  series  to  obj  ts  cat  obj  start  str  ts  index  target  list  ts  obj  cat  cat  return  obj  def  series  to  jsonline  ts  cat  None  return  json  dumps  series  to  obj  ts  cat  encoding  utf  s3filesystem  s3fs  S3FileSystem  with  s3filesystem  open  s3  data  path  train  train  json  wb  as  fp  for  in  range  len  train  data  columns  ts  train  data  iloc  fp  write  series  to  jsonline  ts  encode  encoding  fp  write  encode  encoding  with  s3filesystem  open  s3  data  path  test  test  json  wb  as  fp  for  in  range  len  test  data  columns  ts  test  data  iloc  fp  write  series  to  jsonline  ts  encode  encoding  fp  write  encode  encoding  estimator  sagemaker  estimator  Estimator  sagemaker  session  sagemaker  session  image  name  image  name  r,amazon
ole  role  train  instance  count  train  instance  type  ml  c4  xlarge  base  job  name  DEMO  deepar  output  path  s3  s3  output  path  hyperparameters  time  freq  context  length  72  prediction  length  12  num  cells  40  num  layers  likelihood  gaussian  epochs  20  mini  batch  size  32  learning  rate  001  dropout  rate  05  early  stopping  patience  10  embedding  dimension  10  cardinality  len  train  data  columns  estimator  set  hyperparameters  hyperparameters  data  channels  train  s3  train  format  s3  data  path  test  s3  test  format  s3  data  path  estimator  fit  inputs  data  channels  job  name  estimator  lat  est  training  job  name  endpoint  name  sagemaker  session  endpoint  from  job  job  name  job  name  initial  instance  count  instance  type  ml  m4  xlarge  deployment  image  image  name  role  role  class  DeepARPredictor  sagemaker  predictor  RealTimePredictor  def  set  prediction  parameters  self  freq  prediction  length  Set  the  time  frequency  and  p,amazon
rediction  length  parameters  This  method  must  be  called  before  being  able  to  use  predict  Parameters  freq  string  indicating  the  time  frequency  prediction  length  integer  number  of  predicted  time  points  Return  value  none  self  freq  freq  self  prediction  length  prediction  length  def  predict  self  ts  cat  None  encoding  utf  num  samples  100  quantiles  Requests  the  prediction  of  for  the  time  series  listed  in  ts  each  with  the  optional  corresponding  category  listed  in  cat  Parameters  ts  list  of  pandas  Series  objects  the  time  series  to  predict  cat  list  of  integers  default  None  encoding  string  encoding  to  use  for  the  request  default  utf  num  samples  integer  number  of  samples  to  compute  at  prediction  time  default  100  quantiles  list  of  strings  specifying  the  quantiles  to  compute  default  Return  value  list  of  pandas  DataFrame  objects  each  containing  the  predictions  prediction  times  ts  iloc  index  ,amazon
for  in  range  len  ts  columns  req  self  encode  request  ts  cat  encoding  num  samples  quantiles  res  super  DeepARPredictor  self  predict  req  return  self  decode  response  res  prediction  times  encoding  def  encode  request  self  ts  cat  encoding  num  samples  quantiles  instances  series  to  obj  ts  iloc  for  in  range  len  ts  columns  configuration  num  samples  num  samples  output  types  quantiles  quantiles  quantiles  http  request  data  instances  instances  configuration  configuration  return  json  dumps  http  request  data  encode  encoding  def  decode  response  self  response  prediction  times  encoding  response  data  json  loads  response  decode  encoding  list  of  df  for  in  range  len  prediction  times  prediction  index  pd  DatetimeIndex  start  prediction  times  freq  self  freq  periods  self  prediction  length  list  of  df  append  pd  DataFrame  data  response  data  predictions  quantiles  index  prediction  index  return  list  of  dfpredictor ,amazon
 DeepARPredictor  endpoint  endpoint  name  sagemaker  session  sagemaker  session  content  type  application  json  freq  predictor  set  prediction  parameters  freq  prediction  length  12  2013  2014  print  train  data  list  of  df  predictor  predict  train  data  print  list  of  df  predict  list  of  df  predict2014  predict  predict  index  year  2014  actual2014  test  data  loc  predict2014  index  plt  figure  figsize  12  p10  predict2014  p90  predict2014  plt  fill  between  p10  index  p10  p90  color  alpha  label  80  confidence  interval  plt  plot  p10  index  predict2014  label  prediction  median  plt  plot  p10  index  actual2014  pm2  label  ground  truth  plt  legend  plt  show  start  time  end  time  24  one  step  predict  pd  DataFrame  one  step  time  for  in  range  start  time  end  time  print  prediction  done  format  start  time  end  time  start  time  end  list  of  df  predictor  predict  test  data  iloc  72  one  step  predict  pd  concat  one  step  predict  list ,amazon
 of  df  iloc  one  step  predict2014  one  step  predict  one  step  actual2014  test  data  loc  one  step  predict  index  plt  figure  figsize  12  p10  one  step  predict2014  p90  one  step  predict2014  plt  fill  between  p10  index  p10  p90  color  alpha  label  80  confidence  interval  plt  plot  p10  index  one  step  predict2014  label  prediction  median  plt  plot  p10  index  one  step  actual2014  pm2  label  ground  truth  plt  legend  plt  show  sagemaker  session  delete  endpoint  endpoint  name  ,amazon
import  os  import  sagemaker  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  import  utils  utils  cifar10  download  inputs  sagemaker  session  upload  data  path  tmp  cifar10  data  key  prefix  data  DEMO  cifar10  from  sagemaker  tensorflow  import  TensorFlow  source  dir  os  path  join  os  getcwd  source  dir  estimator  TensorFlow  entry  point  resnet  cifar  10  py  source  dir  source  dir  role  role  hyperparameters  min  eval  frequency  10  training  steps  1000  evaluation  steps  100  train  instance  count  train  instance  type  ml  c4  xlarge  base  job  name  tensorboard  example  estimator  fit  inputs  run  tensorboard  locally  True  predictor  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
import  os  import  sagemaker  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  import  utils  from  tensorflow  contrib  learn  python  learn  datasets  import  mnist  import  tensorflow  as  tf  data  sets  mnist  read  data  sets  data  dtype  tf  uint8  reshape  False  validation  size  5000  utils  convert  to  data  sets  train  train  data  utils  convert  to  data  sets  validation  validation  data  utils  convert  to  data  sets  test  test  data  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  DEMO  mnist  cat  mnist  py  from  sagemaker  tensorflow  import  TensorFlow  mnist  estimator  TensorFlow  entry  point  mnist  py  role  role  training  steps  1000  evaluation  steps  100  train  instance  count  train  instance  type  ml  c4  xlarge  mnist  estimator  fit  inputs  mnist  predictor  mnist  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  import  numpy  as  np  from  tensor,amazon
flow  examples  tutorials  mnist  import  input  data  mnist  input  data  read  data  sets  tmp  data  one  hot  True  for  in  range  10  data  mnist  test  images  tolist  tensor  proto  tf  make  tensor  proto  values  np  asarray  data  shape  len  data  dtype  tf  float32  predict  response  mnist  predictor  predict  tensor  proto  print  label  np  argmax  mnist  test  labels  print  label  is  format  label  prediction  predict  response  outputs  classes  int64Val  print  prediction  is  format  prediction  sagemaker  Session  delete  endpoint  mnist  predictor  endpoint  ,amazon
import  json  Connection  String  ev  namespace  yoichika  eventhub01  ev  name  tweetshub  ev  sas  key  name  managepolicy  ev  sas  key  val  Lg4mzv2qIvnZ  xlam6Gbq  7Whf8ZPMqQN6fRjbqzIhc  conn  string  Endpoint  sb  servicebus  windows  net  EntityPath  SharedAccessKeyName  SharedAccessKey  format  ev  namespace  ev  name  ev  sas  key  name  ev  sas  key  val  ehConf  ehConf  eventhubs  connectionString  conn  string  ehConf  eventhubs  maxEventsPerTrigger  ehConf  eventhubs  consumerGroup  Default  Start  from  beginning  of  stream  startOffset  Create  the  positions  startingEventPosition  offset  startOffset  seqNo  not  in  use  enqueuedTime  None  not  in  use  isInclusive  True  Put  the  positions  into  the  Event  Hub  config  dictionary  ehConf  eventhubs  startingPosition  json  dumps  startingEventPosition  Creating  an  Event  Hubs  Source  for  Streaming  Queries  df  spark  readStream  format  eventhubs  options  ehConf  load  readInStreamBody  df  withColumn  body  df  body  cast  strin,microsoft
g  display  readInStreamBody  Print  Schema  df  printSchema  ,microsoft
import  tensorflow  as  tf  tf  set  random  seed  777  for  reproducibilityx  data  data  Try  to  find  values  for  and  to  compute  data  data  We  know  that  should  be  and  should  be  But  let  use  TensorFlow  to  figure  it  out  tf  Variable  tf  random  normal  name  weight  tf  placeholder  tf  float32  tf  placeholder  tf  float32  Our  hypothesis  for  linear  model  hypothesis  cost  loss  function  cost  tf  reduce  mean  tf  square  hypothesis  Minimize  Gradient  Descent  using  derivative  learning  rate  derivative  learning  rate  gradient  tf  reduce  mean  descent  learning  rate  gradient  update  assign  descent  Launch  the  graph  in  session  sess  tf  Session  Initializes  global  variables  in  the  graph  sess  run  tf  global  variables  initializer  for  step  in  range  21  sess  run  update  feed  dict  data  data  print  step  sess  run  cost  feed  dict  data  data  sess  run  ,ibm
matplotlib  inline  import  os  import  numpy  as  np  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  from  sklearn  datasets  import  load  boston  import  sklearn  model  selectionimport  sagemaker  from  sagemaker  import  get  execution  role  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  from  sagemaker  predictor  import  csv  serializer  This  is  an  object  that  represents  the  SageMaker  session  that  we  are  currently  operating  in  This  object  contains  some  useful  information  that  we  will  need  to  access  later  such  as  our  region  session  sagemaker  Session  This  is  an  object  that  represents  the  IAM  role  that  we  are  currently  assigned  When  we  construct  and  launch  the  training  job  later  we  will  need  to  tell  it  what  IAM  role  it  should  have  Since  our  use  case  is  relatively  simple  we  will  simply  assign  the  training  job  the  role  we  currently  have  role  get  execution  role  boston  load  bosto,amazon
n  First  we  package  up  the  input  data  and  the  target  variable  the  median  value  as  pandas  dataframes  This  will  make  saving  the  data  to  file  little  easier  later  on  bos  pd  pd  DataFrame  boston  data  columns  boston  feature  names  bos  pd  pd  DataFrame  boston  target  We  split  the  dataset  into  training  and  testing  sets  train  test  train  test  sklearn  model  selection  train  test  split  bos  pd  bos  pd  test  size  33  Then  we  split  the  training  set  further  into  training  and  validation  sets  train  val  train  val  sklearn  model  selection  train  test  split  train  train  test  size  33  This  is  our  local  data  directory  We  need  to  make  sure  that  it  exists  data  dir  data  boston  if  not  os  path  exists  data  dir  os  makedirs  data  dir  We  use  pandas  to  save  our  test  train  and  validation  data  to  csv  files  Note  that  we  make  sure  not  to  include  header  information  or  an  index  as  this  is  required  by  the,amazon
  built  in  algorithms  provided  by  Amazon  Also  for  the  train  and  validation  data  it  is  assumed  that  the  first  entry  in  each  row  is  the  target  variable  test  to  csv  os  path  join  data  dir  test  csv  header  False  index  False  pd  concat  val  val  axis  to  csv  os  path  join  data  dir  validation  csv  header  False  index  False  pd  concat  train  train  axis  to  csv  os  path  join  data  dir  train  csv  header  False  index  False  prefix  boston  xgboost  HL  test  location  session  upload  data  os  path  join  data  dir  test  csv  key  prefix  prefix  val  location  session  upload  data  os  path  join  data  dir  validation  csv  key  prefix  prefix  train  location  session  upload  data  os  path  join  data  dir  train  csv  key  prefix  prefix  As  stated  above  we  use  this  utility  method  to  construct  the  image  name  for  the  training  container  container  get  image  uri  session  boto  region  name  xgboost  Now  that  we  know  which  contain,amazon
er  to  use  we  can  construct  the  estimator  object  xgb  sagemaker  estimator  Estimator  container  The  name  of  the  training  container  role  The  IAM  role  to  use  our  current  role  in  this  case  train  instance  count  The  number  of  instances  to  use  for  training  train  instance  type  ml  m4  xlarge  The  type  of  instance  ot  use  for  training  output  path  s3  output  format  session  default  bucket  prefix  Where  to  save  the  output  the  model  artifacts  sagemaker  session  session  The  current  SageMaker  sessionxgb  set  hyperparameters  max  depth  eta  gamma  min  child  weight  subsample  objective  reg  linear  early  stopping  rounds  10  num  round  200  from  sagemaker  tuner  import  IntegerParameter  ContinuousParameter  HyperparameterTuner  xgb  hyperparameter  tuner  HyperparameterTuner  estimator  xgb  The  estimator  object  to  use  as  the  basis  for  the  training  jobs  objective  metric  name  validation  rmse  The  metric  used  to  compare  train,amazon
ed  models  objective  type  Minimize  Whether  we  wish  to  minimize  or  maximize  the  metric  max  jobs  20  The  total  number  of  models  to  train  max  parallel  jobs  The  number  of  models  to  train  in  parallel  hyperparameter  ranges  max  depth  IntegerParameter  12  eta  ContinuousParameter  05  min  child  weight  IntegerParameter  subsample  ContinuousParameter  gamma  ContinuousParameter  10  s3  input  train  sagemaker  s3  input  s3  data  train  location  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  val  location  content  type  csv  xgb  hyperparameter  tuner  fit  train  s3  input  train  validation  s3  input  validation  xgb  hyperparameter  tuner  wait  xgb  hyperparameter  tuner  best  training  job  xgb  attached  sagemaker  estimator  Estimator  attach  xgb  hyperparameter  tuner  best  training  job  xgb  transformer  xgb  attached  transformer  instance  count  instance  type  ml  m4  xlarge  xgb  transformer  transform  test  location  content ,amazon
 type  text  csv  split  type  Line  xgb  transformer  wait  aws  s3  cp  recursive  xgb  transformer  output  path  data  dirY  pred  pd  read  csv  os  path  join  data  dir  test  csv  out  header  None  plt  scatter  test  pred  plt  xlabel  Median  Price  plt  ylabel  Predicted  Price  plt  title  Median  Price  vs  Predicted  Price  First  we  will  remove  all  of  the  files  contained  in  the  data  dir  directory  rm  data  dir  And  then  we  delete  the  directory  itself  rmdir  data  dir  ,amazon
import  os  import  io  import  re  import  boto3  import  pandas  as  pd  import  numpy  as  np  import  time  from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  S3  bucket  prefix  sagemaker  DEMO  parquet  conda  install  conda  forge  fastparquet  scikit  learn  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  from  fastparquet  import  write  from  fastparquet  import  ParquetFile  def  save  as  parquet  file  dataset  filename  label  col  dataset  dataset  data  pd  DataFrame  data  label  col  data  columns  data  columns  astype  str  Parquet  expexts  the  column  names  to  be  strings  write  filename  data  def  read  parquet  file  filename  pf  ParquetFile  filename  return  pf  to  pandas  def  features  and  target  df  label ,amazon
 col  df  loc  df  columns  label  col  values  df  label  col  values  return  trainFile  train  parquet  validFile  valid  parquet  testFile  test  parquet  label  col  target  save  as  parquet  file  train  set  trainFile  label  col  save  as  parquet  file  valid  set  validFile  label  col  save  as  parquet  file  test  set  testFile  label  col  dfTrain  read  parquet  file  trainFile  dfValid  read  parquet  file  validFile  dfTest  read  parquet  file  testFile  train  train  features  and  target  dfTrain  label  col  valid  valid  features  and  target  dfValid  label  col  test  test  features  and  target  dfTest  label  col  import  io  import  numpy  as  np  import  sagemaker  amazon  common  as  smac  trainVectors  np  array  tolist  for  in  train  astype  float32  trainLabels  np  where  np  array  tolist  for  in  train  astype  float32  bufTrain  io  BytesIO  smac  write  numpy  to  dense  tensor  bufTrain  trainVectors  trainLabels  bufTrain  seek  validVectors  np  array  tolist  for  ,amazon
in  valid  astype  float32  validLabels  np  where  np  array  tolist  for  in  valid  astype  float32  bufValid  io  BytesIO  smac  write  numpy  to  dense  tensor  bufValid  validVectors  validLabels  bufValid  seek  import  boto3  import  os  key  recordio  pb  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  bufTrain  s3  train  data  s3  train  format  bucket  prefix  key  print  uploaded  training  data  location  format  s3  train  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  key  upload  fileobj  bufValid  s3  validation  data  s3  validation  format  bucket  prefix  key  print  uploaded  validation  data  location  format  s3  validation  data  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  boto3  Session  region  name  linear  learner  linear  job  DEMO  linear  time  strftime  time  gmtime  print  Job  name  is  linear  job  linear  training  params  Rol,amazon
eArn  role  TrainingJobName  linear  job  AlgorithmSpecification  TrainingImage  container  TrainingInputMode  File  ResourceConfig  InstanceCount  InstanceType  ml  c4  2xlarge  VolumeSizeInGB  10  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  S3DataDistributionType  FullyReplicated  CompressionType  None  RecordWrapperType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  prefix  S3DataDistributionType  FullyReplicated  CompressionType  None  RecordWrapperType  None  OutputDataConfig  S3OutputPath  s3  format  bucket  prefix  HyperParameters  feature  dim  784  mini  batch  size  200  predictor  type  binary  classifier  epochs  10  num  models  32  loss  absolute  loss  StoppingCondition  MaxRuntimeInSeconds  60  60  time  sm  boto3  Session  client  sagemaker  sm  create  training  job  linear  training  params  status  sm  describe  training  job  TrainingJ,amazon
obName  linear  job  TrainingJobStatus  print  status  sm  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  linear  job  if  status  Failed  message  sm  describe  training  job  TrainingJobName  linear  job  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  sm  describe  training  job  TrainingJobName  linear  job  TrainingJobStatus  ,amazon
from  sagemaker  import  get  execution  role  Bucket  location  to  save  your  custom  code  in  tar  gz  format  custom  code  upload  location  s3  bucket  name  customcode  mxnet  Bucket  location  where  results  of  model  training  are  saved  model  artifacts  location  s3  bucket  name  artifacts  IAM  execution  role  that  gives  SageMaker  access  to  resources  in  your  AWS  account  We  can  use  the  SageMaker  Python  SDK  to  get  the  role  from  our  notebook  environment  role  get  execution  role  cat  mnist  pyfrom  sagemaker  mxnet  import  MXNet  mnist  estimator  MXNet  entry  point  mnist  py  role  role  output  path  model  artifacts  location  code  location  custom  code  upload  location  train  instance  count  train  instance  type  ml  m4  xlarge  hyperparameters  learning  rate  time  import  boto3  region  boto3  Session  region  name  train  data  location  s3  sagemaker  sample  data  mxnet  mnist  train  format  region  test  data  location  s3  sagemaker  sample  dat,amazon
a  mxnet  mnist  test  format  region  mnist  estimator  fit  train  train  data  location  test  test  data  location  time  predictor  mnist  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  IPython  display  import  HTML  HTML  open  input  html  read  response  predictor  predict  data  print  Raw  prediction  result  print  response  labeled  predictions  list  zip  range  10  response  print  Labeled  predictions  print  labeled  predictions  labeled  predictions  sort  key  lambda  label  and  prob  label  and  prob  print  Most  likely  answer  format  labeled  predictions  print  Endpoint  name  predictor  endpoint  import  sagemaker  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
import  boto3  import  logging  logger  logging  getLogger  logger  setLevel  logging  INFO  client  boto3  client  sagemaker  candidates  client  list  notebook  instances  candidatesfor  candidate  in  candidates  NotebookInstances  candidateName  candidate  NotebookInstanceName  candidateNameinstanceArn  candidate  NotebookInstanceArn  instanceTags  client  list  tags  ResourceArn  instanceArn  instanceTags  Tags  for  tag  in  instanceTags  Tags  if  tag  Key  InDuty  and  tag  Value  Yes  print  Found  it  client  boto3  client  sagemaker  candidates  client  list  notebook  instances  for  candidate  in  candidates  NotebookInstances  candidateName  candidate  NotebookInstanceName  logger  info  instance  is  in  Service  format  candidateName  instanceArn  candidate  NotebookInstanceArn  instanceTags  client  list  tags  ResourceArn  instanceArn  for  tag  in  instanceTags  Tags  if  tag  Key  InDuty  and  tag  Value  Yes  logger  info  instance  is  in  duty  format  candidateName  response  client  s,amazon
top  notebook  instance  NotebookInstanceName  candidateName  logger  info  instance  was  stopped  format  candidateName  else  logger  info  Not  in  duty  format  candidateName  writefile  lambda  function  py  import  boto3  import  logging  logger  logging  getLogger  logger  setLevel  logging  INFO  client  boto3  client  sagemaker  def  lambda  handler  event  context  candidates  client  list  notebook  instances  for  candidate  in  candidates  NotebookInstances  candidateName  candidate  NotebookInstanceName  logger  info  instance  is  in  Service  format  candidateName  instanceArn  candidate  NotebookInstanceArn  instanceTags  client  list  tags  ResourceArn  instanceArn  for  tag  in  instanceTags  Tags  if  tag  Key  InDuty  and  tag  Value  Yes  logger  info  instance  is  in  duty  format  candidateName  if  event  event  On  response  client  start  notebook  instance  NotebookInstanceName  candidateName  logger  info  instance  was  started  format  candidateName  elif  event  event  Off  r,amazon
esponse  client  stop  notebook  instance  NotebookInstanceName  candidateName  logger  info  instance  was  stopped  format  candidateName  else  logger  info  Not  in  duty  format  candidateName  return  Done  zip  lambda  zip  lambda  function  py  aws  s3  mb  s3  terraform  serverless  repository  region  eu  west  aws  s3  cp  lambda  zip  s3  terraform  serverless  repository  v1  writefile  lambda  tf  provider  aws  region  eu  west  resource  aws  lambda  function  start  stop  sm  function  name  StopStartSageMakerNotebookInstances  The  bucket  name  as  created  earlier  with  aws  s3api  create  bucket  s3  bucket  terraform  serverless  repository  s3  key  v1  lambda  zip  main  is  the  filename  within  the  zip  file  main  js  and  handler  is  the  name  of  the  property  under  which  the  handler  function  was  exported  in  that  file  handler  lambda  function  lambda  handler  runtime  python3  role  aws  iam  role  lambda  exec  arn  writefile  lambda  tf  IAM  role  which  dicta,amazon
tes  what  other  AWS  services  the  Lambda  function  may  access  Testing  working  example  resource  aws  iam  role  lambda  exec  name  assume  role  assume  role  policy  EOF  Version  2012  10  17  Statement  Action  sts  AssumeRole  Principal  Service  lambda  amazonaws  com  Effect  Allow  Sid  EOF  resource  aws  iam  policy  policy  name  control  policy  description  policy  to  control  SageMaker  instances  start  stop  state  policy  EOF  Version  2012  10  17  Statement  Sid  CloudWatchLogs0  Effect  Allow  Action  logs  CreateLogStream  logs  PutLogEvents  Resource  arn  aws  logs  eu  west  217431963147  log  group  aws  lambda  NotebookInstance  Sid  InstanceControl  Effect  Allow  Action  sagemaker  ListTags  ec2  ModifyNetworkInterfaceAttribute  sagemaker  DescribeNotebookInstance  ec2  DeleteNetworkInterface  ec2  DescribeSecurityGroups  ec2  CreateNetworkInterface  ec2  DescribeInternetGateways  ec2  DescribeNetworkInterfaces  ec2  DescribeAvailabilityZones  sagemaker  StopNotebookInst,amazon
ance  ec2  DescribeVpcs  sagemaker  StartNotebookInstance  ec2  AttachNetworkInterface  ec2  DescribeSubnets  sagemaker  ListNotebookInstances  Resource  Sid  CloudWatchLogs1  Effect  Allow  Action  logs  CreateLogGroup  Resource  arn  aws  logs  eu  west  217431963147  EOF  resource  aws  iam  role  policy  attachment  test  attach  role  aws  iam  role  lambda  exec  name  policy  arn  aws  iam  policy  policy  arn  wget  https  releases  hashicorp  com  terraform  11  terraform  11  linux  amd64  zip  unzip  terraform  11  linux  amd64  zip  terraform  init  terraform  validate  terraform  plan  terraform  apply  input  false  auto  approve  aws  lambda  list  functions  aws  lambda  invoke  function  name  StopStartSageMakerNotebookInstances  payload  event  On  invocation  type  RequestResponse  log  type  Tail  dev  null  jq  LogResult  base64  decode  writefile  cloud  watch  tf  Based  on  the  location  of  the  instances  This  is  for  Israel  where  people  are  working  Sunday  to  Thursday  and ,amazon
 5AM  GTM  is  8AM  resource  aws  cloudwatch  event  rule  on  duty  name  on  duty  description  Fires  at  the  beginning  of  the  working  day  schedule  expression  cron  SUN  THU  people  are  working  Sunday  to  Thursday  and  4PM  GTM  is  7PM  resource  aws  cloudwatch  event  rule  off  duty  name  off  duty  description  Fires  at  the  end  of  the  working  day  schedule  expression  cron  16  SUN  THU  resource  aws  cloudwatch  event  target  on  duty  start  instances  rule  aws  cloudwatch  event  rule  on  duty  name  target  id  start  stop  sm  arn  aws  lambda  function  start  stop  sm  arn  input  DOC  event  On  DOC  resource  aws  cloudwatch  event  target  off  duty  start  instances  rule  aws  cloudwatch  event  rule  off  duty  name  target  id  start  stop  sm  arn  aws  lambda  function  start  stop  sm  arn  input  DOC  event  Off  DOC  resource  aws  lambda  permission  allow  cloudwatch  to  call  On  duty  statement  id  AllowExecutionFromCloudWatchOn  action  lambda  Invo,amazon
keFunction  function  name  aws  lambda  function  start  stop  sm  function  name  principal  events  amazonaws  com  source  arn  aws  cloudwatch  event  rule  on  duty  arn  resource  aws  lambda  permission  allow  cloudwatch  to  call  Off  duty  statement  id  AllowExecutionFromCloudWatchOff  action  lambda  InvokeFunction  function  name  aws  lambda  function  start  stop  sm  function  name  principal  events  amazonaws  com  source  arn  aws  cloudwatch  event  rule  off  duty  arn  terraform  apply  input  false  auto  approve  terraform  graph  draw  cycles  dot  Tsvg  graph  svgfrom  IPython  core  display  import  SVG  SVG  filename  graph  svg  usr  bin  env  python2  import  botocore  session  from  datetime  import  datetime  tzinfo  timedelta  import  json  from  os  import  environ  change  region  to  match  desired  region  region  eu  west  class  SimpleUtc  tzinfo  def  tzname  self  return  UTC  def  utcoffset  self  dt  return  timedelta  class  DateTimeEncoder  json  JSONEncoder  def,amazon
  default  self  if  isinstance  datetime  return  utcnow  replace  tzinfo  SimpleUtc  isoformat  return  json  JSONEncoder  default  self  def  clean  response  resp  del  resp  ResponseMetadata  return  resp  output  if  AWS  PROFILE  in  environ  session  botocore  session  Session  profile  environ  AWS  PROFILE  else  session  botocore  session  get  session  ec2  session  create  client  ec2  region  name  region  print  Executing  ec2  describe  instances  output  ec2  clean  response  ec2  describe  instances  print  Executing  ec2  describe  security  groups  output  securitygroup  clean  response  ec2  describe  security  groups  print  Executing  ec2  describe  subnet  output  subnets  clean  response  ec2  describe  subnets  print  Executing  ec2  describe  network  acls  output  acls  clean  response  ec2  describe  network  acls  print  Executing  ec2  describe  vpcs  output  vpc  clean  response  ec2  describe  vpcs  print  Executing  ec2  describe  volumes  output  ebs  clean  response  ec2  d,amazon
escribe  volumes  print  Executing  elb  describe  load  balancers  output  elb  clean  response  session  create  client  elb  region  name  region  describe  load  balancers  elbv2  session  create  client  elbv2  region  name  region  output  elbv2  output  elbv2  TargetHealthDescriptions  print  Executing  elbv2  describe  load  balancers  output  elbv2  LoadBalancers  elbv2  describe  load  balancers  LoadBalancers  print  Executing  elbv2  describe  target  groups  output  elbv2  TargetGroups  elbv2  describe  target  groups  TargetGroups  print  Executing  elbv2  describe  target  health  for  target  group  arn  in  target  group  TargetGroupArn  for  target  group  in  output  elbv2  TargetGroups  output  elbv2  TargetHealthDescriptions  target  group  arn  elbv2  describe  target  health  TargetGroupArn  target  group  arn  TargetHealthDescriptions  print  Executing  autoscaling  describe  auto  scaling  groups  output  autoscale  clean  response  session  create  client  autoscaling  region  name  ,amazon
region  describe  auto  scaling  groups  print  Executing  autoscaling  describe  launch  configurations  output  launchconfig  clean  response  session  create  client  autoscaling  region  name  region  describe  launch  configurations  print  Executing  s3api  list  buckets  output  s3buckets  clean  response  session  create  client  s3  region  name  region  list  buckets  print  Executing  rds  describe  db  instances  output  rds  clean  response  session  create  client  rds  region  name  region  describe  db  instances  print  Executing  cloudfront  describe  db  instances  output  cloudfront  clean  response  session  create  client  cloudfront  region  name  region  list  distributions  print  Executing  sns  list  topics  sns  session  create  client  sns  region  name  region  topic  resp  sns  list  topics  print  Executing  sns  get  topic  attributes  output  sns  clean  response  sns  get  topic  attributes  TopicArn  TopicArn  for  in  topic  resp  get  Topics  print  Executing  sqs  list  ,amazon
queues  sqs  session  create  client  sqs  region  name  region  queue  resp  sqs  list  queues  print  Executing  sqs  get  queue  attributes  urls  queue  resp  get  QueueUrls  output  sqs  Queues  clean  response  sqs  get  queue  attributes  AttributeNames  All  QueueUrl  url  for  url  in  urls  output  importMetaData  region  region  timeStamp  datetime  now  with  open  aws  json  as  json  dump  output  cls  DateTimeEncoder  print  Output  to  aws  json  ,amazon
writefile  Dockerfile  FROM  scikit  base  latest  COPY  model  py  opt  program  writefile  model  py  import  numpy  as  np  import  json  import  os  import  pandas  as  pd  import  re  from  sklearn  import  model  selection  from  sklearn  externals  import  joblib  from  sklearn  linear  model  import  LogisticRegression  from  sklearn  ensemble  import  RandomForestClassifier  prefix  opt  ml  input  path  os  path  join  prefix  input  data  output  path  os  path  join  prefix  output  model  path  os  path  join  prefix  model  param  path  os  path  join  prefix  input  config  hyperparameters  json  model  cache  def  train  print  Training  mode  try  This  algorithm  has  single  channel  of  input  data  called  training  Since  we  run  in  File  mode  the  input  files  are  copied  to  the  directory  specified  here  channel  name  training  training  path  os  path  join  input  path  channel  name  hyper  logistic  hyper  random  forest  Read  in  any  hyperparameters  that  the  user  pa,amazon
ssed  with  the  training  job  with  open  param  path  as  tc  is  float  re  compile  is  integer  re  compile  for  key  value  in  json  load  tc  items  workaround  to  convert  numbers  from  string  if  is  float  match  value  is  not  None  value  float  value  elif  is  integer  match  value  is  not  None  value  int  value  if  key  startswith  logistic  key  key  replace  logistic  hyper  logistic  key  value  if  key  startswith  random  forest  key  key  replace  random  forest  hyper  random  forest  key  value  Take  the  set  of  files  and  read  them  all  into  single  pandas  dataframe  input  files  os  path  join  training  path  file  for  file  in  os  listdir  training  path  if  len  input  files  raise  ValueError  There  are  no  files  in  This  usually  indicates  that  the  channel  was  incorrectly  specified  the  data  specification  in  S3  was  incorrectly  specified  or  the  role  specified  does  not  have  permission  to  access  the  data  format  training  path  ch,amazon
annel  name  raw  data  pd  read  csv  file  sep  header  None  for  file  in  input  files  train  data  pd  concat  raw  data  labels  are  in  the  first  column  train  data  ix  train  data  ix  train  test  train  test  model  selection  train  test  split  test  size  33  random  state  algo  logistic  print  Training  algo  model  LogisticRegression  model  set  params  hyper  logistic  model  fit  train  train  print  format  algo  model  score  test  test  joblib  dump  model  open  os  path  join  model  path  model  pkl  algo  wb  algo  random  forest  print  Training  algo  model  RandomForestClassifier  model  set  params  hyper  random  forest  model  fit  train  train  print  format  algo  model  score  test  test  joblib  dump  model  open  os  path  join  model  path  model  pkl  algo  wb  except  Exception  as  Write  out  an  error  file  This  will  be  returned  as  the  failureReason  in  the  DescribeTrainingJob  result  trc  traceback  format  exc  with  open  os  path  join  output  ,amazon
path  failure  as  write  Exception  during  training  str  trc  Printing  this  causes  the  exception  to  be  in  the  training  job  logs  as  well  print  Exception  during  training  str  trc  file  sys  stderr  non  zero  exit  code  causes  the  training  job  to  be  marked  as  Failed  sys  exit  255  def  predict  request  algo  request  get  algorithm  payload  request  get  payload  if  algo  is  None  or  payload  is  None  raise  ValueError  You  need  to  inform  the  algorithm  and  the  payload  if  model  cache  get  algo  is  None  model  filename  os  path  join  model  path  model  pkl  algo  model  cache  algo  joblib  load  open  model  filename  rb  return  iris  id  model  cache  algo  predict  payload  tolist  writefile  buildspec  yml  version  phases  pre  build  commands  echo  Logging  in  to  Amazon  ECR  aws  ecr  get  login  no  include  email  region  AWS  DEFAULT  REGION  docker  pull  AWS  ACCOUNT  ID  dkr  ecr  AWS  DEFAULT  REGION  amazonaws  com  scikit  base  latest  d,amazon
ocker  tag  AWS  ACCOUNT  ID  dkr  ecr  AWS  DEFAULT  REGION  amazonaws  com  scikit  base  latest  scikit  base  latest  build  commands  echo  Build  started  on  date  echo  Building  the  Docker  image  docker  build  IMAGE  REPO  NAME  IMAGE  TAG  docker  tag  IMAGE  REPO  NAME  IMAGE  TAG  AWS  ACCOUNT  ID  dkr  ecr  AWS  DEFAULT  REGION  amazonaws  com  IMAGE  REPO  NAME  IMAGE  TAG  post  build  commands  echo  Build  completed  on  date  echo  Pushing  the  Docker  image  echo  docker  push  AWS  ACCOUNT  ID  dkr  ecr  AWS  DEFAULT  REGION  amazonaws  com  IMAGE  REPO  NAME  IMAGE  TAG  docker  push  AWS  ACCOUNT  ID  dkr  ecr  AWS  DEFAULT  REGION  amazonaws  com  IMAGE  REPO  NAME  IMAGE  TAG  echo  AWS  ACCOUNT  ID  dkr  ecr  AWS  DEFAULT  REGION  amazonaws  com  IMAGE  REPO  NAME  IMAGE  TAG  image  url  echo  Done  artifacts  files  image  url  name  image  url  discard  paths  yes  docker  build  Dockerfile  iris  model  import  boto3  sts  client  boto3  client  sts  session  boto3  session  S,amazon
ession  account  id  sts  client  get  caller  identity  Account  region  session  region  name  credentials  session  get  credentials  credentials  credentials  get  frozen  credentials  repo  name  iris  model  image  tag  test  mkdir  tests  cp  model  py  Dockerfile  buildspec  yml  tests  with  open  tests  vars  env  as  write  AWS  ACCOUNT  ID  account  id  write  IMAGE  TAG  image  tag  write  IMAGE  REPO  NAME  repo  name  write  AWS  DEFAULT  REGION  region  write  AWS  ACCESS  KEY  ID  credentials  access  key  write  AWS  SECRET  ACCESS  KEY  credentials  secret  key  write  AWS  SESSION  TOKEN  credentials  token  close  cat  tests  vars  env  time  PWD  pwd  PWD  PWD  docker  rm  local  test  docker  run  name  local  test  stdout  rm  var  run  docker  sock  var  run  docker  sock  PWD  tests  LocalBuild  envFile  ENV  VAR  FILE  vars  env  IMAGE  NAME  aws  codebuild  docker  17  09  ARTIFACTS  PWD  tests  output  SOURCE  PWD  tests  amazon  aws  codebuild  local  latesthyperparameters  logis,amazon
tic  max  iter  100  logistic  solver  lbfgs  random  forest  max  depth  10  random  forest  jobs  random  forest  verbose  import  json  mkdir  input  config  hyperparameters  dict  key  str  values  for  key  values  in  hyperparameters  items  with  open  input  config  hyperparameters  json  as  write  json  dumps  hyperparameters  flush  close  import  time  import  sagemaker  import  boto3  sts  client  boto3  client  sts  model  prefix  iris  model  account  id  sts  client  get  caller  identity  Account  region  boto3  session  Session  region  name  training  image  dkr  ecr  amazonaws  com  latest  format  account  id  region  model  prefix  roleArn  arn  aws  iam  role  MLOps  format  account  id  timestamp  time  strftime  time  gmtime  job  name  model  prefix  timestamp  sagemaker  session  sagemaker  Session  training  params  Here  we  set  the  reference  for  the  Image  Classification  Docker  image  stored  on  ECR  https  aws  amazon  com  pt  ecr  training  params  AlgorithmSpecificati,amazon
on  TrainingImage  training  image  TrainingInputMode  File  The  IAM  role  with  all  the  permissions  given  to  Sagemaker  training  params  RoleArn  roleArn  Here  Sagemaker  will  store  the  final  trained  model  training  params  OutputDataConfig  S3OutputPath  s3  format  sagemaker  session  default  bucket  model  prefix  This  is  the  config  of  the  instance  that  will  execute  the  training  training  params  ResourceConfig  InstanceCount  InstanceType  ml  m4  xlarge  VolumeSizeInGB  30  The  job  name  You  ll  see  this  name  in  the  Jobs  section  of  the  Sagemaker  console  training  params  TrainingJobName  job  name  Here  you  will  configure  the  hyperparameters  used  for  training  your  model  training  params  HyperParameters  hyperparameters  Training  timeout  training  params  StoppingCondition  MaxRuntimeInSeconds  360000  The  algorithm  currently  only  supports  fullyreplicated  model  where  data  is  copied  onto  each  machine  training  params  InputDataConfig  P,amazon
lease  notice  that  we  re  using  application  recordio  for  both  training  and  validation  datasets  given  our  dataset  is  formated  in  RecordIO  Here  we  set  training  dataset  Training  data  should  be  inside  subdirectory  called  train  training  params  InputDataConfig  append  ChannelName  training  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  input  format  sagemaker  session  default  bucket  model  prefix  S3DataDistributionType  FullyReplicated  ContentType  text  csv  CompressionType  None  training  params  Tags  with  open  trainingjob  json  as  write  json  dumps  training  params  flush  close  mkdir  input  data  training  import  pandas  as  pd  import  numpy  as  np  from  sklearn  import  datasets  iris  datasets  load  iris  dataset  np  insert  iris  data  iris  target  axis  pd  pd  DataFrame  data  dataset  columns  iris  id  iris  feature  names  pd  to  csv  input  data  training  iris  csv  header  None  index  False  sep  encoding  utf  pd  head  mkdir  ,amazon
model  rm  model  print  Training  docker  run  rm  name  my  model  PWD  model  opt  ml  model  PWD  input  opt  ml  input  iris  model  trainprint  Testing  with  logistic  docker  run  rm  name  my  model  PWD  model  opt  ml  model  PWD  input  opt  ml  input  iris  model  test  logistic  print  Testing  with  random  forest  docker  run  rm  name  my  model  PWD  model  opt  ml  model  PWD  input  opt  ml  input  iris  model  test  random  forest  docker  run  rm  name  my  model  8080  8080  PWD  model  opt  ml  model  PWD  input  opt  ml  input  iris  model  serveimport  sagemaker  Get  the  current  Sagemaker  session  sagemaker  session  sagemaker  Session  default  bucket  sagemaker  session  default  bucket  role  sagemaker  get  execution  role  data  location  sagemaker  session  upload  data  path  input  data  training  key  prefix  iris  model  input  bash  cd  mlops  workshop  images  iris  model  mkdir  assets  cp  OLDPWD  buildspec  yml  OLDPWD  model  py  OLDPWD  Dockerfile  cp  OLDPWD  tr,amazon
ainingjob  json  assets  cp  OLDPWD  assets  iris  model  deploy  yml  assets  git  add  all  git  commit  files  for  building  an  iris  model  image  git  push  ,amazon
import  time  import  numpy  as  np  np  random  seed  import  pandas  as  pd  import  json  import  matplotlib  pyplot  as  plt  conda  install  s3fsimport  boto3  import  s3fs  import  sagemaker  from  sagemaker  import  get  execution  rolebucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  deepar  sagemaker  session  sagemaker  Session  role  get  execution  role  s3  data  path  data  format  bucket  prefix  s3  output  path  output  format  bucket  prefix  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  image  name  get  image  uri  boto3  Session  region  name  forecasting  deepar  freq  prediction  length  48context  length  72t0  2016  01  01  00  00  00  data  length  400  num  ts  200  period  24time  series  for  in  range  num  ts  level  10  np  random  rand  seas  amplitude  np  random  rand  level  sig  05  level  noise  parameter  constant  in  time  time  ticks  np  array  range  data  length  source  level  seas  amplitude  np  sin  time  ticks  np  pi  perio,amazon
d  noise  sig  np  random  randn  data  length  data  source  noise  index  pd  DatetimeIndex  start  t0  freq  freq  periods  data  length  time  series  append  pd  Series  data  data  index  index  time  series  plot  plt  show  time  series  training  for  ts  in  time  series  time  series  training  append  ts  prediction  length  time  series  plot  label  test  time  series  training  plot  label  train  ls  plt  legend  plt  show  def  series  to  obj  ts  cat  None  obj  start  str  ts  index  target  list  ts  if  cat  is  not  None  obj  cat  cat  return  obj  def  series  to  jsonline  ts  cat  None  return  json  dumps  series  to  obj  ts  cat  encoding  utf  s3filesystem  s3fs  S3FileSystem  with  s3filesystem  open  s3  data  path  train  train  json  wb  as  fp  for  ts  in  time  series  training  fp  write  series  to  jsonline  ts  encode  encoding  fp  write  encode  encoding  with  s3filesystem  open  s3  data  path  test  test  json  wb  as  fp  for  ts  in  time  series  fp  write  se,amazon
ries  to  jsonline  ts  encode  encoding  fp  write  encode  encoding  estimator  sagemaker  estimator  Estimator  sagemaker  session  sagemaker  session  image  name  image  name  role  role  train  instance  count  train  instance  type  ml  c4  xlarge  base  job  name  DEMO  deepar  output  path  s3  s3  output  path  hyperparameters  time  freq  freq  context  length  str  context  length  prediction  length  str  prediction  length  num  cells  40  num  layers  likelihood  gaussian  epochs  20  mini  batch  size  32  learning  rate  001  dropout  rate  05  early  stopping  patience  10  estimator  set  hyperparameters  hyperparameters  data  channels  train  s3  train  format  s3  data  path  test  s3  test  format  s3  data  path  estimator  fit  inputs  data  channels  job  name  estimator  latest  training  job  name  endpoint  name  sagemaker  session  endpoint  from  job  job  name  job  name  initial  instance  count  instance  type  ml  m4  xlarge  deployment  image  image  name  role  role  class,amazon
  DeepARPredictor  sagemaker  predictor  RealTimePredictor  def  set  prediction  parameters  self  freq  prediction  length  Set  the  time  frequency  and  prediction  length  parameters  This  method  must  be  called  before  being  able  to  use  predict  Parameters  freq  string  indicating  the  time  frequency  prediction  length  integer  number  of  predicted  time  points  Return  value  none  self  freq  freq  self  prediction  length  prediction  length  def  predict  self  ts  cat  None  encoding  utf  num  samples  100  quantiles  Requests  the  prediction  of  for  the  time  series  listed  in  ts  each  with  the  optional  corresponding  category  listed  in  cat  Parameters  ts  list  of  pandas  Series  objects  the  time  series  to  predict  cat  list  of  integers  default  None  encoding  string  encoding  to  use  for  the  request  default  utf  num  samples  integer  number  of  samples  to  compute  at  prediction  time  default  100  quantiles  list  of  strings  specifying  the ,amazon
 quantiles  to  compute  default  Return  value  list  of  pandas  DataFrame  objects  each  containing  the  predictions  prediction  times  index  for  in  ts  req  self  encode  request  ts  cat  encoding  num  samples  quantiles  res  super  DeepARPredictor  self  predict  req  return  self  decode  response  res  prediction  times  encoding  def  encode  request  self  ts  cat  encoding  num  samples  quantiles  instances  series  to  obj  ts  cat  if  cat  else  None  for  in  range  len  ts  configuration  num  samples  num  samples  output  types  quantiles  quantiles  quantiles  http  request  data  instances  instances  configuration  configuration  return  json  dumps  http  request  data  encode  encoding  def  decode  response  self  response  prediction  times  encoding  response  data  json  loads  response  decode  encoding  list  of  df  for  in  range  len  prediction  times  prediction  index  pd  DatetimeIndex  start  prediction  times  freq  self  freq  periods  self  prediction  length  ,amazon
list  of  df  append  pd  DataFrame  data  response  data  predictions  quantiles  index  prediction  index  return  list  of  dfpredictor  DeepARPredictor  endpoint  endpoint  name  sagemaker  session  sagemaker  session  content  type  application  json  predictor  set  prediction  parameters  freq  prediction  length  list  of  df  predictor  predict  time  series  training  actual  data  time  series  for  in  range  len  list  of  df  plt  figure  figsize  12  actual  data  prediction  length  context  length  plot  label  target  p10  list  of  df  p90  list  of  df  plt  fill  between  p10  index  p10  p90  color  alpha  label  80  confidence  interval  list  of  df  plot  label  prediction  median  plt  legend  plt  show  sagemaker  session  delete  endpoint  endpoint  name  ,amazon
import  pandas  as  pd  import  numpy  as  np  from  bs4  import  BeautifulSoup  import  requests  wikipedia  link  https  en  wikipedia  org  wiki  List  of  postal  codes  of  Canada  raw  wikipedia  page  requests  get  wikipedia  link  text  soup  BeautifulSoup  raw  wikipedia  page  lxml  print  soup  prettify  table  soup  find  table  Postcode  Borough  Neighbourhood  print  table  for  tr  cell  in  table  find  all  tr  counter  Postcode  var  Borough  var  Neighbourhood  var  for  td  cell  in  tr  cell  find  all  td  if  counter  Postcode  var  td  cell  text  if  counter  Borough  var  td  cell  text  tag  Borough  td  cell  find  if  counter  Neighbourhood  var  str  td  cell  text  strip  tag  Neighbourhood  td  cell  find  counter  if  Postcode  var  Not  assigned  or  Borough  var  Not  assigned  or  Neighbourhood  var  Not  assigned  continue  try  if  tag  Borough  is  None  or  tag  Neighbourhood  is  None  continue  except  pass  if  Postcode  var  or  Borough  var  or  Neighbourhood  var,ibm
  continue  Postcode  append  Postcode  var  Borough  append  Borough  var  Neighbourhood  append  Neighbourhood  var  unique  set  Postcode  print  num  of  unique  Postal  codes  len  unique  Postcode  Borough  Neighbourhood  for  postcode  unique  element  in  unique  var  var  var  for  postcode  idx  postcode  element  in  enumerate  Postcode  if  postcode  unique  element  postcode  element  var  postcode  element  var  Borough  postcode  idx  if  var  var  Neighbourhood  postcode  idx  else  var  var  Neighbourhood  postcode  idx  Postcode  append  var  Borough  append  var  Neighbourhood  append  var  pip  install  geocoder  Collecting  geocoder  Downloading  https  files  pythonhosted  org  packages  4f  6b  13166c909ad2f2d76b929a4227c952630ebaf0d729f6317eb09cbceccbab  geocoder  38  py2  py3  none  any  whl  98kB  Requirement  already  satisfied  click  in  programdata  anaconda3  lib  site  packages  from  geocoder  Requirement  already  satisfied  six  in  programdata  anaconda3  lib  site  package,ibm
s  from  geocoder  11  Collecting  ratelim  from  geocoder  Downloading  https  files  pythonhosted  org  packages  f2  98  7e6d147fd16a10a5f821db6e25f192265d6ecca3d82957a4fdd592cad49c  ratelim  py2  py3  none  any  whl  Requirement  already  satisfied  requests  in  programdata  anaconda3  lib  site  packages  from  geocoder  19  Requirement  already  satisfied  future  in  programdata  anaconda3  lib  site  packages  from  geocoder  16  Requirement  already  satisfied  decorator  in  programdata  anaconda3  lib  site  packages  from  ratelim  geocoder  Requirement  already  satisfied  chardet  in  programdata  anaconda3  lib  site  packages  from  requests  geocoder  Requirement  already  satisfied  idna  in  programdata  anaconda3  lib  site  packages  from  requests  geocoder  Requirement  already  satisfied  urllib3  24  21  in  programdata  anaconda3  lib  site  packages  from  requests  geocoder  23  Requirement  already  satisfied  certifi  2017  17  in  programdata  anaconda3  lib  site  packages  fr,ibm
om  requests  geocoder  2018  24  Installing  collected  packages  ratelim  geocoder  Successfully  installed  geocoder  38  ratelim  print  geocoder  has  been  installed  before  import  geocoder  print  geocoder  has  been  successfully  imported  latitude  longitude  for  elem  in  Postcode  initialize  your  variable  to  None  lat  lng  coords  None  loop  until  you  get  the  coordinates  while  lat  lng  coords  is  None  geocoder  google  Toronto  Ontario  format  elem  lat  lng  coords  latlng  print  lat  lng  coords  latitude  append  lat  lng  coords  longitude  append  lat  lng  coords  print  elem  is  RECEIVED  print  lat  lng  coords  print  lat  lng  coords  toronto  dict  Postcode  Postcode  Borough  Borough  Neighbourhood  Neighbourhood  Latitude  latitude  Longitude  longitude  df  toronto  pd  DataFrame  from  dict  toronto  dict  df  toronto  to  csv  toronto  base  csv  df  toronto  head  10  ,ibm
sh  setup  shimport  chainer  import  os  import  shutil  import  numpy  as  np  import  sagemaker  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  Download  MNIST  dataset  train  test  chainer  datasets  get  mnist  Extract  data  and  labels  from  dataset  train  images  np  array  data  for  data  in  train  train  labels  np  array  data  for  data  in  train  test  images  np  array  data  for  data  in  test  test  labels  np  array  data  for  data  in  test  Save  the  data  and  labels  as  npz  into  local  directories  and  upload  them  to  S3  try  os  makedirs  tmp  data  train  os  makedirs  tmp  data  test  np  savez  tmp  data  train  train  npz  images  train  images  labels  train  labels  np  savez  tmp  data  test  test  npz  images  test  images  labels  test  labels  train  input  sagemaker  session  upload  data  path  os  path  join  tmp  data  train  key  prefix  notebook  chainer  mnist  test  input  sagemaker  ses,amazon
sion  upload  data  path  os  path  join  tmp  data  test  key  prefix  notebook  chainer  mnist  finally  shutil  rmtree  tmp  data  pygmentize  chainer  mnist  py  import  subprocess  from  sagemaker  chainer  estimator  import  Chainer  instance  type  ml  m4  xlarge  chainer  estimator  Chainer  entry  point  chainer  mnist  py  role  role  train  instance  count  train  instance  type  instance  type  hyperparameters  epochs  batch  size  128  chainer  estimator  fit  train  train  input  test  test  input  Keep  the  job  name  for  checking  training  loss  later  training  job  chainer  estimator  latest  training  job  nameimport  subprocess  from  sagemaker  chainer  estimator  import  Chainer  instance  type  ml  m4  xlarge  chainer  estimator  Chainer  entry  point  chainer  mnist  py  role  role  train  instance  count  train  instance  type  instance  type  hyperparameters  epochs  batch  size  128  Setting  for  hyper  paramter  optimization  from  sagemaker  tuner  import  HyperparameterTuner ,amazon
 CategoricalParameter  ContinuousParameter  IntegerParameter  hyperparameter  ranges  optimizer  CategoricalParameter  sgd  Adam  An  example  of  further  tuning  hyperparameter  ranges  optimizer  CategoricalParameter  sgd  Adam  learning  rate  ContinuousParameter  01  num  epoch  IntegerParameter  objective  metric  name  Validation  accuracy  metric  definitions  Name  Validation  accuracy  Regex  validation  main  accuracy  tuner  HyperparameterTuner  chainer  estimator  objective  metric  name  hyperparameter  ranges  metric  definitions  max  jobs  max  parallel  jobs  tuner  fit  train  train  input  test  test  input  training  job  tuning  tuner  latest  tuning  job  nameimport  subprocess  from  sagemaker  chainer  estimator  import  Chainer  instance  type  local  chainer  estimator  Chainer  entry  point  chainer  mnist  py  role  role  train  instance  count  train  instance  type  instance  type  hyperparameters  epochs  batch  size  128  chainer  estimator  fit  train  train  input  test  tes,amazon
t  input  desc  tuner  sagemaker  session  sagemaker  client  describe  training  job  TrainingJobName  tuner  best  training  job  selected  optimizer  desc  HyperParameters  optimizer  print  selected  optimizer  instance  type  ml  m4  xlarge  predictor  tuner  deploy  initial  instance  count  instance  type  instance  type  import  random  import  matplotlib  pyplot  as  plt  num  samples  indices  random  sample  range  test  images  shape  num  samples  images  labels  test  images  indices  test  labels  indices  for  in  range  num  samples  plt  subplot  num  samples  plt  imshow  images  reshape  28  28  cmap  gray  plt  title  labels  plt  axis  off  prediction  predictor  predict  images  predicted  label  prediction  argmax  axis  print  The  predicted  labels  are  format  predicted  label  from  IPython  display  import  HTML  HTML  open  input  html  read  image  np  array  data  dtype  np  float32  prediction  predictor  predict  image  predicted  label  prediction  argmax  axis  print  What,amazon
  you  wrote  is  format  predicted  label  tuner  delete  endpoint  ,amazon
cat  Dockerfile  bash  image  sagemaker  fastai  dogscats  FASTAI  VERSION  PY  VERSION  py37  Get  the  account  number  associated  with  the  current  IAM  credentials  account  aws  sts  get  caller  identity  query  Account  output  text  Get  the  region  defined  in  the  current  configuration  default  to  us  west  if  none  defined  region  aws  configure  get  region  region  region  us  west  If  the  repository  doesn  exist  in  ECR  create  it  aws  ecr  describe  repositories  repository  names  image  dev  null  if  ne  then  aws  ecr  create  repository  repository  name  image  dev  null  fi  Get  the  login  command  from  ECR  and  execute  it  directly  aws  ecr  get  login  region  region  no  include  email  Build  the  docker  image  locally  with  the  image  name  and  then  push  it  to  ECR  with  the  full  name  fullname  account  dkr  ecr  region  amazonaws  com  image  FASTAI  VERSION  gpu  PY  VERSION  docker  build  image  FASTAI  VERSION  gpu  PY  VERSION  build  arg  ARCH,amazon
  gpu  docker  tag  image  FASTAI  VERSION  gpu  PY  VERSION  fullname  docker  push  fullname  fullname  account  dkr  ecr  region  amazonaws  com  image  FASTAI  VERSION  cpu  PY  VERSION  docker  build  image  FASTAI  VERSION  cpu  PY  VERSION  build  arg  ARCH  cpu  docker  tag  image  FASTAI  VERSION  cpu  PY  VERSION  fullname  docker  push  fullname  import  os  pwd  os  getcwd  DOGSCATS  DATA  PATH  data  dogscats  mkdir  data  wget  data  http  files  fast  ai  data  dogscats  zip  unzip  data  data  dogscats  zip  rm  data  dogscats  zip  ls  DOGSCATS  DATA  PATH  from  sagemaker  import  get  execution  role  role  get  execution  role  roleimport  os  import  subprocess  instance  type  local  image  name  sagemaker  fastai  dogscats  cpu  py37  if  subprocess  call  nvidia  smi  Set  type  to  GPU  if  one  is  present  instance  type  local  gpu  image  name  sagemaker  fastai  dogscats  gpu  py37  print  Instance  type  instance  type  print  Image  image  name  from  sagemaker  estimator  impo,amazon
rt  Estimator  hyperparameters  epochs  batch  size  256  estimator  Estimator  role  role  train  instance  count  train  instance  type  instance  type  image  name  image  name  hyperparameters  hyperparameters  estimator  fit  file  pwd  DOGSCATS  DATA  PATH  import  os  random  from  PIL  import  Image  TEST  DATA  PATH  DOGSCATS  DATA  PATH  test1  from  sagemaker  predictor  import  RealTimePredictor  json  deserializer  class  ImagePredictor  RealTimePredictor  def  init  self  endpoint  name  sagemaker  session  super  ImagePredictor  self  init  endpoint  name  sagemaker  session  sagemaker  session  serializer  None  deserializer  json  deserializer  content  type  image  jpeg  predictor  estimator  deploy  instance  type  image  sagemaker  fastai  dogscats  cpu  py37  predictor  cls  ImagePredictor  img  file  random  choice  os  listdir  TEST  DATA  PATH  change  dir  name  to  whatever  img  pil  Image  open  TEST  DATA  PATH  img  file  img  pilwith  open  TEST  DATA  PATH  img  file  rb  as  S,amazon
erializes  data  and  makes  prediction  request  to  the  endpoint  local  or  sagemaker  response  predictor  predict  read  responsepredictor  delete  endpoint  S3  prefix  prefix  DEMO  fastai  dogscats  import  sagemaker  as  sage  sess  sage  Session  train  data  location  sess  upload  data  DOGSCATS  DATA  PATH  train  key  prefix  prefix  train  valid  data  location  sess  upload  data  DOGSCATS  DATA  PATH  valid  key  prefix  prefix  valid  data  location  s3  sess  default  bucket  prefiximport  boto3  client  boto3  client  sts  account  client  get  caller  identity  Account  my  session  boto3  session  Session  region  my  session  region  name  algorithm  name  sagemaker  fastai  dogscats  arch  gpu  ecr  image  account  dkr  ecr  region  amazonaws  com  algorithm  name  arch  py37  print  Using  ECR  image  for  training  ecr  image  from  sagemaker  estimator  import  Estimator  hyperparameters  epochs  batch  size  256  instance  type  train  ml  p3  2xlarge  estimator  Estimator  role  ,amazon
role  train  instance  count  train  instance  type  instance  type  train  image  name  ecr  image  hyperparameters  hyperparameters  estimator  fit  data  location  instance  type  deploy  ml  m4  xlarge  arch  cpu  ecr  image  account  dkr  ecr  region  amazonaws  com  algorithm  name  arch  py37  print  Using  ECR  image  for  inference  ecr  image  predictor  estimator  deploy  instance  type  deploy  image  ecr  image  predictor  cls  ImagePredictor  import  os  random  from  PIL  import  Image  TEST  DATA  PATH  DOGSCATS  DATA  PATH  test1  img  file  random  choice  os  listdir  TEST  DATA  PATH  change  dir  name  to  whatever  img  pil  Image  open  TEST  DATA  PATH  img  file  img  pilwith  open  TEST  DATA  PATH  img  file  rb  as  Serializes  data  and  makes  prediction  request  to  the  endpoint  local  or  sagemaker  response  predictor  predict  read  print  response  predictor  delete  endpoint  ,amazon
bin  bash  setup  shimport  sagemaker  sagemaker  session  sagemaker  Session  bucket  sagemaker  session  default  bucket  prefix  sagemaker  DEMO  pytorch  cnn  cifar10  role  sagemaker  get  execution  role  import  os  import  subprocess  instance  type  local  if  subprocess  call  nvidia  smi  Set  type  to  GPU  if  one  is  present  instance  type  local  gpu  print  Instance  type  instance  type  from  utils  cifar  import  get  train  data  loader  get  test  data  loader  imshow  classes  trainloader  get  train  data  loader  testloader  get  test  data  loader  import  numpy  as  np  import  torchvision  torch  get  some  random  training  images  dataiter  iter  trainloader  images  labels  dataiter  next  show  images  imshow  torchvision  utils  make  grid  images  print  labels  print  join  9s  classes  labels  for  in  range  inputs  sagemaker  session  upload  data  path  data  bucket  bucket  key  prefix  data  cifar10  pygmentize  source  cifar10  pyfrom  sagemaker  pytorch  import  PyT,amazon
orch  cifar10  estimator  PyTorch  entry  point  source  cifar10  py  role  role  train  instance  count  train  instance  type  instance  type  cifar10  estimator  fit  inputs  from  sagemaker  pytorch  import  PyTorchModel  cifar10  predictor  cifar10  estimator  deploy  initial  instance  count  instance  type  instance  type  get  some  test  images  dataiter  iter  testloader  images  labels  dataiter  next  print  images  imshow  torchvision  utils  make  grid  images  print  GroundTruth  join  4s  classes  labels  for  in  range  outputs  cifar10  predictor  predict  images  numpy  predicted  torch  max  torch  from  numpy  np  array  outputs  print  Predicted  join  4s  classes  predicted  for  in  range  cifar10  estimator  delete  endpoint  ,amazon
import  os  import  time  import  datetime  measure  time  start  time  time  time  process  code  print  processed  time  2f  sec  format  time  time  start  time  https  docs  python  org  library  time  html  print  date  print  datetime  datetime  now  strftime  print  datetime  datetime  now  strftime  dirpath  assets  sample  create  directory  if  not  os  path  exists  dirpath  os  makedirs  dirpath  else  print  dir  already  exist  browse  directory  for  in  os  listdir  dirpath  if  os  path  isdir  os  path  join  dirpath  continue  filename  os  path  splitext  ext  os  path  splitext  print  filename  ext  wirte  file  dirpath  assets  with  open  os  path  join  dirpath  test  txt  as  file  file  write  test  file  nhello  world  nI  love  python  read  file  with  open  os  path  join  dirpath  test  txt  as  for  line  in  print  line  import  cv2  from  PIL  import  Image  import  matplotlib  pyplot  as  plt  read  image  file  testimg  assets  sample  IMG  0263  face0  JPG  img  cv2  imre,microsoft
ad  testimg  cv2  IMREAD  COLOR  image  resize  http  opencv  python  readthedocs  io  en  latest  doc  10  imageTransformation  imageTransformation  html  upsizing  cv2  INTER  CUBIC  downsize  cv2  INTER  AREA  img  cv2  resize  img  100  100  interpolation  cv2  INTER  AREA  display  using  PIL  need  to  convert  img  cv2  cvtColor  img  cv2  COLOR  BGR2RGB  pilimg  Image  fromarray  img  display  pilimg  img  cv2  imread  testimg  cv2  IMREAD  COLOR  display  using  PLT  1x2  display  format  fig  plt  figure  fig  add  subplot  121  plt  imshow  img  plt  title  default  plt  axis  off  convert  to  RGB  img2  cv2  cvtColor  img  cv2  COLOR  BGR2RGB  fig  add  subplot  122  plt  imshow  img2  plt  title  BGR2RGB  plt  axis  off  import  requests  import  http  import  json  from  io  import  BytesIO  import  numpy  as  np  simple  http  GET  request  test  url  http  ipinfo  io  ip  requests  get  test  url  print  text  print  content  binary  http  GET  request  binary  test  url  http  epilepsyu  com,microsoft
  wp  content  uploads  2014  01  happy  people  1050x600  jpg  requests  get  test  url  img  Image  open  BytesIO  content  rsimg  cv2  resize  np  array  img  480  300  interpolation  cv2  INTER  AREA  plt  imshow  rsimg  update  myconfig  py  import  myconfig  print  myconfig  api  id  myconfig  api  key  http  POST  request  with  binary  data  Azure  custom  ai  prediction  api  testimg  assets  sample  IMG  0263  face0  JPG  img  cv2  imread  testimg  cv2  IMREAD  COLOR  img  cv2  resize  img  100  100  interpolation  cv2  INTER  AREA  img  cv2  cvtColor  img  cv2  COLOR  BGR2RGB  pilimg  Image  fromarray  img  display  pilimg  apiurl  https  southcentralus  api  cognitive  microsoft  com  customvision  v2  Prediction  image  headers  Content  Type  application  octet  stream  Prediction  Key  myconfig  api  key  with  open  testimg  rb  as  roi  requests  post  apiurl  myconfig  api  id  headers  headers  data  roi  print  text  JSON  parse  pred  json  loads  content  decode  utf  conf  float  pred  ,microsoft
predictions  probability  label  pred  predictions  tagName  print  label  confidence  2f  format  label  conf  100  capture  camera  cap  cv2  VideoCapture  print  width  height  format  cap  get  cap  get  cap  set  320  cap  set  240  while  True  ret  frame  cap  read  if  ret  gray  cv2  cvtColor  frame  cv2  COLOR  BGR2GRAY  cv2  imshow  frame  gray  if  cv2  waitKey  0xFF  ord  break  cap  release  cv2  destroyAllWindows  play  video  file  cap  cv2  VideoCapture  assets  ssd  sample3  mp4  print  cap  get  cap  get  while  cap  isOpened  ret  frame  cap  read  if  not  ret  break  gray  cv2  cvtColor  frame  cv2  COLOR  BGR2GRAY  cv2  resize  frame  480  300  interpolation  cv2  INTER  AREA  cv2  imshow  frame  if  cv2  waitKey  0xFF  ord  break  cap  release  cv2  destroyAllWindows  drawing  False  true  if  mouse  is  pressed  ix  iy  mouse  callback  function  def  draw  circle  event  flags  param  global  ix  iy  drawing  mode  if  event  cv2  EVENT  LBUTTONDOWN  drawing  True  ix  iy  elif  even,microsoft
t  cv2  EVENT  MOUSEMOVE  if  drawing  True  cv2  circle  img  255  255  255  elif  event  cv2  EVENT  LBUTTONUP  drawing  False  cv2  circle  img  255  255  255  img  np  full  240  240  np  uint8  cv2  namedWindow  image  cv2  setMouseCallback  image  draw  circle  while  cv2  imshow  image  img  cv2  waitKey  0xFF  if  ord  break  cv2  destroyAllWindows  gray  image  cv2  cvtColor  img  cv2  COLOR  BGR2GRAY  dimg  cv2  resize  gray  image  28  28  interpolation  cv2  INTER  AREA  testx  dimg  astype  np  float32  plt  imshow  dimg  cmap  gray  ,microsoft
S3  bucket  and  prefix  bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  seq2seq  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  from  time  import  gmtime  strftime  import  time  import  numpy  as  np  import  os  import  json  For  plotting  attention  matrix  later  on  import  matplotlib  matplotlib  inline  import  matplotlib  pyplot  as  plt  bash  wget  http  data  statmt  org  wmt17  translation  task  preprocessed  de  en  corpus  tc  de  gz  wget  http  data  statmt  org  wmt17  translation  task  preprocessed  de  en  corpus  tc  en  gz  wait  gunzip  corpus  tc  de  gz  gunzip  corpus  tc  en  gz  wait  mkdir  validation  curl  http  data  statmt  org  wmt17  translation  task  preprocessed  de  en  dev  tgz  tar  xvzf  validation  head  10000  corpus  tc  en  corpus  tc  en  small  head  10000  corpus  tc  de  corpus  tc  de  small  bash  python3  create  vocab  proto  py  time  bash  python3  create  vocab  proto  py  train  ,amazon
source  corpus  tc  en  small  train  target  corpus  tc  de  small  val  source  validation  newstest2014  tc  en  val  target  validation  newstest2014  tc  dedef  upload  to  s3  bucket  prefix  channel  file  s3  boto3  resource  s3  data  open  file  rb  key  prefix  channel  file  s3  Bucket  bucket  put  object  Key  key  Body  data  upload  to  s3  bucket  prefix  train  train  rec  upload  to  s3  bucket  prefix  validation  val  rec  upload  to  s3  bucket  prefix  vocab  vocab  src  json  upload  to  s3  bucket  prefix  vocab  vocab  trg  json  region  name  boto3  Session  region  namecontainers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  seq2seq  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  seq2seq  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  seq2seq  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  seq2seq  latest  container  containers  region  name  print  Using  SageMaker  Seq2Seq  container  format  container ,amazon
 region  name  job  name  DEMO  seq2seq  en  de  strftime  gmtime  print  Training  job  job  name  create  training  params  AlgorithmSpecification  TrainingImage  container  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  s3  format  bucket  prefix  ResourceConfig  Seq2Seq  does  not  support  multiple  machines  Currently  it  only  supports  single  machine  multiple  GPUs  InstanceCount  InstanceType  ml  p2  xlarge  We  suggest  one  of  ml  p2  16xlarge  ml  p2  8xlarge  ml  p2  xlarge  VolumeSizeInGB  50  TrainingJobName  job  name  HyperParameters  Please  refer  to  the  documentation  for  complete  list  of  parameters  max  seq  len  source  60  max  seq  len  target  60  optimized  metric  bleu  batch  size  64  Please  use  larger  batch  size  256  or  512  if  using  ml  p2  8xlarge  or  ml  p2  16xlarge  checkpoint  frequency  num  batches  1000  rnn  num  hidden  512  num  layers  encoder  num  layers  decoder  num  embed  source  512  num  embed  target  512  checkp,amazon
oint  threshold  max  num  batches  2100  Training  will  stop  after  2100  iterations  batches  This  is  just  for  demo  purposes  Remove  the  above  parameter  if  you  want  better  model  StoppingCondition  MaxRuntimeInSeconds  48  3600  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  S3DataDistributionType  FullyReplicated  ChannelName  vocab  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  vocab  format  bucket  prefix  S3DataDistributionType  FullyReplicated  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  prefix  S3DataDistributionType  FullyReplicated  sagemaker  client  boto3  Session  client  service  name  sagemaker  sagemaker  client  create  training  job  create  training  params  status  sagemaker  client  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  status  status  sagemaker  client  describe  training  ,amazon
job  TrainingJobName  job  name  TrainingJobStatus  print  status  if  the  job  failed  determine  why  if  status  Failed  message  sagemaker  client  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  use  pretrained  model  False  use  pretrained  model  True  model  name  DEMO  pretrained  en  de  model  curl  https  s3  us  west  amazonaws  com  gsaur  seq2seq  data  seq2seq  eng  german  full  nb  translation  eng  german  p2  16x  2017  11  24  22  25  53  output  model  tar  gz  model  tar  gz  curl  https  s3  us  west  amazonaws  com  gsaur  seq2seq  data  seq2seq  eng  german  full  nb  translation  eng  german  p2  16x  2017  11  24  22  25  53  output  vocab  src  json  vocab  src  json  curl  https  s3  us  west  amazonaws  com  gsaur  seq2seq  data  seq2seq  eng  german  full  nb  translation  eng  german  p2  16x  2017  11  24  22  25  53  output  vocab  trg  json  ,amazon
vocab  trg  json  upload  to  s3  bucket  prefix  pretrained  model  model  tar  gz  model  data  s3  pretrained  model  model  tar  gz  format  bucket  prefix  time  sage  boto3  client  sagemaker  if  not  use  pretrained  model  info  sage  describe  training  job  TrainingJobName  job  name  model  name  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  name  print  model  data  primary  container  Image  container  ModelDataUrl  model  data  create  model  response  sage  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  endpoint  config  name  DEMO  Seq2SeqEndpointConfig  strftime  gmtime  print  endpoint  config  name  create  endpoint  config  response  sage  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic,amazon
  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  import  time  endpoint  name  DEMO  Seq2SeqEndpoint  strftime  gmtime  print  endpoint  name  create  endpoint  response  sage  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  print  create  endpoint  response  EndpointArn  resp  sage  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  wait  until  the  status  has  changed  sage  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  print  the  status  of  the  endpoint  endpoint  response  sage  describe  endpoint  EndpointName  endpoint  name  status  endpoint  response  EndpointStatus  print  Endpoint  creation  ended  with  EndpointStatus  format  status  if  status  InService  raise  Exception  Endpoint  creation  failed  runtime  boto3  client  service  name  runtime  sagemaker  sentences  you  are  so  good  can  you  drive  car  want  to  watch  m,amazon
ovie  payload  instances  for  sent  in  sentences  payload  instances  append  data  sent  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  json  Body  json  dumps  payload  response  response  Body  read  decode  utf  response  json  loads  response  print  response  sentence  can  you  drive  car  payload  instances  data  sentence  configuration  attention  matrix  true  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  json  Body  json  dumps  payload  response  response  Body  read  decode  utf  response  json  loads  response  predictions  source  sentence  target  response  target  attention  matrix  np  array  response  matrix  print  Source  nTarget  source  target  Define  function  for  plotting  the  attentioan  matrix  def  plot  matrix  attention  matrix  target  source  source  tokens  source  split  target  tokens  target  split  assert  attention  matrix  shape  len  target  tokens  plt  imshow  attention ,amazon
 matrix  transpose  interpolation  nearest  cmap  Greys  plt  xlabel  target  plt  ylabel  source  plt  gca  set  xticks  for  in  range  len  target  tokens  plt  gca  set  yticks  for  in  range  len  source  tokens  plt  gca  set  xticklabels  target  tokens  plt  gca  set  yticklabels  source  tokens  plt  tight  layout  plot  matrix  attention  matrix  target  source  import  io  import  tempfile  from  record  pb2  import  Record  from  create  vocab  proto  import  vocab  from  json  reverse  vocab  write  recordio  list  to  record  bytes  read  next  source  vocab  from  json  vocab  src  json  target  vocab  from  json  vocab  trg  json  source  rev  reverse  vocab  source  target  rev  reverse  vocab  target  sentences  this  is  so  cool  am  having  dinner  am  sitting  in  an  aeroplane  come  let  us  go  for  long  drive  Convert  strings  to  integers  using  source  vocab  mapping  Out  of  vocabulary  strings  are  mapped  to  the  mapping  for  unk  sentences  source  get  token  for  toke,amazon
n  in  sentence  split  for  sentence  in  sentences  io  BytesIO  for  sentence  in  sentences  record  list  to  record  bytes  sentence  write  recordio  record  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  recordio  protobuf  Body  getvalue  response  response  Body  read  def  parse  proto  response  received  bytes  output  file  tempfile  NamedTemporaryFile  output  file  write  received  bytes  output  file  flush  target  sentences  with  open  output  file  name  rb  as  datum  next  record  True  while  next  record  next  record  read  next  datum  if  next  record  rec  Record  rec  ParseFromString  next  record  target  list  rec  features  target  int32  tensor  values  target  sentences  append  target  else  break  return  target  sentencestargets  parse  proto  response  response  resp  join  target  rev  get  token  unk  for  token  in  sentence  for  sentence  in  targets  print  resp  sage  delete  endpoint  EndpointName  endpoint  name  ,amazon
import  mxnet  as  mx  from  movielens  data  import  get  data  iter  max  id  from  matrix  fact  import  train  import  matplotlib  pyplot  as  plt  If  MXNet  is  not  compiled  with  GPU  support  on  OSX  set  to  mx  cpu  Can  be  changed  to  mx  gpu  mx  gpu  mx  gpu  if  there  are  GPUs  ctx  mx  cpu  mx  gpu  train  test  data  get  data  iter  batch  size  100  max  user  max  item  max  id  ml  100k  data  max  user  max  item  def  plain  net  input  user  mx  symbol  Variable  user  item  mx  symbol  Variable  item  score  mx  symbol  Variable  score  user  feature  lookup  user  mx  symbol  Embedding  data  user  input  dim  max  user  output  dim  item  feature  lookup  item  mx  symbol  Embedding  data  item  input  dim  max  item  output  dim  predict  by  the  inner  product  which  is  elementwise  product  and  then  sum  pred  user  item  pred  mx  symbol  sum  axis  data  pred  axis  pred  mx  symbol  Flatten  data  pred  loss  layer  pred  mx  symbol  LinearRegressionOutput  data  pr,amazon
ed  label  score  return  pred  net1  plain  net  64  mx  viz  plot  network  net1  results1  train  net1  train  test  data  num  epoch  25  learning  rate  001  optimizer  adam  ctx  ctx  plt  plot  results1  label  Linear  MF  plt  ylabel  RMSE  plt  xlabel  iterations  per  500  mini  batch  plt  title  RMSE  History  plt  legend  bbox  to  anchor  05  loc  borderaxespad  plt  show  def  get  one  layer  mlp  hidden  input  user  mx  symbol  Variable  user  item  mx  symbol  Variable  item  score  mx  symbol  Variable  score  user  latent  features  user  mx  symbol  Embedding  data  user  input  dim  max  user  output  dim  user  mx  symbol  Activation  data  user  act  type  relu  user  mx  symbol  FullyConnected  data  user  num  hidden  hidden  item  latent  features  item  mx  symbol  Embedding  data  item  input  dim  max  item  output  dim  item  mx  symbol  Activation  data  item  act  type  relu  item  mx  symbol  FullyConnected  data  item  num  hidden  hidden  predict  by  the  inner  product  ,amazon
pred  user  item  pred  mx  symbol  sum  axis  data  pred  axis  pred  mx  symbol  Flatten  data  pred  loss  layer  pred  mx  symbol  LinearRegressionOutput  data  pred  label  score  return  pred  net2  get  one  layer  mlp  64  64  mx  viz  plot  network  net2  results2  train  net2  train  test  data  num  epoch  20  learning  rate  001  optimizer  adam  ctx  ctx  plt  plot  results1  label  Linear  MF  plt  plot  results2  label  Non  Linear  MF  plt  ylabel  RMSE  plt  xlabel  iterations  per  500  mini  batch  plt  title  RMSE  History  plt  legend  bbox  to  anchor  05  loc  borderaxespad  plt  show  def  get  multi  layer  dropout  resnet  hidden  input  user  mx  symbol  Variable  user  item  mx  symbol  Variable  item  score  mx  symbol  Variable  score  user  latent  features  user1  mx  symbol  Embedding  data  user  input  dim  max  user  output  dim  user  mx  symbol  FullyConnected  data  user1  num  hidden  hidden  user  mx  symbol  Activation  data  user  act  type  relu  user  mx  symbol  D,amazon
ropout  data  user  user  mx  symbol  FullyConnected  data  user  num  hidden  hidden  user2  user  user1  user2  mx  symbol  Dropout  data  user2  user  mx  symbol  FullyConnected  data  user2  num  hidden  hidden  user  mx  symbol  Activation  data  user  act  type  relu  user  mx  symbol  Dropout  data  user  user  mx  symbol  FullyConnected  data  user  num  hidden  hidden  user  user  user2  item  latent  features  item1  mx  symbol  Embedding  data  item  input  dim  max  item  output  dim  item  mx  symbol  FullyConnected  data  item1  num  hidden  hidden  item  mx  symbol  Activation  data  item  act  type  relu  item  mx  symbol  Dropout  data  item  item  mx  symbol  FullyConnected  data  item  num  hidden  hidden  item2  item  item1  item2  mx  symbol  Dropout  data  item2  item  mx  symbol  FullyConnected  data  item2  num  hidden  hidden  item  mx  symbol  Activation  data  item  act  type  relu  item  mx  symbol  Dropout  data  item  item  mx  symbol  FullyConnected  data  item  num  hidden  hid,amazon
den  item  item  item2  predict  by  the  inner  product  pred  user  item  pred  mx  symbol  sum  axis  data  pred  axis  pred  mx  symbol  Flatten  data  pred  loss  layer  pred  mx  symbol  LinearRegressionOutput  data  pred  label  score  return  pred  net3  get  multi  layer  dropout  resnet  64  64  mx  viz  plot  network  net3  Larger  batch  size  makes  GPU  more  efficient  for  this  complex  model  train  test  data2  get  data  iter  batch  size  200  results3  train  net3  train  test  data2  num  epoch  25  learning  rate  001  optimizer  adam  ctx  ctx  import  bokeh  import  bokeh  io  import  bokeh  plotting  bokeh  io  output  notebook  import  pandas  as  pd  def  viz  lines  fig  results  legend  color  df  pd  DataFrame  results  data  eval  fig  line  df  elapsed  df  RMSE  color  color  legend  legend  line  width  df  pd  DataFrame  results  data  train  fig  line  df  elapsed  df  RMSE  color  color  line  dash  dotted  alpha  fig  bokeh  plotting  Figure  axis  type  datetime  axis ,amazon
 label  Training  time  axis  label  RMSE  viz  lines  fig  results1  Linear  MF  orange  viz  lines  fig  results2  MLP  blue  viz  lines  fig  results3  ResNet  red  bokeh  io  show  fig  import  bokeh  print  bokeh  version  ,amazon
matplotlib  inline  import  os  import  numpy  as  np  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  from  sklearn  datasets  import  load  boston  import  sklearn  model  selectionimport  sagemaker  from  sagemaker  import  get  execution  role  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  from  sagemaker  predictor  import  csv  serializer  This  is  an  object  that  represents  the  SageMaker  session  that  we  are  currently  operating  in  This  object  contains  some  useful  information  that  we  will  need  to  access  later  such  as  our  region  session  sagemaker  Session  This  is  an  object  that  represents  the  IAM  role  that  we  are  currently  assigned  When  we  construct  and  launch  the  training  job  later  we  will  need  to  tell  it  what  IAM  role  it  should  have  Since  our  use  case  is  relatively  simple  we  will  simply  assign  the  training  job  the  role  we  currently  have  role  get  execution  role  boston  load  bosto,amazon
n  First  we  package  up  the  input  data  and  the  target  variable  the  median  value  as  pandas  dataframes  This  will  make  saving  the  data  to  file  little  easier  later  on  bos  pd  pd  DataFrame  boston  data  columns  boston  feature  names  bos  pd  pd  DataFrame  boston  target  We  split  the  dataset  into  training  and  testing  sets  train  test  train  test  sklearn  model  selection  train  test  split  bos  pd  bos  pd  test  size  33  Then  we  split  the  training  set  further  into  training  and  validation  sets  train  val  train  val  sklearn  model  selection  train  test  split  train  train  test  size  33  This  is  our  local  data  directory  We  need  to  make  sure  that  it  exists  data  dir  data  boston  if  not  os  path  exists  data  dir  os  makedirs  data  dir  We  use  pandas  to  save  our  test  train  and  validation  data  to  csv  files  Note  that  we  make  sure  not  to  include  header  information  or  an  index  as  this  is  required  by  the,amazon
  built  in  algorithms  provided  by  Amazon  Also  for  the  train  and  validation  data  it  is  assumed  that  the  first  entry  in  each  row  is  the  target  variable  test  to  csv  os  path  join  data  dir  test  csv  header  False  index  False  pd  concat  val  val  axis  to  csv  os  path  join  data  dir  validation  csv  header  False  index  False  pd  concat  train  train  axis  to  csv  os  path  join  data  dir  train  csv  header  False  index  False  prefix  boston  xgboost  HL  test  location  session  upload  data  os  path  join  data  dir  test  csv  key  prefix  prefix  val  location  session  upload  data  os  path  join  data  dir  validation  csv  key  prefix  prefix  train  location  session  upload  data  os  path  join  data  dir  train  csv  key  prefix  prefix  As  stated  above  we  use  this  utility  method  to  construct  the  image  name  for  the  training  container  container  get  image  uri  session  boto  region  name  xgboost  Now  that  we  know  which  contain,amazon
er  to  use  we  can  construct  the  estimator  object  xgb  sagemaker  estimator  Estimator  container  The  name  of  the  training  container  role  The  IAM  role  to  use  our  current  role  in  this  case  train  instance  count  The  number  of  instances  to  use  for  training  train  instance  type  ml  m4  xlarge  The  type  of  instance  ot  use  for  training  output  path  s3  output  format  session  default  bucket  prefix  Where  to  save  the  output  the  model  artifacts  sagemaker  session  session  The  current  SageMaker  sessionxgb  set  hyperparameters  max  depth  eta  gamma  min  child  weight  subsample  objective  reg  linear  early  stopping  rounds  10  num  round  200  This  is  wrapper  around  the  location  of  our  train  and  validation  data  to  make  sure  that  SageMaker  knows  our  data  is  in  csv  format  s3  input  train  sagemaker  s3  input  s3  data  train  location  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  val  location  co,amazon
ntent  type  csv  xgb  fit  train  s3  input  train  validation  s3  input  validation  xgb  transformer  xgb  transformer  instance  count  instance  type  ml  m4  xlarge  xgb  transformer  transform  test  location  content  type  text  csv  split  type  Line  xgb  transformer  wait  aws  s3  cp  recursive  xgb  transformer  output  path  data  dirY  pred  pd  read  csv  os  path  join  data  dir  test  csv  out  header  None  plt  scatter  test  pred  plt  xlabel  Median  Price  plt  ylabel  Predicted  Price  plt  title  Median  Price  vs  Predicted  Price  First  we  will  remove  all  of  the  files  contained  in  the  data  dir  directory  rm  data  dir  And  then  we  delete  the  directory  itself  rmdir  data  dir  ,amazon
Cleanup  rm  rf  good  bad  raw  data  rec  data  time  import  boto3  import  re  from  sagemaker  import  get  execution  role  import  urllib  request  role  get  execution  role  bucket  fx  sagemaker  customize  to  your  bucket  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  image  classification  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  image  classification  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  image  classification  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  image  classification  latest  training  image  containers  boto3  Session  region  name  bucket  prefix  demo  syd  Load  and  convert  image  data  set  to  rec  aws  s3  sync  s3  fx  sagemaker  demo  syd  raw  data  raw  data  python  home  ec2  user  anaconda3  envs  mxnet  p27  lib  python2  site  packages  mxnet  tools  im2rec  py  data  raw  data  list  recursive  train  ratio  75  exts  jpg  python  home  ec2  user  anaconda3  envs,amazon
  mxnet  p27  lib  python2  site  packages  mxnet  tools  im2rec  py  data  raw  data  resize  227  num  thread  16  ls  mkdir  rec  data  train  mkdir  rec  data  validate  mv  data  train  rec  data  train  mv  data  val  rec  data  validate  aws  s3  sync  rec  data  s3  fx  sagemaker  demo  syd  rec  data  wc  rec  data  train  data  train  lst  The  algorithm  supports  multiple  network  depth  number  of  layers  They  are  18  34  50  101  152  and  200  For  this  training  we  will  use  18  layers  num  layers  18  image  shape  227  227  num  training  samples  281  num  classes  mini  batch  size  epochs  10  learning  rate  005  augmentation  type  crop  color  transform  time  import  time  import  boto3  from  time  import  gmtime  strftime  s3  boto3  client  s3  create  unique  job  name  job  name  prefix  DEMO  imageclassification  timestamp  time  strftime  time  gmtime  job  name  job  name  prefix  timestamp  training  params  specify  the  training  docker  image  AlgorithmSpecificatio,amazon
n  TrainingImage  training  image  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  s3  output  format  bucket  job  name  prefix  ResourceConfig  InstanceCount  InstanceType  ml  p3  2xlarge  VolumeSizeInGB  50  TrainingJobName  job  name  HyperParameters  image  shape  image  shape  num  layers  str  num  layers  num  training  samples  str  num  training  samples  num  classes  str  num  classes  mini  batch  size  str  mini  batch  size  epochs  str  epochs  learning  rate  str  learning  rate  augmentation  type  augmentation  type  StoppingCondition  MaxRuntimeInSeconds  360000  Training  data  should  be  inside  subdirectory  called  train  Validation  data  should  be  inside  subdirectory  called  validation  The  algorithm  currently  only  supports  fullyreplicated  model  where  data  is  copied  onto  each  machine  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  rec  data  train  format  bucket  bucket  prefix  S3DataDistrib,amazon
utionType  FullyReplicated  ContentType  application  recordio  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  rec  data  validate  format  bucket  bucket  prefix  S3DataDistributionType  FullyReplicated  ContentType  application  recordio  CompressionType  None  print  Training  job  name  format  job  name  print  nInput  Data  Location  format  training  params  InputDataConfig  DataSource  S3DataSource  create  the  Amazon  SageMaker  training  job  sagemaker  boto3  client  service  name  sagemaker  sagemaker  create  training  job  training  params  confirm  that  the  training  job  has  started  status  sagemaker  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  Training  job  current  status  format  status  try  wait  for  the  job  to  finish  and  report  the  ending  status  sagemaker  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  training  info  sagemaker  describe  t,amazon
raining  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  except  print  Training  failed  to  start  if  exception  is  raised  that  means  it  has  failed  message  sagemaker  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  job  name  DEMO  imageclassification  2018  03  26  22  07  58  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  status  status  time  import  boto3  from  time  import  gmtime  strftime  sage  boto3  Session  client  service  name  sagemaker  model  name  DEMO  full  image  classification  model  timestamp  print  model  name  info  sage  describe  training  job  TrainingJobName  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  data  containers  us  west  433757028032  dkr  ecr  us  west  amazonaw,amazon
s  com  image  classification  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  image  classification  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  image  classification  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  image  classification  latest  hosting  image  containers  boto3  Session  region  name  primary  container  Image  hosting  image  ModelDataUrl  model  data  create  model  response  sage  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  timestamp  time  strftime  time  gmtime  endpoint  config  name  job  name  prefix  epc  timestamp  endpoint  config  response  sage  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  configuration  name  format  e,amazon
ndpoint  config  name  print  Endpoint  configuration  arn  format  endpoint  config  response  EndpointConfigArn  time  import  time  timestamp  time  strftime  time  gmtime  endpoint  name  job  name  prefix  ep  timestamp  print  Endpoint  name  format  endpoint  name  endpoint  params  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  endpoint  response  sagemaker  create  endpoint  endpoint  params  print  EndpointArn  format  endpoint  response  EndpointArn  get  the  status  of  the  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  response  EndpointStatus  print  EndpointStatus  format  status  wait  until  the  status  has  changed  sagemaker  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  print  the  status  of  the  endpoint  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  endpoint  response  EndpointStatus  print  Endpoint  creation  ended  with  EndpointStatus  format ,amazon
 status  if  status  InService  raise  Exception  Endpoint  creation  failed  print  the  status  of  the  endpoint  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  endpoint  response  EndpointStatus  print  Endpoint  creation  ended  with  EndpointStatus  format  status  if  status  InService  raise  Exception  Endpoint  creation  failed  import  boto3  runtime  boto3  Session  client  service  name  runtime  sagemaker  wget  tmp  test  jpg  https  encrypted  tbn0  gstatic  com  images  tbn  ANd9GcQzyrbQKSeKAVlVYLERgbl  eHnajtmpENgiY4qpwkJl1yOcG7YqLQ  file  name  tmp  test  jpg  test  image  from  IPython  display  import  Image  Image  file  name  import  json  import  numpy  as  np  with  open  file  name  rb  as  payload  read  payload  bytearray  payload  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  image  Body  payload  result  response  Body  read  result  will  be  in  json  format  and  convert  it  to  ndarray  ,amazon
result  json  loads  result  the  result  will  output  the  probabilities  for  all  classes  find  the  class  with  maximum  probability  and  print  the  class  index  index  np  argmax  result  object  categories  bad  good  none  print  Result  label  object  categories  index  probability  str  result  index  sage  delete  endpoint  EndpointName  endpoint  name  ,amazon
bucket  your  s3  bucket  name  here  prefix  sagemaker  movielens  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  wget  http  files  grouplens  org  datasets  movielens  ml  100k  zip  unzip  ml  100k  zip  cd  ml  100k  shuf  ua  base  ua  base  shuffled  head  10  ua  base  shuffled  head  10  ua  testimport  pandas  as  pd  data  pd  read  csv  ua  base  shuffled  sep  header  None  names  userid  movieid  rating  timestamp  nbUsers  data  userid  max  nbMovies  data  movieid  max  nbFeatures  nbUsers  nbMovies  print  Number  of  Users  nbUsers  print  Number  of  Movies  nbMovies  print  Number  of  Features  nbFeatures  movies  by  user  data  groupby  userid  res  movies  by  user  movieid  count  hist  bins  100  res  set  title  Rating  count  distribution  resdata  rating  hist  nbRatingsTrain  90570  nbRatingsTest  9430import  csv  import  numpy  as  np  from  scipy  sparse  import  lil  matrix  For  each  user  build  list ,amazon
 of  rated  movies  We  need  this  to  add  random  negative  samples  moviesByUser  for  userId  in  range  nbUsers  moviesByUser  str  userId  with  open  ua  base  shuffled  as  samples  csv  reader  delimiter  for  userId  movieId  rating  timestamp  in  samples  moviesByUser  str  int  userId  append  int  movieId  def  loadDataset  filename  lines  columns  Features  are  one  hot  encoded  in  sparse  matrix  lil  matrix  lines  columns  astype  float32  Labels  are  stored  in  vector  line  with  open  filename  as  samples  csv  reader  delimiter  for  userId  movieId  rating  timestamp  in  samples  line  int  userId  line  int  nbUsers  int  movieId  if  int  rating  append  else  append  line  line  np  array  astype  float32  return  train  train  loadDataset  ua  base  shuffled  nbRatingsTrain  nbFeatures  test  test  loadDataset  ua  test  nbRatingsTest  nbFeatures  print  train  shape  print  train  shape  assert  train  shape  nbRatingsTrain  nbFeatures  assert  train  shape  nbRatingsTrain,amazon
  zero  labels  np  count  nonzero  train  print  Training  labels  zeros  ones  zero  labels  nbRatingsTrain  zero  labels  print  test  shape  print  test  shape  assert  test  shape  nbRatingsTest  nbFeatures  assert  test  shape  nbRatingsTest  zero  labels  np  count  nonzero  test  print  Test  labels  zeros  ones  zero  labels  nbRatingsTest  zero  labels  train  key  train  protobuf  train  prefix  format  prefix  train  test  key  test  protobuf  test  prefix  format  prefix  test  output  location  s3  output  format  bucket  prefix  import  sagemaker  amazon  common  as  smac  import  io  def  writeDatasetToProtobuf  bucket  prefix  key  buf  io  BytesIO  smac  write  spmatrix  to  sparse  tensor  buf  buf  seek  obj  format  prefix  key  boto3  resource  s3  Bucket  bucket  Object  obj  upload  fileobj  buf  return  s3  format  bucket  obj  train  data  writeDatasetToProtobuf  train  train  bucket  train  prefix  train  key  test  data  writeDatasetToProtobuf  test  test  bucket  test  prefix  tes,amazon
t  key  print  uploaded  training  data  location  format  train  data  print  uploaded  test  data  location  format  test  data  print  training  artifacts  will  be  uploaded  to  format  output  location  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  boto3  Session  region  name  factorization  machines  import  sagemaker  sess  sagemaker  Session  fm  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  c5  xlarge  output  path  output  location  sagemaker  session  sess  fm  set  hyperparameters  feature  dim  nbFeatures  predictor  type  binary  classifier  mini  batch  size  1000  num  factors  64  epochs  100  fm  fit  train  train  data  test  test  data  fm  predictor  fm  deploy  initial  instance  count  instance  type  ml  c5  xlarge  import  json  from  sagemaker  predictor  import  json  deserializer  def  fm  serializer  data  js  instances  for  row  in  data  js  instances  append  features  r,amazon
ow  tolist  return  json  dumps  js  fm  predictor  content  type  application  json  fm  predictor  serializer  fm  serializer  fm  predictor  deserializer  json  deserializerprediction  test  1000  toarray  result  fm  predictor  predict  prediction  print  test  1000  print  result  import  numpy  as  np  predictions  for  array  in  np  array  split  test  1000  1100  toarray  result  fm  predictor  predict  array  predictions  predicted  label  for  in  result  predictions  predictions  np  array  predictions  import  pandas  as  pd  pd  crosstab  test  1000  1100  predictions  rownames  actuals  colnames  predictions  import  sagemaker  sagemaker  Session  delete  endpoint  fm  predictor  endpoint  ,amazon
time  import  os  import  boto3  import  re  import  copy  import  time  from  time  import  gmtime  strftime  from  sagemaker  import  get  execution  role  role  get  execution  role  region  boto3  Session  region  name  bucket  bucket  name  put  your  s3  bucket  name  here  and  create  s3  bucket  prefix  sagemaker  DEMO  xgboost  multiclass  classification  customize  to  your  bucket  where  you  have  stored  the  data  bucket  path  https  s3  amazonaws  com  format  region  bucket  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  gzip  open  mnist  pkl  gz  rb  train  set  valid  set  test  set  pickle  load  encoding  latin1  close  time  import  struct  import  io  import  boto3  def  to  libsvm  labels  values  write  bytes  join  format  label  join  format  el  for  el  in  enumerate  vec  for  label  vec  in  zip  labels  values  utf  return  def  write  to  s3  fo,amazon
bj  bucket  key  return  boto3  Session  resource  s3  Bucket  bucket  Object  key  upload  fileobj  fobj  def  get  dataset  import  pickle  import  gzip  with  gzip  open  mnist  pkl  gz  rb  as  pickle  Unpickler  encoding  latin1  return  load  def  upload  to  s3  partition  name  partition  labels  tolist  for  in  partition  vectors  tolist  for  in  partition  num  partition  partition  file  into  parts  partition  bound  int  len  labels  num  partition  for  in  range  num  partition  io  BytesIO  to  libsvm  labels  partition  bound  partition  bound  vectors  partition  bound  partition  bound  seek  key  examples  format  prefix  partition  name  str  url  s3n  format  bucket  key  print  Writing  to  format  url  write  to  s3  bucket  key  print  Done  writing  to  format  url  def  download  from  s3  partition  name  number  filename  key  examples  format  prefix  partition  name  number  url  s3n  format  bucket  key  print  Reading  from  format  url  s3  boto3  resource  s3  s3  Bucket  ,amazon
bucket  download  file  key  filename  try  s3  Bucket  bucket  download  file  key  mnist  local  test  except  botocore  exceptions  ClientError  as  if  response  Error  Code  404  print  The  object  does  not  exist  at  format  url  else  raise  def  convert  data  train  set  valid  set  test  set  get  dataset  partitions  train  train  set  validation  valid  set  test  test  set  for  partition  name  partition  in  partitions  print  format  partition  name  partition  shape  partition  shape  upload  to  s3  partition  name  partition  time  convert  data  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  xgboost  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  xgboost  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  xgboost  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  xgboost  latest  container  containers  boto3  Session  region  name  Ensure  that  the  train  and  validation  data  folders  generated  above  ,amazon
are  reflected  in  the  InputDataConfig  parameter  below  common  training  params  AlgorithmSpecification  TrainingImage  container  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  bucket  path  prefix  xgboost  ResourceConfig  InstanceCount  InstanceType  ml  m4  10xlarge  VolumeSizeInGB  HyperParameters  max  depth  eta  gamma  min  child  weight  silent  objective  multi  softmax  num  class  10  num  round  10  StoppingCondition  MaxRuntimeInSeconds  86400  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  bucket  path  prefix  train  S3DataDistributionType  FullyReplicated  ContentType  libsvm  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  bucket  path  prefix  validation  S3DataDistributionType  FullyReplicated  ContentType  libsvm  CompressionType  None  single  machine  job  params  single  machine  job  name  DEMO  xgboost  classification  strftime  gmtime  print  Job  name  i,amazon
s  single  machine  job  name  single  machine  job  params  copy  deepcopy  common  training  params  single  machine  job  params  TrainingJobName  single  machine  job  name  single  machine  job  params  OutputDataConfig  S3OutputPath  bucket  path  prefix  xgboost  single  single  machine  job  params  ResourceConfig  InstanceCount  distributed  job  params  distributed  job  name  DEMO  xgboost  distrib  classification  strftime  gmtime  print  Job  name  is  distributed  job  name  distributed  job  params  copy  deepcopy  common  training  params  distributed  job  params  TrainingJobName  distributed  job  name  distributed  job  params  OutputDataConfig  S3OutputPath  bucket  path  prefix  xgboost  distributed  number  of  instances  used  for  training  distributed  job  params  ResourceConfig  InstanceCount  no  more  than  if  there  are  total  partition  files  generated  above  data  distribution  type  for  train  channel  distributed  job  params  InputDataConfig  DataSource  S3DataSource  S,amazon
3DataDistributionType  ShardedByS3Key  data  distribution  type  for  validation  channel  distributed  job  params  InputDataConfig  DataSource  S3DataSource  S3DataDistributionType  ShardedByS3Key  time  region  boto3  Session  region  name  sm  boto3  Session  client  sagemaker  sm  create  training  job  single  machine  job  params  sm  create  training  job  distributed  job  params  status  sm  describe  training  job  TrainingJobName  distributed  job  name  TrainingJobStatus  print  status  sm  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  distributed  job  name  status  sm  describe  training  job  TrainingJobName  distributed  job  name  TrainingJobStatus  print  Training  job  ended  with  status  status  if  status  Failed  message  sm  describe  training  job  TrainingJobName  distributed  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  print  Single  Machine  sm  describe  trai,amazon
ning  job  TrainingJobName  single  machine  job  name  TrainingJobStatus  print  Distributed  sm  describe  training  job  TrainingJobName  distributed  job  name  TrainingJobStatus  time  import  boto3  from  time  import  gmtime  strftime  model  name  distributed  job  name  mod  print  model  name  info  sm  describe  training  job  TrainingJobName  distributed  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  data  primary  container  Image  container  ModelDataUrl  model  data  create  model  response  sm  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  endpoint  config  name  DEMO  XGBoostEndpointConfig  strftime  gmtime  print  endpoint  config  name  create  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialVariantWeight  InitialInst,amazon
anceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  import  time  endpoint  name  DEMO  XGBoostEndpoint  strftime  gmtime  print  endpoint  name  create  endpoint  response  sm  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  print  create  endpoint  response  EndpointArn  resp  sm  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  while  status  Creating  time  sleep  60  resp  sm  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  print  Arn  resp  EndpointArn  print  Status  status  runtime  client  boto3  client  runtime  sagemaker  download  from  s3  test  mnist  local  test  reading  the  first  part  file  within  test  head  mnist  local  test  mnist  single  test  time  import  json  file  name  mnist  single  test  customize  to  your  test  file  mnist  ,amazon
single  test  if  use  the  data  above  with  open  file  name  as  payload  read  response  runtime  client  invoke  endpoint  EndpointName  endpoint  name  ContentType  text  libsvm  Body  payload  result  response  Body  read  decode  ascii  print  Predicted  label  is  format  result  import  sys  def  do  predict  data  endpoint  name  content  type  payload  join  data  response  runtime  client  invoke  endpoint  EndpointName  endpoint  name  ContentType  content  type  Body  payload  result  response  Body  read  decode  ascii  preds  float  num  for  num  in  result  split  return  preds  def  batch  predict  data  batch  size  endpoint  name  content  type  items  len  data  arrs  for  offset  in  range  items  batch  size  arrs  extend  do  predict  data  offset  min  offset  batch  size  items  endpoint  name  content  type  sys  stdout  write  return  arrs  time  import  json  file  name  mnist  local  test  with  open  file  name  as  payload  read  strip  labels  float  line  split  for  line ,amazon
 in  payload  split  test  data  payload  split  preds  batch  predict  test  data  100  endpoint  name  text  libsvm  print  nerror  rate  sum  for  in  range  len  preds  if  preds  labels  float  len  preds  preds  10  labels  10  import  numpy  def  error  rate  predictions  labels  Return  the  error  rate  and  confusions  correct  numpy  sum  predictions  labels  total  predictions  shape  error  100  100  float  correct  float  total  confusions  numpy  zeros  10  10  numpy  int32  bundled  zip  predictions  labels  for  predicted  actual  in  bundled  confusions  int  predicted  int  actual  return  error  confusionsimport  matplotlib  pyplot  as  plt  matplotlib  inline  NUM  LABELS  10  change  it  according  to  num  class  in  your  dataset  test  error  confusions  error  rate  numpy  asarray  preds  numpy  asarray  labels  print  Test  error  1f  test  error  plt  xlabel  Actual  plt  ylabel  Predicted  plt  grid  False  plt  xticks  numpy  arange  NUM  LABELS  plt  yticks  numpy  arange  NUM  ,amazon
LABELS  plt  imshow  confusions  cmap  plt  cm  jet  interpolation  nearest  for  cas  in  enumerate  confusions  for  count  in  enumerate  cas  if  count  xoff  07  len  str  count  plt  text  xoff  int  count  fontsize  color  white  sm  delete  endpoint  EndpointName  endpoint  name  ,amazon
import  boto3  import  codecs  import  os  from  contextlib  import  closingdef  aws  speech  file  name  text  file  file  name  txt  output  file  file  name  mp3  codecs  open  text  file  encoding  utf  text  for  line  in  text  line  client  boto3  client  polly  eu  west  response  client  synthesize  speech  OutputFormat  mp3  Text  text  VoiceId  Amy  if  AudioStream  in  response  with  closing  response  AudioStream  as  stream  output  output  file  try  with  open  output  wb  as  file  file  write  stream  read  except  IOError  as  error  print  error  return  response  def  get  text  file  name  text  file  file  name  txt  text  codecs  open  text  file  encoding  utf  for  line  in  text  line  close  return  textfile  name  the  raven  text  get  text  file  name  print  text  response  aws  speech  file  name  os  startfile  file  name  mp3  ,microsoft
matplotlib  inline  import  sys  from  urllib  request  import  urlretrieve  import  zipfile  from  dateutil  parser  import  parse  import  json  from  random  import  shuffle  import  random  import  datetime  import  os  import  boto3  import  s3fs  import  sagemaker  import  numpy  as  np  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  from  future  import  print  function  from  ipywidgets  import  interact  interactive  fixed  interact  manual  import  ipywidgets  as  widgets  from  ipywidgets  import  IntSlider  FloatSlider  Checkbox  set  random  seeds  for  reproducibility  np  random  seed  42  random  seed  42  sagemaker  session  sagemaker  Session  s3  bucket  logisticsdemo  bucket  replace  with  an  existing  bucket  if  needed  s3  prefix  data  prefix  used  for  all  data  stored  within  the  bucket  role  sagemaker  get  execution  role  IAM  role  to  use  by  SageMakerregion  sagemaker  session  boto  region  name  s3  data  path  s3  raw  format  s3  bucket  s3  prefix  s3,amazon
  output  path  s3  output  format  s3  bucket  s3  prefix  image  name  sagemaker  amazon  amazon  estimator  get  image  uri  region  forecasting  deepar  latest  DATA  HOST  https  archive  ics  uci  edu  DATA  PATH  ml  machine  learning  databases  00321  ARCHIVE  NAME  LD2011  2014  txt  zip  FILE  NAME  ARCHIVE  NAME  aws  s3  sync  s3  logisticsdemo  bucket  data  raw  lsFILE  NAME  sales  train  csv  gz  data  pd  read  csv  FILE  NAME  compression  gzip  sep  index  col  parse  dates  True  decimal  num  timeseries  data  shape  data  kw  data  resample  2H  sum  timeseries  for  in  range  num  timeseries  timeseries  append  np  trim  zeros  data  kw  iloc  trim  data  groupby  shop  id  item  id  date  block  num  as  index  False  agg  item  cnt  day  sum  item  price  mean  fig  axs  plt  subplots  figsize  20  20  sharex  True  axx  axs  ravel  for  in  range  10  timeseries  loc  2014  01  01  2014  01  14  plot  ax  axx  axx  set  xlabel  date  axx  set  ylabel  kW  consumption  axx  grid  w,amazon
hich  minor  axis  we  use  hour  frequency  for  the  time  series  freq  2H  we  predict  for  days  prediction  length  12  we  also  use  days  as  context  length  this  is  the  number  of  state  updates  accomplished  before  making  predictions  context  length  12start  dataset  pd  Timestamp  2014  01  01  00  00  00  freq  freq  end  training  pd  Timestamp  2014  09  01  00  00  00  freq  freq  training  data  start  str  start  dataset  target  ts  start  dataset  end  training  tolist  We  use  because  pandas  indexing  includes  the  upper  bound  for  ts  in  timeseries  print  len  training  data  num  test  windows  test  data  start  str  start  dataset  target  ts  start  dataset  end  training  prediction  length  tolist  for  in  range  num  test  windows  for  ts  in  timeseries  print  len  test  data  def  write  dicts  to  file  path  data  with  open  path  wb  as  fp  for  in  data  fp  write  json  dumps  encode  utf  fp  write  encode  utf  time  write  dicts  to  file  train  ,amazon
json  training  data  write  dicts  to  file  test  json  test  data  s3  boto3  resource  s3  def  copy  to  s3  local  file  s3  path  override  False  assert  s3  path  startswith  s3  split  s3  path  split  bucket  split  path  join  split  buk  s3  Bucket  bucket  if  len  list  buk  objects  filter  Prefix  path  if  not  override  print  File  s3  already  exists  nSet  override  to  upload  anyway  format  s3  bucket  s3  path  return  else  print  Overwriting  existing  file  with  open  local  file  rb  as  data  print  Uploading  file  to  format  s3  path  buk  put  object  Key  path  Body  data  time  copy  to  s3  train  json  s3  data  path  train  train  json  copy  to  s3  test  json  s3  data  path  test  test  json  s3filesystem  s3fs  S3FileSystem  with  s3filesystem  open  s3  data  path  train  train  json  rb  as  fp  print  fp  readline  decode  utf  100  estimator  sagemaker  estimator  Estimator  sagemaker  session  sagemaker  session  image  name  image  name  role  role  train  in,amazon
stance  count  train  instance  type  ml  c4  2xlarge  base  job  name  deepar  electricity  demo  output  path  s3  output  path  hyperparameters  time  freq  freq  epochs  400  early  stopping  patience  40  mini  batch  size  64  learning  rate  5E  context  length  str  context  length  prediction  length  str  prediction  length  estimator  set  hyperparameters  hyperparameters  time  data  channels  train  train  format  s3  data  path  test  test  format  s3  data  path  estimator  fit  inputs  data  channels  wait  True  class  DeepARPredictor  sagemaker  predictor  RealTimePredictor  def  init  self  args  kwargs  super  init  args  content  type  sagemaker  content  types  CONTENT  TYPE  JSON  kwargs  def  predict  self  ts  cat  None  dynamic  feat  None  num  samples  100  return  samples  False  quantiles  Requests  the  prediction  of  for  the  time  series  listed  in  ts  each  with  the  optional  corresponding  category  listed  in  cat  ts  pandas  Series  object  the  time  series  to  pr,amazon
edict  cat  integer  the  group  associated  to  the  time  series  default  None  num  samples  integer  number  of  samples  to  compute  at  prediction  time  default  100  return  samples  boolean  indicating  whether  to  include  samples  in  the  response  default  False  quantiles  list  of  strings  specifying  the  quantiles  to  compute  default  Return  value  list  of  pandas  DataFrame  objects  each  containing  the  predictions  prediction  time  ts  index  quantiles  str  for  in  quantiles  req  self  encode  request  ts  cat  dynamic  feat  num  samples  return  samples  quantiles  res  super  DeepARPredictor  self  predict  req  return  self  decode  response  res  ts  index  freq  prediction  time  return  samples  def  encode  request  self  ts  cat  dynamic  feat  num  samples  return  samples  quantiles  instance  series  to  dict  ts  cat  if  cat  is  not  None  else  None  dynamic  feat  if  dynamic  feat  else  None  configuration  num  samples  num  samples  output  types  quantil,amazon
es  samples  if  return  samples  else  quantiles  quantiles  quantiles  http  request  data  instances  instance  configuration  configuration  return  json  dumps  http  request  data  encode  utf  def  decode  response  self  response  freq  prediction  time  return  samples  we  only  sent  one  time  series  so  we  only  receive  one  in  return  however  if  possible  one  will  pass  multiple  time  series  as  predictions  will  then  be  faster  predictions  json  loads  response  decode  utf  predictions  prediction  length  len  next  iter  predictions  quantiles  values  prediction  index  pd  DatetimeIndex  start  prediction  time  freq  freq  periods  prediction  length  if  return  samples  dict  of  samples  sample  str  for  in  enumerate  predictions  samples  else  dict  of  samples  return  pd  DataFrame  data  predictions  quantiles  dict  of  samples  index  prediction  index  def  set  frequency  self  freq  self  freq  freq  def  encode  target  ts  return  if  np  isfinite  else  NaN,amazon
  for  in  ts  def  series  to  dict  ts  cat  None  dynamic  feat  None  Given  pandas  Series  object  returns  dictionary  encoding  the  time  series  ts  pands  Series  object  with  the  target  time  series  cat  an  integer  indicating  the  time  series  category  Return  value  dictionary  obj  start  str  ts  index  target  encode  target  ts  if  cat  is  not  None  obj  cat  cat  if  dynamic  feat  is  not  None  obj  dynamic  feat  dynamic  feat  return  objpredictor  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  predictor  cls  DeepARPredictor  predictor  predict  ts  timeseries  120  quantiles  10  90  head  def  plot  predictor  target  ts  cat  None  dynamic  feat  None  forecast  date  end  training  show  samples  False  plot  history  12  confidence  80  print  calling  served  model  to  generate  predictions  starting  from  format  str  forecast  date  assert  confidence  50  and  confidence  100  low  quantile  confidence  005  up  quantile  confidence  ,amazon
005  we  first  construct  the  argument  to  call  our  model  args  ts  target  ts  forecast  date  return  samples  show  samples  quantiles  low  quantile  up  quantile  num  samples  100  if  dynamic  feat  is  not  None  args  dynamic  feat  dynamic  feat  fig  plt  figure  figsize  20  ax  plt  subplot  else  fig  plt  figure  figsize  20  ax  plt  subplot  if  cat  is  not  None  args  cat  cat  ax  text  cat  format  cat  transform  ax  transAxes  call  the  end  point  to  get  the  prediction  prediction  predictor  predict  args  plot  the  samples  if  show  samples  for  key  in  prediction  keys  if  sample  in  key  prediction  key  plot  color  lightskyblue  alpha  label  nolegend  plot  the  target  target  section  target  ts  forecast  date  plot  history  forecast  date  prediction  length  target  section  plot  color  black  label  target  plot  the  confidence  interval  and  the  median  predicted  ax  fill  between  prediction  str  low  quantile  index  prediction  str  low  quantil,amazon
e  values  prediction  str  up  quantile  values  color  alpha  label  confidence  interval  format  confidence  prediction  plot  color  label  P50  ax  legend  loc  fix  the  scale  as  the  samples  may  change  it  ax  set  ylim  target  section  min  target  section  max  if  dynamic  feat  is  not  None  for  in  enumerate  dynamic  feat  start  ax  plt  subplot  len  dynamic  feat  len  dynamic  feat  sharex  ax  feat  ts  pd  Series  index  pd  DatetimeIndex  start  target  ts  index  freq  target  ts  index  freq  periods  len  data  feat  ts  forecast  date  plot  history  forecast  date  prediction  length  plot  ax  ax  color  style  description  width  initial  interact  manual  customer  id  IntSlider  min  max  369  value  91  style  style  forecast  day  IntSlider  min  max  100  value  51  style  style  confidence  IntSlider  min  60  max  95  value  80  step  style  style  history  weeks  plot  IntSlider  min  max  20  value  style  style  show  samples  Checkbox  value  False  continuous  u,amazon
pdate  False  def  plot  interact  customer  id  forecast  day  confidence  history  weeks  plot  show  samples  plot  predictor  target  ts  timeseries  customer  id  forecast  date  end  training  datetime  timedelta  days  forecast  day  show  samples  show  samples  plot  history  history  weeks  plot  12  confidence  confidence  def  create  special  day  feature  ts  fraction  05  First  select  random  day  indices  plus  the  forecast  day  num  days  ts  index  ts  index  days  rand  indices  list  np  random  randint  num  days  int  num  days  num  days  feature  value  np  zeros  like  ts  for  in  rand  indices  feature  value  12  12  feature  pd  Series  index  ts  index  data  feature  value  return  feature  def  drop  at  random  ts  drop  probability  assert  drop  probability  random  mask  np  random  random  len  ts  drop  probability  return  ts  mask  random  mask  special  day  features  create  special  day  feature  ts  for  ts  in  timeseries  timeseries  uplift  ts  feat  for  ts ,amazon
 feat  in  zip  timeseries  special  day  features  time  series  processed  drop  at  random  ts  for  ts  in  timeseries  uplift  fig  axs  plt  subplots  figsize  20  20  sharex  True  axx  axs  ravel  for  in  range  10  ax  axx  ts  time  series  processed  400  ts  plot  ax  ax  ax  set  ylim  ts  max  ts  max  ax2  ax  twinx  special  day  features  400  plot  ax  ax2  color  ax2  set  ylim  time  training  data  new  features  start  str  start  dataset  target  encode  target  ts  start  dataset  end  training  dynamic  feat  special  day  features  start  dataset  end  training  tolist  for  ts  in  enumerate  time  series  processed  print  len  training  data  new  features  as  in  our  previous  example  we  do  rolling  evaluation  over  the  next  days  num  test  windows  test  data  new  features  start  str  start  dataset  target  encode  target  ts  start  dataset  end  training  prediction  length  dynamic  feat  special  day  features  start  dataset  end  training  prediction  length  ,amazon
tolist  for  in  range  num  test  windows  for  ts  in  enumerate  timeseries  uplift  def  check  dataset  consistency  train  dataset  test  dataset  None  train  dataset  has  dynamic  feat  dynamic  feat  in  if  has  dynamic  feat  num  dynamic  feat  len  dynamic  feat  has  cat  cat  in  if  has  cat  num  cat  len  cat  def  check  ds  ds  for  in  enumerate  ds  if  has  dynamic  feat  assert  dynamic  feat  in  assert  num  dynamic  feat  len  dynamic  feat  for  in  dynamic  feat  assert  len  target  len  if  has  cat  assert  cat  in  assert  len  cat  num  cat  check  ds  train  dataset  if  test  dataset  is  not  None  check  ds  test  dataset  check  dataset  consistency  training  data  new  features  test  data  new  features  time  write  dicts  to  file  train  new  features  json  training  data  new  features  write  dicts  to  file  test  new  features  json  test  data  new  features  time  s3  data  path  new  features  s3  new  features  data  format  s3  bucket  s3  prefix  s3  ou,amazon
tput  path  new  features  s3  new  features  output  format  s3  bucket  s3  prefix  print  Uploading  to  S3  this  may  take  few  minutes  depending  on  your  connection  copy  to  s3  train  new  features  json  s3  data  path  new  features  train  train  new  features  json  override  True  copy  to  s3  test  new  features  json  s3  data  path  new  features  test  test  new  features  json  override  True  time  estimator  new  features  sagemaker  estimator  Estimator  sagemaker  session  sagemaker  session  image  name  image  name  role  role  train  instance  count  train  instance  type  ml  c4  2xlarge  base  job  name  deepar  electricity  demo  new  features  output  path  s3  output  path  new  features  hyperparameters  time  freq  freq  context  length  str  context  length  prediction  length  str  prediction  length  epochs  400  learning  rate  5E  mini  batch  size  64  early  stopping  patience  40  num  dynamic  feat  auto  this  will  use  the  dynamic  feat  field  if  it  presen,amazon
t  in  the  data  estimator  new  features  set  hyperparameters  hyperparameters  estimator  new  features  fit  inputs  train  train  format  s3  data  path  new  features  test  test  format  s3  data  path  new  features  wait  True  time  predictor  new  features  estimator  new  features  deploy  initial  instance  count  instance  type  ml  m4  xlarge  predictor  cls  DeepARPredictor  customer  id  120  predictor  new  features  predict  ts  time  series  processed  customer  id  prediction  length  dynamic  feat  special  day  features  customer  id  tolist  quantiles  head  interact  manual  customer  id  IntSlider  min  max  369  value  13  style  style  forecast  day  IntSlider  min  max  100  value  21  style  style  confidence  IntSlider  min  60  max  95  value  80  step  style  style  missing  ratio  FloatSlider  min  max  95  value  step  05  style  style  show  samples  Checkbox  value  False  continuous  update  False  def  plot  interact  customer  id  forecast  day  confidence  missing  ra,amazon
tio  show  samples  forecast  date  end  training  datetime  timedelta  days  forecast  day  target  time  series  processed  customer  id  start  dataset  forecast  date  prediction  length  target  drop  at  random  target  missing  ratio  dynamic  feat  special  day  features  customer  id  start  dataset  forecast  date  prediction  length  tolist  plot  predictor  new  features  target  ts  target  dynamic  feat  dynamic  feat  forecast  date  forecast  date  show  samples  show  samples  plot  history  12  confidence  confidence  predictor  delete  endpoint  predictor  new  features  delete  endpoint  ,amazon
import  os  import  boto3  import  sagemaker  from  sagemaker  mxnet  import  MXNet  from  mxnet  import  gluon  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  gluon  data  vision  MNIST  data  train  train  True  gluon  data  vision  MNIST  data  test  train  False  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  DEMO  mnist  cat  mnist  py  MXNet  mnist  py  role  role  train  instance  count  train  instance  type  ml  c4  xlarge  hyperparameters  batch  size  100  epochs  20  learning  rate  momentum  log  interval  100  fit  inputs  predictor  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  IPython  display  import  HTML  HTML  open  input  html  read  response  predictor  predict  data  print  int  response  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
import  os  import  sagemaker  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  import  utils  from  tensorflow  contrib  learn  python  learn  datasets  import  mnist  import  tensorflow  as  tf  data  sets  mnist  read  data  sets  data  dtype  tf  uint8  reshape  False  validation  size  5000  utils  convert  to  data  sets  train  train  data  utils  convert  to  data  sets  validation  validation  data  utils  convert  to  data  sets  test  test  data  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  DEMO  mnist  cat  mnist  py  from  sagemaker  tensorflow  import  TensorFlow  mnist  estimator  TensorFlow  entry  point  mnist  py  role  role  training  steps  1000  evaluation  steps  100  train  instance  count  train  instance  type  ml  c4  xlarge  mnist  estimator  fit  inputs  mnist  predictor  mnist  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  import  numpy  as  np  from  tensor,amazon
flow  examples  tutorials  mnist  import  input  data  mnist  input  data  read  data  sets  tmp  data  one  hot  True  for  in  range  10  data  mnist  test  images  tolist  tensor  proto  tf  make  tensor  proto  values  np  asarray  data  shape  len  data  dtype  tf  float32  predict  response  mnist  predictor  predict  tensor  proto  print  label  np  argmax  mnist  test  labels  print  label  is  format  label  prediction  predict  response  outputs  classes  int64Val  print  prediction  is  format  prediction  sagemaker  Session  delete  endpoint  mnist  predictor  endpoint  ,amazon
data  bucket  amazon  sagemaker  in  practice  workshop  user  number  CHANGE  TO  YOUR  NUMBER  user  name  user  format  user  number  output  bucket  amazon  sagemaker  in  practice  bucket  format  user  name  path  criteo  display  ad  challenge  key  sample  csv  data  location  s3  format  data  bucket  path  key  import  boto3  from  sagemaker  import  get  execution  role  role  get  execution  role  print  role  import  numpy  as  np  For  matrix  operations  and  numerical  processing  import  pandas  as  pd  For  munging  tabular  data  import  matplotlib  pyplot  as  plt  For  charts  and  visualizations  from  IPython  display  import  Image  For  displaying  images  in  the  notebook  from  IPython  display  import  display  For  displaying  outputs  in  the  notebook  from  time  import  gmtime  strftime  For  labeling  SageMaker  models  endpoints  etc  import  sys  For  writing  outputs  to  notebook  import  math  For  ceiling  function  import  json  For  parsing  hosting  outputs  import ,amazon
 os  For  manipulating  filepath  names  import  sagemaker  Amazon  SageMaker  Python  SDK  provides  helper  functions  from  sagemaker  predictor  import  csv  serializer  Converts  strings  for  HTTP  POST  requests  on  inference  from  sagemaker  tuner  import  IntegerParameter  Importing  HPO  elements  from  sagemaker  tuner  import  CategoricalParameter  from  sagemaker  tuner  import  ContinuousParameter  from  sagemaker  tuner  import  HyperparameterTunerdata  pd  read  csv  data  location  header  None  sep  pd  set  option  display  max  columns  500  Make  sure  we  can  see  all  of  the  columns  pd  set  option  display  max  rows  20  Keep  the  output  on  one  page  Histograms  for  each  numeric  features  display  data  describe  matplotlib  inline  hist  data  hist  bins  30  sharey  True  figsize  10  10  display  data  corr  pd  plotting  scatter  matrix  data  figsize  12  12  plt  show  Frequency  tables  for  each  categorical  feature  for  column  in  data  select  dtypes  include,amazon
  object  columns  display  pd  crosstab  index  data  column  columns  observations  normalize  columns  categorical  feature  data  14  unique  values  data  14  unique  print  Number  of  unique  values  in  14th  feature  format  len  unique  values  print  data  14  for  column  in  data  select  dtypes  include  object  columns  size  data  groupby  column  size  print  Column  number  of  categories  format  column  len  size  for  column  in  data  select  dtypes  include  number  columns  size  data  groupby  column  size  print  Column  number  of  categories  format  column  len  size  for  column  in  data  select  dtypes  include  object  columns  print  Converting  column  to  indexed  values  format  column  indexed  column  index  format  column  data  indexed  column  pd  Categorical  data  column  data  indexed  column  data  indexed  column  cat  codescategorical  feature  data  14  index  unique  values  data  14  index  unique  print  Number  of  unique  values  in  14th  feature  format ,amazon
 len  unique  values  print  data  14  index  for  column  in  data  select  dtypes  include  object  columns  data  drop  column  axis  inplace  True  display  data  Replace  all  to  NaN  for  column  in  data  columns  data  column  data  column  replace  np  nan  testing  data  testing  unique  values  data  unique  print  Number  of  unique  values  in  2nd  feature  format  len  testing  unique  values  print  testing  Randomly  sort  the  data  then  split  out  first  70  second  20  and  last  10  data  len  len  data  sampled  data  data  sample  frac  train  data  validation  data  test  data  np  split  sampled  data  int  data  len  int  data  len  train  data  to  csv  train  sample  csv  index  False  header  False  validation  data  to  csv  validation  sample  csv  index  False  header  False  s3client  boto3  Session  resource  s3  train  csv  file  os  path  join  path  train  train  csv  validation  csv  file  os  path  join  path  validation  validation  csv  s3client  Bucket  output  buc,amazon
ket  Object  train  csv  file  upload  file  train  sample  csv  s3client  Bucket  output  bucket  Object  validation  csv  file  upload  file  validation  sample  csv  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  boto3  Session  region  name  xgboost  train  csv  key  s3  train  train  csv  format  output  bucket  path  validation  csv  key  s3  validation  validation  csv  format  output  bucket  path  s3  input  train  sagemaker  s3  input  s3  data  train  csv  key  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  validation  csv  key  content  type  csv  sess  sagemaker  Session  xgb  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  m4  xlarge  base  job  name  user  name  output  path  s3  output  format  output  bucket  path  sagemaker  session  sess  xgb  set  hyperparameters  eval  metric  logloss  objective  binary  logistic  eta  max  depth  10  colsample  bytree  colsam,amazon
ple  bylevel  min  child  weight  rate  drop  num  round  75  gamma  xgb  fit  train  s3  input  train  validation  s3  input  validation  hpo  sess  sagemaker  Session  hpo  xgb  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  m4  xlarge  output  path  s3  output  hpo  format  output  bucket  path  sagemaker  session  hpo  sess  hpo  xgb  set  hyperparameters  eval  metric  logloss  objective  binary  logistic  colsample  bytree  colsample  bylevel  num  round  75  rate  drop  gamma  hyperparameter  ranges  eta  ContinuousParameter  min  child  weight  ContinuousParameter  10  alpha  ContinuousParameter  max  depth  IntegerParameter  10  objective  metric  name  validation  logloss  objective  type  Minimize  tuner  HyperparameterTuner  hpo  xgb  objective  metric  name  hyperparameter  ranges  base  tuning  job  name  user  name  max  jobs  20  max  parallel  jobs  objective  type  objective  type  tuner  fit  train  s3  input  train  validation  s3  inpu,amazon
t  validation  smclient  boto3  client  sagemaker  job  name  tuner  latest  tuning  job  job  name  hpo  job  smclient  describe  hyper  parameter  tuning  job  HyperParameterTuningJobName  job  name  hpo  job  HyperParameterTuningJobStatus  xgb  predictor  xgb  deploy  initial  instance  count  instance  type  ml  m4  xlarge  xgb  predictor  hpo  tuner  deploy  initial  instance  count  instance  type  ml  m4  xlarge  xgb  predictor  content  type  text  csv  xgb  predictor  serializer  csv  serializerxgb  predictor  hpo  content  type  text  csv  xgb  predictor  hpo  serializer  csv  serializerdef  predict  predictor  data  rows  500  split  array  np  array  split  data  int  data  shape  float  rows  predictions  for  array  in  split  array  predictions  join  predictions  predictor  predict  array  decode  utf  return  np  fromstring  predictions  sep  predictions  predict  xgb  predictor  test  data  drop  axis  values  hpo  predictions  predict  xgb  predictor  hpo  test  data  drop  axis  values  ro,amazon
ws  actuals  cols  predictions  clicks  np  round  predictions  result  pd  crosstab  index  test  data  columns  clicks  rownames  rows  colnames  cols  display  Single  job  results  display  result  display  result  apply  lambda  sum  axis  hpo  clicks  np  round  hpo  predictions  result  hpo  pd  crosstab  index  test  data  columns  hpo  clicks  rownames  rows  colnames  cols  display  HPO  job  results  display  result  hpo  display  result  hpo  apply  lambda  sum  axis  sagemaker  Session  delete  endpoint  xgb  predictor  endpoint  sagemaker  Session  delete  endpoint  xgb  predictor  hpo  endpoint  ,amazon
import  sagemaker  import  tensorflow  as  tf  import  os  from  sagemaker  tensorflow  import  TensorFlow  from  sagemaker  import  get  execution  role  role  get  execution  role  QUESTION  INSTANCE  TYPE  ml  c4  xlarge  sagemaker  session  sagemaker  Session  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  print  sagemaker  session  sagemaker  runtime  client  client  config  user  agent  runs  fine  with  framework  version  set  to  crashes  with  classifier  TensorFlow  entry  point  entry  point  py  role  role  training  steps  1e1  evaluation  steps  train  instance  count  train  instance  type  INSTANCE  TYPE  py  version  py2  framework  version  hyperparameters  words  classifier  fit  inputs  ,amazon
import  sagemaker  from  sagemaker  import  get  execution  role  import  json  import  boto3  sess  sagemaker  Session  role  get  execution  role  print  role  This  is  the  role  that  SageMaker  would  use  to  leverage  AWS  resources  S3  CloudWatch  on  your  behalf  bucket  sess  default  bucket  Replace  with  your  own  bucket  name  if  needed  print  bucket  prefix  blazingtext  supervised  Replace  with  the  prefix  under  which  you  want  to  store  the  data  if  needed  wget  https  github  com  saurabh3949  Text  Classification  Datasets  raw  master  dbpedia  csv  tar  gz  tar  xzvf  dbpedia  csv  tar  gz  head  dbpedia  csv  train  csv  cat  dbpedia  csv  classes  txtindex  to  label  with  open  dbpedia  csv  classes  txt  as  for  label  in  enumerate  readlines  index  to  label  str  label  strip  print  index  to  label  from  random  import  shuffle  import  multiprocessing  from  multiprocessing  import  Pool  import  csv  import  nltk  nltk  download  punkt  def  transform  insta,amazon
nce  row  cur  row  label  label  index  to  label  row  Prefix  the  index  ed  label  with  label  cur  row  append  label  cur  row  extend  nltk  word  tokenize  row  lower  cur  row  extend  nltk  word  tokenize  row  lower  return  cur  rowdef  preprocess  input  file  output  file  keep  all  rows  with  open  input  file  as  csvinfile  csv  reader  csv  reader  csvinfile  delimiter  for  row  in  csv  reader  all  rows  append  row  shuffle  all  rows  all  rows  all  rows  int  keep  len  all  rows  pool  Pool  processes  multiprocessing  cpu  count  transformed  rows  pool  map  transform  instance  all  rows  pool  close  pool  join  with  open  output  file  as  csvoutfile  csv  writer  csv  writer  csvoutfile  delimiter  lineterminator  csv  writer  writerows  transformed  rows  time  Preparing  the  training  dataset  Since  preprocessing  the  whole  dataset  might  take  couple  of  mintutes  we  keep  20  of  the  training  dataset  for  this  demo  Set  keep  to  if  you  want  to  use  the,amazon
  complete  dataset  preprocess  dbpedia  csv  train  csv  dbpedia  train  keep  Preparing  the  validation  dataset  preprocess  dbpedia  csv  test  csv  dbpedia  validation  time  train  channel  prefix  train  validation  channel  prefix  validation  sess  upload  data  path  dbpedia  train  bucket  bucket  key  prefix  train  channel  sess  upload  data  path  dbpedia  validation  bucket  bucket  key  prefix  validation  channel  s3  train  data  s3  format  bucket  train  channel  s3  validation  data  s3  format  bucket  validation  channel  s3  output  location  s3  output  format  bucket  prefix  region  name  boto3  Session  region  namecontainer  sagemaker  amazon  amazon  estimator  get  image  uri  region  name  blazingtext  latest  print  Using  SageMaker  BlazingText  container  format  container  region  name  bt  model  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  c4  4xlarge  train  volume  size  30  train  max  run  360000  input  mode ,amazon
 File  output  path  s3  output  location  sagemaker  session  sess  bt  model  set  hyperparameters  mode  supervised  epochs  10  min  count  learning  rate  05  vector  dim  10  early  stopping  True  patience  min  epochs  word  ngrams  train  data  sagemaker  session  s3  input  s3  train  data  distribution  FullyReplicated  content  type  text  plain  s3  data  type  S3Prefix  validation  data  sagemaker  session  s3  input  s3  validation  data  distribution  FullyReplicated  content  type  text  plain  s3  data  type  S3Prefix  data  channels  train  train  data  validation  validation  data  bt  model  fit  inputs  data  channels  logs  True  text  classifier  bt  model  deploy  initial  instance  count  instance  type  ml  m4  xlarge  sentences  Convair  was  an  american  aircraft  manufacturing  company  which  later  expanded  into  rockets  and  spacecraft  Berwick  secondary  college  is  situated  in  the  outer  melbourne  metropolitan  suburb  of  berwick  using  the  same  nltk  tokenizer ,amazon
 that  we  used  during  data  preparation  for  training  tokenized  sentences  join  nltk  word  tokenize  sent  for  sent  in  sentences  payload  instances  tokenized  sentences  response  text  classifier  predict  json  dumps  payload  predictions  json  loads  response  print  json  dumps  predictions  indent  payload  instances  tokenized  sentences  configuration  response  text  classifier  predict  json  dumps  payload  predictions  json  loads  response  print  json  dumps  predictions  indent  sess  delete  endpoint  text  classifier  endpoint  ,amazon
import  boto3  import  os  from  sagemaker  import  get  execution  role  import  pandas  as  pd  import  botocore  import  string  random  region  boto3  Session  region  name  sagemaker  boto3  Session  client  sagemaker  role  get  execution  role  def  random  string  size  chars  string  ascii  uppercase  string  digits  return  join  random  choice  chars  for  in  range  size  Specify  S3  bucket  bucket  sagemaker  walebadr  prefix  hpo  xgboost  check  if  the  bucket  exists  try  boto3  Session  client  s3  head  bucket  Bucket  bucket  except  botocore  exceptions  ParamValidationError  as  print  Oi  Have  you  forgetten  to  specify  the  bucket  name  or  maybe  your  bucket  name  isn  correct  except  botocore  exceptions  ClientError  as  if  response  Error  Code  403  print  Oi  You  don  have  permission  to  access  the  bucket  Check  the  policy  permissions  format  bucket  elif  response  Error  Code  404  print  Oi  This  bucket  doesn  exist  format  bucket  else  raise  else  prin,amazon
t  Well  Done  Training  input  and  output  will  be  stored  in  s3  format  bucket  prefix  train  pd  read  csv  train  csv  train  head  10  rows  train  shape  columns  train  shape  print  The  train  dataset  contains  rows  and  columns  format  rows  columns  matplotlib  inline  import  plotly  offline  as  py  py  init  notebook  mode  connected  True  import  plotly  graph  objs  as  go  import  plotly  tools  as  tls  data  go  Bar  train  target  value  counts  index  values  train  target  value  counts  values  text  Target  Distribution  layout  go  Layout  title  Target  distribution  fig  go  Figure  data  data  layout  layout  py  iplot  fig  filename  basic  bar  import  seaborn  as  sns  import  matplotlib  pyplot  as  plt  train  float  train  select  dtypes  include  float64  train  int  train  select  dtypes  include  int64  colormap  plt  cm  magma  plt  figure  figsize  16  12  plt  title  Pearson  correlation  of  continuous  features  05  size  15  sns  heatmap  train  float  corr,amazon
  linewidths  vmax  square  False  cmap  colormap  linecolor  white  annot  True  from  time  import  gmtime  strftime  sleep  tuning  job  name  xgboost  smworkshop  strftime  gmtime  tuning  job  config  ParameterRanges  CategoricalParameterRanges  ContinuousParameterRanges  MaxValue  MinValue  Name  eta  MaxValue  10  MinValue  Name  gamma  MaxValue  10  MinValue  Name  min  child  weight  IntegerParameterRanges  MaxValue  10  MinValue  Name  max  depth  ResourceLimits  MaxNumberOfTrainingJobs  10  MaxParallelTrainingJobs  Strategy  Bayesian  HyperParameterTuningJobObjective  MetricName  validation  auc  Type  Maximize  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  xgboost  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  xgboost  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  xgboost  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  xgboost  latest  training  image  containers  region  training  job  definition  Algorithm,amazon
Specification  TrainingImage  training  image  TrainingInputMode  File  InputDataConfig  ChannelName  train  CompressionType  None  ContentType  csv  DataSource  S3DataSource  S3DataDistributionType  FullyReplicated  S3DataType  S3Prefix  S3Uri  s3  public  test  hpo  datasets  sagemaker  kaggle  porto  seguro  xgb  train  format  region  ChannelName  validation  CompressionType  None  ContentType  csv  DataSource  S3DataSource  S3DataDistributionType  FullyReplicated  S3DataType  S3Prefix  S3Uri  s3  public  test  hpo  datasets  sagemaker  kaggle  porto  seguro  xgb  val  format  region  OutputDataConfig  S3OutputPath  s3  format  bucket  ResourceConfig  InstanceCount  InstanceType  ml  c5  2xlarge  VolumeSizeInGB  10  RoleArn  role  StaticHyperParameters  eval  metric  auc  num  round  50  objective  binary  logistic  rate  drop  tweedie  variance  power  StoppingCondition  MaxRuntimeInSeconds  43200  sagemaker  create  hyper  parameter  tuning  job  HyperParameterTuningJobName  tuning  job  name  HyperPara,amazon
meterTuningJobConfig  tuning  job  config  TrainingJobDefinition  training  job  definition  import  pandas  as  pd  from  smhpolib  import  analysis  analytical  library  provided  through  smhpolib  you  can  find  the  source  code  under  smhpolib  folder  tuning  analysis  TuningJob  tuning  job  name  tuning  job  name  HPO  params  tuning  hyperparam  dataframe  if  len  HPO  params  df  HPO  params  HPO  params  FinalObjectiveValue  float  inf  if  len  df  df  df  sort  values  FinalObjectiveValue  ascending  False  print  Valid  objective  len  df  print  lowest  min  df  FinalObjectiveValue  highest  max  df  FinalObjectiveValue  best  model  df  iloc  print  best  model  information  best  model  best  training  job  name  best  model  TrainingJobName  pd  set  option  display  max  colwidth  Don  truncate  TrainingJobName  else  print  Training  jobs  launched  are  not  completed  yet  Try  again  in  few  minutes  dfimport  bokeh  import  bokeh  io  bokeh  io  output  notebook  from  bokeh  plo,amazon
tting  import  figure  show  import  bokeh  palettes  def  big  warp  palette  size  palette  func  warp  setting  warp  exagerates  the  high  end  setting  warp  exagerates  the  low  end  palette  func  256  out  for  in  range  size  size  from  inclusive  warp  idx  int  255  out  append  idx  return  out  if  len  df  palette  big  warp  palette  len  df  bokeh  palettes  plasma  df  color  palette  hover  smhpolib  viz  SmhpoHover  tuning  figure  plot  width  900  plot  height  400  tools  hover  tools  axis  type  datetime  circle  source  df  TrainingCreationTime  FinalObjectiveValue  color  color  show  else  print  Training  jobs  launched  are  not  completed  yet  Try  again  in  few  minutes  Which  hyperparameters  to  look  for  correlations  for  all  hyperparameters  tuning  hyperparam  ranges  keys  all  hyperparameters  figures  for  hp  in  all  hyperparameters  figure  plot  width  900  plot  height  500  title  Final  objective  vs  hp  tools  hover  tools  axis  label  hp  axis  label,amazon
  objective  circle  source  df  hp  FinalObjectiveValue  color  color  figures  append  show  bokeh  layouts  Column  figures  time  from  time  import  gmtime  strftime  model  name  best  training  job  name  print  model  name  info  sagemaker  describe  training  job  TrainingJobName  best  training  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  hosting  image  training  image  For  XGBoost  algorithm  training  and  hosting  share  the  same  image  primary  container  Image  hosting  image  ModelDataUrl  model  data  create  model  response  sagemaker  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  endpoint  config  name  XGBoostEndpointConfig  strftime  gmtime  print  endpoint  config  name  create  endpoint  config  response  sagemaker  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  c5  xlarge  InitialInstanceCount  ModelName  m,amazon
odel  name  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  endpoint  name  XGBoostEndpoint  strftime  gmtime  print  endpoint  name  create  endpoint  response  sagemaker  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  print  create  endpoint  response  EndpointArn  resp  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  try  sagemaker  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  finally  resp  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Create  endpoint  ended  with  status  status  if  status  InService  message  sagemaker  describe  endpoint  EndpointName  endpoint  name  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Endpoint  creation  did  not  succeed  First  row  fr,amazon
om  validation  dataset  sample  record  332648709  10  14  104  445982062  879049073  40620192  11  label  payload  sample  record  split  maxsplit  import  json  from  itertools  import  islice  import  math  import  struct  runtime  client  boto3  client  runtime  sagemaker  response  runtime  client  invoke  endpoint  EndpointName  endpoint  name  ContentType  text  csv  Body  payload  result  response  Body  read  result  float  result  decode  utf  print  Label  label  nPrediction  result  ,amazon
Load  the  required  libraries  import  warnings  import  zipfile  import  os  import  cv2  import  urllib  request  import  sagemaker  import  numpy  as  np  import  mxnet  as  mx  import  pandas  as  pd  import  seaborn  as  sns  import  matplotlib  pyplot  as  plt  import  matplotlib  image  as  mpimg  from  sklearn  model  selection  import  train  test  split  warnings  simplefilter  ignore  matplotlib  inline  Helper  function  def  download  url  Helper  function  to  download  individual  file  from  given  url  Arguments  url  full  URL  of  the  file  to  download  Returns  filename  downloaded  file  name  filename  url  split  if  not  os  path  exists  filename  urllib  request  urlretrieve  url  filename  return  filename  To  download  and  extract  Sample  Data  URL  PROVIDED  BY  INSTRUCTOR  file  download  URL  Extract  the  file  with  zipfile  ZipFile  file  as  zf  zf  extractall  View  the  log  data  data  df  pd  read  csv  data  driving  log  csv  data  df  head  Data  Overview  print,amazon
  Dataset  Shape  format  data  df  shape  print  data  df  describe  Visualize  the  distribution  of  the  data  using  the  seabourne  library  sns  set  rc  figure  figsize  10  10  fig  sns  distplot  data  df  steering  plt  xlabel  Steering  Angle  plt  title  Distribution  of  Steering  Angles  in  the  Dataset  plt  show  fig  Separate  the  features  data  df  center  left  right  values  data  df  steering  values  Image  Transformation  Crop  def  crop  image  Crops  the  image  by  emoving  the  sky  at  the  top  and  the  car  front  at  the  bottom  Arguments  image  numpy  array  representing  an  RGB  image  of  format  Height  Width  Channel  Returns  Cropped  image  image  image  60  25  return  imagedef  resize  image  Resize  the  image  to  the  input  shape  for  the  NVIDIA  model  Arguments  image  numpy  array  representing  image  height  desired  image  height  width  desired  image  width  Returns  Resized  image  image  cv2  resize  image  IMAGE  WIDTH  IMAGE  HEIGHT  cv2  INTER,amazon
  AREA  return  image  Image  Transformation  Convert  from  RGB  to  YUV  def  rgb2yuv  image  Convert  the  image  from  RGB  to  YUV  color  space  Arguments  image  numpy  array  represnting  the  image  Returns  YUV  image  image  cv2  cvtColor  image  cv2  COLOR  RGB2YUV  return  image  Image  Augmentation  Random  Flip  def  random  flip  image  steering  angle  Randomly  flip  the  50  of  the  images  from  left  to  right  and  vice  versa  Additionally  adjust  the  steering  angle  accordingly  Arguments  image  pre  processed  input  image  steering  amngle  pre  processed  steering  angle  Returns  image  flipped  image  steering  angle  adjusted  steering  angle  Randomly  select  of  the  images  if  np  random  rand  Apply  the  flip  function  to  the  vertical  axis  image  cv2  flip  image  Adjust  the  steering  angle  to  the  reverse  of  the  current  steering  angle  steering  angle  steering  angle  Return  the  flipped  image  and  new  steering  angle  return  image  steering  angl,amazon
e  Image  Augmentation  Random  Translate  def  translate  image  steering  angle  range  range  Randomly  shift  translate  the  image  vertically  and  horizontally  Arguments  image  pre  processed  input  image  steering  angle  pre  processed  steering  angle  range  axis  pixels  range  axis  pixels  Returns  image  translated  image  steering  angle  adjusted  steeing  angle  Randomly  adjust  the  and  axis  transform  range  np  random  rand  transform  range  np  random  rand  Adjust  the  steering  angle  steering  angle  transform  002  transform  np  float32  transform  transform  height  width  image  shape  image  cv2  warpAffine  image  transform  width  height  return  image  steering  angle  Image  Augmentation  Random  Distortion  def  distort  image  Add  distortion  to  random  images  and  adjust  the  brightness  Arguments  image  pre  processed  input  image  Returns  new  image  distorted  image  Create  placeholder  numpy  array  for  the  new  image  new  img  image  astype  float  ,amazon
Add  random  brightness  value  np  random  randint  28  28  if  value  mask  new  img  value  255  if  value  mask  new  img  value  new  img  np  where  mask  value  Add  random  shadow  new  img  shape  mid  np  random  randint  factor  np  random  uniform  if  np  random  rand  new  img  mid  factor  else  new  img  mid  factor  Randomly  shift  the  horizon  new  img  shape  horizon  shift  np  random  randint  pts1  np  float32  horizon  horizon  pts2  np  float32  horizon  shift  horizon  shift  cv2  getPerspectiveTransform  pts1  pts2  new  img  cv2  warpPerspective  new  img  borderMode  cv2  BORDER  REPLICATE  return  new  img  astype  np  uint8  Image  Augmentation  Random  Brightness  def  brightness  image  Randomly  adjust  brightness  of  the  image  Convert  image  to  HSV  hsv  cv2  cvtColor  image  cv2  COLOR  RGB2HSV  Randomly  adjust  the  brightness  ratio  and  apply  it  to  the  image  ratio  np  random  rand  hsv  hsv  ratio  Convert  back  to  RGB  and  return  the  image  return  cv,amazon
2  cvtColor  hsv  cv2  COLOR  HSV2RGB  Helper  function  def  load  data  dir  image  file  Load  RGB  images  from  file  return  mpimg  imread  os  path  join  data  dir  image  file  strip  def  transform  image  Combine  all  preprocess  functions  into  one  image  crop  image  image  resize  image  image  rgb2yuv  image  return  image  Origional  left  image  IMAGE  HEIGHT  IMAGE  WIDTH  IMAGE  CHANNELS  66  200  INPUT  SHAPE  IMAGE  HEIGHT  IMAGE  WIDTH  IMAGE  CHANNELS  random  image  100  img  load  data  random  image  img  random  image  plt  rcParams  figure  figsize  11  10  plt  imshow  img  Create  Subpluts  for  Augmented  Images  plt  close  all  fig  plt  figure  figsize  18  10  sub1  fig  add  subplot  221  sub1  set  title  Bright  sub1  imshow  brightness  img  sub2  fig  add  subplot  222  sub2  set  title  Horizontal  Flip  sub2  imshow  cv2  flip  img  sub3  fig  add  subplot  223  sub3  set  title  Random  Distortion  sub3  imshow  distort  img  sub4  fig  add  subplot  224  sub4  se,amazon
t  title  YUV  Image  sub4  imshow  transform  img  plt  show  Batch  Image  Configurations  HEIGHT  WIDTH  CHANNELS  66  200  INPUT  SHAPE  HEIGHT  WIDTH  CHANNELS  Aumentation  Pipeline  Functions  def  choose  data  dir  center  left  right  steering  angle  Randomly  choose  an  image  from  the  center  left  or  right  and  adjust  the  steering  angle  choice  np  random  choice  if  choice  return  load  data  dir  left  steering  angle  elif  choice  return  load  data  dir  right  steering  angle  return  load  data  dir  center  steering  angle  def  augment  data  dir  center  left  right  steering  angle  range  100  range  10  Generate  an  augumented  image  and  adjust  steering  angle  The  steering  angle  is  associated  with  the  center  image  image  steering  angle  choose  data  dir  center  left  right  steering  angle  image  steering  angle  random  flip  image  steering  angle  image  steering  angle  translate  image  steering  angle  range  range  image  brightness  image  image ,amazon
 distort  image  return  image  steering  angle  def  aug  pipeline  data  dir  image  paths  steering  angles  size  is  training  Generate  training  image  give  image  paths  and  associated  steering  angles  images  np  empty  size  HEIGHT  WIDTH  CHANNELS  steering  np  empty  size  while  True  for  index  in  np  random  permutation  image  paths  shape  center  left  right  image  paths  index  steering  angle  steering  angles  index  argumentation  if  is  training  and  np  random  rand  image  steering  angle  augment  data  dir  center  left  right  steering  angle  else  image  load  data  dir  center  add  the  image  and  steering  angle  to  the  batch  images  transform  image  steering  steering  angle  if  size  break  return  np  array  images  astype  np  float32  np  array  steering  astype  np  float32  sample  sample  aug  pipeline  data  len  True  Plot  New  Distribution  of  training  examples  fig  sns  distplot  sample  plt  xlabel  Steering  Angle  plt  title  New  Distributio,amazon
n  of  Steering  Angles  in  the  Training  Dataset  plt  show  fig  Libraries  and  SageMaker  configuration  sagemaker  session  sagemaker  Session  role  sagemaker  get  execution  role  Create  Training  and  Validation  datasets  train  valid  train  valid  train  test  split  test  size  random  state  42  Preprocess  through  the  pipline  train  train  aug  pipeline  data  train  train  len  train  True  valid  valid  aug  pipeline  data  valid  valid  len  valid  False  View  resultant  shape  print  Training  Dataset  Shape  format  train  shape  print  Validation  Dataset  Shape  format  valid  shape  Create  local  repository  for  Numpy  Arrays  if  not  os  path  exists  tmp  data  os  mkdir  tmp  data  Save  the  Dataset  as  Numpy  Arrays  np  save  tmp  data  train  npy  train  np  save  tmp  data  train  npy  train  np  save  tmp  data  valid  npy  valid  np  save  tmp  data  valid  npy  valid  ,amazon
data  analysis  and  wrangling  import  pandas  as  pd  import  numpy  as  np  import  random  as  rnd  visualization  import  seaborn  as  sns  import  matplotlib  pyplot  as  plt  matplotlib  inline  machine  learning  from  sklearn  linear  model  import  LogisticRegression  from  sklearn  svm  import  SVC  LinearSVC  from  sklearn  ensemble  import  RandomForestClassifier  from  sklearn  neighbors  import  KNeighborsClassifier  from  sklearn  naive  bayes  import  GaussianNB  from  sklearn  linear  model  import  Perceptron  from  sklearn  linear  model  import  SGDClassifier  from  sklearn  tree  import  DecisionTreeClassifiertrain  df  pd  read  csv  dataset  titanic  train  csv  test  df  pd  read  csv  dataset  titanic  test  csv  combine  train  df  test  df  preview  the  data  train  df  head  preview  the  data  train  df  tail  ,microsoft
def  getSumOfDivisors  number  ,microsoft
bucket  um  ds  testbucket  currently  in  the  same  region  as  the  notebook  instance  prefix  sagemaker  DEMO  byo  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  print  role  print  client  boto3  client  sagemaker  print  client  describe  notebook  instance  NotebookInstanceName  um  inna  testing  v004  access  any  s3  import  time  import  json  import  os  import  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  import  json  For  training  you  want  your  Amazon  S3  bucket  for  training  data  to  be  in  the  same  region  as  your  Amazon  SageMaker  training  job  and  your  training  container  For  inference  you  will  need  to  push  your  model  and  your  inference  container  to  each  Region  in  which  you  want  to  have  it  available  in  Amazon  SageMaker  sh  some  addtional  notes  on  what  happenes  in  cell  below  algorithm  name  rmars  account  aws  sts  get  caller  ide,amazon
ntity  query  Account  output  text  region  aws  configure  get  region  region  region  us  west  fullname  account  dkr  ecr  region  amazonaws  com  algorithm  name  latest  aws  configure  get  region  echo  account  echo  region  echo  fullname  aws  ecr  describe  repositories  aws  ecr  create  repository  repository  name  algorithm  name  aws  ecr  describe  repositories  repository  names  algorithm  name  set  up  policy  this  might  be  to  permissive  Add  custom  role  IAM  Roles  AmazonSageMaker  ExecutionRole  20180905T101877  the  role  used  to  create  the  notebook  add  inline  policy  JSON  print  Version  2012  10  17  Statement  Effect  Allow  Action  ecr  sagemaker  ec2  Resource  without  the  triple  quotes  sh  The  name  of  our  algorithm  algorithm  name  rmars  set  stop  if  anything  fails  account  aws  sts  get  caller  identity  query  Account  output  text  Get  the  region  defined  in  the  current  configuration  default  to  us  west  if  none  defined  region  aws ,amazon
 configure  get  region  region  region  us  west  fullname  account  dkr  ecr  region  amazonaws  com  algorithm  name  latest  If  the  repository  doesn  exist  in  ECR  create  it  aws  ecr  describe  repositories  repository  names  algorithm  name  dev  null  if  ne  then  aws  ecr  create  repository  repository  name  algorithm  name  dev  null  fi  Get  the  login  command  from  ECR  and  execute  it  directly  aws  ecr  get  login  region  region  no  include  email  Build  the  docker  image  locally  with  the  image  name  and  then  push  it  to  ECR  with  the  full  name  docker  build  algorithm  name  docker  tag  algorithm  name  fullname  docker  push  fullname  bash  echo  show  all  running  containers  on  the  notebook  instance  in  this  case  docker  container  ls  show  all  running  containers  on  the  notebook  instance  in  this  case  docker  container  ls  show  all  containers  echo  echo  show  all  image  files  on  local  disk  docker  image  ls  echo  echo  show  all  r,amazon
egistered  images  in  rmars  repository  aws  ecr  describe  images  repository  name  rmarstrain  file  iris  csv  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  train  file  upload  file  train  file  aws  s3  ls  um  ds  testbucket  sagemaker  DEMO  byo  train  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  train  file  for  object  in  boto3  Session  resource  s3  Bucket  bucket  objects  all  print  object  region  boto3  Session  region  name  account  boto3  client  sts  get  caller  identity  get  Account  print  account  job  DEMO  byo  time  strftime  time  gmtime  print  Training  job  job  training  params  RoleArn  role  TrainingJobName  job  AlgorithmSpecification  TrainingImage  dkr  ecr  amazonaws  com  rmars  latest  format  account  region  TrainingInputMode  File  ResourceConfig  InstanceCount  InstanceType  ml  m4  xlarge  VolumeSizeInGB  10  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataTy,amazon
pe  S3Prefix  S3Uri  s3  train  format  bucket  prefix  S3DataDistributionType  FullyReplicated  CompressionType  None  RecordWrapperType  None  OutputDataConfig  S3OutputPath  s3  output  format  bucket  prefix  HyperParameters  target  Sepal  Length  degree  StoppingCondition  MaxRuntimeInSeconds  60  60  print  training  params  print  json  dumps  training  params  indent  time  sm  boto3  client  sagemaker  sm  create  training  job  training  params  status  sm  describe  training  job  TrainingJobName  job  TrainingJobStatus  print  status  sm  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  status  sm  describe  training  job  TrainingJobName  job  TrainingJobStatus  print  Training  job  ended  with  status  status  if  status  Failed  message  sm  describe  training  job  TrainingJobName  job  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  trained  model  now  saved  in  S3  bucket  sagema,amazon
ker  DEMO  byo  output  DEMO  byo  2018  09  05  12  37  22  output  aws  s3  ls  um  ds  testbucket  sagemaker  DEMO  byo  output  DEMO  byo  2018  09  05  12  37  22  output  hosting  container  Image  dkr  ecr  amazonaws  com  rmars  latest  format  account  region  ModelDataUrl  sm  describe  training  job  TrainingJobName  job  ModelArtifacts  S3ModelArtifacts  print  json  dumps  hosting  container  indent  print  create  model  response  sm  create  model  ModelName  job  ExecutionRoleArn  role  PrimaryContainer  hosting  container  print  create  model  response  ModelArn  print  create  model  response  ModelArn  list  registered  models  aws  sagemaker  list  modelsr  endpoint  config  DEMO  byo  config  time  strftime  time  gmtime  print  endpoint  config  create  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  endpoint  config  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  job  VariantName  AllTraffic  print  Endpoint  Config  Arn,amazon
  create  endpoint  config  response  EndpointConfigArn  list  registered  endpoint  configurations  aws  sagemaker  list  endpoint  configs  time  endpoint  DEMO  endpoint  time  strftime  time  gmtime  print  endpoint  create  endpoint  response  sm  create  endpoint  EndpointName  endpoint  EndpointConfigName  endpoint  config  print  create  endpoint  response  EndpointArn  resp  sm  describe  endpoint  EndpointName  endpoint  status  resp  EndpointStatus  print  Status  status  try  sm  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  finally  resp  sm  describe  endpoint  EndpointName  endpoint  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Status  status  if  status  InService  raise  Exception  Endpoint  creation  did  not  succeed  list  registered  endpoint  configurations  aws  sagemaker  list  endpointsiris  pd  read  csv  iris  csv  runtime  boto3  Session  client  runtime  sagemaker  payload  iris  drop  Sepal  Length  axis  to  csv  index  False  print  pa,amazon
yload  102  response  runtime  invoke  endpoint  EndpointName  endpoint  ContentType  text  csv  Body  payload  result  json  loads  response  Body  read  decode  result  using  curl  https  runtime  sagemaker  eu  central  amazonaws  com  endpoints  DEMO  endpoint  201809051316  invocations  curl  POST  endpoints  DEMO  endpoint  201809051316  invocations  HTTP  Content  Type  ContentType  Accept  Accept  Amzn  SageMaker  Custom  Attributes  CustomAttributes  Body  curl  https  runtime  sagemaker  eu  central  amazonaws  com  endpoints  DEMO  endpoint  201809051316  invocations  8080  pingplt  scatter  iris  Sepal  Length  np  fromstring  result  sep  plt  show  delete  endpoint  sm  delete  endpoint  EndpointName  endpoint  delete  endpoint  configuration  sm  delete  endpoint  config  EndpointConfigName  endpoint  config  delete  registered  model  sm  delete  model  ModelName  job  ,amazon
import  os  import  boto3  import  sagemaker  from  sagemaker  mxnet  import  MXNet  from  mxnet  import  gluon  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  bash  sample  notebooks  sagemaker  python  sdk  mxnet  gluon  mnist  setup  sh  role  get  execution  role  gluon  data  vision  MNIST  data  train  train  True  gluon  data  vision  MNIST  data  test  train  False  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  mnist  cat  mnist  py  MXNet  mnist  py  role  role  train  instance  count  train  instance  type  local  hyperparameters  batch  size  100  epochs  10  learning  rate  momentum  log  interval  100  fit  inputs  predictor  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  IPython  display  import  HTML  HTML  open  input  html  read  response  predictor  predict  data  print  int  response  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
spark  conf  set  fs  azure  account  key  storage  account  name  blob  core  windows  net  storage  access  key  spark  conf  set  fs  azure  account  key  databrickstore  blob  core  windows  net  S1PtMWvUw5If1Z8FMzXAxC7OMw9G5Go8BGCXJ81qpFVYpZ9dpXOnU4zlg0PbldKkbLIbmbv02WoJsgYLGKIfgg  dbutils  fs  ls  wasbs  your  container  name  your  storage  account  name  blob  core  windows  net  your  directory  name  dbutils  fs  ls  wasbs  dbdemo01  databrickstore  blob  core  windows  net  Mount  Blob  storage  container  or  folder  inside  container  dbutils  fs  mount  source  wasbs  your  container  name  your  storage  account  name  blob  core  windows  net  your  directory  name  mount  point  mount  point  path  extra  configs  conf  key  conf  value  note  mount  point  is  DBFS  path  and  the  path  must  be  under  mnt  dbutils  fs  mount  source  wasbs  dbdemo01  databrickstore  blob  core  windows  net  mount  point  mnt  dbdemo01  extra  configs  fs  azure  account  key  databrickstore  blob  core  ,microsoft
windows  net  S1PtMWvUw5If1Z8FMzXAxC7OMw9G5Go8BGCXJ81qpFVYpZ9dpXOnU4zlg0PbldKkbLIbmbv02WoJsgYLGKIfgg  Access  files  in  your  container  as  if  they  were  local  files  TEXT  df  spark  read  text  mnt  mount  point  path  JSON  df  spark  read  json  mnt  mount  point  path  df  spark  read  json  mnt  small  radio  json  json  dbdemo01  display  df  df  show  unmount  if  needed  dbutils  fs  unmount  mount  point  path  dbutils  fs  unmount  mnt  dbdemo01  specificColumnsDf  df  select  firstname  lastname  gender  location  level  specificColumnsDf  show  renamedColumnsDF  specificColumnsDf  withColumnRenamed  level  subscription  type  renamedColumnsDF  show  writeConfig  Endpoint  https  cosmosdb  account  name  documents  azure  com  443  Masterkey  Cosmosdb  master  key  string  Database  database  name  Collection  collection  name  Upsert  true  Write  configuration  writeConfig  Endpoint  https  dbstreamdemo  documents  azure  com  443  Masterkey  ekRLXkETPJ93s6XZz4YubZOw1mjSnoO5Bhz1Gk29bVxCbtgt,microsoft
KmiyRz4SogOSxLOGTouXbwlaAHcHOzct4JVwtQ  Database  etl  Collection  outcol01  Upsert  true  Write  to  Cosmos  DB  from  the  renamedColumnsDF  DataFrame  renamedColumnsDF  write  format  com  microsoft  azure  cosmosdb  spark  options  writeConfig  save  ,microsoft
import  sagemaker  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  Get  SageMaker  compatible  role  used  by  this  Notebook  Instance  role  get  execution  role  bin  bash  setup  shimport  chainer  train  test  chainer  datasets  get  mnist  import  os  import  shutil  import  numpy  as  np  train  images  np  array  data  for  data  in  train  train  labels  np  array  data  for  data  in  train  test  images  np  array  data  for  data  in  test  test  labels  np  array  data  for  data  in  test  try  os  makedirs  tmp  data  train  os  makedirs  tmp  data  test  np  savez  tmp  data  train  train  npz  images  train  images  labels  train  labels  np  savez  tmp  data  test  test  npz  images  test  images  labels  test  labels  train  input  sagemaker  session  upload  data  path  os  path  join  tmp  data  train  key  prefix  notebook  chainer  mnist  test  input  sagemaker  session  upload  data  path  os  path  join  tmp  data  test  key  prefix  notebook  ch,amazon
ainer  mnist  finally  shutil  rmtree  tmp  data  pygmentize  chainer  mnist  single  machine  py  import  subprocess  from  sagemaker  chainer  estimator  import  Chainer  instance  type  local  if  subprocess  call  nvidia  smi  Set  type  to  GPU  if  one  is  present  instance  type  local  gpu  print  Instance  type  instance  type  chainer  estimator  Chainer  entry  point  chainer  mnist  single  machine  py  role  role  train  instance  count  train  instance  type  instance  type  hyperparameters  epochs  batch  size  128  chainer  estimator  fit  train  train  input  test  test  input  import  glob  import  shutil  try  os  makedirs  output  single  machine  mnist  except  OSError  pass  chainer  training  job  chainer  estimator  latest  training  job  name  desc  chainer  estimator  sagemaker  session  sagemaker  client  describe  training  job  TrainingJobName  chainer  training  job  output  data  desc  ModelArtifacts  S3ModelArtifacts  replace  model  output  for  file  in  glob  glob  output  ,amazon
data  png  recursive  True  shutil  copy  file  output  single  machine  mnist  from  IPython  display  import  Image  from  IPython  display  import  display  accuracy  graph  Image  filename  output  single  machine  mnist  accuracy  png  width  800  height  800  loss  graph  Image  filename  output  single  machine  mnist  loss  png  width  800  height  800  display  accuracy  graph  loss  graph  predictor  chainer  estimator  deploy  initial  instance  count  instance  type  instance  type  import  random  import  matplotlib  pyplot  as  plt  num  samples  indices  random  sample  range  test  images  shape  num  samples  images  labels  test  images  indices  test  labels  indices  for  in  range  num  samples  plt  subplot  num  samples  plt  imshow  images  reshape  28  28  cmap  gray  plt  title  labels  plt  axis  off  prediction  predictor  predict  images  predicted  label  prediction  argmax  axis  print  The  predicted  labels  are  format  predicted  label  from  IPython  display  import  HTML  ,amazon
HTML  open  input  html  read  image  np  array  data  dtype  np  float32  prediction  predictor  predict  image  predicted  label  prediction  argmax  axis  print  What  you  wrote  is  format  predicted  label  chainer  estimator  delete  endpoint  ,amazon
import  sagemaker  import  boto3  from  sagemaker  tuner  import  IntegerParameter  CategoricalParameter  ContinuousParameter  HyperparameterTuner  import  numpy  as  np  For  matrix  operations  and  numerical  processing  import  pandas  as  pd  For  munging  tabular  data  import  os  region  boto3  Session  region  name  smclient  boto3  Session  client  sagemaker  role  sagemaker  get  execution  role  bucket  sagemaker  Session  default  bucket  prefix  sagemaker  DEMO  hpo  xgboost  dm  wget  https  archive  ics  uci  edu  ml  machine  learning  databases  00222  bank  additional  zip  unzip  bank  additional  zipdata  pd  read  csv  bank  additional  bank  additional  full  csv  sep  pd  set  option  display  max  columns  500  Make  sure  we  can  see  all  of  the  columns  pd  set  option  display  max  rows  50  Keep  the  output  on  one  page  datadata  no  previous  contact  np  where  data  pdays  999  Indicator  variable  to  capture  when  pdays  takes  value  of  999  data  not  working  np,amazon
  where  np  in1d  data  job  student  retired  unemployed  Indicator  for  individuals  not  actively  employed  model  data  pd  get  dummies  data  Convert  categorical  variables  to  sets  of  indicators  model  datamodel  data  model  data  drop  duration  emp  var  rate  cons  price  idx  cons  conf  idx  euribor3m  nr  employed  axis  train  data  validation  data  test  data  np  split  model  data  sample  frac  random  state  1729  int  len  model  data  int  len  model  data  pd  concat  train  data  yes  train  data  drop  no  yes  axis  axis  to  csv  train  csv  index  False  header  False  pd  concat  validation  data  yes  validation  data  drop  no  yes  axis  axis  to  csv  validation  csv  index  False  header  False  pd  concat  test  data  yes  test  data  drop  no  yes  axis  axis  to  csv  test  csv  index  False  header  False  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  train  csv  upload  file  train  csv  boto3  Session  resource  s3  Bucket,amazon
  bucket  Object  os  path  join  prefix  validation  validation  csv  upload  file  validation  csv  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  sess  sagemaker  Session  container  get  image  uri  region  xgboost  xgb  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  m4  xlarge  output  path  s3  output  format  bucket  prefix  sagemaker  session  sess  xgb  set  hyperparameters  eval  metric  auc  objective  binary  logistic  num  round  100  rate  drop  tweedie  variance  power  hyperparameter  ranges  eta  ContinuousParameter  min  child  weight  ContinuousParameter  10  alpha  ContinuousParameter  max  depth  IntegerParameter  10  objective  metric  name  validation  auc  tuner  HyperparameterTuner  xgb  objective  metric  name  hyperparameter  ranges  max  jobs  20  max  parallel  jobs  s3  input  train  sagemaker  s3  input  s3  data  s3  train  format  bucket  prefix  content  type  csv  s3  input  validation  sagemaker  s3,amazon
  input  s3  data  s3  validation  format  bucket  prefix  content  type  csv  tuner  fit  train  s3  input  train  validation  s3  input  validation  boto3  client  sagemaker  describe  hyper  parameter  tuning  job  HyperParameterTuningJobName  tuner  latest  tuning  job  job  name  HyperParameterTuningJobStatus  ,amazon
mkdir  data  wget  data  aclImdb  v1  tar  gz  http  ai  stanford  edu  amaas  data  sentiment  aclImdb  v1  tar  gz  tar  zxf  data  aclImdb  v1  tar  gz  dataimport  os  import  glob  def  read  imdb  data  data  dir  data  aclImdb  data  labels  for  data  type  in  train  test  data  data  type  labels  data  type  for  sentiment  in  pos  neg  data  data  type  sentiment  labels  data  type  sentiment  path  os  path  join  data  dir  data  type  sentiment  txt  files  glob  glob  path  for  in  files  with  open  as  review  data  data  type  sentiment  append  review  read  Here  we  represent  positive  review  by  and  negative  review  by  labels  data  type  sentiment  append  if  sentiment  pos  else  assert  len  data  data  type  sentiment  len  labels  data  type  sentiment  data  size  does  not  match  labels  size  format  data  type  sentiment  return  data  labelsdata  labels  read  imdb  data  print  IMDB  reviews  train  pos  neg  test  pos  neg  format  len  data  train  pos  len  data ,amazon
 train  neg  len  data  test  pos  len  data  test  neg  from  sklearn  utils  import  shuffle  def  prepare  imdb  data  data  labels  Prepare  training  and  test  sets  from  IMDb  movie  reviews  Combine  positive  and  negative  reviews  and  labels  data  train  data  train  pos  data  train  neg  data  test  data  test  pos  data  test  neg  labels  train  labels  train  pos  labels  train  neg  labels  test  labels  test  pos  labels  test  neg  Shuffle  reviews  and  corresponding  labels  within  training  and  test  sets  data  train  labels  train  shuffle  data  train  labels  train  data  test  labels  test  shuffle  data  test  labels  test  Return  unified  training  data  test  data  training  labels  test  labets  return  data  train  data  test  labels  train  labels  testtrain  test  train  test  prepare  imdb  data  data  labels  print  IMDb  reviews  combined  train  test  format  len  train  len  test  train  100  import  nltk  nltk  download  stopwords  from  nltk  corpus  import  stop,amazon
words  from  nltk  stem  porter  import  stemmer  PorterStemmer  import  re  from  bs4  import  BeautifulSoup  def  review  to  words  review  text  BeautifulSoup  review  html  parser  get  text  Remove  HTML  tags  text  re  sub  zA  Z0  text  lower  Convert  to  lower  case  words  text  split  Split  string  into  words  words  for  in  words  if  not  in  stopwords  words  english  Remove  stopwords  words  PorterStemmer  stem  for  in  words  stem  return  wordsimport  pickle  cache  dir  os  path  join  cache  sentiment  analysis  where  to  store  cache  files  os  makedirs  cache  dir  exist  ok  True  ensure  cache  directory  exists  def  preprocess  data  data  train  data  test  labels  train  labels  test  cache  dir  cache  dir  cache  file  preprocessed  data  pkl  Convert  each  review  to  words  read  from  cache  if  available  If  cache  file  is  not  None  try  to  read  from  it  first  cache  data  None  if  cache  file  is  not  None  try  with  open  os  path  join  cache  dir  cach,amazon
e  file  rb  as  cache  data  pickle  load  print  Read  preprocessed  data  from  cache  file  cache  file  except  pass  unable  to  read  from  cache  but  that  okay  If  cache  is  missing  then  do  the  heavy  lifting  if  cache  data  is  None  Preprocess  training  and  test  data  to  obtain  words  for  each  review  words  train  list  map  review  to  words  data  train  words  test  list  map  review  to  words  data  test  words  train  review  to  words  review  for  review  in  data  train  words  test  review  to  words  review  for  review  in  data  test  Write  to  cache  file  for  future  runs  if  cache  file  is  not  None  cache  data  dict  words  train  words  train  words  test  words  test  labels  train  labels  train  labels  test  labels  test  with  open  os  path  join  cache  dir  cache  file  wb  as  pickle  dump  cache  data  print  Wrote  preprocessed  data  to  cache  file  cache  file  else  Unpack  data  loaded  from  cache  file  words  train  words  test  labels  tr,amazon
ain  labels  test  cache  data  words  train  cache  data  words  test  cache  data  labels  train  cache  data  labels  test  return  words  train  words  test  labels  train  labels  test  Preprocess  data  train  test  train  test  preprocess  data  train  test  train  test  import  numpy  as  np  from  sklearn  feature  extraction  text  import  CountVectorizer  from  sklearn  externals  import  joblib  joblib  is  an  enhanced  version  of  pickle  that  is  more  efficient  for  storing  NumPy  arrays  def  extract  BoW  features  words  train  words  test  vocabulary  size  5000  cache  dir  cache  dir  cache  file  bow  features  pkl  Extract  Bag  of  Words  for  given  set  of  documents  already  preprocessed  into  words  If  cache  file  is  not  None  try  to  read  from  it  first  cache  data  None  if  cache  file  is  not  None  try  with  open  os  path  join  cache  dir  cache  file  rb  as  cache  data  joblib  load  print  Read  features  from  cache  file  cache  file  except  pass  una,amazon
ble  to  read  from  cache  but  that  okay  If  cache  is  missing  then  do  the  heavy  lifting  if  cache  data  is  None  Fit  vectorizer  to  training  documents  and  use  it  to  transform  them  NOTE  Training  documents  have  already  been  preprocessed  and  tokenized  into  words  pass  in  dummy  functions  to  skip  those  steps  preprocessor  lambda  vectorizer  CountVectorizer  max  features  vocabulary  size  preprocessor  lambda  tokenizer  lambda  already  preprocessed  features  train  vectorizer  fit  transform  words  train  toarray  Apply  the  same  vectorizer  to  transform  the  test  documents  ignore  unknown  words  features  test  vectorizer  transform  words  test  toarray  NOTE  Remember  to  convert  the  features  using  toarray  for  compact  representation  Write  to  cache  file  for  future  runs  store  vocabulary  as  well  if  cache  file  is  not  None  vocabulary  vectorizer  vocabulary  cache  data  dict  features  train  features  train  features  test  features  ,amazon
test  vocabulary  vocabulary  with  open  os  path  join  cache  dir  cache  file  wb  as  joblib  dump  cache  data  print  Wrote  features  to  cache  file  cache  file  else  Unpack  data  loaded  from  cache  file  features  train  features  test  vocabulary  cache  data  features  train  cache  data  features  test  cache  data  vocabulary  Return  both  the  extracted  features  as  well  as  the  vocabulary  return  features  train  features  test  vocabulary  Extract  Bag  of  Words  features  for  both  training  and  test  datasets  train  test  vocabulary  extract  BoW  features  train  test  import  pandas  as  pd  val  pd  DataFrame  train  10000  train  pd  DataFrame  train  10000  val  pd  DataFrame  train  10000  train  pd  DataFrame  train  10000  First  we  make  sure  that  the  local  directory  in  which  we  like  to  store  the  training  and  validation  csv  files  exists  data  dir  data  xgboost  if  not  os  path  exists  data  dir  os  makedirs  data  dir  First  save  the  test  ,amazon
data  to  test  csv  in  the  data  dir  directory  Note  that  we  do  not  save  the  associated  ground  truth  labels  instead  we  will  use  them  later  to  compare  with  our  model  output  Solution  The  test  data  shouldn  contain  the  ground  truth  labels  as  they  are  what  the  model  is  trying  to  predict  We  will  end  up  using  them  afterward  to  compare  the  predictions  to  pd  concat  test  test  axis  to  csv  os  path  join  data  dir  test  csv  header  False  index  False  pd  DataFrame  test  to  csv  os  path  join  data  dir  test  csv  header  False  index  False  pd  concat  val  val  axis  to  csv  os  path  join  data  dir  validation  csv  header  False  index  False  pd  concat  train  train  axis  to  csv  os  path  join  data  dir  train  csv  header  False  index  False  To  save  bit  of  memory  we  can  set  text  train  val  train  and  val  to  None  test  train  val  train  val  Noneimport  sagemaker  session  sagemaker  Session  Store  the  current  SageM,amazon
aker  session  S3  prefix  which  folder  will  we  use  prefix  sentiment  xgboost  test  location  session  upload  data  os  path  join  data  dir  test  csv  key  prefix  prefix  val  location  session  upload  data  os  path  join  data  dir  validation  csv  key  prefix  prefix  train  location  session  upload  data  os  path  join  data  dir  train  csv  key  prefix  prefix  from  sagemaker  import  get  execution  role  Our  current  execution  role  is  require  when  creating  the  model  as  the  training  and  inference  code  will  need  to  access  the  model  artifacts  role  get  execution  role  We  need  to  retrieve  the  location  of  the  container  which  is  provided  by  Amazon  for  using  XGBoost  As  matter  of  convenience  the  training  and  inference  code  both  use  the  same  container  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  session  boto  region  name  xgboost  TODO  Create  SageMaker  estimator  using  the  containe,amazon
r  location  determined  in  the  previous  cell  It  is  recommended  that  you  use  single  training  instance  of  type  ml  m4  xlarge  It  is  also  recommended  that  you  use  s3  output  format  session  default  bucket  prefix  as  the  output  path  xgb  None  Solution  xgb  sagemaker  estimator  Estimator  container  The  location  of  the  container  we  wish  to  use  role  What  is  our  current  IAM  Role  train  instance  count  How  many  compute  instances  train  instance  type  ml  m4  xlarge  What  kind  of  compute  instances  output  path  s3  output  format  session  default  bucket  prefix  sagemaker  session  session  TODO  Set  the  XGBoost  hyperparameters  in  the  xgb  object  Don  forget  that  in  this  case  we  have  binary  label  so  we  should  be  using  the  binary  logistic  objective  Solution  xgb  set  hyperparameters  max  depth  eta  gamma  min  child  weight  subsample  silent  objective  binary  logistic  early  stopping  rounds  10  num  round  500  First  make,amazon
  sure  to  import  the  relevant  objects  used  to  construct  the  tuner  from  sagemaker  tuner  import  IntegerParameter  ContinuousParameter  HyperparameterTuner  TODO  Create  the  hyperparameter  tuner  object  xgb  hyperparameter  tuner  None  Solution  xgb  hyperparameter  tuner  HyperparameterTuner  estimator  xgb  The  estimator  object  to  use  as  the  basis  for  the  training  jobs  objective  metric  name  validation  rmse  The  metric  used  to  compare  trained  models  objective  type  Minimize  Whether  we  wish  to  minimize  or  maximize  the  metric  max  jobs  20  The  total  number  of  models  to  train  max  parallel  jobs  The  number  of  models  to  train  in  parallel  hyperparameter  ranges  max  depth  IntegerParameter  12  eta  ContinuousParameter  05  min  child  weight  IntegerParameter  subsample  ContinuousParameter  gamma  ContinuousParameter  10  s3  input  train  sagemaker  s3  input  s3  data  train  location  content  type  csv  s3  input  validation  sagemaker  s3,amazon
  input  s3  data  val  location  content  type  csv  xgb  hyperparameter  tuner  fit  train  s3  input  train  validation  s3  input  validation  xgb  hyperparameter  tuner  wait  TODO  Create  new  estimator  object  attached  to  the  best  training  job  found  during  hyperparameter  tuning  xgb  attached  None  Solution  xgb  attached  sagemaker  estimator  Estimator  attach  xgb  hyperparameter  tunerparameter  tuner  best  training  job  TODO  Create  transformer  object  from  the  attached  estimator  Using  an  instance  count  of  and  an  instance  type  of  ml  m4  xlarge  should  be  more  than  enough  xgb  transformer  None  Solution  xgb  transformer  xgb  attached  transformer  instance  count  instance  type  ml  m4  xlarge  TODO  Start  the  transform  job  Make  sure  to  specify  the  content  type  and  the  split  type  of  the  test  data  xgb  transformer  transform  test  location  content  type  text  csv  split  type  Line  xgb  transformer  wait  aws  s3  cp  recursive  xgb  tra,amazon
nsformer  output  path  data  dirpredictions  pd  read  csv  os  path  join  data  dir  test  csv  out  header  None  predictions  round  num  for  num  in  predictions  squeeze  values  from  sklearn  metrics  import  accuracy  score  accuracy  score  test  predictions  First  we  will  remove  all  of  the  files  contained  in  the  data  dir  directory  rm  data  dir  And  then  we  delete  the  directory  itself  rmdir  data  dir  Similarly  we  will  remove  the  files  in  the  cache  dir  directory  and  the  directory  itself  rm  cache  dir  rmdir  cache  dir  ,amazon
az  login  az  account  list  output  table  az  account  set  subscription  subscription  id  subscription  name  location  West  Europe  resourceGroup  group1  appserviceplanName  appserviceplan1  appserviceplanSKU  S2  webappName  testwebappm3  whos  az  group  create  name  resourceGroup  location  location  az  appservice  plan  create  name  appserviceplanName  resource  group  resourceGroup  sku  appserviceplanSKU  az  appservice  web  create  name  webappName  resource  group  resourceGroup  plan  appserviceplanName  az  appservice  web  source  control  config  repo  url  https  github  com  prashanthmadi  azure  flask  httpplatformhandler  no  site  ext  git  name  webappName  resource  group  resourceGroup  ,microsoft
import  os  import  time  import  boto3  import  numpy  as  np  import  pandas  as  pd  from  numpy  import  genfromtxt  import  keras  from  keras  models  import  Sequential  from  keras  layers  import  Dense  Activation  Dropout  from  keras  layers  import  LSTM  from  keras  models  import  load  model  from  sklearn  utils  import  shuffle  import  re  import  string  import  nltk  from  nltk  corpus  import  stopwords  from  nltk  tokenize  import  word  tokenize  from  nltk  stem  porter  import  PorterStemmer  import  time  nltk  download  punkt  nltk  download  stopwords  stop  words  set  stopwords  words  english  mkdir  download  cd  download  wget  https  www  ssa  gov  oact  babynames  names  zip  unzip  qq  names  zip  cat  download  yob  txt  download  allnames  txt  filename  download  allnames  txt  data  pd  read  csv  filename  sep  names  Name  Gender  Count  rm  rf  download  pip  install  tqdmfrom  tqdm  import  tqdm  trange  grouped  data  data  groupby  Name  Gender  apply  lambda  ,amazon
Count  sum  to  frame  grouped  data  columns  Count  names  max  count  len  grouped  data  index  values  for  in  tqdm  range  max  count  unit  records  if  and  grouped  data  index  grouped  data  index  if  grouped  data  values  grouped  data  values  names  grouped  data  index  grouped  data  index  else  names  grouped  data  index  grouped  data  index  else  names  grouped  data  index  grouped  data  index  clean  data  pd  DataFrame  list  names  items  columns  Name  Gender  sample  frac  reset  index  drop  True  clean  data  clean  data  sample  frac  reset  index  drop  True  print  clean  data  shape  print  clean  data  loc  clean  data  Name  Mary  print  clean  data  loc  clean  data  Name  John  mkdir  data  clean  data  to  csv  data  name  gender  txt  index  False  header  False  s3bucketname  input  Enter  the  Name  of  your  S3  bucket  s3  boto3  resource  s3  s3  meta  client  upload  file  data  name  gender  txt  s3bucketname  data  name  gender  txt  s3  ObjectAcl  s3bucketn,amazon
ame  data  name  gender  txt  put  ACL  public  read  filename  https  s3  amazonaws  com  data  name  gender  txt  format  s3bucketname  training  data  pd  read  csv  filename  sep  names  Name  Gender  training  data  shuffle  training  data  number  of  names  num  names  training  data  shape  print  Number  of  names  format  num  names  length  of  longest  name  max  name  length  training  data  Name  map  len  max  print  Maximum  length  of  name  format  max  name  length  Concatenate  all  names  into  one  long  string  case  insensitive  names  training  data  Name  values  txt  for  in  names  txt  lower  Created  sorted  set  of  alphabets  where  each  alphabet  appears  only  once  chars  sorted  set  txt  alphabet  size  len  chars  print  Alphabet  size  format  alphabet  size  print  Alphabets  used  format  chars  Assign  indices  to  characters  char  indices  dict  str  chr  for  in  enumerate  range  97  123  alphabet  size  123  97  char  indices  max  name  length  max  name  lengt,amazon
h  print  Character  indices  char  indices  np  zeros  num  names  max  name  length  alphabet  size  print  shape  for  name  in  enumerate  names  name  name  lower  for  char  in  enumerate  name  char  indices  char  np  set  printoptions  linewidth  200  print  np  where  names  Mary  np  ones  num  names  training  data  Gender  training  data  Gender  print  shape  data  dim  alphabet  size  timesteps  max  name  length  num  classes  2model  Sequential  model  add  LSTM  512  return  sequences  True  input  shape  timesteps  data  dim  model  add  Dropout  model  add  LSTM  512  return  sequences  False  model  add  Dropout  model  add  Dense  num  classes  model  add  Activation  sigmoid  model  compile  loss  categorical  crossentropy  optimizer  adam  metrics  accuracy  num  epochs  int  input  How  many  epochs  do  you  want  to  run  model  fit  validation  split  10  epochs  num  epochs  batch  size  128  test  name  input  Enter  name  val  no  special  re  compile  re  escape  string  punctu,amazon
ation  sub  test  name  val  space  collapse  re  sub  val  no  special  strip  names  test  word  tokenize  val  space  collapse  num  test  len  names  test  test  np  zeros  num  test  max  name  length  alphabet  size  for  name  in  enumerate  names  test  name  name  lower  for  char  in  enumerate  name  test  char  indices  char  predictions  model  predict  test  for  name  in  enumerate  names  test  print  is  format  names  test  Male  if  predictions  predictions  else  Female  mkdir  model  model  save  model  lstm  gender  classifier  model  h5  np  save  model  lstm  gender  classifier  indices  npy  char  indices  tar  zcvf  model  tar  gz  model  s3  meta  client  upload  file  model  tar  gz  s3bucketname  model  model  tar  gz  rm  rf  modelos  chdir  container  os  getcwd  ls  Rl  writefile  byoa  predictor  py  This  is  the  file  that  implements  flask  server  to  do  inferences  It  the  file  that  you  will  modify  to  implement  the  scoring  for  your  own  algorithm  from  fut,amazon
ure  import  print  function  import  os  import  json  import  pickle  from  io  import  StringIO  import  sys  import  signal  import  traceback  import  numpy  as  np  import  keras  from  keras  models  import  Sequential  from  keras  layers  import  Dense  Dropout  from  keras  layers  import  Embedding  from  keras  layers  import  LSTM  from  keras  models  import  load  model  import  flask  import  tensorflow  as  tf  import  pandas  as  pd  from  os  import  listdir  sep  from  os  path  import  abspath  basename  isdir  from  sys  import  argv  prefix  opt  ml  model  model  path  os  path  join  prefix  model  singleton  for  holding  the  model  This  simply  loads  the  model  and  holds  it  It  has  predict  function  that  does  prediction  based  on  the  model  and  the  input  data  class  ScoringService  object  model  type  None  Where  we  keep  the  model  type  qualified  by  hyperparameters  used  during  training  model  None  Where  we  keep  the  model  when  it  loaded  graph  N,amazon
one  indices  None  Where  we  keep  the  indices  of  Alphabet  when  it  loaded  classmethod  def  get  indices  cls  Get  the  indices  for  Alphabet  for  this  instance  loading  it  if  it  not  already  loaded  if  cls  indices  None  model  type  lstm  gender  classifier  index  path  os  path  join  model  path  indices  npy  format  model  type  if  os  path  exists  index  path  cls  indices  np  load  index  path  item  else  print  Character  Indices  not  found  return  cls  indices  classmethod  def  get  model  cls  Get  the  model  object  for  this  instance  loading  it  if  it  not  already  loaded  if  cls  model  None  model  type  lstm  gender  classifier  mod  path  os  path  join  model  path  model  h5  format  model  type  if  os  path  exists  mod  path  cls  model  load  model  mod  path  cls  model  make  predict  function  cls  graph  tf  get  default  graph  else  print  LSTM  Model  not  found  return  cls  model  classmethod  def  predict  cls  input  mod  cls  get  model  in,amazon
d  cls  get  indices  result  if  mod  None  print  Model  not  loaded  else  if  max  name  length  not  in  ind  max  name  length  15  alphabet  size  26  else  max  name  length  ind  max  name  length  ind  pop  max  name  length  None  alphabet  size  len  ind  inputs  list  input  strip  split  num  inputs  len  inputs  list  test  np  zeros  num  inputs  max  name  length  alphabet  size  for  name  in  enumerate  inputs  list  name  name  lower  strip  for  char  in  enumerate  name  if  char  in  ind  test  ind  char  with  cls  graph  as  default  predictions  mod  predict  test  for  name  in  enumerate  inputs  list  result  name  if  predictions  predictions  else  print  format  inputs  list  if  predictions  predictions  else  return  json  dumps  result  The  flask  app  for  serving  predictions  app  flask  Flask  name  app  route  ping  methods  GET  def  ping  Determine  if  the  container  is  working  and  healthy  Declare  it  healthy  if  we  can  load  the  model  successfully  model,amazon
  ScoringService  get  model  indices  ScoringService  get  indices  health  model  is  not  None  and  indices  is  not  None  status  200  if  health  else  404  return  flask  Response  response  status  status  mimetype  application  json  app  route  invocations  methods  POST  def  transformation  Do  an  inference  on  single  batch  of  data  data  None  Convert  from  CSV  to  pandas  if  flask  request  content  type  text  csv  data  flask  request  data  decode  utf  else  return  flask  Response  response  This  predictor  only  supports  CSV  data  status  415  mimetype  text  plain  print  Invoked  with  records  format  data  count  Do  the  prediction  predictions  ScoringService  predict  data  result  for  prediction  in  predictions  result  result  prediction  return  flask  Response  response  result  status  200  mimetype  text  csv  run  type  cpu  instance  class  p2  if  run  type  lower  gpu  else  t2  instance  type  ml  xlarge  format  instance  class  pipeline  name  gender  clas,amazon
sifier  run  input  Enter  run  version  run  name  pipeline  name  run  if  run  type  cpu  cp  Dockerfile  cpu  Dockerfile  if  run  type  gpu  cp  Dockerfile  gpu  Dockerfile  sh  build  and  push  sh  run  nameimport  sagemaker  sm  role  sagemaker  get  execution  role  print  Using  Role  format  sm  role  acc  boto3  client  sts  get  caller  identity  get  Account  reg  boto3  session  Session  region  name  sagemaker  boto3  client  sagemaker  Check  if  model  already  exists  model  name  model  format  run  name  models  sagemaker  list  models  NameContains  model  name  Models  model  exists  False  if  len  models  for  model  in  models  if  model  ModelName  model  name  model  exists  True  break  Delete  model  if  chosen  if  model  exists  True  choice  input  Model  already  exists  do  you  want  to  delete  and  create  fresh  one  if  choice  upper  sagemaker  delete  model  ModelName  model  name  model  exists  False  else  print  Model  already  exists  format  model  name  if  mod,amazon
el  exists  False  model  response  sagemaker  create  model  ModelName  model  name  PrimaryContainer  Image  dkr  ecr  amazonaws  com  latest  format  acc  reg  run  name  ModelDataUrl  s3  model  model  tar  gz  format  s3bucketname  ExecutionRoleArn  sm  role  Tags  Key  Name  Value  model  name  print  Created  at  format  model  response  ModelArn  model  response  ResponseMetadata  HTTPHeaders  date  Check  if  endpoint  configuration  already  exists  endpoint  config  name  endpoint  config  format  run  name  endpoint  configs  sagemaker  list  endpoint  configs  NameContains  endpoint  config  name  EndpointConfigs  endpoint  config  exists  False  if  len  endpoint  configs  for  endpoint  config  in  endpoint  configs  if  endpoint  config  EndpointConfigName  endpoint  config  name  endpoint  config  exists  True  break  Delete  endpoint  configuration  if  chosen  if  endpoint  config  exists  True  choice  input  Endpoint  Configuration  already  exists  do  you  want  to  delete  and  create ,amazon
 fresh  one  if  choice  upper  sagemaker  delete  endpoint  config  EndpointConfigName  endpoint  config  name  endpoint  config  exists  False  else  print  Endpoint  Configuration  already  exists  format  endpoint  config  name  if  endpoint  config  exists  False  endpoint  config  response  sagemaker  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  VariantName  default  ModelName  model  name  InitialInstanceCount  InstanceType  instance  type  InitialVariantWeight  Tags  Key  Name  Value  endpoint  config  name  print  Created  at  format  endpoint  config  response  EndpointConfigArn  endpoint  config  response  ResponseMetadata  HTTPHeaders  date  from  ipywidgets  import  widgets  from  IPython  display  import  display  Check  if  endpoint  already  exists  endpoint  name  endpoint  format  run  name  endpoints  sagemaker  list  endpoints  NameContains  endpoint  name  Endpoints  endpoint  exists  False  if  len  endpoints  for  endpoint  in  endpoints  if ,amazon
 endpoint  EndpointName  endpoint  name  endpoint  exists  True  break  Delete  endpoint  if  chosen  if  endpoint  exists  True  choice  input  Endpoint  already  exists  do  you  want  to  delete  and  create  fresh  one  if  choice  upper  sagemaker  delete  endpoint  EndpointName  endpoint  name  print  Deleting  Endpoint  format  endpoint  name  waiter  sagemaker  get  waiter  endpoint  deleted  waiter  wait  EndpointName  endpoint  name  WaiterConfig  Delay  MaxAttempts  100  endpoint  exists  False  print  Endpoint  deleted  format  endpoint  name  else  print  Endpoint  already  exists  format  endpoint  name  if  endpoint  exists  False  endpoint  response  sagemaker  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  Tags  Key  string  Value  endpoint  name  status  Creating  sleep  print  Endpoint  format  status  endpoint  name  bar  widgets  FloatProgress  min  description  Progress  instantiate  the  bar  display  bar  display  the  bar  while  status  In,amazon
Service  and  status  Failed  and  status  OutOfService  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  endpoint  response  EndpointStatus  time  sleep  sleep  bar  value  bar  value  if  bar  value  bar  max  bar  max  int  bar  max  05  if  status  InService  and  status  Failed  and  status  OutOfService  print  end  bar  max  bar  value  html  widgets  HTML  value  H2  Endpoint  H2  format  endpoint  response  EndpointName  status  display  html  test  name  input  Enter  name  val  no  special  re  compile  re  escape  string  punctuation  sub  test  name  val  space  collapse  re  sub  val  no  special  strip  names  test  word  tokenize  val  space  collapse  request  body  join  names  test  aws  sagemaker  runtime  invoke  endpoint  endpoint  name  run  name  endpoint  body  request  body  content  type  text  csv  outfile  cat  outfilelambda  client  boto3  client  lambda  lambda  client  update  function  configuration  FunctionName  IdentifyCustomerGender ,amazon
 Environment  Variables  sagemaker  endpoint  endpoint  response  EndpointName  table  name  UnicornCustomerFeedback  ,amazon
imports  import  boto3  AWS  python  SDK  for  accessing  AWS  services  import  numpy  as  np  Array  libraru  with  probability  and  statistics  capabilities  import  io  import  sagemaker  amazon  common  as  smac  Amazon  Sagemaker  common  library  that  includes  data  formats  import  sagemaker  sagemaker  python  sdk  import  os  from  sagemaker  predictor  import  csv  serializer  json  deserializer  sagemaker  prediction  sdk  from  sagemaker  import  get  execution  role  bucket  cyrusmv  sagemaker  demos  replace  this  with  your  own  bucket  local  data  processed  replace  with  your  local  destination  dist  visa  kaggle  data  replace  with  your  top  directory  in  S3  for  all  the  data  files  files  role  get  execution  role  this  is  SageMaker  role  that  would  be  later  used  for  authorizing  SageMaker  to  access  S3  print  role  sagemaker  session  sagemaker  Session  def  download  dir  client  resource  dist  local  bucket  paginator  client  get  paginator  list  object,amazon
s  for  result  in  paginator  paginate  Bucket  bucket  Delimiter  Prefix  dist  if  result  get  CommonPrefixes  is  not  None  for  subdir  in  result  get  CommonPrefixes  download  dir  client  resource  subdir  get  Prefix  local  bucket  if  result  get  Contents  is  not  None  for  file  in  result  get  Contents  if  not  os  path  exists  os  path  dirname  local  os  sep  file  get  Key  os  makedirs  os  path  dirname  local  os  sep  file  get  Key  print  bucket  source  file  local  format  bucket  file  get  Key  local  os  sep  file  get  Key  try  dest  local  os  sep  file  get  Key  key  dest  rsplit  key  key  rsplit  resource  meta  client  download  file  bucket  file  get  Key  dest  files  key  dest  except  IsADirectoryError  NotADirectoryError  print  WARNING  is  directory  skipping  download  operation  format  bucket  file  get  Key  def  start  client  boto3  client  s3  resource  boto3  resource  s3  download  dir  client  resource  local  local  bucket  bucket  dist  dist  pr,amazon
int  ndownload  completed  start  files  train  data  np  load  files  train  data  train  label  np  load  files  train  label  val  data  np  load  files  val  data  val  label  np  load  files  val  label  print  training  data  shape  training  label  shape  nValidation  data  shape  validation  label  shape  format  train  data  shape  train  label  shape  val  data  shape  val  label  shape  train  set  train  data  train  label  test  set  val  data  val  label  vectors  np  array  tolist  for  in  train  set  astype  float32  labels  np  array  tolist  for  in  train  set  astype  float32  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  vectors  labels  buf  seek  key  recordio  pb  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  dist  key  upload  fileobj  buf  s3  train  data  s3  format  bucket  dist  key  print  uploaded  training  data  location  format  s3  train  data  output  location  s3  output  format  bucket  prefix  print  training  artifacts  will  be  up,amazon
loaded  to  format  output  location  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  linear  learner  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  linear  learner  latest  sess  sagemaker  Session  linear  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  role  S3  role  so  the  notebook  can  read  the  data  and  upload  the  model  train  instance  count  number  of  instances  for  training  train  instance  type  ml  c4  8xlarge  type  of  training  instance  output  path  output  location  s3  location  for  uploading  trained  mdoel  sagemaker  session  sess  linear  set  hyperparameters  feature  dim  30  dataset  has  30  columns  features  predictor  type  binary  classifier  we  predict  binary  value  it  could  have  been  regressor  mini  batch  size  200 ,amazon
 making  recall  the  selection  criteria  and  changin  calibration  samples  that  are  used  for  threshold  setting  binary  classifier  model  selection  criteria  recall  at  target  precision  linear  fit  train  s3  train  data  linear  predictor  linear  deploy  initial  instance  count  Initial  number  of  instances  Autoscaling  can  increase  the  number  of  instances  instance  type  ml  m4  xlarge  instance  typetype  linear  predictor  linear  predictor  content  type  text  csv  linear  predictor  serializer  csv  serializer  linear  predictor  deserializer  json  deserializerpredictions  for  array  in  np  array  split  test  set  100  result  linear  predictor  predict  array  predictions  predicted  label  for  in  result  predictions  predictions  np  array  predictions  import  pandas  as  pd  pd  crosstab  test  set  predictions  rownames  actuals  colnames  predictions  print  false  positive  after  Hyper  Parameter  change  format  119  28  119  print  false  positive  before  Hype,amazon
r  Parameter  change  format  118  29  118  linear  delete  endpoint  ,amazon
The  code  was  removed  by  DSX  for  sharing  The  code  was  removed  by  DSX  for  sharing  import  matplotlib  pyplot  as  plt  matplotlib  inline  from  pandas  plotting  import  scatter  matrix  from  pandas  plotting  import  autocorrelation  plotdf  data  viz  df  data  copy  df  data  viz  pd  DataFrame  np  random  randn  1000  columns  Temperature  Light  Level  Relative  Humidity  Noise  Level  Pressure  params  Temperature  Light  Level  Relative  Humidity  Noise  Level  Pressure  scatter  matrix  df  data  viz  params  alpha  figsize  10  10  diagonal  kde  plt  show  pd  options  display  mpl  style  default  df  data  box  df  data  copy  df  data  box  boxplot  df  data  hist  plt  figure  temp  data  df  data  Temperature  autocorrelation  plot  temp  data  df  001  LNW  df  data  loc  df  data  sensor  H001LNW  df  001  LW  df  data  loc  df  data  sensor  H001LW  merging  the  files  into  one  file  to  get  all  the  values  for  room  We  will  also  sort  the  values  based  on  their,ibm
  timestamp  index  later  on  assuring  no  inconsistency  with  the  original  data  df  001  unsorted  pd  concat  df  001  LNW  df  001  LW  df  002  LNW  df  data  loc  df  data  sensor  H002LNW  df  002  LSE  df  data  loc  df  data  sensor  H002LSE  df  002  LSW  df  data  loc  df  data  sensor  H002LSW  df  002  unsorted  pd  concat  df  002  LNW  df  002  LSE  df  002  LSW  df  004  LE  df  data  loc  df  data  sensor  H004LE  df  004  LSW  df  data  loc  df  data  sensor  H004LSW  df  004  unsorted  pd  concat  df  004  LE  df  004  LSW  df  008  LE  df  data  loc  df  data  sensor  H008LE  df  008  LNE  df  data  loc  df  data  sensor  H008LNE  df  008  LNW  df  data  loc  df  data  sensor  H008LNW  df  008  LS  df  data  loc  df  data  sensor  H008LS  df  008  unsorted  pd  concat  df  008  LE  df  008  LNE  df  008  LNW  df  008  LS  df  013  LE  df  data  loc  df  data  sensor  H013LE  df  013  LNE  df  data  loc  df  data  sensor  H013LNE  df  013  LS  df  data  loc  df  data  sensor  H013LS  d,ibm
f  013  LSE  df  data  loc  df  data  sensor  H013LSE  df  013  LSW  df  data  loc  df  data  sensor  H013LSW  df  013  unsorted  pd  concat  df  013  LE  df  013  LNE  df  013  LS  df  013  LSE  df  013  LSW  df  015  LE  df  data  loc  df  data  sensor  H015LE  df  015  LW  df  data  loc  df  data  sensor  H015LW  df  015  unsorted  pd  concat  df  015  LE  df  015  LW  df  019  LN  df  data  loc  df  data  sensor  H019LN  df  019  LS  df  data  loc  df  data  sensor  H019LS  df  019  unsorted  pd  concat  df  019  LN  df  019  LS  setting  the  index  to  timestamp  to  facilitate  sorting  the  data  df  001  indexed  df  001  unsorted  set  index  timestamp  df  002  indexed  df  002  unsorted  set  index  timestamp  df  004  indexed  df  004  unsorted  set  index  timestamp  df  008  indexed  df  008  unsorted  set  index  timestamp  df  013  indexed  df  013  unsorted  set  index  timestamp  df  015  indexed  df  015  unsorted  set  index  timestamp  df  019  indexed  df  019  unsorted  set  index  tim,ibm
estamp  sorting  the  data  to  get  back  the  original  order  just  in  each  room  df  001  df  001  indexed  sort  index  df  002  df  002  indexed  sort  index  df  004  df  004  indexed  sort  index  df  008  df  008  indexed  sort  index  df  013  df  013  indexed  sort  index  df  015  df  015  indexed  sort  index  df  019  df  019  indexed  sort  index  df  002  head  resetting  the  indexes  back  to  normal  numerical  values  df  001  df  001  reset  index  df  002  df  002  reset  index  df  004  df  004  reset  index  df  008  df  008  reset  index  df  013  df  013  reset  index  df  015  df  015  reset  index  df  019  df  019  reset  index  df  002  head  import  numpy  as  np  from  numpy  import  genfromtxt  from  scipy  stats  import  multivariate  normal  from  sklearn  metrics  import  f1  score  from  sklearn  import  datasetsdf  001  Temp  df  001  Temperature  df  001  Temp  tail  Getting  rid  of  the  last  value  since  it  not  available  df  Temperature  df  Temperature  drop  ,ibm
df  Temperature  index  len  df  Temperature  df  001  Temp  mean  df  001  Temp  mean  df  001  Temp  df  001  Temp  fillna  df  001  Temp  mean  df  Temperature  anomalies  df  Temperature  copy  df  anomalies  mean  df  Temperature  anomalies  mean  within  zScore  df  Temperature  anomalies  np  abs  df  Temperature  anomalies  df  anomalies  mean  df  Temperature  anomalies  std  outside  zScore  df  Temperature  anomalies  np  abs  df  Temperature  anomalies  df  anomalies  mean  df  Temperature  anomalies  std  within  zScore  head  outside  zScore  df  001  temp  df  001  Temp  copy  dropping  the  outliers  df  001  Temp  df  001  Temp  np  abs  df  001  Temp  df  001  Temp  mean  df  001  Temp  std  df  001  Temp  head  df  002  Temp  df  002  Temperature  df  002  Temp  mean  df  002  Temp  mean  df  002  Temp  df  002  Temp  fillna  df  002  Temp  mean  df  002  Temp  df  002  Temp  np  abs  df  002  Temp  df  002  Temp  mean  df  002  Temp  std  df  004  Temp  df  004  Temperature  df  004  Temp ,ibm
 mean  df  004  Temp  mean  df  004  Temp  df  004  Temp  fillna  df  004  Temp  mean  df  004  Temp  df  004  Temp  np  abs  df  004  Temp  df  004  Temp  mean  df  004  Temp  std  df  008  Temp  df  008  Temperature  df  008  Temp  mean  df  008  Temp  mean  df  008  Temp  df  008  Temp  fillna  df  008  Temp  mean  df  008  Temp  df  008  Temp  np  abs  df  008  Temp  df  008  Temp  mean  df  008  Temp  std  df  013  Temp  df  013  Temperature  df  013  Temp  mean  df  013  Temp  mean  df  013  Temp  df  013  Temp  fillna  df  013  Temp  mean  df  013  Temp  df  013  Temp  np  abs  df  013  Temp  df  013  Temp  mean  df  013  Temp  std  df  015  Temp  df  015  Temperature  df  015  Temp  mean  df  015  Temp  mean  df  015  Temp  df  015  Temp  fillna  df  015  Temp  mean  df  015  Temp  df  015  Temp  np  abs  df  015  Temp  df  015  Temp  mean  df  015  Temp  std  df  019  Temp  df  019  Temperature  df  019  Temp  mean  df  019  Temp  mean  df  019  Temp  df  019  Temp  fillna  df  019  Temp  mean  df  0,ibm
19  Temp  df  019  Temp  np  abs  df  019  Temp  df  019  Temp  mean  df  019  Temp  std  from  pandas  import  read  csv  from  pandas  import  datetime  from  pandas  import  DataFrame  from  pandas  import  concat  from  pandas  import  Series  from  sklearn  metrics  import  mean  squared  error  from  sklearn  preprocessing  import  MinMaxScaler  from  keras  models  import  Sequential  from  keras  layers  import  Dense  from  keras  layers  import  LSTM  from  math  import  sqrt  import  numpy  from  matplotlib  import  pyplot  from  sklearn  model  selection  import  StratifiedKFold  frame  sequence  as  supervised  learning  problem  def  timeseries  to  supervised  data  lag  df  DataFrame  data  columns  df  shift  for  in  range  lag  columns  append  df  df  concat  columns  axis  df  fillna  inplace  True  return  df  create  differenced  series  def  difference  dataset  interval  diff  list  for  in  range  interval  len  dataset  value  dataset  dataset  interval  diff  append  value  return ,ibm
 Series  diff  invert  differenced  value  def  inverse  difference  history  yhat  interval  return  yhat  history  interval  scale  train  and  test  data  to  def  scale  train  test  fit  scaler  scaler  MinMaxScaler  feature  range  scaler  scaler  fit  train  transform  train  train  train  reshape  train  shape  train  shape  train  scaled  scaler  transform  train  transform  test  test  test  reshape  test  shape  test  shape  test  scaled  scaler  transform  test  return  scaler  train  scaled  test  scaled  inverse  scaling  for  forecasted  value  def  invert  scale  scaler  value  new  row  for  in  value  array  numpy  array  new  row  array  array  reshape  len  array  inverted  scaler  inverse  transform  array  return  inverted  fit  an  LSTM  network  to  training  data  def  fit  lstm  train  batch  size  nb  epoch  neurons  train  train  reshape  shape  shape  model  Sequential  model  add  LSTM  neurons  batch  input  shape  batch  size  shape  shape  stateful  True  model  add  Dense  mo,ibm
del  compile  loss  mean  squared  error  optimizer  adam  for  in  range  nb  epoch  model  fit  epochs  batch  size  batch  size  verbose  shuffle  False  model  reset  states  return  model  make  one  step  forecast  def  forecast  lstm  model  batch  size  reshape  len  yhat  model  predict  batch  size  batch  size  return  yhat  ann  data  to  be  used  001  df  001  Temp  1000  out  of  10  861  values  ann  data  to  be  used  002  df  002  Temp  1600  out  of  16  982  values  ann  data  to  be  used  004  df  004  Temp  1800  out  of  18  743  values  ann  data  to  be  used  008  df  008  Temp  2000  out  of  21  943  values  ann  data  to  be  used  015  df  015  Temp  2000  out  of  21  005  values  transform  data  to  be  stationary  raw  values  001  ann  data  to  be  used  001  values  diff  values  difference  raw  values  001  raw  values  001  transform  data  to  be  supervised  learning  supervised  timeseries  to  supervised  diff  values  supervised  values  supervised  values  split,ibm
  data  into  train  and  test  sets  we  chose  the  80  20  rule  for  test  train  since  massive  amounts  of  data  are  not  present  train  test  supervised  values  200  supervised  values  200  transform  the  scale  of  the  data  scaler  train  scaled  test  scaled  scale  train  test  fit  the  model  lstm  model  fit  lstm  train  scaled  1500  forecast  the  entire  training  dataset  to  build  up  state  for  forecasting  train  reshaped  train  scaled  reshape  len  train  scaled  lstm  model  predict  train  reshaped  batch  size  walk  forward  validation  on  the  test  data  predictions  list  for  in  range  len  test  scaled  make  one  step  forecast  test  scaled  test  scaled  yhat  forecast  lstm  lstm  model  invert  scaling  yhat  invert  scale  scaler  yhat  invert  differencing  yhat  inverse  difference  raw  values  001  yhat  len  test  scaled  store  forecast  predictions  append  yhat  expected  raw  values  001  len  train  print  Predicted  Expected  yhat  expected  Cross,ibm
  validating  the  data  with  10  fold  cross  validation  fix  random  seed  for  reproducibility  seed  numpy  random  seed  seed  dataset  dataset  define  10  fold  cross  validation  test  harness  kfold  StratifiedKFold  splits  10  shuffle  True  random  state  seed  cvscores  for  train  test  in  kfold  split  create  model  model  Sequential  model  add  Dense  12  input  dim  activation  relu  model  add  Dense  activation  relu  model  add  Dense  activation  sigmoid  Compile  model  model  compile  loss  binary  crossentropy  optimizer  adam  metrics  accuracy  Fit  the  model  model  fit  train  train  epochs  150  batch  size  10  verbose  evaluate  the  model  scores  model  evaluate  test  test  verbose  print  2f  model  metrics  names  scores  100  cvscores  append  scores  100  print  2f  2f  numpy  mean  cvscores  numpy  std  cvscores  report  performance  rmse  sqrt  mean  squared  error  raw  values  001  200  predictions  print  Test  RMSE  3f  rmse  line  plot  of  observed  vs  pred,ibm
icted  pyplot  plot  raw  values  001  200  pyplot  plot  predictions  pyplot  show  repeat  experiment  repeats  30  error  scores  list  for  in  range  repeats  fit  the  model  lstm  model  fit  lstm  train  scaled  1500  forecast  the  entire  training  dataset  to  build  up  state  for  forecasting  train  reshaped  train  scaled  reshape  len  train  scaled  lstm  model  predict  train  reshaped  batch  size  walk  forward  validation  on  the  test  data  predictions  list  for  in  range  len  test  scaled  make  one  step  forecast  test  scaled  test  scaled  yhat  forecast  lstm  lstm  model  invert  scaling  yhat  invert  scale  scaler  yhat  invert  differencing  yhat  inverse  difference  raw  values  yhat  len  test  scaled  store  forecast  predictions  append  yhat  report  performance  rmse  sqrt  mean  squared  error  raw  values  81  predictions  print  Test  RMSE  3f  rmse  error  scores  append  rmse  summarize  results  results  DataFrame  results  rmse  error  scores  print  results ,ibm
 describe  results  boxplot  pyplot  show  raw  values  002  ann  data  to  be  used  002  values  diff  values  difference  raw  values  002  supervised  timeseries  to  supervised  diff  values  supervised  values  supervised  values  train  test  supervised  values  320  supervised  values  320  scaler  train  scaled  test  scaled  scale  train  test  lstm  model  fit  lstm  train  scaled  1500  train  reshaped  train  scaled  reshape  len  train  scaled  lstm  model  predict  train  reshaped  batch  size  predictions  list  for  in  range  len  test  scaled  test  scaled  test  scaled  yhat  forecast  lstm  lstm  model  yhat  invert  scale  scaler  yhat  yhat  inverse  difference  raw  values  002  yhat  len  test  scaled  predictions  append  yhat  expected  raw  values  002  len  train  print  Predicted  Expected  yhat  expected  seed  numpy  random  seed  seed  dataset  dataset  kfold  StratifiedKFold  splits  10  shuffle  True  random  state  seed  cvscores  for  train  test  in  kfold  split  model  ,ibm
Sequential  model  add  Dense  12  input  dim  activation  relu  model  add  Dense  activation  relu  model  add  Dense  activation  sigmoid  model  compile  loss  binary  crossentropy  optimizer  adam  metrics  accuracy  model  fit  train  train  epochs  150  batch  size  10  verbose  scores  model  evaluate  test  test  verbose  print  2f  model  metrics  names  scores  100  cvscores  append  scores  100  print  2f  2f  numpy  mean  cvscores  numpy  std  cvscores  rmse  sqrt  mean  squared  error  raw  values  002  320  predictions  print  Test  RMSE  3f  rmse  pyplot  plot  raw  values  002  320  pyplot  plot  predictions  pyplot  show  raw  values  004  ann  data  to  be  used  004  values  diff  values  difference  raw  values  004  supervised  timeseries  to  supervised  diff  values  supervised  values  supervised  values  train  test  supervised  values  360  supervised  values  360  scaler  train  scaled  test  scaled  scale  train  test  lstm  model  fit  lstm  train  scaled  1500  train  reshaped  ,ibm
train  scaled  reshape  len  train  scaled  lstm  model  predict  train  reshaped  batch  size  predictions  list  for  in  range  len  test  scaled  test  scaled  test  scaled  yhat  forecast  lstm  lstm  model  yhat  invert  scale  scaler  yhat  yhat  inverse  difference  raw  values  004  yhat  len  test  scaled  predictions  append  yhat  expected  raw  values  004  len  train  print  Predicted  Expected  yhat  expected  seed  numpy  random  seed  seed  dataset  dataset  kfold  StratifiedKFold  splits  10  shuffle  True  random  state  seed  cvscores  for  train  test  in  kfold  split  model  Sequential  model  add  Dense  12  input  dim  activation  relu  model  add  Dense  activation  relu  model  add  Dense  activation  sigmoid  model  compile  loss  binary  crossentropy  optimizer  adam  metrics  accuracy  model  fit  train  train  epochs  150  batch  size  10  verbose  scores  model  evaluate  test  test  verbose  print  2f  model  metrics  names  scores  100  cvscores  append  scores  100  print  2,ibm
f  2f  numpy  mean  cvscores  numpy  std  cvscores  rmse  sqrt  mean  squared  error  raw  values  004  360  predictions  print  Test  RMSE  3f  rmse  pyplot  plot  raw  values  004  360  pyplot  plot  predictions  pyplot  show  raw  values  008  ann  data  to  be  used  008  values  diff  values  difference  raw  values  008  supervised  timeseries  to  supervised  diff  values  supervised  values  supervised  values  train  test  supervised  values  400  supervised  values  400  scaler  train  scaled  test  scaled  scale  train  test  lstm  model  fit  lstm  train  scaled  1500  train  reshaped  train  scaled  reshape  len  train  scaled  lstm  model  predict  train  reshaped  batch  size  predictions  list  for  in  range  len  test  scaled  test  scaled  test  scaled  yhat  forecast  lstm  lstm  model  yhat  invert  scale  scaler  yhat  yhat  inverse  difference  raw  values  008  yhat  len  test  scaled  predictions  append  yhat  expected  raw  values  008  len  train  print  Predicted  Expected  yhat  ,ibm
expected  seed  numpy  random  seed  seed  dataset  dataset  kfold  StratifiedKFold  splits  10  shuffle  True  random  state  seed  cvscores  for  train  test  in  kfold  split  model  Sequential  model  add  Dense  12  input  dim  activation  relu  model  add  Dense  activation  relu  model  add  Dense  activation  sigmoid  model  compile  loss  binary  crossentropy  optimizer  adam  metrics  accuracy  model  fit  train  train  epochs  150  batch  size  10  verbose  scores  model  evaluate  test  test  verbose  print  2f  model  metrics  names  scores  100  cvscores  append  scores  100  print  2f  2f  numpy  mean  cvscores  numpy  std  cvscores  rmse  sqrt  mean  squared  error  raw  values  008  400  predictions  print  Test  RMSE  3f  rmse  pyplot  plot  raw  values  008  400  pyplot  plot  predictions  pyplot  show  raw  values  015  ann  data  to  be  used  015  values  diff  values  difference  raw  values  015  supervised  timeseries  to  supervised  diff  values  supervised  values  supervised  valu,ibm
es  train  test  supervised  values  400  supervised  values  400  scaler  train  scaled  test  scaled  scale  train  test  lstm  model  fit  lstm  train  scaled  1500  train  reshaped  train  scaled  reshape  len  train  scaled  lstm  model  predict  train  reshaped  batch  size  predictions  list  for  in  range  len  test  scaled  test  scaled  test  scaled  yhat  forecast  lstm  lstm  model  yhat  invert  scale  scaler  yhat  yhat  inverse  difference  raw  values  015  yhat  len  test  scaled  predictions  append  yhat  expected  raw  values  015  len  train  print  Predicted  Expected  yhat  expected  seed  numpy  random  seed  seed  dataset  dataset  kfold  StratifiedKFold  splits  10  shuffle  True  random  state  seed  cvscores  for  train  test  in  kfold  split  model  Sequential  model  add  Dense  12  input  dim  activation  relu  model  add  Dense  activation  relu  model  add  Dense  activation  sigmoid  model  compile  loss  binary  crossentropy  optimizer  adam  metrics  accuracy  model  fit ,ibm
 train  train  epochs  150  batch  size  10  verbose  scores  model  evaluate  test  test  verbose  print  2f  model  metrics  names  scores  100  cvscores  append  scores  100  print  2f  2f  numpy  mean  cvscores  numpy  std  cvscores  rmse  sqrt  mean  squared  error  raw  values  015  400  predictions  print  Test  RMSE  3f  rmse  pyplot  plot  raw  values  015  400  pyplot  plot  predictions  pyplot  show  from  repository  mlrepositoryclient  import  MLRepositoryClient  from  repository  mlrepositoryartifact  import  MLRepositoryArtifact  from  repository  mlrepository  import  MetaProps  MetaNames  import  urllib3wml  credentials  url  https  ibm  watson  ml  eu  gb  bluemix  net  access  key  username  password  instance  id  ml  repository  client  MLRepositoryClient  wml  credentials  url  ml  repository  client  authorize  wml  credentials  username  wml  credentials  password  props  MetaProps  MetaNames  AUTHOR  NAME  IBM  MetaNames  AUTHOR  EMAIL  ibm  ibm  com  model  artifact  MLRepositoryArti,ibm
fact  model  name  Hand  written  digits  recognition  meta  props  props  saved  model  ml  repository  client  models  save  model  artifact  saved  model  meta  available  props  print  modelType  saved  model  meta  prop  modelType  print  runtime  saved  model  meta  prop  runtime  print  creationTime  str  saved  model  meta  prop  creationTime  print  modelVersionHref  saved  model  meta  prop  modelVersionHref  loadedModelArtifact  ml  repository  client  models  get  saved  model  uid  print  loadedModelArtifact  name  print  saved  model  uid  predictions  loadedModelArtifact  model  instance  predict  score  data  print  predictions  headers  urllib3  util  make  headers  basic  auth  username  password  format  username  wml  credentials  username  password  wml  credentials  password  url  v3  identity  token  format  wml  credentials  url  response  requests  get  url  headers  headers  mltoken  json  loads  response  text  get  token  endpoint  instance  wml  credentials  url  v3  wml  instance,ibm
s  wml  credentials  instance  id  header  Content  Type  application  json  Authorization  Bearer  mltoken  response  get  instance  requests  get  endpoint  instance  headers  header  print  response  get  instance  print  response  get  instance  text  endpoint  published  models  json  loads  response  get  instance  text  get  entity  get  published  models  get  url  print  endpoint  published  models  header  Content  Type  application  json  Authorization  Bearer  mltoken  response  get  requests  get  endpoint  published  models  headers  header  print  response  get  print  response  get  text  endpoint  deployments  get  entity  get  deployments  get  url  for  in  json  loads  response  get  text  get  resources  if  get  metadata  get  guid  saved  model  uid  print  endpoint  deployments  payload  online  name  Hand  written  digits  recognition  description  Hand  Written  Digits  Deployment  type  online  response  online  requests  post  endpoint  deployments  json  payload  online  headers  ,ibm
header  print  response  online  print  response  online  text  scoring  url  json  loads  response  online  text  get  entity  get  scoring  url  print  scoring  url  digit  list  digits  data  digit2  list  digits  data  payload  scoring  values  digit  digit2  print  payload  scoring  response  scoring  requests  post  scoring  url  json  payload  scoring  headers  header  print  response  scoring  text  ,ibm
import  os  import  boto3  import  sagemaker  from  sagemaker  mxnet  import  MXNet  from  mxnet  import  gluon  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  gluon  data  vision  MNIST  data  train  train  True  gluon  data  vision  MNIST  data  test  train  False  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  DEMO  mnist  cat  mnist  py  MXNet  mnist  py  role  role  train  instance  count  train  instance  type  ml  c4  xlarge  hyperparameters  batch  size  100  epochs  20  learning  rate  momentum  log  interval  100  fit  inputs  predictor  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  IPython  display  import  HTML  HTML  open  input  html  read  response  predictor  predict  data  print  int  response  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
import  boto3  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  mxnet  as  mx  data  mx  test  utils  get  mnist  from  mnist  import  train  model  train  data  data  num  cpus  num  gpus  import  os  import  json  os  mkdir  model  model  save  checkpoint  model  model  0000  with  open  model  model  shapes  json  as  shapes  json  dump  shape  model  data  shapes  name  data  shapes  import  tarfile  def  flatten  tarinfo  tarinfo  name  os  path  basename  tarinfo  name  return  tarinfo  tar  tarfile  open  model  tar  gz  gz  tar  add  model  filter  flatten  tar  close  import  sagemaker  sagemaker  session  sagemaker  Session  inputs  sagemaker  session  upload  data  path  model  tar  gz  key  prefix  model  from  sagemaker  mxnet  model  import  MXNetModel  sagemaker  model  MXNetModel  model  data  s3  sagemaker  session  default  bucket  model  model  tar  gz  role  role  entry  point  mnist  py  import  logging  logging  getLogger  setLevel  logging  WARNING ,amazon
 predictor  sagemaker  model  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  IPython  display  import  HTML  HTML  open  input  html  read  response  predictor  predict  data  print  Raw  prediction  result  print  response  labeled  predictions  list  zip  range  10  response  print  Labeled  predictions  print  labeled  predictions  labeled  predictions  sort  key  lambda  label  and  prob  label  and  prob  print  Most  likely  answer  format  labeled  predictions  print  predictor  endpoint  sagemaker  Session  delete  endpoint  predictor  endpoint  os  remove  model  tar  gz  import  shutil  shutil  rmtree  model  ,amazon
Timing  function  from  datetime  import  datetime  Timings  def  timing  tag  fromTag  None  Timings  tag  datetime  now  if  fromTag  print  at  elapsed  since  format  tag  Timings  tag  fromTag  Timings  tag  Timings  fromTag  else  print  at  format  tag  Timings  tag  timing  Start  hidden  cell  The  following  code  contains  the  credentials  for  file  in  your  IBM  Cloud  Object  Storage  You  might  want  to  remove  those  credentials  before  you  share  your  notebook  YourCredentials  IBM  API  KEY  ID  ZATI1oq  TlsWqN  oEz3Wo1IPXWwOkCx0PXV3gj0d5eui  IAM  SERVICE  ID  iam  ServiceId  521e4bd0  5161  49f0  9e11  474c674a5fe7  ENDPOINT  https  s3  api  us  geo  objectstorage  service  networklayer  com  IBM  AUTH  ENDPOINT  https  iam  ng  bluemix  net  oidc  token  BUCKET  watstudworkshop  donotdelete  pr  basx79wonvxlys  FILE  201701  citibike  tripdata  csv  NOTTHISONE  Insert  YOUR  OWN  Credentials  as  YourCredentials  in  the  cell  above  Setup  access  to  COS  import  sys  import  typ,ibm
es  import  pandas  as  pd  from  botocore  client  import  Config  import  ibm  boto3  def  iter  self  return  try  raise  Exception  YourCredentials  NOTTHISONE  except  KeyError  pass  bucket  name  YourCredentials  BUCKET  object  name  2017  csv  cos  ibm  boto3  client  s3  ibm  api  key  id  YourCredentials  IBM  API  KEY  ID  ibm  auth  endpoint  YourCredentials  IBM  AUTH  ENDPOINT  config  Config  signature  version  oauth  endpoint  url  YourCredentials  ENDPOINT  timing  BeforeFTPAndCOS  Get  the  file  via  FTP  download  rm  2017  csv  wget  ftp  ftp  ncdc  noaa  gov  pub  data  ghcn  daily  by  year  2017  csv  gz  ls  2017  Unzip  the  file  gunzip  2017  csv  gz  ls  2017  Send  the  file  to  COS  from  file  system  with  open  object  name  rb  as  cos  upload  fileobj  bucket  name  object  name  print  Done  file  uploaded  to  COS  bucket  format  object  name  bucket  name  timing  AfterFTPAndCOS  BeforeFTPAndCOS  import  ibmos2spark  timing  BeforeDownloadingFromCOS  Create  credenti,ibm
als  in  the  format  required  by  CloudObjectStorage  credentials  endpoint  YourCredentials  ENDPOINT  api  key  YourCredentials  IBM  API  KEY  ID  service  id  YourCredentials  IAM  SERVICE  ID  configuration  name  wheather  data  2017  config  os  See  https  github  com  ibm  watson  data  lab  ibmos2spark  tree  master  python  cos  ibmos2spark  CloudObjectStorage  sc  credentials  configuration  name  configuration  name  cos  type  bluemix  cos  The  sc  object  is  your  SparkContext  object  The  cos  object  will  provide  the  URL  for  SparkContext  to  retrieve  your  data  Get  the  URL  data  url  cos  url  object  name  bucket  name  timing  AfterDownloadingFromCOS  BeforeDownloadingFromCOS  print  COS  URL  for  the  data  asset  format  data  url  timing  BeforeLoadingData  Now  we  load  the  data  Note  that  Spark  uses  lazy  evaluation  so  the  lengthy  operation  will  be  take  weather  sc  textFile  data  url  take  triggers  the  Spark  job  that  loads  the  data  See  the  Sp,ibm
ark  Job  Progress  gauge  weather  take  timing  AfterLoadingData  BeforeLoadingData  print  Total  records  in  the  data  set  format  weather  count  print  The  first  row  in  the  data  set  format  weather  first  Create  RDD  from  Python  array  to  represent  Header  see  https  spark  apache  org  docs  latest  api  python  pyspark  html  pyspark  SparkContext  parallelize  header  sc  parallelize  STATION  DATE  METRIC  VALUE  C5  C6  C7  C8  Append  the  header  and  data  into  single  RDD  weather  header  union  weather  weather  take  timing  BeforeParsing  weatherParse  weather  map  lambda  line  line  split  weatherParse  first  weatherParse  first  weatherParse  first  timing  AfterParsing  BeforeParsing  Create  new  RDD  which  holds  only  rows  which  represent  precipitation  events  weatherPrecp  weatherParse  filter  lambda  PRCP  Display  first  lines  weatherPrecp  take  Map  to  tuples  with  station  ID  as  first  element  then  tuple  made  of  the  precipitation  measure  a,ibm
nd  constant  is  the  station  is  the  precipitation  value  weatherPrecpCountByKey  weatherPrecp  map  lambda  int  weatherPrecpCountByKey  take  timing  AfterFiltering  AfterParsing  weatherPrecpAddByKey  weatherPrecpCountByKey  reduceByKey  lambda  v1  v2  v1  v2  v1  v2  weatherPrecpAddByKey  first  weatherAverages  weatherPrecpAddByKey  map  lambda  float  weatherAverages  first  for  pair  in  weatherAverages  top  10  print  Station  had  average  precipitations  of  2f  format  pair  pair  precTop10  stationsTop10  for  pair  in  weatherAverages  map  lambda  top  10  precTop10  append  pair  stationsTop10  append  pair  print  Station  had  average  precipitations  of  2f  format  pair  pair  timing  AfterAverages  AfterFiltering  matplotlib  inline  import  numpy  as  np  import  matplotlib  pyplot  as  plt  10  index  np  arange  bar  width  plt  bar  index  precTop10  bar  width  color  plt  xlabel  Stations  plt  ylabel  Precipitations  plt  title  10  stations  with  the  highest  average  pre,ibm
cipitation  plt  xticks  index  bar  width  stationsTop10  rotation  90  plt  show  timing  BeginSparkSQL  Filter  where  type  is  snow  weatherSnow  weatherParse  filter  lambda  SNOW  print  There  are  SNOW  events  format  weatherSnow  count  timing  BeginBuildSparkSQLDataFrame  from  datetime  import  datetime  from  pyspark  sql  import  Row  spark  SparkSession  builder  getOrCreate  Convert  each  line  of  snowWeather  RDD  into  Row  object  snowRows  weatherSnow  map  lambda  Row  station  month  datetime  strptime  month  date  datetime  strptime  day  metric  value  int  Apply  Row  schema  to  create  Spark  DataFrame  snowSchema  spark  createDataFrame  snowRows  Register  snow2017  table  with  columns  station  month  date  metric  and  value  snowSchema  registerTempTable  snow2017  timing  EndBuildSparkSQLDataFrame  BeginBuildSparkSQLDataFrame  timing  BeginCountSnowDays  snow  US10chey021  spark  sql  SELECT  month  COUNT  AS  snowdays  FROM  snow2017  WHERE  station  US10chey021  GROUP  ,ibm
BY  month  ORDER  BY  month  collect  timing  EndCountSnowDays  BeginCountSnowDays  snow  US10chey021  Convert  to  python  array  of  12  elements  initialized  to  US10chey021  snowdays  12  fill  in  array  with  snow  days  per  month  notice  the  indexed  array  versus  indexed  months  ranks  for  row  in  snow  US10chey021  US10chey021  snowdays  row  month  row  snowdays  print  Snow  days  per  month  US10chey021  snowdays  timing  BeginCountSnowDays  snow  USW00094985  spark  sql  SELECT  month  COUNT  AS  snowdays  FROM  snow2017  WHERE  station  USW00094985  GROUP  BY  month  ORDER  BY  month  collect  timing  EndCountSnowDays  BeginCountSnowDays  Create  array  of  12  to  start  with  USW00094985  snowdays  12  For  each  row  compute  number  of  snow  days  for  row  in  snow  USW00094985  USW00094985  snowdays  row  month  row  snowdays  print  USW00094985  snowdays  matplotlib  inline  import  matplotlib  import  numpy  as  np  import  matplotlib  pyplot  as  plt  12  ind  np  arange  width,ibm
  35  pUS10chey021  plt  bar  ind  US10chey021  snowdays  width  color  label  US10chey021  pUSW00094985  plt  bar  ind  width  USW00094985  snowdays  width  color  label  USW00094985  plt  ylabel  SNOW  DAYS  plt  xlabel  MONTH  plt  title  Snow  Days  in  2017  at  Stations  US10chey021  vs  USW00094985  plt  xticks  ind  width  Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec  plt  legend  plt  show  timing  BeginCountSnowDays  snowStations  spark  sql  SELECT  station  COUNT  AS  snowdays  FROM  snow2017  WHERE  station  LIKE  US  GROUP  BY  station  ORDER  BY  station  LIMIT  100  snowStations  head  timing  EndCountSnowDays  BeginCountSnowDays  snowStations  registerTempTable  snowdays  2017  snowStations  top5  spark  sql  SELECT  station  snowdays  FROM  snowdays  2017  ORDER  BY  snowdays  DESC  LIMIT  collect  for  row  in  snowStations  top5  print  row  Query  the  station  snowdays  station  snowdays  spark  sql  SELECT  station  snowdays  FROM  snowdays  2017  ORDER  BY  snowdays  Make,ibm
  RDD  with  snowdays  as  first  column  used  as  key  snowday  station  station  snowdays  rdd  map  lambda  snowdays  station  Collapse  by  key  snowdays  and  make  list  of  stations  as  second  column  snowday  stationsList  snowday  station  reduceByKey  lambda  sortByKey  collect  for  snowday  in  snowday  stationsList  print  Snow  days  Stations  format  snowday  snowday  timing  EndSparkSQLQueries  BeginSparkSQL  Save  as  parquet  file  If  you  are  running  this  cell  multiple  times  you  will  need  to  overwrite  the  data  in  the  parquet  file  snowStations  write  mode  overwrite  parquet  bmos  url  CONTAINER  snowStations  parquet  snowStations  url  cos  url  snowStations  parquet  format  int  datetime  now  timestamp  bucket  name  snowStations  write  parquet  snowStations  url  timing  EndWriteToCOS  EndSparkSQLQueries  snowDaysParquetFile  spark  read  parquet  cos  url  snowStations  parquet  bucket  name  snowDaysParquetFile  registerTempTable  snow  from  parquet  timing  ,ibm
ReadFromCOS  EndWriteToCOS  Display  structure  of  the  DataFrame  snowDaysParquetFile  describe  station  snowdays  spark  sql  SELECT  DISTINCT  COUNT  AS  countSnow  FROM  snow  from  parquet  print  There  are  stations  format  station  snowdays  first  countSnow  timing  EndSQLFromCOS  ReadFromCOS  timing  End  Start  ,ibm
import  numpy  as  np  import  matplotlib  pyplot  as  plt  from  sklearn  cross  validation  import  train  test  split  cross  val  score  from  sklearn  metrics  import  accuracy  score  roc  auc  score  average  precision  score  from  sklearn  preprocessing  import  LabelEncoder  import  seaborn  as  sns  from  sklearn  grid  search  import  RandomizedSearchCV  from  sklearn  ensemble  import  RandomForestClassifier  from  scipy  stats  import  randint  as  sp  randint  from  scikitplot  metrics  import  plot  lift  curve  plot  cumulative  gain  pip  install  scikit  plot  import  warnings  warnings  filterwarnings  ignore  Gr  fica  de  variable  predecir  churn  count  sns  countplot  df  data  Exited  churn  count  set  xlabel  El  cliente  abandon  el  banco  ylabel  mero  de  clientes  title  Distribuci  del  abandono  plt  show  print  Taza  de  abandono  str  100  sum  df  data  Exited  len  df  data  Exited  Se  crean  las  matrices  df  data  drop  Exited  axis  Eliminamos  columnas  df  data  ,ibm
Exited  Definimos  variable  predecir  Se  define  funci  que  crea  diccionarios  para  transformar  los  factores  de  las  variables  categ  ricas  meros  viceversa  def  label  encoder  cols  from  collections  import  namedtuple  encode  decode  for  in  cols  le  LabelEncoder  le  fit  transform  encode  dict  zip  le  classes  le  transform  le  classes  decode  dict  zip  le  transform  le  classes  le  classes  encoder  tuple  namedtuple  encoder  tuple  encode  decode  dictionaries  encoder  tuple  encode  decode  return  dictionaries  categoric  select  dtypes  include  object  columns  Guarda  el  nombre  de  las  columnas  categ  ricas  dictionaries  label  encoder  categoric  Label  encoder  dictionaries  replace  dictionaries  encode  inplace  True  Codificamos  las  variables  categ  ricas  train  test  train  test  train  test  split  test  size  random  state  Dividimos  los  datos  en  entrenamiento  prueba  train  shape  Extraemos  las  dimensiones  de  la  matriz  de  entrenamiento  head ,ibm
 Observamos  mo  quedan  las  primeras  filas  luego  de  la  transformaci  param  dist  max  features  sp  randint  int  np  sqrt  int  min  samples  leaf  sp  randint  20  Se  definen  los  hiperpar  metros  para  sintonizar  el  modelo  clf  RandomForestClassifier  jobs  estimators  COLOCAR  AQU  MERO  DE  RBOLES  Se  inicializa  la  instancia  de  Random  Forest  Corre  squeda  aleatorio  por  iteraciones  iter  search  COLOQUE  AQU  EL  MERO  DE  ITERACIONES  random  search  clf  RandomizedSearchCV  clf  param  distributions  param  dist  iter  iter  search  cv  scoring  accuracy  random  state  10  random  search  clf  fit  train  train  Se  entrena  el  modelo  con  los  mejores  par  metros  encontrados  best  estimator  random  search  clf  best  estimator  Guarda  el  modelo  best  score  random  search  clf  best  score  Guarda  el  puntaje  del  mejor  modelo  print  Best  params  best  estimator  Best  score  best  score  pred  best  estimator  predict  test  Predicciones  binarias  en  el  conju,ibm
nto  de  prueba  pred  proba  best  estimator  predict  proba  test  Predicciones  de  probabilidad  en  el  conjunto  de  prueba  accuracy  accuracy  score  pred  pred  true  test  Exactitud  AUC  roc  auc  score  score  pred  proba  true  test  AUC  average  precision  score  average  precision  score  true  test  score  pred  proba  print  Accuracy  str  np  round  accuracy  100  AUC  str  np  round  AUC  100  average  precision  score  str  np  round  average  precision  score  100  from  repository  mlrepositoryclient  import  MLRepositoryClient  from  repository  mlrepositoryartifact  import  MLRepositoryArtifact  from  repository  mlrepository  import  MetaProps  MetaNameswml  credentials  ml  repository  client  MLRepositoryClient  wml  credentials  url  ml  repository  client  authorize  wml  credentials  username  wml  credentials  password  Check  if  props  is  mandatory  props  MetaProps  MetaNames  AUTHOR  NAME  XXXXXXX  MetaNames  AUTHOR  EMAIL  XXXXXXX  model  artifact  MLRepositoryArtifact  b,ibm
est  estimator  name  XXXXXXX  meta  props  props  saved  model  ml  repository  client  models  save  model  artifact  saved  model  meta  available  props  import  urllib3  requests  json  headers  urllib3  util  make  headers  basic  auth  format  wml  credentials  username  wml  credentials  password  url  v3  identity  token  format  wml  credentials  url  response  requests  get  url  headers  headers  mltoken  json  loads  response  text  get  token  header  Content  Type  application  json  Authorization  Bearer  mltoken  endpoint  instance  wml  credentials  url  v3  wml  instances  wml  credentials  instance  id  header  Content  Type  application  json  Authorization  Bearer  mltoken  response  get  instance  requests  get  endpoint  instance  headers  header  endpoint  published  models  json  loads  response  get  instance  text  get  entity  get  published  models  get  url  print  endpoint  published  models  header  Content  Type  application  json  Authorization  Bearer  mltoken  response  ge,ibm
t  requests  get  endpoint  published  models  headers  header  endpoint  deployments  get  entity  get  deployments  get  url  for  in  json  loads  response  get  text  get  resources  if  get  metadata  get  guid  saved  model  uid  print  endpoint  deployments  payload  online  name  XXXXXXX  description  Churn  predction  using  Random  Forest  type  online  response  online  requests  post  endpoint  deployments  json  payload  online  headers  header  print  response  online  text  scoring  url  json  loads  response  online  text  get  entity  get  scoring  url  print  scoring  url  Credit  score  int  input  Credit  score  Geography  input  Geography  Gender  input  Gender  Age  int  input  Age  Tenure  int  input  Tenure  Balance  int  input  Balance  Num  products  int  input  Number  of  products  HasCrCard  input  Has  credit  card  IsActiveMember  input  Is  active  member  EstimadedSalary  int  input  Estimated  salary  new  observation  np  array  Credit  score  Geography  Gender  Age  Tenure ,ibm
 Balance  Num  products  HasCrCard  IsActiveMember  EstimadedSalary  dtype  object  new  observation  pd  DataFrame  new  observation  columns  columns  new  observation  replace  dictionaries  encode  inplace  True  payload  scoring  values  list  new  observation  values  print  payload  scoring  response  scoring  requests  post  scoring  url  json  payload  scoring  headers  header  response  json  loads  response  scoring  text  probabilidad  abandono  response  values  print  Este  cliente  tiene  una  probabilidad  de  np  round  probabilidad  abandono  de  abandonar  el  banco  def  uplift  true  pred  percentile  expected  response  rate  int  np  round  len  true  percentile  true  true  astype  int  temp  pd  DataFrame  pred  pred  true  true  temp  temp  sort  values  by  pred  ascending  False  iloc  response  rate  sum  temp  true  len  temp  true  lift  response  rate  expected  response  rate  return  lift  percentile  expected  response  rate  uplift  score  make  scorer  uplift  greater  is ,ibm
 better  True  needs  proba  True  percentile  percentile  expected  response  rate  expected  response  rate  uplift  cv  np  mean  cross  val  score  best  estimator  train  train  astype  int  cv  scoring  uplift  score  print  Cross  validation  uplift  uplift  cv  Lift  and  gain  cumulative  gain  charts  pred  best  estimator  predict  proba  test  print  uplift  pred  pred  true  test  percentile  percentile  expected  response  rate  expected  response  rate  plot  cumulative  gain  test  pred  title  Cumulative  Gains  Curve  plt  show  plot  lift  curve  test  pred  title  Lift  curve  plt  show  ,ibm
Permissions  and  environment  variable  from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  mlfirstdatabucket  arn  aws  s3  mlfirstdatabucket  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  Data  Inspection  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  30  This  is  format  train  set  30  Means  Model  from  sagemaker  import  KMeans  data  location  s3  kmeans  highlevel  example  data  format  bucket  output  location  s3  kmeans  example  output  format  bucket  print  traini,amazon
ng  data  will  be  uploaded  to  format  data  location  print  training  artifacts  will  be  uploaded  to  format  output  location  kmeans  KMeans  role  role  train  instance  count  train  instance  type  ml  c4  8xlarge  output  path  output  location  10  data  location  data  location  time  kmeans  fit  kmeans  record  set  train  set  time  kmeans  predictor  kmeans  deploy  initial  instance  count  instance  type  ml  m4  xlarge  result  kmeans  predictor  predict  train  set  30  31  print  result  time  result  kmeans  predictor  predict  valid  set  100  clusters  label  closest  cluster  float32  tensor  values  for  in  result  for  cluster  in  range  10  print  nCluster  format  int  cluster  digits  img  for  img  in  zip  clusters  valid  set  if  int  cluster  height  len  digits  width  plt  rcParams  figure  figsize  width  height  subplots  plt  subplots  height  width  subplots  numpy  ndarray  flatten  subplots  for  subplot  image  in  zip  subplots  digits  show  digit  image  su,amazon
bplot  subplot  for  subplot  in  subplots  len  digits  subplot  axis  off  plt  show  print  kmeans  predictor  endpoint  import  sagemaker  sagemaker  Session  delete  endpoint  kmeans  predictor  endpoint  ,amazon
import  os  import  boto3  import  io  import  sagemaker  matplotlib  inline  import  pandas  as  pd  import  numpy  as  np  import  mxnet  as  mx  import  matplotlib  pyplot  as  plt  import  matplotlib  import  seaborn  as  sns  matplotlib  style  use  ggplot  import  pickle  gzip  urllib  json  import  csvfrom  sagemaker  import  get  execution  role  role  get  execution  role  roles3  client  boto3  client  s3  data  bucket  name  aws  ml  blog  sagemaker  census  segmentation  obj  list  s3  client  list  objects  Bucket  data  bucket  name  file  for  contents  in  obj  list  Contents  file  append  contents  Key  print  file  file  data  file  response  s3  client  get  object  Bucket  data  bucket  name  Key  file  data  response  body  response  Body  read  counties  pd  read  csv  io  BytesIO  response  body  header  delimiter  low  memory  False  counties  head  counties  shapecounties  dropna  inplace  True  counties  shapecounties  index  counties  State  counties  County  counties  head  drop  ,amazon
CensusId  State  County  counties  drop  drop  axis  inplace  True  counties  head  import  seaborn  as  sns  for  in  Professional  Service  Office  ax  plt  subplots  figsize  ax  sns  distplot  counties  title  Histogram  of  ax  set  title  title  fontsize  12  plt  show  from  sklearn  preprocessing  import  MinMaxScaler  scaler  MinMaxScaler  counties  scaled  pd  DataFrame  scaler  fit  transform  counties  counties  scaled  columns  counties  columns  counties  scaled  index  counties  indexcounties  scaled  describe  from  sagemaker  import  PCA  bucket  name  my  s3  bucket  name  here  num  components  33  pca  SM  PCA  role  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  s3  bucket  name  counties  num  components  num  components  train  data  counties  scaled  values  astype  float32  time  pca  SM  fit  pca  SM  record  set  train  data  job  name  your  SageMaker  PCA  job  name  here  model  key  counties  job  name  output  model  tar  gz  boto3  resource ,amazon
 s3  Bucket  bucket  name  download  file  model  key  model  tar  gz  os  system  tar  zxvf  model  tar  gz  os  system  unzip  model  algo  import  mxnet  as  mx  pca  model  params  mx  ndarray  load  model  algo  pd  DataFrame  pca  model  params  asnumpy  pd  DataFrame  pca  model  params  asnumpy  iloc  28  apply  lambda  sum  apply  lambda  sum  iloc  28  iloc  28  columns  component  num  first  comp  component  num  comps  pd  DataFrame  list  zip  first  comp  counties  scaled  columns  columns  weights  features  comps  abs  weights  comps  weights  apply  lambda  np  abs  ax  sns  barplot  data  comps  sort  values  abs  weights  ascending  False  head  10  weights  features  palette  Blues  ax  set  title  PCA  Component  Makeup  str  component  num  plt  show  PCA  list  comp  comp  comp  comp  comp  PCA  list  Poverty  Unemployment  Self  Employment  Public  Workers  High  Income  Professional  Office  Workers  Black  Native  Am  Populations  Public  Professional  Workers  Construction  Commute,amazon
rs  time  pca  predictor  pca  SM  deploy  initial  instance  count  instance  type  ml  t2  medium  time  result  pca  predictor  predict  train  data  counties  transformed  pd  DataFrame  for  in  result  label  projection  float32  tensor  values  counties  transformed  counties  transformed  append  list  counties  transformed  index  counties  scaled  index  counties  transformed  counties  transformed  iloc  28  counties  transformed  columns  PCA  listcounties  transformed  head  train  data  counties  transformed  values  astype  float32  from  sagemaker  import  KMeans  num  clusters  kmeans  KMeans  role  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  s3  bucket  name  counties  num  clusters  time  kmeans  fit  kmeans  record  set  train  data  time  kmeans  predictor  kmeans  deploy  initial  instance  count  instance  type  ml  t2  medium  time  result  kmeans  predictor  predict  train  data  cluster  labels  label  closest  cluster  float32  tensor  values  ,amazon
for  in  result  pd  DataFrame  cluster  labels  value  counts  ax  plt  subplots  figsize  ax  sns  distplot  cluster  labels  kde  False  title  Histogram  of  Cluster  Counts  ax  set  title  title  fontsize  12  plt  show  job  name  your  SageMaker  KMeans  job  name  here  model  key  counties  job  name  output  model  tar  gz  boto3  resource  s3  Bucket  bucket  name  download  file  model  key  model  tar  gz  os  system  tar  zxvf  model  tar  gz  os  system  unzip  model  algo  Kmeans  model  params  mx  ndarray  load  model  algo  cluster  centroids  pd  DataFrame  Kmeans  model  params  asnumpy  cluster  centroids  columns  counties  transformed  columnscluster  centroidsplt  figure  figsize  16  ax  sns  heatmap  cluster  centroids  cmap  YlGnBu  ax  set  xlabel  Cluster  plt  yticks  fontsize  16  plt  xticks  fontsize  16  ax  set  title  Attribute  Value  by  Centroid  plt  show  counties  transformed  labels  list  map  int  cluster  labels  counties  transformed  head  cluster  counties  t,amazon
ransformed  counties  transformed  labels  cluster  head  sagemaker  Session  delete  endpoint  pca  predictor  endpoint  sagemaker  Session  delete  endpoint  kmeans  predictor  endpoint  ,amazon
cat  container  Dockerfile  sh  The  name  of  our  algorithm  algorithm  name  ann  churn  cd  container  chmod  ann  train  chmod  ann  serve  account  aws  sts  get  caller  identity  query  Account  output  text  Get  the  region  defined  in  the  current  configuration  default  to  us  west  if  none  defined  region  aws  configure  get  region  region  region  us  west  fullname  account  dkr  ecr  region  amazonaws  com  algorithm  name  latest  If  the  repository  doesn  exist  in  ECR  create  it  aws  ecr  describe  repositories  repository  names  algorithm  name  dev  null  if  ne  then  aws  ecr  create  repository  repository  name  algorithm  name  dev  null  fi  Get  the  login  command  from  ECR  and  execute  it  directly  aws  ecr  get  login  region  region  no  include  email  Build  the  docker  image  locally  with  the  image  name  and  then  push  it  to  ECR  with  the  full  name  docker  build  algorithm  name  docker  tag  algorithm  name  fullname  docker  push  fullname  S,amazon
3  prefix  prefix  ann  churn  Define  IAM  role  import  boto3  import  re  import  os  import  numpy  as  np  import  pandas  as  pd  from  sagemaker  import  get  execution  role  role  get  execution  role  import  sagemaker  as  sage  from  time  import  gmtime  strftime  sess  sage  Session  WORK  DIRECTORY  data  data  location  sess  upload  data  WORK  DIRECTORY  key  prefix  prefix  account  sess  boto  session  client  sts  get  caller  identity  Account  region  sess  boto  session  region  name  image  account  dkr  ecr  region  amazonaws  com  prefix  latest  classifier  sage  estimator  Estimator  image  role  ml  c4  2xlarge  output  path  s3  output  format  sess  default  bucket  sagemaker  session  sess  classifier  fit  data  location  from  sagemaker  predictor  import  csv  serializer  predictor  classifier  deploy  ml  m4  xlarge  serializer  csv  serializer  shape  pd  read  csv  data  churn  csv  header  None  import  itertools  50  for  in  range  40  for  in  range  10  indices  for,amazon
  in  itertools  product  test  data  shape  iloc  indices  dict  test  col  val  for  col  val  in  zip  dataset  columns  test  dict  test  pd  DataFrame  from  dict  dict  test  orient  index  pd  Series  dict  test  name  test  df  pd  DataFrame  test  print  predictor  predict  test  data  values  decode  utf  sess  delete  endpoint  predictor  endpoint  ,amazon
time  import  os  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  region  boto3  Session  region  name  bucket  bucket  name  put  your  s3  bucket  name  here  and  create  s3  bucket  prefix  sagemaker  DEMO  xgboost  regression  customize  to  your  bucket  where  you  have  stored  the  data  bucket  path  https  s3  amazonaws  com  format  region  bucket  time  import  io  import  boto3  import  random  def  data  split  FILE  DATA  FILE  TRAIN  FILE  VALIDATION  FILE  TEST  PERCENT  TRAIN  PERCENT  VALIDATION  PERCENT  TEST  data  for  in  open  FILE  DATA  train  file  open  FILE  TRAIN  valid  file  open  FILE  VALIDATION  tests  file  open  FILE  TEST  num  of  data  len  data  num  train  int  PERCENT  TRAIN  100  num  of  data  num  valid  int  PERCENT  VALIDATION  100  num  of  data  num  tests  int  PERCENT  TEST  100  num  of  data  data  fractions  num  train  num  valid  num  tests  split  data  rand  data  ind  for  split  ind  fraction  i,amazon
n  enumerate  data  fractions  for  in  range  fraction  rand  data  ind  random  randint  len  data  split  data  split  ind  append  data  rand  data  ind  data  pop  rand  data  ind  for  in  split  data  train  file  write  for  in  split  data  valid  file  write  for  in  split  data  tests  file  write  train  file  close  valid  file  close  tests  file  close  def  write  to  s3  fobj  bucket  key  return  boto3  Session  resource  s3  Bucket  bucket  Object  key  upload  fileobj  fobj  def  upload  to  s3  bucket  channel  filename  fobj  open  filename  rb  key  prefix  channel  url  s3  format  bucket  key  filename  print  Writing  to  format  url  write  to  s3  fobj  bucket  key  time  import  urllib  request  Load  the  dataset  FILE  DATA  abalone  urllib  request  urlretrieve  https  www  csie  ntu  edu  tw  cjlin  libsvmtools  datasets  regression  abalone  FILE  DATA  split  the  downloaded  data  into  train  test  validation  files  FILE  TRAIN  abalone  train  FILE  VALIDATION  abalone ,amazon
 validation  FILE  TEST  abalone  test  PERCENT  TRAIN  70  PERCENT  VALIDATION  15  PERCENT  TEST  15  data  split  FILE  DATA  FILE  TRAIN  FILE  VALIDATION  FILE  TEST  PERCENT  TRAIN  PERCENT  VALIDATION  PERCENT  TEST  upload  the  files  to  the  S3  bucket  upload  to  s3  bucket  train  FILE  TRAIN  upload  to  s3  bucket  validation  FILE  VALIDATION  upload  to  s3  bucket  test  FILE  TEST  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  boto3  Session  region  name  xgboost  time  import  boto3  from  time  import  gmtime  strftime  job  name  DEMO  xgboost  regression  strftime  gmtime  print  Training  job  job  name  Ensure  that  the  training  and  validation  data  folders  generated  above  are  reflected  in  the  InputDataConfig  parameter  below  create  training  params  AlgorithmSpecification  TrainingImage  container  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  bucket  path  prefix  single  xgboost  ResourceC,amazon
onfig  InstanceCount  InstanceType  ml  m4  4xlarge  VolumeSizeInGB  TrainingJobName  job  name  HyperParameters  max  depth  eta  gamma  min  child  weight  subsample  silent  objective  reg  linear  num  round  50  StoppingCondition  MaxRuntimeInSeconds  3600  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  bucket  path  prefix  train  S3DataDistributionType  FullyReplicated  ContentType  libsvm  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  bucket  path  prefix  validation  S3DataDistributionType  FullyReplicated  ContentType  libsvm  CompressionType  None  client  boto3  client  sagemaker  client  create  training  job  create  training  params  import  time  status  client  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  status  while  status  Completed  and  status  Failed  time  sleep  60  status  client  describe  training  job  TrainingJobName  job  name  TrainingJobStat,amazon
us  print  status  time  import  boto3  from  time  import  gmtime  strftime  model  name  job  name  model  print  model  name  info  client  describe  training  job  TrainingJobName  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  data  primary  container  Image  container  ModelDataUrl  model  data  create  model  response  client  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  endpoint  config  name  DEMO  XGBoostEndpointConfig  strftime  gmtime  print  endpoint  config  name  create  endpoint  config  response  client  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialVariantWeight  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  import  time  endpoint  na,amazon
me  DEMO  XGBoostEndpoint  strftime  gmtime  print  endpoint  name  create  endpoint  response  client  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  print  create  endpoint  response  EndpointArn  resp  client  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  while  status  Creating  time  sleep  60  resp  client  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  print  Arn  resp  EndpointArn  print  Status  status  runtime  client  boto3  client  runtime  sagemaker  head  abalone  test  abalone  single  test  time  import  json  from  itertools  import  islice  import  math  import  struct  file  name  abalone  single  test  customize  to  your  test  file  with  open  file  name  as  payload  read  strip  response  runtime  client  invoke  endpoint  EndpointName  endpoint  name  ContentType  text  libsvm  Body  payload  result  response  Body  read  result,amazon
  result  decode  utf  result  result  split  result  math  ceil  float  for  in  result  label  payload  strip  split  print  Label  label  nPrediction  result  import  sys  import  math  def  do  predict  data  endpoint  name  content  type  payload  join  data  response  runtime  client  invoke  endpoint  EndpointName  endpoint  name  ContentType  content  type  Body  payload  result  response  Body  read  result  result  decode  utf  result  result  split  preds  float  num  for  num  in  result  preds  math  ceil  num  for  num  in  preds  return  preds  def  batch  predict  data  batch  size  endpoint  name  content  type  items  len  data  arrs  for  offset  in  range  items  batch  size  if  offset  batch  size  items  results  do  predict  data  offset  offset  batch  size  endpoint  name  content  type  arrs  extend  results  else  arrs  extend  do  predict  data  offset  items  endpoint  name  content  type  sys  stdout  write  return  arrs  time  import  json  import  numpy  as  np  with  open  FI,amazon
LE  TEST  as  payload  read  strip  labels  int  line  split  for  line  in  payload  split  test  data  line  for  line  in  payload  split  preds  batch  predict  test  data  100  endpoint  name  text  libsvm  print  Median  Absolute  Percent  Error  MdAPE  np  median  np  abs  np  array  labels  np  array  preds  np  array  labels  client  delete  endpoint  EndpointName  endpoint  name  ,amazon
from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  maggiesagemaker  ucdml  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  30  This  is  format  train  set  30  from  sagemaker  import  KMeans  data  location  s3  maggiesagemaker  ucdml  kmeans  highlevel  example  data  format  bucket  output  location  s3  maggiesagemaker  ucdml  kmeans  example  output  format  bucket  print  training  data  will  be  uploaded  to  format  data  locat,amazon
ion  print  training  artifacts  will  be  uploaded  to  format  output  location  kmeans  KMeans  role  role  train  instance  count  train  instance  type  ml  c4  8xlarge  output  path  output  location  10  data  location  data  location  time  kmeans  fit  kmeans  record  set  train  set  time  kmeans  predictor  kmeans  deploy  initial  instance  count  instance  type  ml  m4  xlarge  result  kmeans  predictor  predict  train  set  30  31  print  result  time  result  kmeans  predictor  predict  valid  set  100  clusters  label  closest  cluster  float32  tensor  values  for  in  result  for  cluster  in  range  10  print  nCluster  format  int  cluster  digits  img  for  img  in  zip  clusters  valid  set  if  int  cluster  height  len  digits  width  plt  rcParams  figure  figsize  width  height  subplots  plt  subplots  height  width  subplots  numpy  ndarray  flatten  subplots  for  subplot  image  in  zip  subplots  digits  show  digit  image  subplot  subplot  for  subplot  in  subplots  len  digi,amazon
ts  subplot  axis  off  plt  show  print  kmeans  predictor  endpoint  import  sagemaker  sagemaker  Session  delete  endpoint  kmeans  predictor  endpoint  ,amazon
reload  ext  autoreload  autoreload  matplotlib  inlinefrom  fastai  imports  import  from  fastai  transforms  import  from  fastai  conv  learner  import  from  fastai  model  import  from  fastai  dataset  import  from  fastai  sgdr  import  from  fastai  plots  import  import  json  import  boto3PATH  data  caltech  256  sz  224  arch  resnext50  bs  64def  read  dirs  path  folder  Fetches  name  of  all  files  in  path  in  long  form  and  labels  associated  by  extrapolation  of  directory  names  lbls  fnames  all  lbls  full  path  os  path  join  path  folder  for  lbl  in  sorted  os  listdir  full  path  if  lbl  not  in  ipynb  checkpoints  DS  Store  all  lbls  append  lbl  for  fname  in  os  listdir  os  path  join  full  path  lbl  if  fname  endswith  jpg  fnames  append  os  path  join  lbl  fname  lbls  append  lbl  return  fnames  lbls  all  lblsfnames  lbls  all  lbls  read  dirs  PATH  train  len  fnames  len  lbls  len  all  lbls  label  df  pd  DataFrame  id  fnames  label  lbls  l,amazon
abel  df  head  label  df  pivot  table  index  label  aggfunc  len  sort  values  id  ascending  False  label  df  to  csv  PATH  labels  csv  index  False  label  df  pd  read  csv  PATH  labels  csv  label  df  size  val  idxs  get  cv  idxs  len  val  idxs  def  get  data  sz  bs  tfms  tfms  from  model  arch  sz  aug  tfms  transforms  side  on  max  zoom  data  ImageClassifierData  from  csv  PATH  train  PATH  labels  csv  num  workers  val  idxs  val  idxs  tfms  tfms  bs  bs  return  data  if  sz  300  else  data  resize  340  tmp  data  get  data  224  bs  learn  ConvLearner  pretrained  arch  data  precompute  True  ps  lrf  learn  lr  find  learn  sched  plot  lr  learn  sched  plot  LEARN  RATE  3e  plt  axvline  LEARN  RATE  color  red  learn  fit  LEARN  RATE  learn  precompute  Falselearn  precompute  False  learn  fit  LEARN  RATE  cycle  len  learn  unfreeze  lr  np  array  3e  3e  3e  learn  fit  lr  cycle  len  cycle  mult  torch  save  learn  model  PATH  models  caltech  256  resnext50 ,amazon
 224  pt  pickle  module  dill  learn  save  224  learn  load  224  lr  np  array  1e  1e  1e  learn  set  data  get  data  299  bs  learn  fit  lr  cycle  len  cycle  mult  torch  save  learn  model  PATH  models  caltech  256  resnext50  299  pt  pickle  module  dill  learn  save  299  learn  load  299  log  preds  learn  TTA  probs  np  mean  np  exp  log  preds  accuracy  np  probs  preds  np  argmax  probs  axis  from  sklearn  metrics  import  confusion  matrix  cm  confusion  matrix  preds  plot  confusion  matrix  cm  data  classes  with  open  PATH  models  classes  json  as  outfile  json  dump  json  dumps  all  lbls  outfile  tar  czvf  data  caltech  256  model  tar  gz  data  caltech  256  models  caltech  256  resnext50  299  h5  classes  jsonbucket  sagemaker  934676248949  eu  west  customize  to  the  name  of  your  S3  bucket  key  models  caltech256  fastai  model  tar  gz  prefix  of  the  S3  bucket  of  the  model  fileboto3  client  s3  upload  file  PATH  model  tar  gz  bucket  key ,amazon
 print  Uploaded  model  artefacts  to  s3  bucket  key  ,amazon
import  numpy  as  np  import  pandas  as  pd  Define  IAM  role  import  boto3  import  re  import  sagemaker  from  sagemaker  import  get  execution  rolebucket  name  fish  dsci  training  file  key  sagemaker  tutorial  biketrain  bike  train  csv  validation  file  key  sagemaker  tutorial  biketrain  bike  validation  csv  test  file  key  sagemaker  tutorial  biketrain  bike  test  csv  s3  model  output  location  s3  sagemaker  tutorial  biketrain  model  format  bucket  name  s3  training  file  location  s3  format  bucket  name  training  file  key  s3  validation  file  location  s3  format  bucket  name  validation  file  key  s3  test  file  location  s3  format  bucket  name  test  file  key  print  s3  model  output  location  print  s3  training  file  location  print  s3  validation  file  location  print  s3  test  file  location  function  to  write  data  to  s3  def  write  to  s3  filename  bucket  key  with  open  filename  rb  as  Read  in  binary  mode  return  boto3  Session  reso,amazon
urce  s3  Bucket  bucket  Object  key  upload  fileobj  write  to  s3  bike  train  csv  bucket  name  training  file  key  write  to  s3  bike  validation  csv  bucket  name  validation  file  key  write  to  s3  bike  test  csv  bucket  name  test  file  key  Registry  Path  for  algorithms  provided  by  SageMaker  https  docs  aws  amazon  com  sagemaker  latest  dg  sagemaker  algo  docker  registry  paths  html  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  xgboost  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  xgboost  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  xgboost  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  xgboost  latest  role  get  execution  role  This  role  contains  the  permissions  needed  to  train  deploy  models  SageMaker  Service  is  trusted  to  assume  this  role  print  role  sess  sagemaker  Session  Access  appropriate  algorithm  container  image  Specify  how  many  instances  to,amazon
  use  for  distributed  training  and  what  type  of  machine  to  use  Finally  specify  where  the  trained  model  artifacts  needs  to  be  stored  Reference  http  sagemaker  readthedocs  io  en  latest  estimators  html  estimator  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  role  train  instance  count  train  instance  type  ml  m4  xlarge  output  path  s3  model  output  location  sagemaker  session  sess  base  job  name  xgboost  biketrain  v1  Specify  hyper  parameters  that  appropriate  for  the  training  algorithm  XGBoost  Training  Parameter  Reference  https  github  com  dmlc  xgboost  blob  master  doc  parameter  md  max  depth  eta  subsample  num  round  150  estimator  set  hyperparameters  max  depth  objective  reg  linear  eta  subsample  num  round  150  estimator  hyperparameters  content  type  can  be  libsvm  or  csv  for  XGBoost  training  input  config  sagemaker  session  s3  input  s3  data  s3  training  file  location  content  type  c,amazon
sv  validation  input  config  sagemaker  session  s3  input  s3  data  s3  validation  file  location  content  type  csv  print  training  input  config  config  print  validation  input  config  config  XGBoost  supports  train  validation  channels  Reference  Supported  channels  by  algorithm  https  docs  aws  amazon  com  sagemaker  latest  dg  sagemaker  algo  docker  registry  paths  html  estimator  fit  train  training  input  config  validation  validation  input  config  Ref  http  sagemaker  readthedocs  io  en  latest  estimators  html  predictor  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  endpoint  name  xgboost  biketrain  v1  ,amazon
from  sagemaker  import  get  execution  role  IAM  role  get  execution  role  bucket  amathon  seon  s3  s3  time  import  pickle  gzip  numpy  urllib  request  json  MNIST  Dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  30  This  is  format  train  set  30  from  sagemaker  import  KMeans  SageMaker  kmeans  data  location  s3  kmeans  data  format  bucket  training  output  location  s3  kmeans  model  output  format  bucket  print  training  data  will  be  uploaded  to  format  data  location  print  training  artifacts  will  be,amazon
  uploaded  to  format  output  location  kmeans  KMeans  role  role  IAM  train  instance  count  train  instance  type  ml  c4  8xlarge  output  path  output  location  10  data  location  data  location  Training  time  kmeans  fit  kmeans  record  set  train  set  time  kmeans  predictor  kmeans  deploy  initial  instance  count  instance  type  ml  m4  xlarge  time  result  kmeans  predictor  predict  valid  set  100  clusters  label  closest  cluster  float32  tensor  values  for  in  result  for  cluster  in  range  10  print  nCluster  format  int  cluster  digits  img  for  img  in  zip  clusters  valid  set  if  int  cluster  height  len  digits  width  plt  rcParams  figure  figsize  width  height  subplots  plt  subplots  height  width  subplots  numpy  ndarray  flatten  subplots  for  subplot  image  in  zip  subplots  digits  show  digit  image  subplot  subplot  for  subplot  in  subplots  len  digits  subplot  axis  off  plt  show  import  json  Simple  function  to  create  csv  from  our  nu,amazon
mpy  array  def  np2csv  arr  csv  io  BytesIO  numpy  savetxt  csv  arr  delimiter  fmt  return  csv  getvalue  decode  rstrip  runtime  boto3  Session  client  sagemaker  runtime  payload  np2csv  train  set  30  31  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  text  csv  Body  payload  result  json  loads  response  Body  read  decode  print  result  ,amazon
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  pca  mnist  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  30  This  is  format  train  set  30  import  io  import  numpy  as  np  import  sagemaker  amazon  common  as  smac  vectors  np  array  tolist  for  in  train  set  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  ,amazon
vectors  buf  seek  time  import  boto3  import  os  key  recordio  pb  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  buf  s3  train  data  s3  train  format  bucket  prefix  key  print  uploaded  training  data  location  format  s3  train  data  output  location  s3  output  format  bucket  prefix  print  training  artifacts  will  be  uploaded  to  format  output  location  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  pca  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  pca  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  pca  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  pca  latest  import  boto3  import  sagemaker  sess  sagemaker  Session  pca  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  output  location  sagemaker  session  sess  pca  set  hyper,amazon
parameters  feature  dim  50000  num  components  10  subtract  mean  True  algorithm  mode  randomized  mini  batch  size  200  pca  fit  train  s3  train  data  pca  predictor  pca  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  sagemaker  predictor  import  csv  serializer  json  deserializer  pca  predictor  content  type  text  csv  pca  predictor  serializer  csv  serializer  pca  predictor  deserializer  json  deserializerresult  pca  predictor  predict  train  set  print  result  import  numpy  as  np  eigendigits  for  array  in  np  array  split  train  set  50  result  pca  predictor  predict  array  eigendigits  projection  for  in  result  projections  eigendigits  np  array  eigendigits  Tfor  in  enumerate  eigendigits  show  digit  eigendigit  format  import  sagemaker  sagemaker  Session  delete  endpoint  pca  predictor  endpoint  ,amazon
reload  ext  autoreload  autoreload  matplotlib  inline  ,amazon
from  sqlalchemy  import  create  engine  from  sqlalchemy  utils  import  database  exists  create  database  import  psycopg2  import  pandas  as  pd  import  numpy  as  np  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  import  io  import  os  import  time  import  json  import  glob  Define  database  name  we  re  using  dataset  on  births  so  we  ll  call  it  birth  db  Set  your  postgres  username  dbname  timeseries  username  jb53856  change  this  to  your  username  engine  is  connection  to  database  Here  we  re  using  postgres  but  sqlalchemy  can  connect  to  other  things  too  engine  create  engine  postgres  localhost  username  dbname  print  engine  url  create  database  if  it  doesn  exist  if  not  database  exists  engine  url  create  database  engine  url  print  database  exists  engine  url  read  database  from  CSV  and  load  it  into  pandas  dataframe  use  your  path  files  glob  glob  os  path  join  rnd  2013  csv  files  first200  files  120  dfs ,amazon
 pd  read  csv  fp  sep  assign  VM  os  path  basename  fp  split  for  fp  in  files  df  pd  concat  dfs  ignore  index  True  files2  glob  glob  os  path  join  rnd  2013  csv  files2  first200  files2  120  dfs2  pd  read  csv  fp  sep  assign  VM  os  path  basename  fp  split  for  fp  in  files2  df2  pd  concat  dfs2  ignore  index  True  files3  glob  glob  os  path  join  rnd  2013  csv  files3  first200  files3  120  dfs3  pd  read  csv  fp  sep  assign  VM  os  path  basename  fp  split  for  fp  in  files3  df3  pd  concat  dfs3  ignore  index  True  newdat  df  append  df2  newerdat  newdat  append  df3  concatenated  df  newerdat  insert  data  into  database  from  Python  proof  of  concept  this  won  be  useful  for  big  data  of  course  concatenated  df  to  sql  concatenated  df  engine  if  exists  replace  Connect  to  make  queries  using  psycopg2  con  None  con  psycopg2  connect  database  dbname  user  username  query  sql  query  SELECT  FROM  concatenated  df  WHERE  VM  199,amazon
  tsdatafromsql  pd  read  sql  query  sql  query  con  birth  data  from  sql  head  ,amazon
matplotlib  inline  import  numpy  as  np  linear  algebra  import  pandas  as  pd  data  processing  CSV  file  pd  read  csv  import  matplotlib  pyplot  as  plt  import  seaborn  as  sns  Input  data  files  are  available  in  the  input  directory  For  example  running  this  by  clicking  run  or  pressing  Shift  Enter  will  list  the  files  in  the  input  directory  import  os  print  os  listdir  inputs  pd  read  csv  inputs  train  csv  pd  read  csv  inputs  test  csv  pd  options  display  max  columns  999  head  10  from  sklearn  preprocessing  import  LabelEncoder  as  le  Alley  le  fit  transform  Alley  astype  str  drop  PoolQC  Alley  MiscFeature  axis  drop  PoolQC  Alley  MiscFeature  axis  shape  LotFrontage  fillna  LotFrontage  median  inplace  True  LotFrontage  fillna  LotFrontage  median  inplace  True  drop  Fence  axis  drop  Fence  axis  info  for  in  zip  columns  columns  if  dtype  object  le  fit  transform  astype  str  le  fit  transform  astype  str  shape  GarageY,amazon
rBlt  GarageYrBlt  fillna  GarageYrBlt  median  MSZoning  le  fit  transform  MSZoning  astype  str  apply  lambda  fillna  median  dropna  inplace  True  shape  Exterior  loc  Exterior1st  loc  Exterior2nd  FlrSF  loc  1stFlrSF  loc  2ndFlrSF  drop  Condition1  Condition2  Exterior1st  Exterior2nd  BsmtFinSF1  BsmtUnfSF  axis  Exterior  loc  Exterior1st  loc  Exterior2nd  FlrSF  loc  1stFlrSF  loc  2ndFlrSF  drop  Condition1  Condition2  Exterior1st  Exterior2nd  BsmtFinSF1  BsmtUnfSF  axis  Bath  loc  BsmtFullBath  loc  FullBath  loc  BsmtHalfBath  loc  HalfBath  Bath  loc  BsmtFullBath  loc  FullBath  loc  BsmtHalfBath  loc  HalfBath  Sold  loc  YrSold  loc  MoSold  12  astype  float  Porch  loc  OpenPorchSF  loc  EnclosedPorch  loc  3SsnPorch  loc  ScreenPorch  Porch  loc  OpenPorchSF  loc  EnclosedPorch  loc  3SsnPorch  loc  ScreenPorch  Sold  loc  YrSold  loc  MoSold  12  astype  float  drop  BsmtExposure  BsmtFullBath  BsmtHalfBath  FullBath  HalfBath  YrSold  MoSold  MiscVal  OpenPorchSF  EnclosedPorc,amazon
h  3SsnPorch  ScreenPorch  axis  drop  BsmtExposure  BsmtFullBath  BsmtHalfBath  FullBath  HalfBath  YrSold  MoSold  MiscVal  OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  axis  drop  GarageCars  GarageFinish  BedroomAbvGr  KitchenAbvGr  1stFlrSF  2ndFlrSF  axis  drop  GarageCars  GarageFinish  KitchenAbvGr  BedroomAbvGr  1stFlrSF  2ndFlrSF  axis  drop  GarageYrBlt  axis  drop  GarageYrBlt  axis  drop  Id  axis  drop  Id  axis  corr  SalePrice  np  log  SalePrice  def  match  if  75  or  75  return  else  return  applymap  match  values  reshape  shape  shape  zip  np  where  for  in  if  append  print  for  in  append  list  columns  list  columns  print  corr  loc  SalePrice  OverallQual  GrLivArea  TotRmsAbvGrd  FlrSF  drop  FlrSF  GrLivArea  axis  drop  FlrSF  GrLivArea  axis  head  SalePrice  drop  SalePrice  axis  shapey  to  csv  data  test  csv  to  csv  data  train  csv  to  csv  data  label  csv  shapez  head  ,amazon
import  sagemaker  from  sagemaker  import  get  execution  role  from  sagemaker  session  import  Session  sagemaker  session  sagemaker  Session  region  sagemaker  session  boto  session  region  name  role  get  execution  role  import  utils  from  tensorflow  contrib  learn  python  learn  datasets  import  mnist  import  tensorflow  as  tf  data  sets  mnist  read  data  sets  data  dtype  tf  uint8  reshape  False  validation  size  5000  utils  convert  to  data  sets  train  train  data  utils  convert  to  data  sets  validation  validation  data  utils  convert  to  data  sets  test  test  data  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  DEMO  mnist  cat  mnist  py  from  sagemaker  tensorflow  import  TensorFlow  mnist  estimator  TensorFlow  entry  point  mnist  py  role  role  framework  version  training  steps  1000  evaluation  steps  100  train  instance  count  train  instance  type  ml  c4  xlarge  mnist  estimator  fit  inputs  transformer  mnist  estimator,amazon
  transformer  instance  count  instance  type  ml  m4  xlarge  input  bucket  name  sagemaker  sample  data  format  region  input  file  path  batch  transform  mnist  1000  samples  transformer  transform  s3  format  input  bucket  name  input  file  path  content  type  text  csv  transformer  wait  print  transformer  output  path  import  json  from  urllib  parse  import  urlparse  import  boto3  parsed  url  urlparse  transformer  output  path  bucket  name  parsed  url  netloc  prefix  parsed  url  path  s3  boto3  resource  s3  predictions  for  in  range  10  file  key  data  csv  out  format  prefix  output  obj  s3  Object  bucket  name  file  key  output  output  obj  get  Body  read  decode  utf  predictions  extend  json  loads  output  outputs  classes  int64Val  import  os  import  matplotlib  pyplot  as  plt  from  numpy  import  genfromtxt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  ,amazon
28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  tmp  dir  tmp  data  if  not  os  path  exists  tmp  dir  os  makedirs  tmp  dir  for  in  range  10  input  file  name  data  csv  format  input  file  key  format  input  file  path  input  file  name  s3  Bucket  input  bucket  name  download  file  input  file  key  os  path  join  tmp  dir  input  file  name  input  data  genfromtxt  os  path  join  tmp  dir  input  file  name  delimiter  show  digit  input  data  print  join  predictions  ,amazon
import  tensorflow  as  tf  tf  set  random  seed  777  for  reprducibilty  Try  to  find  value  for  and  to  compute  data  data  We  know  that  should  be  and  should  be  But  let  TensorFlow  figure  it  out  tf  Variable  tf  random  normal  name  weight  tf  Variable  tf  random  normal  name  bias  Now  we  can  use  and  in  place  of  data  and  data  placeholders  for  tensor  that  will  be  always  fed  using  feed  dict  See  http  stackoverflow  com  questions  36693740  tf  placeholder  tf  float32  shape  None  tf  placeholder  tf  float32  shape  None  Out  hypothesis  XW  hypothesis  cost  loss  function  cost  tf  reduce  mean  tf  square  hypothesis  optimizer  tf  train  GradientDescentOptimizer  learning  rate  01  train  optimizer  minimize  cost  Launch  the  graph  in  session  sess  tf  Session  Initializes  global  variables  in  the  graph  sess  run  tf  global  variables  initializer  for  step  in  range  2001  cost  val  val  val  sess  run  cost  train  feed  dict  if  ste,ibm
p  200  print  step  cost  val  val  val  print  sess  run  hypothesis  feed  dict  print  sess  run  hypothesis  feed  dict  print  sess  run  hypothesis  feed  dict  for  step  in  range  2001  cost  val  val  val  sess  run  cost  train  feed  dict  if  step  200  print  step  cost  val  val  val  print  sess  run  hypothesis  feed  dict  print  sess  run  hypothesis  feed  dict  print  sess  run  hypothesis  feed  dict  ,ibm
import  adal  from  msrestazure  azure  active  directory  import  AADTokenCredentials  from  dotenv  import  load  dotenv  find  dotenv  import  os  load  dotenv  find  dotenv  import  pandas  as  pd  import  numpy  as  npimport  requests  Parameters  need  for  API  subscription  os  getenv  SUBSCRIPTION  tenant  os  getenv  TENANT  client  id  os  getenv  CLIENT  ID  client  secret  os  getenv  CLIENT  SECRET  offer  MS  AZR  0003P  currency  USD  locale  en  US  region  US  using  authentication  with  secret  To  configure  user  service  principal  see  https  docs  microsoft  com  en  us  azure  role  based  access  control  role  assignments  portal  https  docs  microsoft  com  en  us  azure  azure  resource  manager  resource  group  create  service  principal  portal  def  authenticate  client  key  tenant  client  id  client  secret  Authenticate  using  service  principal  key  authority  host  uri  https  login  microsoftonline  com  authority  uri  authority  host  uri  tenant  resource  uri  h,microsoft
ttps  management  core  windows  net  context  adal  AuthenticationContext  authority  uri  api  version  None  mgmt  token  context  acquire  token  with  client  credentials  resource  uri  client  id  client  secret  credentials  AADTokenCredentials  mgmt  token  client  id  return  credentials  credentials  authenticate  client  key  tenant  client  id  client  secret  access  token  credentials  token  get  access  token  azure  mgmt  uri  https  management  azure  com  443  subscriptions  subscriptionId  format  subscriptionId  subscription  uri  str  azure  mgmt  uri  providers  Microsoft  Commerce  UsageAggregates  api  version  2015  06  01  preview  aggregationGranularity  Daily  reportedstartTime  2018  06  10  00  3a00  3a00Z  reportedEndTime  2018  07  14  00  3a00  3a00Z  usage  url  uri  str  format  azure  mgmt  uri  azure  mgmt  uri  usage  urlresponse  requests  get  usage  url  allow  redirects  False  headers  Authorization  Bearer  access  token  usage  response  json  pull  the  properti,microsoft
es  key  from  each  usage  record  and  create  dataframe  df  daily  usage  api  pd  DataFrame  properties  for  in  usage  value  the  API  doesn  actually  return  aggregates  by  day  the  date  has  to  be  summarized  to  get  result  by  day  by  resource  df  by  day  group  df  daily  usage  api  groupby  meterId  usageStartTime  df  daily  usage  df  by  day  group  agg  usageEndTime  np  max  meterCategory  np  max  meterRegion  np  max  meterName  np  max  meterSubCategory  np  max  subscriptionId  np  max  unit  np  max  quantity  np  sum  df  daily  usage  df  daily  usage  reset  index  clean  up  column  data  types  for  col  in  meterCategory  meterRegion  meterName  meterSubCategory  subscriptionId  unit  df  daily  usage  col  df  daily  usage  col  astype  category  df  daily  usage  usageEndTime  pd  to  datetime  df  daily  usage  usageEndTime  df  daily  usage  usageStartTime  pd  to  datetime  df  daily  usage  usageStartTime  df  daily  usage  dtypesdf  daily  usagelen  df  daily  u,microsoft
sage  filter  usage  to  dates  from  12  2018  to  11  2018  import  datetime  from  date  pd  Timestamp  datetime  date  2018  12  to  date  pd  Timestamp  datetime  date  2018  10  df  invoice  daily  usage  df  daily  usage  loc  df  daily  usage  usageStartTime  from  date  df  daily  usage  usageEndTime  to  date  len  df  invoice  daily  usage  df  invoice  daily  usage  usageEndTime  unique  pickle  everything  import  pickle  pickle  dump  df  invoice  daily  usage  open  df  invoice  daily  usage  wb  pickle  dump  df  daily  usage  api  open  df  daily  usage  api  wb  2018  06  16  65d4ded2  41ae  43a8  bb68  3c200e1ba864  import  datetime  usage  date  pd  Timestamp  datetime  date  2018  16  df  daily  usage  loc  df  daily  usage  meterId  65d4ded2  41ae  43a8  bb68  3c200e1ba864  df  daily  usage  usageStartTime  usage  date  ,microsoft
bucket  SageMakerS3Bucket  prefix  sagemaker  DEMO  linear  mnist  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  30  This  is  format  train  set  30  import  io  import  numpy  as  np  import  sagemaker  amazon  common  as  smac  vectors  np  array  tolist  for  in  train  set  astype  float32  labels  np  where  np  array  tolist  for  in  train ,amazon
 set  astype  float32  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  vectors  labels  buf  seek  import  boto3  import  os  key  recordio  pb  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  buf  s3  train  data  s3  train  format  bucket  prefix  key  print  uploaded  training  data  location  format  s3  train  data  output  location  s3  output  format  bucket  prefix  print  training  artifacts  will  be  uploaded  to  format  output  location  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  boto3  Session  region  name  linear  learner  import  boto3  import  sagemaker  sess  sagemaker  Session  linear  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  output  location  sagemaker  session  sess  linear  set  hyperparameters  feature  dim  784  predictor  type  binary  classifier  mini  batch  size  200  linear,amazon
  fit  train  s3  train  data  linear  predictor  linear  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  sagemaker  predictor  import  csv  serializer  json  deserializer  linear  predictor  content  type  text  csv  linear  predictor  serializer  csv  serializer  linear  predictor  deserializer  json  deserializerresult  linear  predictor  predict  train  set  30  31  print  result  import  numpy  as  np  predictions  for  array  in  np  array  split  test  set  100  result  linear  predictor  predict  array  predictions  predicted  label  for  in  result  predictions  predictions  np  array  predictions  import  pandas  as  pd  pd  crosstab  np  where  test  set  predictions  rownames  actuals  colnames  predictions  import  sagemaker  sagemaker  Session  delete  endpoint  linear  predictor  endpoint  ,amazon
import  os  import  sagemaker  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  import  utils  utils  cifar10  download  inputs  sagemaker  session  upload  data  path  tmp  cifar10  data  key  prefix  data  DEMO  cifar10  from  sagemaker  tensorflow  import  TensorFlow  source  dir  os  path  join  os  getcwd  source  dir  estimator  TensorFlow  entry  point  resnet  cifar  10  py  source  dir  source  dir  role  role  hyperparameters  min  eval  frequency  10  training  steps  1000  evaluation  steps  100  train  instance  count  train  instance  type  ml  c4  xlarge  base  job  name  tensorboard  example  estimator  fit  inputs  run  tensorboard  locally  True  predictor  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  import  numpy  as  np  random  image  data  np  random  rand  32  32  predictor  predict  random  image  data  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
import  os  import  boto3  from  pyspark  import  SparkContext  SparkConf  from  pyspark  sql  import  SparkSession  import  sagemaker  from  sagemaker  import  get  execution  role  import  sagemaker  pyspark  role  get  execution  role  Configure  Spark  to  use  the  SageMaker  Spark  dependency  jars  jars  sagemaker  pyspark  classpath  jars  classpath  join  sagemaker  pyspark  classpath  jars  See  the  SageMaker  Spark  Github  to  learn  how  to  connect  to  EMR  from  notebook  instance  spark  SparkSession  builder  config  spark  driver  extraClassPath  classpath  master  local  getOrCreate  sparkimport  boto3  region  boto3  Session  region  name  trainingData  spark  read  format  libsvm  option  numFeatures  784  load  s3a  sagemaker  sample  data  spark  mnist  train  format  region  testData  spark  read  format  libsvm  option  numFeatures  784  load  s3a  sagemaker  sample  data  spark  mnist  test  format  region  trainingData  show  from  pyspark  ml  import  Pipeline  from  sagemaker  p,amazon
yspark  algorithms  import  PCASageMakerEstimator  KMeansSageMakerEstimator  from  sagemaker  pyspark  import  RandomNamePolicyFactory  IAMRole  EndpointCreationPolicy  from  sagemaker  pyspark  transformation  serializers  import  ProtobufRequestRowSerializer  ML  pipeline  with  stages  PCA  and  Means  1st  stage  PCA  on  SageMaker  pcaSageMakerEstimator  PCASageMakerEstimator  sagemakerRole  IAMRole  role  trainingInstanceType  ml  m4  xlarge  trainingInstanceCount  endpointInstanceType  ml  t2  large  endpointInitialInstanceCount  namePolicyFactory  RandomNamePolicyFactory  sparksm  3p  Set  parameters  for  PCA  number  of  features  in  input  and  the  number  of  principal  components  to  find  pcaSageMakerEstimator  setFeatureDim  784  pcaSageMakerEstimator  setNumComponents  50  2nd  stage  Means  on  SageMaker  kMeansSageMakerEstimator  KMeansSageMakerEstimator  sagemakerRole  IAMRole  role  trainingSparkDataFormatOptions  featuresColumnName  projection  Default  output  column  generated  by  P,amazon
CASageMakerEstimator  requestRowSerializer  ProtobufRequestRowSerializer  featuresColumnName  projection  Default  output  column  generated  by  PCASageMakerEstimator  trainingInstanceType  ml  m4  xlarge  trainingInstanceCount  endpointInstanceType  ml  t2  large  endpointInitialInstanceCount  namePolicyFactory  RandomNamePolicyFactory  sparksm  3k  endpointCreationPolicy  EndpointCreationPolicy  CREATE  ON  TRANSFORM  Set  parameters  for  Means  kMeansSageMakerEstimator  setFeatureDim  50  kMeansSageMakerEstimator  setK  10  Define  the  stages  of  the  Pipeline  in  order  pipelineSM  Pipeline  stages  pcaSageMakerEstimator  kMeansSageMakerEstimator  Train  pipelineModelSM  pipelineSM  fit  trainingData  transformedData  pipelineModelSM  transform  testData  transformedData  show  from  pyspark  sql  types  import  DoubleType  import  matplotlib  pyplot  as  plt  import  numpy  as  np  import  string  Helper  function  to  display  digit  def  showDigit  img  caption  xlabel  subplot  None  if  subplot ,amazon
 None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axes  get  xaxis  set  ticks  subplot  axes  get  yaxis  set  ticks  plt  title  caption  plt  xlabel  xlabel  subplot  imshow  imgr  cmap  gray  def  displayClusters  data  images  np  array  data  select  features  cache  take  250  clusters  data  select  closest  cluster  cache  take  250  for  cluster  in  range  10  print  nCluster  format  string  ascii  uppercase  cluster  digits  img  for  img  in  zip  clusters  images  if  int  closest  cluster  cluster  height  len  digits  width  plt  rcParams  figure  figsize  width  height  subplots  plt  subplots  height  width  subplots  np  ndarray  flatten  subplots  for  subplot  image  in  zip  subplots  digits  showDigit  image  subplot  subplot  for  subplot  in  subplots  len  digits  subplot  axis  off  plt  show  displayClusters  transformedData  Delete  the  resources  from  sagemaker  pyspark  import  SageMakerResourceCleanup  from  sagemaker  pyspark  import  SageMakerModel  def  ,amazon
cleanUp  model  resource  cleanup  SageMakerResourceCleanup  model  sagemakerClient  resource  cleanup  deleteResources  model  getCreatedResources  Delete  the  SageMakerModel  in  pipeline  for  in  pipelineModelSM  stages  if  isinstance  SageMakerModel  cleanUp  ,amazon
import  boto3  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  mxnet  as  mx  data  mx  test  utils  get  mnist  from  mnist  import  train  model  train  data  data  num  cpus  num  gpus  import  os  import  json  os  mkdir  model  model  save  checkpoint  model  model  0000  with  open  model  model  shapes  json  as  shapes  json  dump  shape  model  data  shapes  name  data  shapes  import  tarfile  def  flatten  tarinfo  tarinfo  name  os  path  basename  tarinfo  name  return  tarinfo  tar  tarfile  open  model  tar  gz  gz  tar  add  model  filter  flatten  tar  close  import  sagemaker  sagemaker  session  sagemaker  Session  inputs  sagemaker  session  upload  data  path  model  tar  gz  key  prefix  model  from  sagemaker  mxnet  model  import  MXNetModel  sagemaker  model  MXNetModel  model  data  s3  sagemaker  session  default  bucket  model  model  tar  gz  role  role  entry  point  mnist  py  import  logging  logging  getLogger  setLevel  logging  WARNING ,amazon
 predictor  sagemaker  model  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  IPython  display  import  HTML  HTML  open  input  html  read  response  predictor  predict  data  print  Raw  prediction  result  print  response  labeled  predictions  list  zip  range  10  response  print  Labeled  predictions  print  labeled  predictions  labeled  predictions  sort  key  lambda  label  and  prob  label  and  prob  print  Most  likely  answer  format  labeled  predictions  print  predictor  endpoint  sagemaker  Session  delete  endpoint  predictor  endpoint  os  remove  model  tar  gz  import  shutil  shutil  rmtree  model  ,amazon
time  import  os  import  io  import  boto3  import  pandas  as  pd  import  numpy  as  np  import  re  from  sagemaker  import  get  execution  role  region  boto3  Session  region  name  role  get  execution  role  kms  key  id  your  kms  key  id  bucket  s3  bucket  put  your  s3  bucket  name  here  and  create  s3  bucket  prefix  sagemaker  DEMO  kms  customize  to  your  bucket  where  you  have  stored  the  data  bucket  path  https  s3  amazonaws  com  format  region  bucket  from  sklearn  datasets  import  load  boston  boston  load  boston  boston  data  boston  target  feature  names  boston  feature  names  data  pd  DataFrame  columns  feature  names  target  pd  DataFrame  columns  MEDV  data  MEDV  local  file  name  boston  csv  data  to  csv  local  file  name  header  False  index  False  from  sklearn  model  selection  import  train  test  split  train  test  train  test  train  test  split  test  size  random  state  test  val  test  val  train  test  split  test  test  test  size  ra,amazon
ndom  state  def  write  file  fname  feature  names  boston  feature  names  data  pd  DataFrame  columns  feature  names  target  pd  DataFrame  columns  MEDV  data  MEDV  bring  this  column  to  the  front  before  writing  the  files  cols  data  columns  tolist  cols  cols  cols  data  data  cols  data  to  csv  fname  header  False  index  False  train  file  train  csv  validation  file  val  csv  test  file  test  csv  write  file  train  train  train  file  write  file  val  val  validation  file  write  file  test  test  test  file  s3  boto3  client  s3  data  train  open  train  file  rb  key  train  train  format  prefix  train  file  print  Put  object  s3  put  object  Bucket  bucket  Key  key  train  Body  data  train  ServerSideEncryption  aws  kms  SSEKMSKeyId  kms  key  id  print  Done  uploading  the  training  dataset  data  validation  open  validation  file  rb  key  validation  validation  format  prefix  validation  file  print  Put  object  s3  put  object  Bucket  bucket  Key  key ,amazon
 validation  Body  data  validation  ServerSideEncryption  aws  kms  SSEKMSKeyId  kms  key  id  print  Done  uploading  the  validation  dataset  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  xgboost  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  xgboost  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  xgboost  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  xgboost  latest  container  containers  boto3  Session  region  name  time  from  time  import  gmtime  strftime  import  time  job  name  DEMO  xgboost  single  regression  strftime  gmtime  print  Training  job  job  name  create  training  params  AlgorithmSpecification  TrainingImage  container  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  bucket  path  prefix  output  ResourceConfig  InstanceCount  InstanceType  ml  m4  4xlarge  VolumeSizeInGB  TrainingJobName  job  name  HyperParameters  max  depth  eta  gamma  min  child  weight  subsa,amazon
mple  silent  objective  reg  linear  num  round  StoppingCondition  MaxRuntimeInSeconds  86400  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  bucket  path  prefix  train  S3DataDistributionType  FullyReplicated  ContentType  csv  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  bucket  path  prefix  validation  S3DataDistributionType  FullyReplicated  ContentType  csv  CompressionType  None  client  boto3  client  sagemaker  client  create  training  job  create  training  params  try  wait  for  the  job  to  finish  and  report  the  ending  status  client  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  training  info  client  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  except  print  Training  failed  to  start  if  exception  is  raised  that  means  it  has  failed ,amazon
 message  client  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  time  import  boto3  from  time  import  gmtime  strftime  model  name  job  name  model  print  model  name  info  client  describe  training  job  TrainingJobName  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  data  primary  container  Image  container  ModelDataUrl  model  data  create  model  response  client  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  endpoint  config  name  DEMO  XGBoostEndpointConfig  strftime  gmtime  print  endpoint  config  name  create  endpoint  config  response  client  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialVariantWeight  InitialInstanceCount  ModelName  model  name  Vari,amazon
antName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  import  time  endpoint  name  DEMO  XGBoostEndpoint  strftime  gmtime  print  endpoint  name  create  endpoint  response  client  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  print  create  endpoint  response  EndpointArn  print  EndpointArn  format  create  endpoint  response  EndpointArn  get  the  status  of  the  endpoint  response  client  describe  endpoint  EndpointName  endpoint  name  status  response  EndpointStatus  print  EndpointStatus  format  status  wait  until  the  status  has  changed  client  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  print  the  status  of  the  endpoint  endpoint  response  client  describe  endpoint  EndpointName  endpoint  name  status  endpoint  response  EndpointStatus  print  Endpoint  creation  ended  with  EndpointStatus  format  status  if  status  InService  raise  Exception  E,amazon
ndpoint  creation  failed  runtime  client  boto3  client  runtime  sagemaker  import  sys  import  math  def  do  predict  data  endpoint  name  content  type  payload  join  data  response  runtime  client  invoke  endpoint  EndpointName  endpoint  name  ContentType  content  type  Body  payload  result  response  Body  read  result  result  decode  utf  result  result  split  return  result  def  batch  predict  data  batch  size  endpoint  name  content  type  items  len  data  arrs  for  offset  in  range  items  batch  size  if  offset  batch  size  items  results  do  predict  data  offset  offset  batch  size  endpoint  name  content  type  arrs  extend  results  else  arrs  extend  do  predict  data  offset  items  endpoint  name  content  type  sys  stdout  write  return  arrs  time  import  json  import  numpy  as  np  with  open  test  csv  as  lines  readlines  remove  the  labels  labels  line  split  for  line  in  lines  features  line  split  for  line  in  lines  features  str  join  row  fo,amazon
r  row  in  features  preds  batch  predict  features  str  100  endpoint  name  text  csv  print  Median  Absolute  Percent  Error  MdAPE  np  median  np  abs  np  asarray  labels  dtype  float  np  asarray  preds  dtype  float  np  asarray  labels  dtype  float  client  delete  endpoint  EndpointName  endpoint  name  ,amazon
mkdir  data  wget  data  aclImdb  v1  tar  gz  http  ai  stanford  edu  amaas  data  sentiment  aclImdb  v1  tar  gz  tar  zxf  data  aclImdb  v1  tar  gz  dataimport  os  import  glob  def  read  imdb  data  data  dir  data  aclImdb  data  labels  for  data  type  in  train  test  data  data  type  labels  data  type  for  sentiment  in  pos  neg  data  data  type  sentiment  labels  data  type  sentiment  path  os  path  join  data  dir  data  type  sentiment  txt  files  glob  glob  path  for  in  files  with  open  as  review  data  data  type  sentiment  append  review  read  Here  we  represent  positive  review  by  and  negative  review  by  labels  data  type  sentiment  append  if  sentiment  pos  else  assert  len  data  data  type  sentiment  len  labels  data  type  sentiment  data  size  does  not  match  labels  size  format  data  type  sentiment  return  data  labelsdata  labels  read  imdb  data  print  IMDB  reviews  train  pos  neg  test  pos  neg  format  len  data  train  pos  len  data ,amazon
 train  neg  len  data  test  pos  len  data  test  neg  from  sklearn  utils  import  shuffle  def  prepare  imdb  data  data  labels  Prepare  training  and  test  sets  from  IMDb  movie  reviews  Combine  positive  and  negative  reviews  and  labels  data  train  data  train  pos  data  train  neg  data  test  data  test  pos  data  test  neg  labels  train  labels  train  pos  labels  train  neg  labels  test  labels  test  pos  labels  test  neg  Shuffle  reviews  and  corresponding  labels  within  training  and  test  sets  data  train  labels  train  shuffle  data  train  labels  train  data  test  labels  test  shuffle  data  test  labels  test  Return  unified  training  data  test  data  training  labels  test  labets  return  data  train  data  test  labels  train  labels  testtrain  test  train  test  prepare  imdb  data  data  labels  print  IMDb  reviews  combined  train  test  format  len  train  len  test  train  100  import  re  REPLACE  NO  SPACE  re  compile  REPLACE  WITH  SPACE  re  co,amazon
mpile  br  br  def  review  to  words  review  words  REPLACE  NO  SPACE  sub  review  lower  words  REPLACE  WITH  SPACE  sub  words  return  wordsreview  to  words  train  100  import  pickle  cache  dir  os  path  join  cache  sentiment  web  app  where  to  store  cache  files  os  makedirs  cache  dir  exist  ok  True  ensure  cache  directory  exists  def  preprocess  data  data  train  data  test  labels  train  labels  test  cache  dir  cache  dir  cache  file  preprocessed  data  pkl  Convert  each  review  to  words  read  from  cache  if  available  If  cache  file  is  not  None  try  to  read  from  it  first  cache  data  None  if  cache  file  is  not  None  try  with  open  os  path  join  cache  dir  cache  file  rb  as  cache  data  pickle  load  print  Read  preprocessed  data  from  cache  file  cache  file  except  pass  unable  to  read  from  cache  but  that  okay  If  cache  is  missing  then  do  the  heavy  lifting  if  cache  data  is  None  Preprocess  training  and  test  data  t,amazon
o  obtain  words  for  each  review  words  train  list  map  review  to  words  data  train  words  test  list  map  review  to  words  data  test  words  train  review  to  words  review  for  review  in  data  train  words  test  review  to  words  review  for  review  in  data  test  Write  to  cache  file  for  future  runs  if  cache  file  is  not  None  cache  data  dict  words  train  words  train  words  test  words  test  labels  train  labels  train  labels  test  labels  test  with  open  os  path  join  cache  dir  cache  file  wb  as  pickle  dump  cache  data  print  Wrote  preprocessed  data  to  cache  file  cache  file  else  Unpack  data  loaded  from  cache  file  words  train  words  test  labels  train  labels  test  cache  data  words  train  cache  data  words  test  cache  data  labels  train  cache  data  labels  test  return  words  train  words  test  labels  train  labels  test  Preprocess  data  train  test  train  test  preprocess  data  train  test  train  test  import  numpy ,amazon
 as  np  from  sklearn  feature  extraction  text  import  CountVectorizer  from  sklearn  externals  import  joblib  joblib  is  an  enhanced  version  of  pickle  that  is  more  efficient  for  storing  NumPy  arrays  def  extract  BoW  features  words  train  words  test  vocabulary  size  5000  cache  dir  cache  dir  cache  file  bow  features  pkl  Extract  Bag  of  Words  for  given  set  of  documents  already  preprocessed  into  words  If  cache  file  is  not  None  try  to  read  from  it  first  cache  data  None  if  cache  file  is  not  None  try  with  open  os  path  join  cache  dir  cache  file  rb  as  cache  data  joblib  load  print  Read  features  from  cache  file  cache  file  except  pass  unable  to  read  from  cache  but  that  okay  If  cache  is  missing  then  do  the  heavy  lifting  if  cache  data  is  None  Fit  vectorizer  to  training  documents  and  use  it  to  transform  them  NOTE  Training  documents  have  already  been  preprocessed  and  tokenized  into  words,amazon
  pass  in  dummy  functions  to  skip  those  steps  preprocessor  lambda  vectorizer  CountVectorizer  max  features  vocabulary  size  features  train  vectorizer  fit  transform  words  train  toarray  Apply  the  same  vectorizer  to  transform  the  test  documents  ignore  unknown  words  features  test  vectorizer  transform  words  test  toarray  NOTE  Remember  to  convert  the  features  using  toarray  for  compact  representation  Write  to  cache  file  for  future  runs  store  vocabulary  as  well  if  cache  file  is  not  None  vocabulary  vectorizer  vocabulary  cache  data  dict  features  train  features  train  features  test  features  test  vocabulary  vocabulary  with  open  os  path  join  cache  dir  cache  file  wb  as  joblib  dump  cache  data  print  Wrote  features  to  cache  file  cache  file  else  Unpack  data  loaded  from  cache  file  features  train  features  test  vocabulary  cache  data  features  train  cache  data  features  test  cache  data  vocabulary  Return  b,amazon
oth  the  extracted  features  as  well  as  the  vocabulary  return  features  train  features  test  vocabulary  Extract  Bag  of  Words  features  for  both  training  and  test  datasets  train  test  vocabulary  extract  BoW  features  train  test  len  train  100  import  pandas  as  pd  Earlier  we  shuffled  the  training  dataset  so  to  make  things  simple  we  can  just  assign  the  first  10  000  reviews  to  the  validation  set  and  use  the  remaining  reviews  for  training  val  pd  DataFrame  train  10000  train  pd  DataFrame  train  10000  val  pd  DataFrame  train  10000  train  pd  DataFrame  train  10000  First  we  make  sure  that  the  local  directory  in  which  we  like  to  store  the  training  and  validation  csv  files  exists  data  dir  data  sentiment  web  app  if  not  os  path  exists  data  dir  os  makedirs  data  dir  pd  DataFrame  test  to  csv  os  path  join  data  dir  test  csv  header  False  index  False  pd  concat  val  val  axis  to  csv  os  path  jo,amazon
in  data  dir  validation  csv  header  False  index  False  pd  concat  train  train  axis  to  csv  os  path  join  data  dir  train  csv  header  False  index  False  To  save  bit  of  memory  we  can  set  text  train  val  train  and  val  to  None  test  train  val  train  val  Noneimport  sagemaker  session  sagemaker  Session  Store  the  current  SageMaker  session  S3  prefix  which  folder  will  we  use  prefix  sentiment  web  app  test  location  session  upload  data  os  path  join  data  dir  test  csv  key  prefix  prefix  val  location  session  upload  data  os  path  join  data  dir  validation  csv  key  prefix  prefix  train  location  session  upload  data  os  path  join  data  dir  train  csv  key  prefix  prefix  from  sagemaker  import  get  execution  role  Our  current  execution  role  is  require  when  creating  the  model  as  the  training  and  inference  code  will  need  to  access  the  model  artifacts  role  get  execution  role  We  need  to  retrieve  the  location ,amazon
 of  the  container  which  is  provided  by  Amazon  for  using  XGBoost  As  matter  of  convenience  the  training  and  inference  code  both  use  the  same  container  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  session  boto  region  name  xgboost  First  we  create  SageMaker  estimator  object  for  our  model  xgb  sagemaker  estimator  Estimator  container  The  location  of  the  container  we  wish  to  use  role  What  is  our  current  IAM  Role  train  instance  count  How  many  compute  instances  train  instance  type  ml  m4  xlarge  What  kind  of  compute  instances  output  path  s3  output  format  session  default  bucket  prefix  sagemaker  session  session  And  then  set  the  algorithm  specific  parameters  xgb  set  hyperparameters  max  depth  eta  gamma  min  child  weight  subsample  silent  objective  binary  logistic  early  stopping  rounds  10  num  round  500  s3  input  train  sagemaker  s3  input  s3  data  train  lo,amazon
cation  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  val  location  content  type  csv  xgb  fit  train  s3  input  train  validation  s3  input  validation  xgb  transformer  xgb  transformer  instance  count  instance  type  ml  m4  xlarge  xgb  transformer  transform  test  location  content  type  text  csv  split  type  Line  xgb  transformer  wait  aws  s3  cp  recursive  xgb  transformer  output  path  data  dirpredictions  pd  read  csv  os  path  join  data  dir  test  csv  out  header  None  predictions  round  num  for  num  in  predictions  squeeze  values  from  sklearn  metrics  import  accuracy  score  accuracy  score  test  predictions  xgb  predictor  xgb  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  sagemaker  predictor  import  csv  serializer  We  need  to  tell  the  endpoint  what  format  the  data  we  are  sending  is  in  so  that  SageMaker  can  perform  the  serialization  xgb  predictor  content  type  text  csv  xgb  predi,amazon
ctor  serializer  csv  serializer  We  split  the  data  into  chunks  and  send  each  chunk  seperately  accumulating  the  results  def  predict  data  rows  512  split  array  np  array  split  data  int  data  shape  float  rows  predictions  for  array  in  split  array  predictions  join  predictions  xgb  predictor  predict  array  decode  utf  return  np  fromstring  predictions  sep  test  pd  read  csv  os  path  join  data  dir  test  csv  header  None  values  predictions  predict  test  predictions  round  num  for  num  in  predictions  from  sklearn  metrics  import  accuracy  score  accuracy  score  test  predictions  xgb  predictor  delete  endpoint  test  review  Nothing  but  disgusting  materialistic  pageant  of  glistening  abed  remote  control  greed  zombies  totally  devoid  of  any  heart  or  heat  romantic  comedy  that  has  zero  romantic  chemestry  and  zero  laughs  test  words  review  to  words  test  review  print  test  words  def  bow  encoding  words  vocabulary  bow  ,amazon
len  vocabulary  Start  by  setting  the  count  for  each  word  in  the  vocabulary  to  zero  for  word  in  words  split  For  each  word  in  the  string  if  word  in  vocabulary  If  the  word  is  one  that  occurs  in  the  vocabulary  increase  its  count  bow  vocabulary  word  return  bowtest  bow  bow  encoding  test  words  vocabulary  print  test  bow  len  test  bow  xgb  predictor  xgb  deploy  initial  instance  count  instance  type  ml  m4  xlarge  import  boto3  runtime  boto3  Session  client  sagemaker  runtime  xgb  predictor  endpointresponse  runtime  invoke  endpoint  EndpointName  xgb  predictor  endpoint  The  name  of  the  endpoint  we  created  ContentType  text  csv  The  data  format  that  is  expected  Body  test  bow  response  runtime  invoke  endpoint  EndpointName  xgb  predictor  endpoint  The  name  of  the  endpoint  we  created  ContentType  text  csv  The  data  format  that  is  expected  Body  join  str  val  for  val  in  test  bow  encode  utf  print  response ,amazon
 response  response  Body  read  decode  utf  print  response  xgb  predictor  endpointprint  str  vocabulary  xgb  predictor  delete  endpoint  First  we  will  remove  all  of  the  files  contained  in  the  data  dir  directory  rm  data  dir  And  then  we  delete  the  directory  itself  rmdir  data  dir  Similarly  we  remove  the  files  in  the  cache  dir  directory  and  the  directory  itself  rm  cache  dir  rmdir  cache  dir  ,amazon
import  os  import  boto3  from  pyspark  import  SparkContext  SparkConf  from  pyspark  sql  import  SparkSession  import  sagemaker  from  sagemaker  import  get  execution  role  import  sagemaker  pyspark  role  get  execution  role  Configure  Spark  to  use  the  SageMaker  Spark  dependency  jars  jars  sagemaker  pyspark  classpath  jars  classpath  join  sagemaker  pyspark  classpath  jars  See  the  SageMaker  Spark  Github  to  learn  how  to  connect  to  EMR  from  notebook  instance  spark  SparkSession  builder  config  spark  driver  extraClassPath  classpath  master  local  getOrCreate  sparkimport  boto3  region  boto3  Session  region  name  trainingData  spark  read  format  libsvm  option  numFeatures  784  load  s3a  sagemaker  sample  data  spark  mnist  train  format  region  testData  spark  read  format  libsvm  option  numFeatures  784  load  s3a  sagemaker  sample  data  spark  mnist  test  format  region  trainingData  show  from  pyspark  ml  import  Pipeline  from  pyspark  ml ,amazon
 feature  import  PCA  from  sagemaker  pyspark  algorithms  import  KMeansSageMakerEstimator  from  sagemaker  pyspark  import  IAMRole  EndpointCreationPolicy  RandomNamePolicyFactory  from  sagemaker  pyspark  transformation  serializers  import  ProtobufRequestRowSerializer  ML  pipeline  with  stages  PCA  and  Means  1st  stage  PCA  pcaSparkEstimator  PCA  inputCol  features  outputCol  projectedFeatures  50  2nd  stage  Means  on  SageMaker  kMeansSageMakerEstimator  KMeansSageMakerEstimator  sagemakerRole  IAMRole  role  trainingSparkDataFormatOptions  featuresColumnName  projectedFeatures  use  the  output  column  of  PCA  requestRowSerializer  ProtobufRequestRowSerializer  featuresColumnName  projectedFeatures  use  the  output  column  of  PCA  trainingInstanceType  ml  m4  xlarge  trainingInstanceCount  endpointInstanceType  ml  t2  medium  endpointInitialInstanceCount  namePolicyFactory  RandomNamePolicyFactory  sparksm  endpointCreationPolicy  EndpointCreationPolicy  CREATE  ON  TRANSFORM  Set,amazon
  parameters  for  Means  kMeansSageMakerEstimator  setFeatureDim  50  kMeansSageMakerEstimator  setK  10  Define  the  stages  of  the  Pipeline  in  order  pipelineSparkSM  Pipeline  stages  pcaSparkEstimator  kMeansSageMakerEstimator  Train  pipelineModelSparkSM  pipelineSparkSM  fit  trainingData  Run  predictions  transformedData  pipelineModelSparkSM  transform  testData  transformedData  show  from  pyspark  sql  types  import  DoubleType  import  matplotlib  pyplot  as  plt  import  numpy  as  np  import  string  Helper  function  to  display  digit  def  showDigit  img  caption  xlabel  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axes  get  xaxis  set  ticks  subplot  axes  get  yaxis  set  ticks  plt  title  caption  plt  xlabel  xlabel  subplot  imshow  imgr  cmap  gray  def  displayClusters  data  images  np  array  data  select  features  cache  take  250  clusters  data  select  closest  cluster  cache  take  250  for  cluster  in  range  10  pr,amazon
int  nCluster  format  string  ascii  uppercase  cluster  digits  img  for  img  in  zip  clusters  images  if  int  closest  cluster  cluster  height  len  digits  width  plt  rcParams  figure  figsize  width  height  subplots  plt  subplots  height  width  subplots  np  ndarray  flatten  subplots  for  subplot  image  in  zip  subplots  digits  showDigit  image  subplot  subplot  for  subplot  in  subplots  len  digits  subplot  axis  off  plt  show  displayClusters  transformedData  Delete  the  resources  from  sagemaker  pyspark  import  SageMakerResourceCleanup  from  sagemaker  pyspark  import  SageMakerModel  def  cleanUp  model  resource  cleanup  SageMakerResourceCleanup  model  sagemakerClient  resource  cleanup  deleteResources  model  getCreatedResources  Delete  the  SageMakerModel  in  pipeline  for  in  pipelineModelSparkSM  stages  if  isinstance  SageMakerModel  cleanUp  ,amazon
pip  install  upgrade  user  numpy  scipy  matplotlib  ipython  jupyter  pandas  sympy  pip  install  user  upgrade  pixiedustfrom  io  import  StringIO  from  scipy  import  signal  from  matplotlib  import  pylab  as  pl  from  mpl  toolkits  mplot3d  axes3d  import  Axes3D  import  requests  import  json  import  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  import  pixiedust  import  sys  import  types  import  pandas  as  pd  from  botocore  client  import  Config  import  ibm  boto3  from  project  lib  import  Project  The  project  token  is  an  authorization  token  that  is  used  to  access  project  resources  like  data  sources  connections  and  used  by  platform  APIs  project  Project  project  id  8cec06cd  ebbe  4f7a  9f66  8e1e40722f95  project  access  token  1298f2f1ff374652829046e40c4f21f2e59886f1  pc  project  project  context  The  code  was  removed  by  Watson  Studio  for  sharing  body  client  8a4fc8dc8bf54832ab62f63af0c03772  get  object  Bucket  ,ibm
breastcancerproteomes  donotdelete  pr  zmcmbnmwpo1wza  Key  clinical  data  breast  cancer  csv  Body  add  missing  iter  method  so  pandas  accepts  body  as  file  like  object  if  not  hasattr  body  iter  body  iter  types  MethodType  iter  body  patientData  pd  read  csv  body  patientData  head  reshapedData  pd  melt  orgData  id  vars  RefSeq  accession  number  gene  symbol  gene  name  var  name  Patient  value  name  Value  reshapedData  head  reshapedData  Gene  reshapedData  RefSeq  accession  number  fillna  reshapedData  gene  symbol  fillna  reshapedData  gene  name  fillna  reshapedData  Patient  ID  TCGA  reshapedData  Patient  str  slice  reshapedData  head  verify  what  separator  to  use  in  out  CSV  to  avoid  further  db  load  problem  coma  count  reshapedData  Gene  str  contains  sum  print  coma  count  project  save  data  data  reshapedData  to  csv  index  False  sep  encoding  utf  file  name  Reshaped  data1  csv  overwrite  True  reimport  cleaned  CSV  body  client ,ibm
 8a4fc8dc8bf54832ab62f63af0c03772  get  object  Bucket  breastcancerproteomes  donotdelete  pr  zmcmbnmwpo1wza  Key  77  cancer  proteomes  CPTAC  itraq  NODouble  csv  Body  add  missing  iter  method  so  pandas  accepts  body  as  file  like  object  if  not  hasattr  body  iter  body  iter  types  MethodType  iter  body  nodoubled  data  pd  read  csv  body  nodoubled  data  head  reshapedData1  pd  melt  nodoubled  data  id  vars  RefSeq  accession  number  gene  symbol  gene  name  var  name  Patient  value  name  Value  reshapedData1  head  reshapedData1  Gene  reshapedData1  RefSeq  accession  number  fillna  reshapedData1  gene  symbol  fillna  reshapedData1  gene  name  fillna  reshapedData1  Patient  ID  TCGA  reshapedData1  Patient  str  slice  reshapedData1  head  project  save  data  data  reshapedData1  to  csv  index  False  sep  encoding  utf  file  name  Reshaped  data  nodouble  csv  overwrite  True  ,ibm
import  json  from  urllib  import  request  base  url  http  localhost  8080  resp  request  urlopen  ping  base  url  print  Response  code  resp  getcode  time  payload  logistic  json  dumps  algorithm  logistic  payload  encode  utf  payload  random  forest  json  dumps  algorithm  random  forest  payload  encode  utf  headers  Content  type  application  json  def  predict  payload  req  request  Request  invocations  base  url  data  payload  headers  headers  resp  request  urlopen  req  print  Response  code  Payload  resp  getcode  json  loads  resp  read  predict  payload  logistic  predict  payload  random  forest  ,amazon
time  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  bucket  name  customize  to  your  bucket  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  image  classification  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  image  classification  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  image  classification  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  image  classification  latest  training  image  containers  boto3  Session  region  name  import  os  import  urllib  request  import  boto3  def  download  url  filename  url  split  if  not  os  path  exists  filename  urllib  request  urlretrieve  url  filename  def  upload  to  s3  channel  file  s3  boto3  resource  s3  data  open  file  rb  key  channel  file  s3  Bucket  bucket  put  object  Key  key  Body  data  caltech  256  download  http  data  mxnet  io  data  caltech  256  caltech  256  60  train  rec ,amazon
 upload  to  s3  train  caltech  256  60  train  rec  download  http  data  mxnet  io  data  caltech  256  caltech  256  60  val  rec  upload  to  s3  validation  caltech  256  60  val  rec  The  algorithm  supports  multiple  network  depth  number  of  layers  They  are  18  34  50  101  152  and  200  For  this  training  we  will  use  18  layers  num  layers  18  we  need  to  specify  the  input  image  shape  for  the  training  data  image  shape  224  224  we  also  need  to  specify  the  number  of  training  samples  in  the  training  set  for  caltech  it  is  15420  num  training  samples  15420  specify  the  number  of  output  classes  num  classes  257  batch  size  for  training  mini  batch  size  64  number  of  epochs  epochs  learning  rate  learning  rate  01  time  import  time  import  boto3  from  time  import  gmtime  strftime  s3  boto3  client  s3  create  unique  job  name  job  name  prefix  DEMO  imageclassification  timestamp  time  strftime  time  gmtime  job  name  job  na,amazon
me  prefix  timestamp  training  params  specify  the  training  docker  image  AlgorithmSpecification  TrainingImage  training  image  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  s3  output  format  bucket  job  name  prefix  ResourceConfig  InstanceCount  InstanceType  ml  p2  xlarge  VolumeSizeInGB  50  TrainingJobName  job  name  HyperParameters  image  shape  image  shape  num  layers  str  num  layers  num  training  samples  str  num  training  samples  num  classes  str  num  classes  mini  batch  size  str  mini  batch  size  epochs  str  epochs  learning  rate  str  learning  rate  StoppingCondition  MaxRuntimeInSeconds  360000  Training  data  should  be  inside  subdirectory  called  train  Validation  data  should  be  inside  subdirectory  called  validation  The  algorithm  currently  only  supports  fullyreplicated  model  where  data  is  copied  onto  each  machine  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  tra,amazon
in  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  recordio  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  recordio  CompressionType  None  print  Training  job  name  format  job  name  print  nInput  Data  Location  format  training  params  InputDataConfig  DataSource  S3DataSource  create  the  Amazon  SageMaker  training  job  sagemaker  boto3  client  service  name  sagemaker  sagemaker  create  training  job  training  params  confirm  that  the  training  job  has  started  status  sagemaker  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  Training  job  current  status  format  status  try  wait  for  the  job  to  finish  and  report  the  ending  status  sagemaker  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  training  info  sagemaker  des,amazon
cribe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  except  print  Training  failed  to  start  if  exception  is  raised  that  means  it  has  failed  message  sagemaker  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  time  import  boto3  from  time  import  gmtime  strftime  sage  boto3  Session  client  service  name  sagemaker  model  name  DEMO  full  image  classification  model  print  model  name  info  sage  describe  training  job  TrainingJobName  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  data  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  image  classification  latest  us  east  81128,amazon
4229777  dkr  ecr  us  east  amazonaws  com  image  classification  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  image  classification  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  image  classification  latest  hosting  image  containers  boto3  Session  region  name  primary  container  Image  hosting  image  ModelDataUrl  model  data  create  model  response  sage  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  timestamp  time  strftime  time  gmtime  endpoint  config  name  job  name  prefix  epc  timestamp  endpoint  config  response  sage  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  configuration  name  format  endpoint  config  name  print  Endpoint  configuration ,amazon
 arn  format  endpoint  config  response  EndpointConfigArn  time  import  time  timestamp  time  strftime  time  gmtime  endpoint  name  job  name  prefix  ep  timestamp  print  Endpoint  name  format  endpoint  name  endpoint  params  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  endpoint  response  sagemaker  create  endpoint  endpoint  params  print  EndpointArn  format  endpoint  response  EndpointArn  get  the  status  of  the  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  response  EndpointStatus  print  EndpointStatus  format  status  wait  until  the  status  has  changed  sagemaker  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  print  the  status  of  the  endpoint  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  endpoint  response  EndpointStatus  print  Endpoint  creation  ended  with  EndpointStatus  format  status  if  status  InService  raise  Exception  Endp,amazon
oint  creation  failed  import  boto3  runtime  boto3  Session  client  service  name  runtime  sagemaker  wget  tmp  test  jpg  http  www  vision  caltech  edu  Image  Datasets  Caltech256  images  008  bathtub  008  0007  jpg  file  name  tmp  test  jpg  test  image  from  IPython  display  import  Image  Image  file  name  import  json  import  numpy  as  np  with  open  file  name  rb  as  payload  read  payload  bytearray  payload  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  image  Body  payload  result  response  Body  read  result  will  be  in  json  format  and  convert  it  to  ndarray  result  json  loads  result  the  result  will  output  the  probabilities  for  all  classes  find  the  class  with  maximum  probability  and  print  the  class  index  index  np  argmax  result  object  categories  ak47  american  flag  backpack  baseball  bat  baseball  glove  basketball  hoop  bat  bathtub  bear  beer  mug  billiards  binoculars  birdbath  blimp ,amazon
 bonsai  101  boom  box  bowling  ball  bowling  pin  boxing  glove  brain  101  breadmaker  buddha  101  bulldozer  butterfly  cactus  cake  calculator  camel  cannon  canoe  car  tire  cartman  cd  centipede  cereal  box  chandelier  101  chess  board  chimp  chopsticks  cockroach  coffee  mug  coffin  coin  comet  computer  keyboard  computer  monitor  computer  mouse  conch  cormorant  covered  wagon  cowboy  hat  crab  101  desk  globe  diamond  ring  dice  dog  dolphin  101  doorknob  drinking  straw  duck  dumb  bell  eiffel  tower  electric  guitar  101  elephant  101  elk  ewer  101  eyeglasses  fern  fighter  jet  fire  extinguisher  fire  hydrant  fire  truck  fireworks  flashlight  floppy  disk  football  helmet  french  horn  fried  egg  frisbee  frog  frying  pan  galaxy  gas  pump  giraffe  goat  golden  gate  bridge  goldfish  golf  ball  goose  gorilla  grand  piano  101  grapes  grasshopper  guitar  pick  hamburger  hammock  harmonica  harp  harpsichord  hawksbill  101  head  phones  helicop,amazon
ter  101  hibiscus  homer  simpson  horse  horseshoe  crab  hot  air  balloon  hot  dog  hot  tub  hourglass  house  fly  human  skeleton  hummingbird  ibis  101  ice  cream  cone  iguana  ipod  iris  jesus  christ  joy  stick  kangaroo  101  kayak  ketch  101  killer  whale  knife  ladder  laptop  101  lathe  leopards  101  license  plate  lightbulb  light  house  lightning  llama  101  mailbox  mandolin  mars  mattress  megaphone  menorah  101  microscope  microwave  minaret  minotaur  motorbikes  101  mountain  bike  mushroom  mussels  necktie  octopus  ostrich  owl  palm  pilot  palm  tree  paperclip  paper  shredder  pci  card  penguin  people  pez  dispenser  photocopier  picnic  table  playing  card  porcupine  pram  praying  mantis  pyramid  raccoon  radio  telescope  rainbow  refrigerator  revolver  101  rifle  rotary  phone  roulette  wheel  saddle  saturn  school  bus  scorpion  101  screwdriver  segway  self  propelled  lawn  mower  sextant  sheet  music  skateboard  skunk  skyscraper  smokestack ,amazon
 snail  snake  sneaker  snowmobile  soccer  ball  socks  soda  can  spaghetti  speed  boat  spider  spoon  stained  glass  starfish  101  steering  wheel  stirrups  sunflower  101  superman  sushi  swan  swiss  army  knife  sword  syringe  tambourine  teapot  teddy  bear  teepee  telephone  box  tennis  ball  tennis  court  tennis  racket  theodolite  toaster  tomato  tombstone  top  hat  touring  bike  tower  pisa  traffic  light  treadmill  triceratops  tricycle  trilobite  101  tripod  shirt  tuning  fork  tweezer  umbrella  101  unicorn  vcr  video  projector  washing  machine  watch  101  waterfall  watermelon  welding  mask  wheelbarrow  windmill  wine  bottle  xylophone  yarmulke  yo  yo  zebra  airplanes  101  car  side  101  faces  easy  101  greyhound  tennis  shoes  toad  clutter  print  Result  label  object  categories  index  probability  str  result  index  sage  delete  endpoint  EndpointName  endpoint  name  ,amazon
time  import  os  import  boto3  import  re  import  json  from  sagemaker  import  get  execution  role  region  boto3  Session  region  name  role  get  execution  role  bucket  s3  bucket  put  your  s3  bucket  name  here  and  create  s3  bucket  prefix  sagemaker  DEMO  xgboost  byo  bucket  path  https  s3  amazonaws  com  format  region  bucket  customize  to  your  bucket  where  you  have  stored  the  data  conda  install  conda  forge  xgboost  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  gzip  open  mnist  pkl  gz  rb  train  set  valid  set  test  set  pickle  load  encoding  latin1  close  time  import  struct  import  io  import  boto3  def  get  dataset  import  pickle  import  gzip  with  gzip  open  mnist  pkl  gz  rb  as  pickle  Unpickler  encoding  latin1  return  load  train  set  valid  set  test  set  get  dataset  train  train  set  train  train  set  v,amazon
alid  valid  set  valid  valid  set  test  test  set  test  test  set  import  xgboost  as  xgb  import  sklearn  as  sk  bt  xgb  XGBClassifier  max  depth  learning  rate  estimators  10  objective  multi  softmax  Setup  xgboost  model  bt  fit  train  train  Train  it  to  our  data  eval  set  valid  valid  verbose  False  model  file  name  DEMO  local  xgboost  model  bt  Booster  save  model  model  file  name  tar  czvf  model  tar  gz  model  file  namefObj  open  model  tar  gz  rb  key  os  path  join  prefix  model  file  name  model  tar  gz  boto3  Session  resource  s3  Bucket  bucket  Object  key  upload  fileobj  fObj  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  xgboost  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  xgboost  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  xgboost  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  xgboost  latest  container  containers  boto3  Session  region  name  time  ,amazon
from  time  import  gmtime  strftime  model  name  model  file  name  strftime  gmtime  model  url  https  s3  amazonaws  com  format  region  bucket  key  sm  client  boto3  client  sagemaker  print  model  url  primary  container  Image  container  ModelDataUrl  model  url  create  model  response2  sm  client  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response2  ModelArn  from  time  import  gmtime  strftime  endpoint  config  name  DEMO  XGBoostEndpointConfig  strftime  gmtime  print  endpoint  config  name  create  endpoint  config  response  sm  client  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  InitialVariantWeight  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  import  time  endpoint  name  DEMO  XGBoostEndpoint  strftime  gmtime  pr,amazon
int  endpoint  name  create  endpoint  response  sm  client  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  print  create  endpoint  response  EndpointArn  resp  sm  client  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  while  status  Creating  time  sleep  60  resp  sm  client  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  print  Arn  resp  EndpointArn  print  Status  status  runtime  client  boto3  client  runtime  sagemaker  import  numpy  as  np  point  test  point  np  expand  dims  point  axis  point  test  np  savetxt  test  point  csv  point  delimiter  time  import  json  file  name  test  point  csv  customize  to  your  test  file  will  be  mnist  single  test  if  use  data  above  with  open  file  name  as  payload  read  strip  response  runtime  client  invoke  endpoint  EndpointName  endpoint  name  ContentType  text  csv  Body  payloa,amazon
d  result  response  Body  read  decode  ascii  print  Predicted  Class  Probabilities  format  result  floatArr  np  array  json  loads  result  predictedLabel  np  argmax  floatArr  print  Predicted  Class  Label  format  predictedLabel  print  Actual  Class  Label  format  point  sm  client  delete  endpoint  EndpointName  endpoint  name  ,amazon
import  time  import  numpy  as  np  np  random  seed  import  pandas  as  pd  import  json  import  matplotlib  pyplot  as  plt  conda  install  s3fsimport  boto3  import  s3fs  import  sagemaker  from  sagemaker  import  get  execution  rolebucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  deepar  sagemaker  session  sagemaker  Session  role  get  execution  role  s3  data  path  data  format  bucket  prefix  s3  output  path  output  format  bucket  prefix  containers  us  east  522234722520  dkr  ecr  us  east  amazonaws  com  forecasting  deepar  latest  us  east  566113047672  dkr  ecr  us  east  amazonaws  com  forecasting  deepar  latest  us  west  156387875391  dkr  ecr  us  west  amazonaws  com  forecasting  deepar  latest  eu  west  224300973850  dkr  ecr  eu  west  amazonaws  com  forecasting  deepar  latest  image  name  containers  boto3  Session  region  name  freq  prediction  length  48context  length  72t0  2016  01  01  00  00  00  data  length  400  num  ts  200  period  24tim,amazon
e  series  for  in  range  num  ts  level  10  np  random  rand  seas  amplitude  np  random  rand  level  sig  05  level  noise  parameter  constant  in  time  time  ticks  np  array  range  data  length  source  level  seas  amplitude  np  sin  time  ticks  np  pi  period  noise  sig  np  random  randn  data  length  data  source  noise  index  pd  DatetimeIndex  start  t0  freq  freq  periods  data  length  time  series  append  pd  Series  data  data  index  index  time  series  plot  plt  show  time  series  training  for  ts  in  time  series  time  series  training  append  ts  prediction  length  time  series  plot  label  test  time  series  training  plot  label  train  ls  plt  legend  plt  show  def  series  to  obj  ts  cat  None  obj  start  str  ts  index  target  list  ts  if  cat  obj  cat  cat  return  obj  def  series  to  jsonline  ts  cat  None  return  json  dumps  series  to  obj  ts  cat  encoding  utf  s3filesystem  s3fs  S3FileSystem  with  s3filesystem  open  s3  data  path  train  ,amazon
train  json  wb  as  fp  for  ts  in  time  series  training  fp  write  series  to  jsonline  ts  encode  encoding  fp  write  encode  encoding  with  s3filesystem  open  s3  data  path  test  test  json  wb  as  fp  for  ts  in  time  series  fp  write  series  to  jsonline  ts  encode  encoding  fp  write  encode  encoding  estimator  sagemaker  estimator  Estimator  sagemaker  session  sagemaker  session  image  name  image  name  role  role  train  instance  count  train  instance  type  ml  c4  xlarge  base  job  name  DEMO  deepar  output  path  s3  s3  output  path  hyperparameters  time  freq  freq  context  length  str  context  length  prediction  length  str  prediction  length  num  cells  40  num  layers  likelihood  gaussian  epochs  20  mini  batch  size  32  learning  rate  001  dropout  rate  05  early  stopping  patience  10  estimator  set  hyperparameters  hyperparameters  data  channels  train  s3  train  format  s3  data  path  test  s3  test  format  s3  data  path  estimator  fit  inp,amazon
uts  data  channels  job  name  estimator  latest  training  job  name  endpoint  name  sagemaker  session  endpoint  from  job  job  name  job  name  initial  instance  count  instance  type  ml  m4  xlarge  deployment  image  image  name  role  role  class  DeepARPredictor  sagemaker  predictor  RealTimePredictor  def  set  prediction  parameters  self  freq  prediction  length  Set  the  time  frequency  and  prediction  length  parameters  This  method  must  be  called  before  being  able  to  use  predict  Parameters  freq  string  indicating  the  time  frequency  prediction  length  integer  number  of  predicted  time  points  Return  value  none  self  freq  freq  self  prediction  length  prediction  length  def  predict  self  ts  cat  None  encoding  utf  num  samples  100  quantiles  Requests  the  prediction  of  for  the  time  series  listed  in  ts  each  with  the  optional  corresponding  category  listed  in  cat  Parameters  ts  list  of  pandas  Series  objects  the  time  series  to  ,amazon
predict  cat  list  of  integers  default  None  encoding  string  encoding  to  use  for  the  request  default  utf  num  samples  integer  number  of  samples  to  compute  at  prediction  time  default  100  quantiles  list  of  strings  specifying  the  quantiles  to  compute  default  Return  value  list  of  pandas  DataFrame  objects  each  containing  the  predictions  prediction  times  index  for  in  ts  req  self  encode  request  ts  cat  encoding  num  samples  quantiles  res  super  DeepARPredictor  self  predict  req  return  self  decode  response  res  prediction  times  encoding  def  encode  request  self  ts  cat  encoding  num  samples  quantiles  instances  series  to  obj  ts  cat  if  cat  else  None  for  in  range  len  ts  configuration  num  samples  num  samples  output  types  quantiles  quantiles  quantiles  http  request  data  instances  instances  configuration  configuration  return  json  dumps  http  request  data  encode  encoding  def  decode  response  self  response ,amazon
 prediction  times  encoding  response  data  json  loads  response  decode  encoding  list  of  df  for  in  range  len  prediction  times  prediction  index  pd  DatetimeIndex  start  prediction  times  freq  self  freq  periods  self  prediction  length  list  of  df  append  pd  DataFrame  data  response  data  predictions  quantiles  index  prediction  index  return  list  of  dfpredictor  DeepARPredictor  endpoint  endpoint  name  sagemaker  session  sagemaker  session  content  type  application  json  predictor  set  prediction  parameters  freq  prediction  length  list  of  df  predictor  predict  time  series  training  actual  data  time  series  for  in  range  len  list  of  df  plt  figure  figsize  12  actual  data  prediction  length  context  length  plot  label  target  p10  list  of  df  p90  list  of  df  plt  fill  between  p10  index  p10  p90  color  alpha  label  80  confidence  interval  list  of  df  plot  label  prediction  median  plt  legend  plt  show  sagemaker  session  delete,amazon
  endpoint  endpoint  name  ,amazon
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  pca  mnist  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  30  This  is  format  train  set  30  import  io  import  numpy  as  np  import  sagemaker  amazon  common  as  smac  vectors  np  array  tolist  for  in  train  set  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  ,amazon
vectors  buf  seek  time  import  boto3  import  os  key  recordio  pb  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  buf  s3  train  data  s3  train  format  bucket  prefix  key  print  uploaded  training  data  location  format  s3  train  data  output  location  s3  output  format  bucket  prefix  print  training  artifacts  will  be  uploaded  to  format  output  location  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  boto3  Session  region  name  pca  import  boto3  import  sagemaker  sess  sagemaker  Session  pca  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  output  location  sagemaker  session  sess  pca  set  hyperparameters  feature  dim  50000  num  components  10  subtract  mean  True  algorithm  mode  randomized  mini  batch  size  200  pca  fit  train  s3  train  data  pca  predictor  pca  deploy  initial  in,amazon
stance  count  instance  type  ml  m4  xlarge  from  sagemaker  predictor  import  csv  serializer  json  deserializer  pca  predictor  content  type  text  csv  pca  predictor  serializer  csv  serializer  pca  predictor  deserializer  json  deserializerresult  pca  predictor  predict  train  set  print  result  import  numpy  as  np  eigendigits  for  array  in  np  array  split  train  set  100  result  pca  predictor  predict  array  eigendigits  projection  for  in  result  projections  eigendigits  np  array  eigendigits  Tfor  in  enumerate  eigendigits  show  digit  eigendigit  format  import  sagemaker  sagemaker  Session  delete  endpoint  pca  predictor  endpoint  ,amazon
time  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  your  bucket  name  customize  to  your  bucket  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  image  classification  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  image  classification  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  image  classification  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  image  classification  latest  training  image  containers  boto3  Session  region  name  print  training  image  The  algorithm  supports  multiple  network  depth  number  of  layers  They  are  18  34  50  101  152  and  200  For  this  training  we  will  use  18  layers  num  layers  18  we  need  to  specify  the  input  image  shape  for  the  training  data  image  shape  32  32  we  also  need  to  specify  the  number  of  training  samples  in  the  training  set  for  caltech  it  is  25000  num  ,amazon
training  samples  25000  specify  the  number  of  output  classes  dogs  vs  cat  num  classes  batch  size  for  training  mini  batch  size  128  number  of  epochs  epochs  learning  rate  learning  rate  01  top  Since  we  are  using  transfer  learning  we  set  use  pretrained  model  to  so  that  weights  can  be  initialized  with  pre  trained  weights  use  pretrained  model  time  import  time  import  boto3  from  time  import  gmtime  strftime  s3  boto3  client  s3  create  unique  job  name  job  name  prefix  sagemaker  dogvscat  notebook  timestamp  time  strftime  time  gmtime  job  name  job  name  prefix  timestamp  training  params  specify  the  training  docker  image  AlgorithmSpecification  TrainingImage  training  image  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  s3  output  format  bucket  job  name  prefix  ResourceConfig  InstanceCount  InstanceType  ml  p2  xlarge  VolumeSizeInGB  50  TrainingJobName  job  name  HyperParameters  image  shape  imag,amazon
e  shape  num  layers  str  num  layers  num  training  samples  str  num  training  samples  num  classes  str  num  classes  mini  batch  size  str  mini  batch  size  epochs  str  epochs  learning  rate  str  learning  rate  use  pretrained  model  str  use  pretrained  model  StoppingCondition  MaxRuntimeInSeconds  360000  Training  data  should  be  inside  subdirectory  called  train  Validation  data  should  be  inside  subdirectory  called  validation  The  algorithm  currently  only  supports  fullyreplicated  model  where  data  is  copied  onto  each  machine  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  dog  vs  cat  train  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  recordio  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  dog  vs  cat  validation  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  recordio  Compressi,amazon
onType  None  print  Training  job  name  format  job  name  print  nInput  Data  Location  format  training  params  InputDataConfig  DataSource  S3DataSource  create  the  Amazon  SageMaker  training  job  sagemaker  boto3  client  service  name  sagemaker  sagemaker  create  training  job  training  params  confirm  that  the  training  job  has  started  status  sagemaker  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  Training  job  current  status  format  status  try  wait  for  the  job  to  finish  and  report  the  ending  statua  sagemaker  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  except  print  Training  failed  to  start  if  exception  is  raised  that  means  it  has  failed  message  sagemaker  describe  training  job  TrainingJobName  job  name  ,amazon
FailureReason  print  Training  failed  with  the  following  error  format  message  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  time  import  boto3  from  time  import  gmtime  strftime  sage  boto3  Session  client  service  name  sagemaker  model  name  dogvscat  print  model  name  info  sage  describe  training  job  TrainingJobName  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  data  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  image  classification  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  image  classification  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  image  classification  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  image  classification  latest  hosting  image  containers  boto3  Session  region  name  primary  container  Image  hosting,amazon
  image  ModelDataUrl  model  data  create  model  response  sage  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  timestamp  time  strftime  time  gmtime  endpoint  config  name  job  name  prefix  epc  timestamp  endpoint  config  response  sage  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  configuration  name  format  endpoint  config  name  print  Endpoint  configuration  arn  format  endpoint  config  response  EndpointConfigArn  time  import  time  timestamp  time  strftime  time  gmtime  endpoint  name  job  name  prefix  ep  timestamp  print  Endpoint  name  format  endpoint  name  endpoint  params  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  endpoint  response  sagemaker  create  ,amazon
endpoint  endpoint  params  print  EndpointArn  format  endpoint  response  EndpointArn  get  the  status  of  the  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  response  EndpointStatus  print  EndpointStatus  format  status  wait  until  the  status  has  changed  sagemaker  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  print  the  status  of  the  endpoint  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  endpoint  response  EndpointStatus  print  Endpoint  creation  ended  with  EndpointStatus  format  status  if  status  InService  raise  Exception  Endpoint  creation  failed  import  boto3  runtime  boto3  Session  client  service  name  runtime  sagemaker  wget  tmp  test  jpg  https  s3  amazonaws  com  sm  demo  iad  dog  vs  cat  test  10019  jpg  file  name  tmp  test  jpg  test  image  from  IPython  display  import  Image  Image  file  name  import  json  import  numpy  as  np  with  open ,amazon
 file  name  rb  as  payload  read  payload  bytearray  payload  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  image  Body  payload  result  response  Body  read  result  will  be  in  json  format  and  convert  it  to  ndarray  result  json  loads  result  the  result  will  output  the  probabilities  for  all  classes  find  the  class  with  maximum  probability  and  print  the  class  index  index  np  argmax  result  object  categories  cat  dog  print  Result  label  object  categories  index  probability  str  result  index  ,amazon
Write  program  for  game  where  the  computer  generates  random  starting  number  between  20  and  30  The  player  and  the  computer  can  remove  or  from  the  number  in  turns  Something  like  this  Starting  number  25  How  many  do  you  want  to  remove  22  left  Computer  removes  20  left  The  player  who  has  to  remove  the  last  value  to  bring  the  number  down  to  is  the  loser  left  Computer  removes  You  win  Easy  Get  the  computer  to  choose  number  between  at  random  import  random  starting  num  random  randint  20  30  win  True  print  Starting  number  format  starting  num  while  True  choice  player  int  input  How  many  do  you  want  to  remove  if  player  not  in  choice  break  starting  num  player  print  left  format  starting  num  if  starting  num  win  False  break  cpu  random  randint  starting  num  cpu  print  Computer  removes  format  cpu  print  left  format  starting  num  if  starting  num  break  if  win  True  print  You  win  else  p,microsoft
rint  CPU  wins  Write  program  for  game  where  the  computer  generates  random  starting  number  between  20  and  30  The  player  and  the  computer  can  remove  or  from  the  number  in  turns  Something  like  this  Starting  number  25  How  many  do  you  want  to  remove  22  left  Computer  removes  20  left  The  player  who  has  to  remove  the  last  value  to  bring  the  number  down  to  is  the  loser  left  Computer  removes  You  win  Hard  Get  the  computer  to  employ  strategy  to  try  and  win  Write  program  for  Higher  Lower  guessing  game  The  computer  randomly  generates  sequence  of  up  to  10  numbers  between  and  13  The  player  after  seeing  each  number  in  turn  has  to  decide  whether  the  next  number  is  higher  or  lower  If  you  get  10  guesses  right  you  win  the  game  Starting  number  12  Higher  or  lower  Next  number  Higher  or  lower  Next  number  11  You  lose  Hints  Use  condition  controlled  loop  do  until  while  etc  to  contr,microsoft
ol  the  game  You  do  not  need  to  remember  all  10  numbers  but  just  the  current  and  the  next  number  Remember  to  keep  count  of  the  number  of  guesses  from  random  import  randint  start  randint  13  print  Starting  number  format  start  correct  win  True  while  correct  10  choice  input  Higher  or  lower  nextnum  randint  13  print  Next  number  format  nextnum  if  choice  and  nextnum  start  or  choice  and  nextnum  start  correct  else  win  False  break  start  nextnum  if  win  print  You  win  else  print  You  lose  Extend  the  above  Higher  Lower  number  guessing  game  by  Giving  the  player  two  lives  Allowing  only  or  to  be  entered  from  random  import  randint  start  randint  13  print  Starting  number  format  start  correct  win  True  lives  while  correct  10  choices  choice  input  Higher  or  lower  if  choice  not  in  choices  print  Invalid  input  break  nextnum  randint  13  print  Next  number  format  nextnum  if  choice  and  nextnum  ,microsoft
start  or  choice  and  nextnum  start  correct  elif  lives  lives  print  life  left  else  win  False  break  start  nextnum  if  win  print  You  win  else  print  You  lose  Write  program  to  count  the  number  of  words  in  sentence  The  user  enters  sentence  The  program  outputs  the  number  of  words  in  the  sentence  Hint  Look  for  spaces  and  full  stops  in  the  string  sentence  input  Enter  sentence  sentence  sentence  split  count  for  in  sentence  count  print  Number  of  words  format  count  Write  program  that  will  display  sentence  backwards  after  being  entered  Note  You  may  not  use  the  Python  list  reverse  function  def  reverse  string  string  if  string  return  string  else  return  reverse  string  string  string  print  reverse  string  Hello  ,microsoft
import  numpy  as  np  from  keras  datasets  import  mnist  test  test  mnist  load  data  from  random  import  randint  image  id  randint  test  shape  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  imshow  test  image  id  cmap  plt  cm  binary  plt  show  score  payload  data  test  reshape  test  shape  test  shape  test  shape  score  payload  data  score  payload  data  astype  float32  255  score  payload  data  score  payload  data  image  id  tolist  scoring  payload  values  score  payload  data  import  json  print  json  dumps  scoring  payload  ,ibm
import  pandas  as  pd  import  numpy  as  np  from  sklearn  import  preprocessing  from  sklearn  model  selection  import  train  test  split  from  sklearn  metrics  import  accuracy  score  from  sklearn  import  svm  from  sklearn  import  tree  from  sklearn  linear  model  import  LogisticRegression  import  seaborn  as  sns  import  matplotlib  pyplot  as  pltabalone  data  https  archive  ics  uci  edu  ml  machine  learning  databases  abalone  abalone  data  names  Sex  Length  Diameter  Height  Whole  Weight  Shucked  Weight  Viscera  Weight  Shell  Weight  Rings  abalone  df  pd  read  csv  abalone  data  header  None  names  names  abalone  df  head  abalone  df  count  sns  pairplot  abalone  df  hue  Sex  plt  show  plt  subplots  figsize  15  10  sns  heatmap  abalone  df  corr  annot  True  plt  show  abalone  df  corr  abalone  df  groupby  Sex  mean  plt  figure  figsize  10  sns  kdeplot  abalone  df  Rings  abalone  df  Sex  color  darkturquoise  shade  True  sns  kdeplot  abalone  df  ,amazon
Rings  abalone  df  Sex  color  salmon  shade  True  sns  kdeplot  abalone  df  Rings  abalone  df  Sex  color  2ecc71  shade  True  plt  legend  plt  title  Density  Plot  of  Rings  Sex  plt  show  le  preprocessing  LabelEncoder  abalone  df  Sex  le  fit  transform  abalone  df  Sex  abalone  df  head  abalone  df  Diameter  Length  Rate  abalone  df  Diameter  abalone  df  Length  abalone  df  drop  Diameter  Length  axis  inplace  True  abalone  df  head  abalone  df  loc  Length  Diameter  Length  Rate  abalone  df  loc  Sex  train  test  train  test  train  test  split  test  size  svc  model  svm  SVC  kernel  rbf  gamma  001  svc  model  fit  train  train  pred  svc  model  predict  test  svc  model  score  train  train  accuracy  score  test  pred  dt  model  tree  DecisionTreeClassifier  dt  model  fit  train  train  pred  dt  model  predict  test  dt  model  score  train  train  accuracy  score  test  pred  log  model  LogisticRegression  penalty  l2  log  model  fit  train  train  pred  log  mod,amazon
el  predict  test  log  model  score  train  train  accuracy  score  test  pred  ,amazon
import  google  datalab  bigquery  as  bq  bq  query  conversions  device  with  visits  as  SELECT  visitId  device  deviceCategory  as  device  hits  transaction  transactionRevenue  as  revenue  FROM  bigquery  public  data  google  analytics  sample  ga  sessions  UNNEST  hits  AS  hits  where  TABLE  SUFFIX  between  20170701  and  20170801  and  hits  transaction  transactionId  is  not  null  and  geoNetwork  region  not  in  not  available  in  demo  dataset  select  device  IF  revenue  is  not  null  completed  abandoned  as  outcome  COUNT  AS  count  FROM  visits  GROUP  BY  device  outcome  ORDER  BY  count  desc  limit  10  bq  execute  conversions  device  chart  sankey  data  conversions  device  sankey  node  colors  yellow  red  green  red  brown  ,google
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  byo  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  time  import  json  import  os  import  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  sh  The  name  of  our  algorithm  algorithm  name  rmars  set  stop  if  anything  fails  account  aws  sts  get  caller  identity  query  Account  output  text  Get  the  region  defined  in  the  current  configuration  default  to  us  west  if  none  defined  region  aws  configure  get  region  region  region  us  west  fullname  account  dkr  ecr  region  amazonaws  com  algorithm  name  latest  If  the  repository  doesn  exist  in  ECR  create  it  aws  ecr  describe  repositories  repository  names  algorithm  name  dev  null  if  ne  then  aws  ecr  create  repository  repository  name  algorithm  name  dev  null  fi  Get  the  login  command  from  ECR  and  execute  it  directly  aws  ecr ,amazon
 get  login  region  region  no  include  email  Build  the  docker  image  locally  with  the  image  name  and  then  push  it  to  ECR  with  the  full  name  docker  build  algorithm  name  docker  tag  algorithm  name  fullname  docker  push  fullname  train  file  iris  csv  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  train  file  upload  file  train  file  region  boto3  Session  region  name  account  boto3  client  sts  get  caller  identity  get  Account  job  DEMO  byo  time  strftime  time  gmtime  print  Training  job  job  training  params  RoleArn  role  TrainingJobName  job  AlgorithmSpecification  TrainingImage  dkr  ecr  amazonaws  com  rmars  latest  format  account  region  TrainingInputMode  File  ResourceConfig  InstanceCount  InstanceType  ml  m4  xlarge  VolumeSizeInGB  10  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  S3DataDistributionType  FullyReplicated  Compre,amazon
ssionType  None  RecordWrapperType  None  OutputDataConfig  S3OutputPath  s3  output  format  bucket  prefix  HyperParameters  target  Sepal  Length  degree  StoppingCondition  MaxRuntimeInSeconds  60  60  time  sm  boto3  client  sagemaker  sm  create  training  job  training  params  status  sm  describe  training  job  TrainingJobName  job  TrainingJobStatus  print  status  sm  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  status  sm  describe  training  job  TrainingJobName  job  TrainingJobStatus  print  Training  job  ended  with  status  status  if  status  Failed  message  sm  describe  training  job  TrainingJobName  job  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  hosting  container  Image  dkr  ecr  amazonaws  com  rmars  latest  format  account  region  ModelDataUrl  sm  describe  training  job  TrainingJobName  job  ModelArtifacts  S3ModelArtifacts  create  model  response  sm  cre,amazon
ate  model  ModelName  job  ExecutionRoleArn  role  PrimaryContainer  hosting  container  print  create  model  response  ModelArn  endpoint  config  DEMO  byo  config  time  strftime  time  gmtime  print  endpoint  config  create  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  endpoint  config  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  job  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  endpoint  DEMO  endpoint  time  strftime  time  gmtime  print  endpoint  create  endpoint  response  sm  create  endpoint  EndpointName  endpoint  EndpointConfigName  endpoint  config  print  create  endpoint  response  EndpointArn  resp  sm  describe  endpoint  EndpointName  endpoint  status  resp  EndpointStatus  print  Status  status  try  sm  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  finally  resp  sm  describe  endpoint  EndpointName  endpoint  status  resp  ,amazon
EndpointStatus  print  Arn  resp  EndpointArn  print  Status  status  if  status  InService  raise  Exception  Endpoint  creation  did  not  succeed  iris  pd  read  csv  iris  csv  runtime  boto3  Session  client  runtime  sagemaker  payload  iris  drop  Sepal  Length  axis  to  csv  index  False  response  runtime  invoke  endpoint  EndpointName  endpoint  ContentType  text  csv  Body  payload  result  json  loads  response  Body  read  decode  result  plt  scatter  iris  Sepal  Length  np  fromstring  result  sep  plt  show  sm  delete  endpoint  EndpointName  endpoint  ,amazon
install  packages  rugarch  if  require  ccgarch  install  packages  ccgarch  library  ccgarch  nobs  1000  cut  1000  003  005  001  diag  15  diag  75  uncR  matrix  12  12  dcc  para  01  98  dcc  data  dcc  sim  nobs  uncR  dcc  para  model  diagonal  dcc  data  zdcc  results  dcc  estimation  inia  iniA  iniB  ini  dcc  dcc  para  dvar  dcc  data  eps  model  diagonal  Parameter  estimates  and  their  robust  standard  errors  dcc  results  out  ,microsoft
pip  install  eikon  Run  in  cmd  window  Or  in  Visual  Studio  PTVSimport  eikon  as  ekek  set  app  id  D163218EE154B9D1851F8C9  ek  get  news  headlines  TD  TO  date  from  2017  10  19T09  00  00  date  to  2017  10  20T18  00  00  df  ek  get  timeseries  MSFT  start  date  2016  01  01  end  date  2016  01  10  dfdata  grid1  err  ek  get  data  IBM  GOOG  MSFT  TR  PriceClose  TR  Volume  TR  PriceLow  data  grid2  err  ek  get  data  IBM  TR  Employees  TR  GrossProfit  params  Scale  Curn  EUR  sort  dir  asc  fields  ek  TR  Field  tr  revenue  ek  TR  Field  tr  open  None  asc  ek  TR  Field  TR  GrossProfit  Scale  Curn  EUR  asc  data  grid3  err  ek  get  data  IBM  MSFT  fields  data  grid3df  err  ek  get  data  GOOG  MSFT  FB  TR  Revenue  TR  GrossProfit  dfdf  err  ek  get  data  GOOG  MSFT  FB  AMZN  TWTR  TR  Revenue  date  TR  Revenue  TR  GrossProfit  Scale  SDate  EDate  FRQ  FY  Curn  EUR  dfdf  err  ek  get  data  IBM  TR  RevenueActValue  params  Period  FY0  Scale  Curn  USD ,microsoft
 TR  RevenueMeanEstimate  params  Period  FY1  Scale  Curn  USD  TR  RevenueMeanEstimate  params  Period  FY2  Scale  Curn  USD  dfdf1  err  ek  get  data  IBM  TR  RevenueActValue  params  Period  FY0  Scale  Curn  USD  TR  RevenueMeanEstimate  params  Period  FY1  Scale  Curn  USD  TR  RevenueMeanEstimate  params  Period  FY2  Scale  Curn  USD  raw  output  False  debug  True  df1df2  err  ek  get  data  IBM  TR  RevenueHigh  params  Period  FY1  Scale  Curn  USD  TR  RevenueHigh  params  Period  FY2  Scale  Curn  USD  TR  RevenueLow  params  Period  FY1  Scale  Curn  USD  TR  RevenueLow  params  Period  FY2  Scale  Curn  USD  debug  True  df2df3  err  ek  get  data  IBM  TR  RevenueLow  params  Period  FY1  Scale  Curn  USD  TR  RevenueLow  params  Period  FY2  Scale  Curn  USD  debug  True  df3df4  err  ek  get  data  IBM  TR  RevenueMeanEstimate  params  Period  FY1  RollPeriods  False  Scale  Curn  USD  TR  RevenueMeanEstimate  params  Period  FY2  RollPeriods  False  Scale  Curn  USD  debug  True  df4c,microsoft
onstituents  data  err  ek  get  data  instruments  ALLCOCO  fields  OFFCL  CODE  constituents  dataek  get  data  USGDPF  ECI  GN  TXT16  ek  get  timeseries  aCNFRTRRAW  interval  monthly  ,microsoft
time  import  boto3  from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  bucket  name  customize  to  your  bucket  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  image  classification  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  image  classification  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  image  classification  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  image  classification  latest  training  image  containers  boto3  Session  region  name  import  os  import  urllib  request  def  download  url  filename  url  split  if  not  os  path  exists  filename  urllib  request  urlretrieve  url  filename  Caltech  256  image  files  download  http  www  vision  caltech  edu  Image  Datasets  Caltech256  256  ObjectCategories  tar  tar  xf  256  ObjectCategories  tar  Tool  for  creating  lst  file  download  https  raw  githubusercontent  com  apache  incubator  mxnet  master  tool,amazon
s  im2rec  py  bash  mkdir  caltech  256  train  60  for  in  256  ObjectCategories  do  basename  mkdir  caltech  256  train  60  for  in  ls  jpg  shuf  head  60  do  mv  caltech  256  train  60  done  done  python  im2rec  py  list  recursive  caltech  256  60  train  caltech  256  train  60  python  im2rec  py  list  recursive  caltech  256  60  val  256  ObjectCategories  head  caltech  256  60  train  lst  example  lst  open  example  lst  lst  content  read  print  lst  content  Four  channels  train  validation  train  lst  and  validation  lst  s3train  s3  train  format  bucket  s3validation  s3  validation  format  bucket  s3train  lst  s3  train  lst  format  bucket  s3validation  lst  s3  validation  lst  format  bucket  upload  the  image  files  to  train  and  validation  channels  aws  s3  cp  caltech  256  train  60  s3train  recursive  quiet  aws  s3  cp  256  ObjectCategories  s3validation  recursive  quiet  upload  the  lst  files  to  train  lst  and  validation  lst  channels  aws  s3  ,amazon
cp  caltech  256  60  train  lst  s3train  lst  quiet  aws  s3  cp  caltech  256  60  val  lst  s3validation  lst  quiet  bash  python  im2rec  py  resize  256  quality  90  num  thread  16  caltech  256  60  val  256  ObjectCategories  python  im2rec  py  resize  256  quality  90  num  thread  16  caltech  256  60  train  caltech  256  train  60  The  algorithm  supports  multiple  network  depth  number  of  layers  They  are  18  34  50  101  152  and  200  For  this  training  we  will  use  18  layers  num  layers  18  we  need  to  specify  the  input  image  shape  for  the  training  data  image  shape  224  224  we  also  need  to  specify  the  number  of  training  samples  in  the  training  set  num  training  samples  15240  specify  the  number  of  output  classes  num  classes  257  batch  size  for  training  mini  batch  size  128  number  of  epochs  epochs  learning  rate  learning  rate  01  report  top  accuracy  top  resize  image  before  training  resize  256  period  to  store  mode,amazon
l  parameters  in  number  of  epochs  in  this  case  we  will  save  parameters  from  epoch  and  checkpoint  frequency  Since  we  are  using  transfer  learning  we  set  use  pretrained  model  to  so  that  weights  can  be  initialized  with  pre  trained  weights  use  pretrained  model  time  import  time  import  boto3  from  time  import  gmtime  strftime  s3  boto3  client  s3  create  unique  job  name  job  name  prefix  sagemaker  imageclassification  notebook  timestamp  time  strftime  time  gmtime  job  name  job  name  prefix  timestamp  training  params  specify  the  training  docker  image  AlgorithmSpecification  TrainingImage  training  image  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  s3  output  format  bucket  job  name  prefix  ResourceConfig  InstanceCount  InstanceType  ml  p2  xlarge  VolumeSizeInGB  50  TrainingJobName  job  name  HyperParameters  image  shape  image  shape  num  layers  str  num  layers  num  training  samples  str  num  training ,amazon
 samples  num  classes  str  num  classes  mini  batch  size  str  mini  batch  size  epochs  str  epochs  learning  rate  str  learning  rate  top  str  top  resize  str  resize  checkpoint  frequency  str  checkpoint  frequency  use  pretrained  model  str  use  pretrained  model  StoppingCondition  MaxRuntimeInSeconds  360000  Training  data  should  be  inside  subdirectory  called  train  Validation  data  should  be  inside  subdirectory  called  validation  The  algorithm  currently  only  supports  fullyreplicated  model  where  data  is  copied  onto  each  machine  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  image  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  image  CompressionType  None  ChannelName  trai,amazon
n  lst  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  lst  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  image  CompressionType  None  ChannelName  validation  lst  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  lst  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  image  CompressionType  None  print  Training  job  name  format  job  name  print  nInput  Data  Location  format  training  params  InputDataConfig  DataSource  S3DataSource  create  the  Amazon  SageMaker  training  job  sagemaker  boto3  client  service  name  sagemaker  sagemaker  create  training  job  training  params  confirm  that  the  training  job  has  started  status  sagemaker  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  Training  job  current  status  format  status  try  wait  for  the  job  to  finish  and  report  the  ending  status  sagemaker  get  waiter  training  job  completed,amazon
  or  stopped  wait  TrainingJobName  job  name  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  except  print  Training  failed  to  start  if  exception  is  raised  that  means  it  has  failed  message  sagemaker  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  print  training  info  time  import  boto3  from  time  import  gmtime  strftime  sage  boto3  Session  client  service  name  sagemaker  timestamp  time  strftime  time  gmtime  model  name  image  classification  model  timestamp  print  model  name  info  sage  describe  training  job  TrainingJobName  job  name  model  data  info  ModelArtifacts  S3ModelArtifac,amazon
ts  print  model  data  hosting  image  containers  boto3  Session  region  name  primary  container  Image  hosting  image  ModelDataUrl  model  data  create  model  response  sage  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  timestamp  time  strftime  time  gmtime  endpoint  config  name  job  name  prefix  epc  timestamp  endpoint  config  response  sage  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  p2  xlarge  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  configuration  name  format  endpoint  config  name  print  Endpoint  configuration  arn  format  endpoint  config  response  EndpointConfigArn  time  import  time  timestamp  time  strftime  time  gmtime  endpoint  name  job  name  prefix  ep  timestamp  print  Endpoint  name  format  endpoint  name  endpoint  param,amazon
s  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  endpoint  response  sagemaker  create  endpoint  endpoint  params  print  EndpointArn  format  endpoint  response  EndpointArn  get  the  status  of  the  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  response  EndpointStatus  print  EndpointStatus  format  status  try  sagemaker  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  finally  resp  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Create  endpoint  ended  with  status  status  if  status  InService  message  sagemaker  describe  endpoint  EndpointName  endpoint  name  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Endpoint  creation  did  not  succeed  import  boto3  runtime  boto3  Session  client  service  name  runtime  sagemaker  wget  tmp  test  jpg  http  www  vision ,amazon
 caltech  edu  Image  Datasets  Caltech256  images  008  bathtub  008  0007  jpg  file  name  tmp  test  jpg  test  image  from  IPython  display  import  Image  Image  file  name  import  json  import  numpy  as  np  with  open  file  name  rb  as  payload  read  payload  bytearray  payload  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  image  Body  payload  result  response  Body  read  result  will  be  in  json  format  and  convert  it  to  ndarray  result  json  loads  result  the  result  will  output  the  probabilities  for  all  classes  find  the  class  with  maximum  probability  and  print  the  class  index  index  np  argmax  result  object  categories  ak47  american  flag  backpack  baseball  bat  baseball  glove  basketball  hoop  bat  bathtub  bear  beer  mug  billiards  binoculars  birdbath  blimp  bonsai  101  boom  box  bowling  ball  bowling  pin  boxing  glove  brain  101  breadmaker  buddha  101  bulldozer  butterfly  cactus  cake  calcu,amazon
lator  camel  cannon  canoe  car  tire  cartman  cd  centipede  cereal  box  chandelier  101  chess  board  chimp  chopsticks  cockroach  coffee  mug  coffin  coin  comet  computer  keyboard  computer  monitor  computer  mouse  conch  cormorant  covered  wagon  cowboy  hat  crab  101  desk  globe  diamond  ring  dice  dog  dolphin  101  doorknob  drinking  straw  duck  dumb  bell  eiffel  tower  electric  guitar  101  elephant  101  elk  ewer  101  eyeglasses  fern  fighter  jet  fire  extinguisher  fire  hydrant  fire  truck  fireworks  flashlight  floppy  disk  football  helmet  french  horn  fried  egg  frisbee  frog  frying  pan  galaxy  gas  pump  giraffe  goat  golden  gate  bridge  goldfish  golf  ball  goose  gorilla  grand  piano  101  grapes  grasshopper  guitar  pick  hamburger  hammock  harmonica  harp  harpsichord  hawksbill  101  head  phones  helicopter  101  hibiscus  homer  simpson  horse  horseshoe  crab  hot  air  balloon  hot  dog  hot  tub  hourglass  house  fly  human  skeleton  hummingb,amazon
ird  ibis  101  ice  cream  cone  iguana  ipod  iris  jesus  christ  joy  stick  kangaroo  101  kayak  ketch  101  killer  whale  knife  ladder  laptop  101  lathe  leopards  101  license  plate  lightbulb  light  house  lightning  llama  101  mailbox  mandolin  mars  mattress  megaphone  menorah  101  microscope  microwave  minaret  minotaur  motorbikes  101  mountain  bike  mushroom  mussels  necktie  octopus  ostrich  owl  palm  pilot  palm  tree  paperclip  paper  shredder  pci  card  penguin  people  pez  dispenser  photocopier  picnic  table  playing  card  porcupine  pram  praying  mantis  pyramid  raccoon  radio  telescope  rainbow  refrigerator  revolver  101  rifle  rotary  phone  roulette  wheel  saddle  saturn  school  bus  scorpion  101  screwdriver  segway  self  propelled  lawn  mower  sextant  sheet  music  skateboard  skunk  skyscraper  smokestack  snail  snake  sneaker  snowmobile  soccer  ball  socks  soda  can  spaghetti  speed  boat  spider  spoon  stained  glass  starfish  101  steering ,amazon
 wheel  stirrups  sunflower  101  superman  sushi  swan  swiss  army  knife  sword  syringe  tambourine  teapot  teddy  bear  teepee  telephone  box  tennis  ball  tennis  court  tennis  racket  theodolite  toaster  tomato  tombstone  top  hat  touring  bike  tower  pisa  traffic  light  treadmill  triceratops  tricycle  trilobite  101  tripod  shirt  tuning  fork  tweezer  umbrella  101  unicorn  vcr  video  projector  washing  machine  watch  101  waterfall  watermelon  welding  mask  wheelbarrow  windmill  wine  bottle  xylophone  yarmulke  yo  yo  zebra  airplanes  101  car  side  101  faces  easy  101  greyhound  tennis  shoes  toad  clutter  print  Result  label  object  categories  index  probability  str  result  index  sage  delete  endpoint  EndpointName  endpoint  name  ,amazon
Imports  the  Google  Cloud  client  library  from  google  cloud  import  language  from  google  cloud  language  import  enums  from  google  cloud  language  import  typesdef  sentiment  analysis  file  doc  open  file  output  doc  readlines  whole  doc  join  map  str  output  client  language  LanguageServiceClient  document  types  Document  content  whole  doc  type  enums  Document  Type  PLAIN  TEXT  entities  client  analyze  entities  document  entities  return  entitiesfile  the  little  prince  txt  sentiment  analysis  file  ,amazon
pip  install  ABCEimport  abcesimulation  abce  Simulation  name  ipythonsimulation  processes  class  Agent  abce  Agent  def  init  self  parameters  agent  parameter  self  family  name  agent  parameter  self  world  size  parameters  world  size  def  say  self  print  hello  am  my  id  and  my  group  is  it  is  the  round  self  family  name  self  id  self  group  self  round  agents  simulation  build  agents  Agent  agent  parameters  world  size  10  agent  parameters  fred  astaire  altair  deurich  for  in  range  simulation  advance  round  agents  say  class  Kid  abce  Agent  def  init  self  parameters  agent  parameters  self  num  kids  parameters  num  kids  if  self  id  self  create  ball  def  whether  have  the  ball  self  if  self  ball  print  end  flush  True  else  print  end  flush  True  def  give  the  ball  to  the  next  kid  self  next  kid  self  id  self  num  kids  the  id  of  the  next  kid  if  am  the  last  the  first  kid  if  self  ball  self  give  kid  next  ki,microsoft
d  good  ball  quantity  num  kids  5simulation  abce  Simulation  name  ipythonsimulation  processes  kids  simulation  build  agents  Kid  kid  number  num  kids  parameters  num  kids  num  kids  for  in  range  simulation  advance  round  kids  whether  have  the  ball  print  kids  give  the  ball  to  the  next  kid  from  random  import  randrangeclass  NewKid  abce  Agent  def  init  self  parameters  agent  parameters  self  num  dealers  parameters  num  dealers  self  create  money  100  don  we  all  wish  you  this  function  in  real  live  def  buy  drugs  self  drug  dealer  id  randrange  self  num  dealers  self  buy  drug  dealer  drug  dealer  id  good  drugs  quantity  price  10  def  print  possessions  self  print  self  group  str  dict  self  possessions  class  DrugDealer  abce  Agent  def  init  self  parameters  agent  parameters  self  create  drugs  def  sell  to  customers  self  for  offer  in  self  get  offers  drugs  if  offer  price  10  and  self  drugs  self  accept  offe,microsoft
r  def  print  possessions  self  print  self  group  str  dict  self  possessions  simulation  parameters  num  dealers  num  customers  rounds  simulation  abce  Simulation  name  school  yard  processes  drug  dealers  simulation  build  agents  DrugDealer  drug  dealer  number  simulation  parameters  num  dealers  customers  simulation  build  agents  NewKid  customer  number  simulation  parameters  num  customers  parameters  simulation  parameters  kids  drug  dealers  customersfor  in  range  simulation  parameters  rounds  simulation  advance  round  print  Customer  offers  10  dollar  customers  buy  drugs  kids  print  possessions  print  Drug  Dealer  accepts  or  rejects  the  offer  drug  dealers  sell  to  customers  kids  print  possessions  print  from  math  import  sin  class  DataDealer  abce  Agent  def  init  self  simulation  parameters  agent  parameters  self  count  self  create  money  def  counting  self  self  count  self  curve  sin  self  count  100  self  create  money  self ,microsoft
 curve  self  id  simulation  abce  Simulation  name  gatherdata  processes  datadealers  simulation  build  agents  DataDealer  datadealer  number  10  for  in  range  100  simulation  advance  round  datadealers  counting  datadealers  agg  log  variables  count  datadealers  panel  log  possessions  money  variables  curve  simulation  finalize  print  simulation  path  import  os  os  listdir  simulation  path  import  pandas  as  pd  matplotlib  inlinedf  pd  read  csv  simulation  path  panel  datadealer  csv  df  head  20  df  pivot  index  round  columns  id  values  money  plot  simulation  abce  Simulation  class  Communicator  abce  Agent  def  report  my  name  self  return  self  name  def  do  as  told  self  time  text  print  it  is  clock  and  have  to  say  time  text  communicators  simulation  build  agents  Communicator  communicator  number  for  time  in  range  simulation  advance  round  time  communicators  report  my  name  communicators  do  as  told  time  time  text  Right  says,microsoft
  fred  ,microsoft
import  pickle  df  usage  pickle  load  open  df  usage  rb  df  usage  ,microsoft
Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  import  sagemaker  bucket  sagemaker  Session  default  bucket  prefix  sagemaker  DEMO  xgboost  churn  role  get  execution  role  import  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  import  io  import  os  import  sys  import  time  import  json  from  IPython  display  import  display  from  time  import  strftime  gmtime  import  sagemaker  from  sagemaker  predictor  import  csv  serializer  wget  http  dataminingconsultant  com  DKD2e  data  sets  zip  unzip  DKD2e  data  sets  zipchurn  pd  read  csv  Data  sets  churn  txt  pd  set  option  display  max  columns  500  churn  Frequency  tables  for  each  categorical  feature  for  column  in  churn  select  dtypes  include  object  columns  display  pd  crosstab  index  churn  column  columns  observations  normalize  columns  Histograms  for  each  numeric  features  display  churn  describe  matplotlib  inline  hist  churn  h,amazon
ist  bins  30  sharey  True  figsize  10  10  churn  churn  drop  Phone  axis  churn  Area  Code  churn  Area  Code  astype  object  for  column  in  churn  select  dtypes  include  object  columns  if  column  Churn  display  pd  crosstab  index  churn  column  columns  churn  Churn  normalize  columns  for  column  in  churn  select  dtypes  exclude  object  columns  print  column  hist  churn  column  Churn  hist  by  Churn  bins  30  plt  show  display  churn  corr  pd  plotting  scatter  matrix  churn  figsize  12  12  plt  show  churn  churn  drop  Day  Charge  Eve  Charge  Night  Charge  Intl  Charge  axis  model  data  pd  get  dummies  churn  model  data  pd  concat  model  data  Churn  True  model  data  drop  Churn  False  Churn  True  axis  axis  train  data  validation  data  test  data  np  split  model  data  sample  frac  random  state  1729  int  len  model  data  int  len  model  data  train  data  to  csv  train  csv  header  False  index  False  validation  data  to  csv  validation  csv  ,amazon
header  False  index  False  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  train  csv  upload  file  train  csv  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  validation  csv  upload  file  validation  csv  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  training  image  get  image  uri  boto3  Session  region  name  xgboost  s3  input  train  s3  train  format  bucket  prefix  s3  input  validation  s3  validation  format  bucket  prefix  print  s3  input  train  from  time  import  gmtime  strftime  sleep  tuning  job  name  xgboost  tuningjob  strftime  gmtime  print  tuning  job  name  tuning  job  config  ParameterRanges  CategoricalParameterRanges  ContinuousParameterRanges  MaxValue  MinValue  Name  eta  MaxValue  10  MinValue  Name  min  child  weight  MaxValue  MinValue  Name  alpha  IntegerParameterRanges  MaxValue  10  MinValue  Name  max  depth  ResourceLimits  MaxNumberOfTrainingJobs  40  MaxPa,amazon
rallelTrainingJobs  Strategy  Bayesian  HyperParameterTuningJobObjective  MetricName  validation  auc  Type  Maximize  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  training  job  definition  AlgorithmSpecification  TrainingImage  training  image  TrainingInputMode  File  InputDataConfig  ChannelName  train  CompressionType  None  ContentType  csv  DataSource  S3DataSource  S3DataDistributionType  FullyReplicated  S3DataType  S3Prefix  S3Uri  s3  input  train  ChannelName  validation  CompressionType  None  ContentType  csv  DataSource  S3DataSource  S3DataDistributionType  FullyReplicated  S3DataType  S3Prefix  S3Uri  s3  input  validation  OutputDataConfig  S3OutputPath  s3  output  format  bucket  prefix  ResourceConfig  InstanceCount  InstanceType  ml  m4  xlarge  VolumeSizeInGB  10  RoleArn  role  StaticHyperParameters  eval  metric  auc  num  round  100  objective  binary  logistic  rate  drop  tweedie  variance  power  StoppingCondition  MaxRuntimeInSeconds  43200  region  boto3 ,amazon
 Session  region  name  client  boto3  Session  client  sagemaker  client  create  hyper  parameter  tuning  job  HyperParameterTuningJobName  tuning  job  name  HyperParameterTuningJobConfig  tuning  job  config  TrainingJobDefinition  training  job  definition  client  describe  hyper  parameter  tuning  job  HyperParameterTuningJobName  tuning  job  name  HyperParameterTuningJobStatus  xgb  predictor  sagemaker  predictor  RealTimePredictor  XGboost  churn  best2  endpoint  xgb  predictor  content  type  text  csv  xgb  predictor  serializer  csv  serializer  xgb  predictor  deserializer  Nonedef  predict  data  rows  500  split  array  np  array  split  data  int  data  shape  float  rows  predictions  for  array  in  split  array  predictions  join  predictions  xgb  predictor  predict  array  decode  utf  return  np  fromstring  predictions  sep  predictions  predict  test  data  as  matrix  pd  crosstab  index  test  data  iloc  columns  np  round  predictions  rownames  actual  colnames  predictions  ,amazon
plt  hist  predictions  plt  show  pd  crosstab  index  test  data  iloc  columns  np  where  predictions  target  clients  np  where  predictions  print  Following  clients  have  been  determined  as  high  risk  of  churn  and  should  be  engaged  target  clients  from  sklearn  metrics  import  accuracy  score  f1  score  precision  score  recall  score  classification  report  confusion  matrix  roc  curve  from  sklearn  metrics  import  precision  recall  curve  average  precision  score  precision  recall  fscore  supportdef  f1  get  label  bin  if  cont  else  for  cont  in  change  the  prob  to  class  output  return  f1  f1  score  bin  def  print  evaluation  metric  true  pred  precision  recall  fscore  support  precision  recall  fscore  support  true  pred  print  Precision  format  precision  print  Recall  format  recall  print  score  format  fscore  print  Support  format  support  return  def  plot  roc  curve  true  prob  fpr  tpr  threshold  roc  curve  true  prob  fig  plt  gcf  fig,amazon
  set  size  inches  10  plt  title  Receiver  Operating  Characteristic  ROC  plt  plot  fpr  tpr  plt  plot  plt  xlim  plt  ylim  plt  ylabel  True  Positive  Rate  plt  xlabel  False  Positive  Rate  plt  show  returnprint  evaluation  metric  test  data  iloc  np  where  predictions  plot  roc  curve  test  data  iloc  predictions  sagemaker  Session  delete  endpoint  xgb  predictor  endpoint  ,amazon
conda  install  scipy  matplotlib  inline  import  os  re  import  boto3  import  matplotlib  pyplot  as  plt  import  numpy  as  np  np  set  printoptions  precision  suppress  True  some  helpful  utility  functions  are  defined  in  the  Python  module  generate  example  data  located  in  the  same  directory  as  this  notebook  from  generate  example  data  import  generate  griffiths  data  plot  lda  match  estimated  topics  accessing  the  SageMaker  Python  SDK  import  sagemaker  from  sagemaker  amazon  common  import  numpy  to  record  serializer  from  sagemaker  predictor  import  csv  serializer  json  deserializerfrom  sagemaker  import  get  execution  role  role  get  execution  role  bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  lda  introduction  print  Training  input  output  will  be  stored  in  format  bucket  prefix  print  nIAM  Role  format  role  print  Generating  example  data  num  documents  6000  num  topics  known  alpha  known  beta  documents  topic ,amazon
 mixtures  generate  griffiths  data  num  documents  num  documents  num  topics  num  topics  vocabulary  size  len  documents  separate  the  generated  data  into  training  and  tests  subsets  num  documents  training  int  num  documents  num  documents  test  num  documents  num  documents  training  documents  training  documents  num  documents  training  documents  test  documents  num  documents  training  topic  mixtures  training  topic  mixtures  num  documents  training  topic  mixtures  test  topic  mixtures  num  documents  training  print  documents  training  shape  format  documents  training  shape  print  documents  test  shape  format  documents  test  shape  print  First  training  document  format  documents  print  nVocabulary  size  format  vocabulary  size  print  Known  topic  mixture  of  first  document  format  topic  mixtures  training  print  nNumber  of  topics  format  num  topics  print  Sum  of  elements  format  topic  mixtures  training  sum  matplotlib  inline  fig  p,amazon
lot  lda  documents  training  nrows  ncols  cmap  gray  with  colorbar  True  fig  suptitle  Example  Document  Word  Counts  fig  set  dpi  160  convert  documents  training  to  Protobuf  RecordIO  format  recordio  protobuf  serializer  numpy  to  record  serializer  fbuffer  recordio  protobuf  serializer  documents  training  upload  to  S3  in  bucket  prefix  train  fname  lda  data  s3  object  os  path  join  prefix  train  fname  boto3  Session  resource  s3  Bucket  bucket  Object  s3  object  upload  fileobj  fbuffer  s3  train  data  s3  format  bucket  s3  object  print  Uploaded  data  to  S3  format  s3  train  data  select  the  algorithm  container  based  on  this  notebook  current  location  containers  us  west  266724342769  dkr  ecr  us  west  amazonaws  com  lda  latest  us  east  766337827248  dkr  ecr  us  east  amazonaws  com  lda  latest  us  east  999911452149  dkr  ecr  us  east  amazonaws  com  lda  latest  eu  west  999678624901  dkr  ecr  eu  west  amazonaws  com  lda  lates,amazon
t  region  name  boto3  Session  region  name  container  containers  region  name  print  Using  SageMaker  LDA  container  format  container  region  name  session  sagemaker  Session  specify  general  training  job  information  lda  sagemaker  estimator  Estimator  container  role  output  path  s3  output  format  bucket  prefix  train  instance  count  train  instance  type  ml  c4  2xlarge  sagemaker  session  session  set  algorithm  specific  hyperparameters  lda  set  hyperparameters  num  topics  num  topics  feature  dim  vocabulary  size  mini  batch  size  num  documents  training  alpha0  run  the  training  job  on  input  data  stored  in  S3  lda  fit  train  s3  train  data  print  Training  job  name  format  lda  latest  training  job  job  name  lda  inference  lda  deploy  initial  instance  count  instance  type  ml  m4  xlarge  LDA  inference  may  work  better  at  scale  on  ml  c4  instances  print  Endpoint  name  format  lda  inference  endpoint  lda  inference  content  type  t,amazon
ext  csv  lda  inference  serializer  csv  serializer  lda  inference  deserializer  json  deserializerresults  lda  inference  predict  documents  test  12  print  results  computed  topic  mixtures  np  array  prediction  topic  mixture  for  prediction  in  results  predictions  print  computed  topic  mixtures  print  topic  mixtures  test  known  test  topic  mixture  print  computed  topic  mixtures  computed  topic  mixture  topics  permuted  sagemaker  Session  delete  endpoint  lda  inference  endpoint  ,amazon
imports  import  boto3  import  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  import  seaborn  as  sns  import  io  import  sagemaker  amazon  common  as  smac  import  os  import  sagemaker  from  sagemaker  predictor  import  csv  serializer  json  deserializerbucket  cyrusmv  sagemaker  demos  replace  this  with  your  own  bucket  prefix  visa  kaggle  original  csv  replace  this  with  your  own  file  inside  the  bucket  protocol  s3  datafile  data  original  csv  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  print  role  sagemaker  session  sagemaker  Session  Downloading  the  file  to  local  folder  client  boto3  client  s3  with  open  datafile  wb  as  client  download  fileobj  bucket  prefix  loading  data  into  pandas  for  inspection  df  pd  read  csv  datafile  df  head  df  Class  count  df  df  Class  Class  count  Converting  Data  Into  Numpy  raw  data  df  as  matrix  print  r,amazon
aw  data  shape  Shuffling  the  data  np  random  seed  123  np  random  shuffle  raw  data  label  raw  data  data  raw  data  print  label  shape  data  shape  format  label  shape  data  shape  There  are  very  few  fraudulant  transactions  in  the  dataset  so  am  putting  their  indexes  in  an  array  to  plot  and  ensure  they  are  evenly  distributed  so  when  split  the  dataset  into  test  and  training  don  end  up  with  dispropostionate  distribution  for  in  range  len  label  if  label  append  sns  distplot  kde  True  rug  True  hist  False  plt  show  ls  data  Splitting  data  into  validation  and  training  and  breaking  dataset  into  data  and  label  60  40  training  to  validation  train  size  int  data  shape  train  data  data  train  size  val  data  data  train  size  train  label  label  train  size  val  label  label  train  size  Saving  arrays  for  later  use  np  save  data  processed  train  train  data  npy  train  data  allow  pickle  True  np  save  data  pr,amazon
ocessed  train  train  label  npy  train  label  allow  pickle  True  np  save  data  processed  test  val  data  npy  val  data  allow  pickle  True  np  save  data  processed  test  val  label  npy  val  label  allow  pickle  True  print  training  data  shape  training  label  shape  nValidation  data  shape  validation  label  shape  format  train  data  shape  train  label  shape  val  data  shape  val  label  shape  sagemaker  session  upload  data  path  data  processed  bucket  cyrusmv  sagemaker  demos  key  prefix  visa  kaggle  data  train  data  np  load  data  train  data  npy  train  label  np  load  data  train  label  npy  val  data  np  load  data  val  data  npy  val  label  np  load  data  val  label  npy  print  training  data  shape  training  label  shape  nValidation  data  shape  validation  label  shape  format  train  data  shape  train  label  shape  val  data  shape  val  label  shape  train  set  train  data  train  label  test  set  val  data  val  label  vectors  np  array  toli,amazon
st  for  in  train  set  astype  float32  labels  np  array  tolist  for  in  train  set  astype  float32  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  vectors  labels  buf  seek  key  recordio  pb  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  buf  s3  train  data  s3  train  format  bucket  prefix  key  print  uploaded  training  data  location  format  s3  train  data  output  location  s3  output  format  bucket  prefix  print  training  artifacts  will  be  uploaded  to  format  output  location  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  linear  learner  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  linear  learner  latest  sess  sagemaker  Session  linear  sagemaker  estimator  Estimator  containers  boto3  Sess,amazon
ion  region  name  role  S3  role  so  the  notebook  can  read  the  data  and  upload  the  model  train  instance  count  number  of  instances  for  training  train  instance  type  ml  p2  xlarge  type  of  training  instance  output  path  output  location  s3  location  for  uploading  trained  mdoel  sagemaker  session  sess  linear  set  hyperparameters  feature  dim  30  dataset  has  30  columns  features  predictor  type  binary  classifier  we  predict  binary  value  it  could  have  been  regressor  mini  batch  size  200  linear  fit  train  s3  train  data  linear  predictor  linear  deploy  initial  instance  count  Initial  number  of  instances  Autoscaling  can  increase  the  number  of  instances  instance  type  ml  m4  xlarge  instance  typelinear  predictor  content  type  text  csv  linear  predictor  serializer  csv  serializer  linear  predictor  deserializer  json  deserializer  since  score  is  very  rare  we  want  to  make  sure  we  can  correctly  predict  fradulant  transa,amazon
ction  First  we  print  lost  of  all  labels  where  score  then  then  run  prediction  for  in  range  len  train  label  if  train  label  append  print  print  linear  predictor  predict  train  set  1294  1299  non  zero  np  count  nonzero  test  set  zero  len  test  set  non  zero  print  validation  set  includes  non  zero  and  items  woth  value  zero  format  non  zero  zero  predictions  for  array  in  np  array  split  test  set  100  result  linear  predictor  predict  array  predictions  predicted  label  for  in  result  predictions  predictions  np  array  predictions  import  pandas  as  pd  pd  crosstab  test  set  predictions  rownames  actuals  colnames  predictions  ,amazon
import  numpy  as  np  from  random  import  randomdef  find  nearest  array  value  idx  np  abs  array  value  argmin  idk  guess  there  something  wrong  with  this  line  Fork  python  return  array  idx  IrregularTimebase  15  20  25  30  IrregularData  98  99  90  97  100  95  90  RegularTimebase  10  15  20  25  30  RegularData  TimeDifference  for  in  RegularTimebase  print  25  print  Regular  Time  str  NearestIndex  find  nearest  IrregularTimebase  find  nearest  timepoint  on  irregular  basis  NearestIrregTime  IrregularTimebase  NearestIndex  note  nearest  irregular  time  RegularData  append  IrregularData  NearestIndex  record  nearest  value  from  irregular  data  TimeDifference  append  NearestIrregTime  print  Nearest  irregular  time  str  IrregularTimebase  NearestIndex  print  dt  str  NearestIrregTime  print  Closest  data  value  IrregularData  NearestIndex  print  RegularTimebase  print  RegularData  ,amazon
file  mx  lenet  sagemaker  py  import  logging  from  os  import  path  as  op  import  os  import  mxnet  as  mx  import  numpy  as  np  data  path  batch  size  50  num  cpus  num  gpus  def  prep  data  data  path  Convert  numpy  array  to  mx  Nd  array  Parameters  path  the  directory  that  save  data  npz  data  np  load  find  file  data  path  data  npz  train  data  train  train  data  train  only  take  the  second  column  of  train  test  data  test  test  data  test  train  train  astype  float32  test  test  astype  float32  img  mean  np  mean  train  axis  img  std  np  std  train  axis  train  img  mean  train  img  std  test  img  mean  test  img  std  img  rows  256  img  cols  256  train  train  reshape  train  shape  img  rows  img  cols  reshape  it  to  448  instead  of  448  test  test  reshape  test  shape  img  rows  img  cols  train  train  reshape  train  shape  test  test  reshape  test  shape  print  train  shape  test  shape  train  shape  test  shape  train  iter  mx  io  N,amazon
DArrayIter  train  train  batch  size  shuffle  True  val  iter  mx  io  NDArrayIter  test  test  batch  size  return  train  iter  val  iter  def  find  file  root  path  file  name  Searching  for  data  npz  at  its  root  director  and  return  full  path  for  the  file  Parameters  root  path  the  root  directory  for  data  npz  file  name  refers  to  data  npz  for  root  dirs  files  in  os  walk  root  path  if  file  name  in  files  return  os  path  join  root  file  name  def  mx  lenet  Building  three  layer  LeNet  sytle  Convolutional  Neural  Net  using  MXNet  data  mx  sym  var  data  data  dp  mx  symbol  Dropout  data  20  of  the  input  that  gets  dropped  out  during  training  time  first  conv  layer  conv1  mx  sym  Convolution  data  data  dp  kernel  num  filter  20  tanh1  mx  sym  Activation  data  conv1  act  type  tanh  pool1  mx  sym  Pooling  data  tanh1  pool  type  max  kernel  stride  second  conv  layer  conv2  mx  sym  Convolution  data  pool1  kernel  num  filter ,amazon
 50  tanh2  mx  sym  Activation  data  conv2  act  type  tanh  pool2  mx  sym  Pooling  data  tanh2  pool  type  max  kernel  stride  third  conv  layer  conv3  mx  sym  Convolution  data  pool1  kernel  num  filter  50  tanh3  mx  sym  Activation  data  conv2  act  type  tanh  pool3  mx  sym  Pooling  data  tanh2  pool  type  max  kernel  stride  first  fullc  layer  flatten  mx  sym  flatten  data  pool3  fc1  mx  symbol  FullyConnected  data  flatten  num  hidden  500  tanh4  mx  sym  Activation  data  fc1  act  type  tanh  second  fullc  fc2  mx  sym  FullyConnected  data  tanh4  num  hidden  softmax  loss  return  mx  sym  SoftmaxOutput  data  fc2  name  softmax  def  train  num  cpus  num  gpus  kwargs  Train  the  image  classification  neural  net  Parameters  num  cpus  If  train  the  model  on  an  aws  GPS  machine  num  cpus  and  num  gpus  vice  versa  num  gpus  apply  to  the  same  rule  above  train  iter  val  iter  prep  data  data  path  lenet  mx  lenet  lenet  model  mx  mod  Module  s,amazon
ymbol  lenet  context  get  train  context  num  cpus  num  gpus  logging  getLogger  setLevel  logging  DEBUG  lenet  model  fit  train  iter  eval  data  val  iter  optimizer  sgd  optimizer  params  learning  rate  eval  metric  acc  batch  end  callback  mx  callback  Speedometer  batch  size  16  num  epoch  100  return  lenet  model  def  get  train  context  num  cpus  num  gpus  Define  the  model  training  instance  Parameters  num  cpus  If  train  the  model  on  an  aws  GPS  machine  num  cpus  and  num  gpus  vice  versa  num  gpus  apply  to  the  same  rule  above  if  num  gpus  return  mx  gpu  return  mx  cpu  def  get  train  context  num  cpus  num  gpus  if  num  gpus  print  It  instance  format  num  gpus  return  mx  gpu  print  It  instance  format  num  cpus  return  mx  cpu  time  from  sagemaker  mxnet  import  MXNet  from  sagemaker  import  get  execution  role  mxnet  estimator  MXNet  mx  lenet  sagemaker  py  role  get  execution  role  train  instance  type  ml  p2  xlarge ,amazon
 train  instance  count  mxnet  estimator  fit  s3  data  754487812300  give  your  s3  bucket  URL  here  time  predictor  mxnet  estimator  deploy  initial  instance  count  instance  type  ml  p2  xlarge  import  numpy  as  np  import  boto3  import  botocore  import  matplotlib  pyplot  as  plt  from  PIL  import  Image  matplotlib  inline  def  pred  img  bucket  img  s3  img  nn  image  mean  image  std  s3  boto3  resource  s3  try  s3  Bucket  bucket  download  file  img  s3  img  nn  print  download  under  current  directory  format  img  s3  except  botocore  exceptions  ClientError  as  if  response  Error  Code  404  print  The  object  does  not  exist  else  raise  img  Image  open  img  nn  img  np  np  array  img  astype  float32  RBG  chanels  are  between  255  so  rescale  it  to  be  between  img  np  255  img  np  image  mean  img  np  image  std  reshape  the  image  to  256  256  to  pass  to  the  predictor  of  MXNet  Estimator  img  np  img  np  reshape  img  np  shape  img  np  sha,amazon
pe  img  np  shape  pred  predictor  predict  img  np  return  pred  replace  with  your  bucket  name  bucket  data  754487812300  full  path  to  the  test  image  img  s3  tiles  65935  63265  17  jpg  the  new  given  name  for  the  image  you  wan  na  save  in  current  work  directory  img  nn  Image  jpg  Image  mean  and  image  std  are  obtained  from  data  npz  by  running  img  mean  np  mean  train  axis  img  std  np  std  train  axis  image  mean  np  array  14663778  1624524  21313858  astype  float32  image  std  np  array  86569798  77569193  79192579  astype  float32  pred  pred  img  bucket  img  s3  img  nn  image  mean  image  std  image  plt  imread  Image  jpg  plt  imshow  image  print  The  overall  prediction  is  format  pred  for  item  in  pred  print  item  item  print  item  item  if  item  item  print  It  background  tile  else  print  The  tile  contains  building  ,amazon
load  ext  autoreload  autoreload  2import  utils  import  tensorflow  as  tf  from  tensorflow  contrib  learn  python  learn  datasets  import  mnist  import  numpy  as  np  import  os  os  environ  TF  CPP  MIN  LOG  LEVEL  prep  data  data  sets  mnist  read  data  sets  data  dtype  tf  uint8  reshape  False  validation  size  5000  utils  convert  to  data  sets  train  train  data  utils  convert  to  data  sets  validation  validation  data  utils  convert  to  data  sets  test  test  data  import  mnist  keras  as  mysagemaker  model  fn  mysagemaker  model  fn  read  and  decode  mysagemaker  read  and  decode  INPUT  TENSOR  NAME  mysagemaker  INPUT  TENSOR  NAME  batch  size  10  params  TRIAN  DIR  data  def  train  input  fn  training  filename  train  tfrecords  training  dir  TRIAN  DIR  test  file  os  path  join  training  dir  training  filename  filename  queue  tf  train  string  input  producer  test  file  image  label  read  and  decode  filename  queue  images  labels  tf  train  batc,amazon
h  image  label  batch  size  batch  size  capacity  1000  batch  size  return  INPUT  TENSOR  NAME  images  labels  def  eval  input  fn  training  filename  validation  tfrecords  training  dir  TRIAN  DIR  test  file  os  path  join  training  dir  training  filename  filename  queue  tf  train  string  input  producer  test  file  image  label  read  and  decode  filename  queue  images  labels  tf  train  batch  image  label  batch  size  batch  size  capacity  1000  batch  size  return  INPUT  TENSOR  NAME  images  labels  model  dir  modelbottleneck  sess  tf  Session  init  tf  global  variables  initializer  sess  run  init  estimator  tf  estimator  Estimator  model  fn  model  dir  model  dir  params  params  out  bottleneck  estimator  train  train  input  fn  steps  10000  tensorboard  logdir  modelbottlneckimport  mnist  keras  fcnn  as  mysagemaker  model  fn  mysagemaker  model  fn  read  and  decode  mysagemaker  read  and  decode  INPUT  TENSOR  NAME  mysagemaker  INPUT  TENSOR  NAME  batch ,amazon
 size  10  params  TRIAN  DIR  data  model  fn  mysagemaker  model  fn  read  and  decode  mysagemaker  read  and  decode  INPUT  TENSOR  NAME  mysagemaker  INPUT  TENSOR  NAME  model  dir  modelfcnn  sess  tf  Session  init  tf  global  variables  initializer  sess  run  init  estimator  tf  estimator  Estimator  model  fn  model  dir  model  dir  params  params  train  spec  tf  estimator  TrainSpec  input  fn  train  input  fn  max  steps  10000  eval  spec  tf  estimator  EvalSpec  input  fn  eval  input  fn  out  fcnn  tf  estimator  train  and  evaluate  estimator  train  spec  eval  spec  tensorboard  logdir  modelfcnn  ,amazon
from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  sagemaker  examples  download  MNIST  Dataset  time  import  pickle  gzip  numpy  urllib  request  json  load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  exploring  the  training  dataset  typically  you  would  clean  up  and  transform  dataset  before  trainin  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  calling  31st  image  from  the  training  dataset  show  digit  train  set  30  This  is  format  test  set  30  transform  the  training  dataset  to  upload  it  into  S3  transform  from  numpy  array  to ,amazon
 RecordIO  protobuf  format  fit  method  for  model  training  performs  necesary  transformation  training  model  using  high  level  python  lib  from  aws  sagemaker  choose  the  training  algrorithm  CreateTrainingjob  from  sagemaker  import  KMeans  data  location  s3  kmeans  highlevel  example  data  format  bucket  output  location  s3  kmeans  example  output  format  bucket  print  training  data  will  be  uploaded  to  format  data  location  print  training  artifacts  will  be  uploaded  to  format  output  location  kmeans  KMeans  role  role  train  instance  count  train  instance  type  ml  c4  8xlarge  output  path  output  location  10  data  location  data  location  time  start  training  model  with  fit  kmeans  fit  kmeans  record  set  train  set  time  sagemaker  amazon  kmeans  KMeans  provides  deploy  meathod  for  deploying  model  kmeans  predictor  kmeans  deploy  initial  instance  count  instance  type  ml  m4  xlarge  get  inference  for  the  30th  image  in  valid  se,amazon
t  result  kmeans  predictor  predict  train  set  30  31  print  result  time  get  inference  for  first  100  images  result  kmeans  predictor  predict  valid  set  100  clusters  label  closest  cluster  float32  tensor  values  for  in  result  visualize  the  results  for  cluster  in  range  10  print  nCluster  format  int  cluster  digits  img  for  img  in  zip  clusters  valid  set  if  int  cluster  height  len  digits  width  plt  rcParams  figure  figsize  width  height  subplots  plt  subplots  height  width  subplots  numpy  ndarray  flatten  subplots  for  subplot  image  in  zip  subplots  digits  show  digit  image  subplot  subplot  for  subplot  in  subplots  len  digits  subplot  axis  off  plt  show  ,amazon
imports  import  numpy  as  np  import  mxnet  as  mx  from  mxnet  import  nd  gluon  autograd  import  pandas  as  pd  from  sagemaker  mxnet  import  MXNet  import  sagemaker  from  sagemaker  import  get  execution  rolesagemaker  session  sagemaker  Session  role  get  execution  role  role  inputs  sagemaker  session  upload  data  path  data  processed  bucket  cyrusmv  sagemaker  demos  key  prefix  visa  kaggle  data  inputsdata  files  train  data  data  train  data  npy  train  label  data  train  label  npy  val  data  data  val  data  npy  val  label  data  val  label  npy  hyper  parameters  batch  size  100  learning  rate  01  num  epochs  optimizer  sgd  momentum  num  dims  30  train  ctx  mx  cpu  num  examples  10000  num  input  30  num  outputs  type  data  files  aws  s3  ls  cyrusmv  sagemaker  demos  visa  kaggle  data  cat  linear  model  gluon  pym  MXNet  linear  model  gluon  py  role  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  s3  cyrusmv  ,amazon
sagemaker  demos  visa  kaggle  output  model  hyperparameters  hyper  parameters  import  os  os  listdir  opt  fit  inputs  predictor  deploy  initial  instance  count  instance  type  ml  m4  xlarge  train  data  np  load  data  processed  train  train  data  npy  train  label  np  load  data  processed  train  train  label  npy  test  data  np  load  data  processed  test  val  data  npy  test  label  np  load  data  processed  test  val  label  npy  print  train  data  shape  train  label  shape  test  data  shape  test  label  shape  predictor  predict  train  data  since  score  is  very  rare  we  want  to  make  sure  we  can  correctly  predict  fradulant  transaction  First  we  print  lost  of  all  labels  where  score  then  then  run  prediction  for  in  range  len  train  label  if  train  label  append  print  hyper  parameters  batch  size  100  learning  rate  01  num  epochs  optimizer  sgd  momentum  num  dims  30  import  linear  model  gluon  as  pyfilepyfile  train  training  data  pr,amazon
ocessed  hyperparameters  hyper  parameters  hosts  local  num  gpus  output  path  cyrusmv  sagemaker  demos  visa  kaggle  output  model  ,amazon
time  import  os  import  boto3  import  re  import  copy  import  time  from  time  import  gmtime  strftime  from  sagemaker  import  get  execution  role  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  gzip  open  mnist  pkl  gz  rb  train  set  valid  set  test  set  pickle  load  encoding  latin1  close  time  import  struct  import  io  import  boto3  import  pickle  import  gzip  import  sagemaker  sess  sagemaker  Session  role  get  execution  role  bucket  sess  default  bucket  prefix  notebook  xgboost  mnist  def  to  libsvm  name  labels  values  with  open  name  encoding  utf  as  content  join  format  label  join  format  el  for  el  in  enumerate  vec  for  label  vec  in  zip  labels  values  write  content  with  gzip  open  mnist  pkl  gz  rb  as  pickle  Unpickler  encoding  latin1  train  set  valid  set  test  set  load  to  libsvm  data  train  train  set  tra,amazon
in  set  to  libsvm  data  valid  valid  set  valid  set  to  libsvm  data  test  test  set  test  set  train  input  sess  upload  data  path  data  train  key  prefix  prefix  valid  input  sess  upload  data  path  data  valid  key  prefix  prefix  test  input  sess  upload  data  path  data  test  key  prefix  prefix  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  xgboost  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  xgboost  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  xgboost  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  xgboost  latest  ap  northeast  501404015308  dkr  ecr  ap  northeast  amazonaws  com  xgboost  latest  training  job  name  DEMO  xgboost  classification  strftime  gmtime  xgb  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  role  train  instance  count  train  instance  type  ml  m4  4xlarge  output  path  s3  output  format  bucket  prefix  sagemaker  session  ses,amazon
s  xgb  set  hyperparameters  eta  objective  multi  softmax  num  class  10  num  round  25  xgb  fit  train  train  input  validation  valid  input  job  name  training  job  name  xgboost  try  import  xgboost  as  xg  print  XGboost  has  already  been  installed  format  xg  version  except  source  activate  python3  conda  install  conda  forge  xgboost  from  IPython  display  import  clear  output  clear  output  import  xgboost  as  xg  print  XGboost  is  installed  format  xg  version  S3  import  boto3  import  botocore  s3  boto3  resource  s3  model  location  prefix  output  training  job  name  output  model  tar  gz  print  The  model  is  saved  at  format  s3  bucket  model  location  try  s3  Bucket  bucket  download  file  model  location  model  tar  gz  tar  zxvf  model  tar  gz  print  Downloading  and  extracting  the  model  are  done  except  botocore  exceptions  ClientError  as  if  response  Error  Code  404  print  The  object  does  not  exist  import  sys  sys  exit  else  ra,amazon
ise  import  pickle  as  pkl  xgb  model  pkl  load  open  xgboost  model  rb  print  Loading  model  is  done  dtest  xg  DMatrix  data  test  prediction  xgb  model  predict  dtest  import  numpy  as  np  from  matplotlib  import  pyplot  as  plt  from  matplotlib  import  cm  select  row  np  random  choice  range  len  test  set  feature  test  set  select  row  pred  result  int  prediction  select  row  plt  imshow  feature  reshape  28  28  cmap  cm  gray  print  Prediction  format  pred  result  xgb  predictor  xgb  deploy  initial  instance  count  instance  type  ml  m4  xlarge  content  from  sagemaker  predictor  import  csv  serializer  xgb  predictor  content  type  text  csv  xgb  predictor  serializer  csv  serializer  xgb  predictor  deserializer  None  predict  select  row  np  random  choice  range  len  test  set  feature  test  set  select  row  pred  result  xgb  predictor  predict  test  set  select  row  pred  result  int  float  pred  result  decode  utf  plt  imshow  feature  reshape,amazon
  28  28  cmap  cm  gray  print  Prediction  format  pred  result  xgb  predictor  delete  endpoint  ,amazon
import  sagemaker  from  sagemaker  import  get  execution  role  import  boto3  import  json  sess  sagemaker  Session  role  get  execution  role  print  role  This  is  the  role  that  SageMaker  would  use  to  leverage  AWS  resources  S3  CloudWatch  on  your  behalf  bucket  sess  default  bucket  Replace  with  your  own  bucket  name  if  needed  print  bucket  prefix  SagemakerBlazingText  Replace  with  the  prefix  under  which  you  want  to  store  the  data  if  neededs3  train  data  s3  format  sagemaker  test  ninja  BlazingTextInput  s3  output  location  s3  BlazingText  Model  Output  word2vec  pitchfork  2018  09  21  format  sagemaker  test  ninja  region  name  boto3  Session  region  namecontainer  sagemaker  amazon  amazon  estimator  get  image  uri  region  name  blazingtext  latest  print  Using  SageMaker  BlazingText  container  format  container  region  name  bt  model  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  c4  2x,amazon
large  train  volume  size  train  max  run  360000  input  mode  File  output  path  s3  output  location  sagemaker  session  sess  bt  model  set  hyperparameters  mode  supervised  epochs  min  count  learning  rate  05  vector  dim  100  window  size  word  ngrams  ngram  feature  specificationtrain  data  sagemaker  session  s3  input  s3  train  data  distribution  FullyReplicated  content  type  text  plain  s3  data  type  S3Prefix  data  channels  train  train  data  bt  model  fit  inputs  data  channels  logs  True  bt  endpoint  bt  model  deploy  initial  instance  count  instance  type  ml  m4  xlarge  words  addictive  recorded  payload  instances  words  response  bt  endpoint  predict  json  dumps  payload  vecs  json  loads  response  print  vecs  s3  boto3  resource  s3  key  bt  model  model  data  bt  model  model  data  find  s3  Bucket  sagemaker  test  ninja  download  file  key  model  tar  gz  tar  xvzf  model  tar  gz  cat  eval  jsonimport  numpy  as  np  from  sklearn  preprocess,amazon
ing  import  normalize  Read  the  1600  most  frequent  word  vectors  The  vectors  in  the  file  are  in  descending  order  of  frequency  num  points  1600  first  line  True  index  to  word  with  open  vectors  txt  as  for  line  num  line  in  enumerate  if  first  line  dim  int  line  strip  split  word  vecs  np  zeros  num  points  dim  dtype  float  first  line  False  continue  line  line  strip  word  line  split  vec  word  vecs  line  num  for  index  vec  val  in  enumerate  line  split  vec  index  float  vec  val  index  to  word  append  word  if  line  num  num  points  break  word  vecs  normalize  word  vecs  copy  False  return  norm  False  from  sklearn  manifold  import  TSNE  tsne  TSNE  perplexity  40  components  init  pca  iter  10000  two  embeddings  tsne  fit  transform  word  vecs  num  points  labels  index  to  word  num  points  import  pandas  as  pd  embeddings  frame  pd  DataFrame  two  embeddings  labels  frame  pd  DataFrame  labels  columns  Word  labels  frame,amazon
  embedding  pd  Series  embeddings  frame  index  labels  frame  index  labels  frame  embedding  pd  Series  embeddings  frame  index  labels  frame  index  labels  frame  head  import  csv  labels  frame  to  csv  ModelResults  labels  frame  2grams  0921  1600words  csv  index  False  from  matplotlib  import  pylab  matplotlib  inline  def  plot  embeddings  labels  pylab  figure  figsize  20  20  for  label  in  enumerate  labels  embeddings  pylab  scatter  pylab  annotate  label  xy  xytext  textcoords  offset  points  ha  right  va  bottom  pylab  show  plot  two  embeddings  labels  sess  delete  endpoint  bt  endpoint  endpoint  ,amazon
S3  bucket  and  prefix  bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  seq2seq  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  from  time  import  gmtime  strftime  import  time  import  numpy  as  np  import  os  import  json  For  plotting  attention  matrix  later  on  import  matplotlib  matplotlib  inline  import  matplotlib  pyplot  as  plt  bash  wget  http  data  statmt  org  wmt17  translation  task  preprocessed  de  en  corpus  tc  de  gz  wget  http  data  statmt  org  wmt17  translation  task  preprocessed  de  en  corpus  tc  en  gz  wait  gunzip  corpus  tc  de  gz  gunzip  corpus  tc  en  gz  wait  mkdir  validation  curl  http  data  statmt  org  wmt17  translation  task  preprocessed  de  en  dev  tgz  tar  xvzf  validation  head  10000  corpus  tc  en  corpus  tc  en  small  head  10000  corpus  tc  de  corpus  tc  de  small  bash  python3  create  vocab  proto  py  time  bash  python3  create  vocab  proto  py  train  ,amazon
source  corpus  tc  en  small  train  target  corpus  tc  de  small  val  source  validation  newstest2014  tc  en  val  target  validation  newstest2014  tc  dedef  upload  to  s3  bucket  prefix  channel  file  s3  boto3  resource  s3  data  open  file  rb  key  prefix  channel  file  s3  Bucket  bucket  put  object  Key  key  Body  data  upload  to  s3  bucket  prefix  train  train  rec  upload  to  s3  bucket  prefix  validation  val  rec  upload  to  s3  bucket  prefix  vocab  vocab  src  json  upload  to  s3  bucket  prefix  vocab  vocab  trg  json  region  name  boto3  Session  region  namefrom  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  region  name  seq2seq  print  Using  SageMaker  Seq2Seq  container  format  container  region  name  job  name  DEMO  seq2seq  en  de  strftime  gmtime  print  Training  job  job  name  create  training  params  AlgorithmSpecification  TrainingImage  container  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3O,amazon
utputPath  s3  format  bucket  prefix  ResourceConfig  Seq2Seq  does  not  support  multiple  machines  Currently  it  only  supports  single  machine  multiple  GPUs  InstanceCount  InstanceType  ml  p2  xlarge  We  suggest  one  of  ml  p2  16xlarge  ml  p2  8xlarge  ml  p2  xlarge  VolumeSizeInGB  50  TrainingJobName  job  name  HyperParameters  Please  refer  to  the  documentation  for  complete  list  of  parameters  max  seq  len  source  60  max  seq  len  target  60  optimized  metric  bleu  batch  size  64  Please  use  larger  batch  size  256  or  512  if  using  ml  p2  8xlarge  or  ml  p2  16xlarge  checkpoint  frequency  num  batches  1000  rnn  num  hidden  512  num  layers  encoder  num  layers  decoder  num  embed  source  512  num  embed  target  512  checkpoint  threshold  max  num  batches  2100  Training  will  stop  after  2100  iterations  batches  This  is  just  for  demo  purposes  Remove  the  above  parameter  if  you  want  better  model  StoppingCondition  MaxRuntimeInSeconds  4,amazon
8  3600  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  S3DataDistributionType  FullyReplicated  ChannelName  vocab  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  vocab  format  bucket  prefix  S3DataDistributionType  FullyReplicated  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  prefix  S3DataDistributionType  FullyReplicated  sagemaker  client  boto3  Session  client  service  name  sagemaker  sagemaker  client  create  training  job  create  training  params  status  sagemaker  client  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  status  status  sagemaker  client  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  status  if  the  job  failed  determine  why  if  status  Failed  message  sagemaker  client  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training ,amazon
 failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  use  pretrained  model  False  use  pretrained  model  True  model  name  DEMO  pretrained  en  de  model  curl  https  s3  us  west  amazonaws  com  seq2seq  data  model  tar  gz  model  tar  gz  curl  https  s3  us  west  amazonaws  com  seq2seq  data  vocab  src  json  vocab  src  json  curl  https  s3  us  west  amazonaws  com  seq2seq  data  vocab  trg  json  vocab  trg  json  upload  to  s3  bucket  prefix  pretrained  model  model  tar  gz  model  data  s3  pretrained  model  model  tar  gz  format  bucket  prefix  time  sage  boto3  client  sagemaker  if  not  use  pretrained  model  info  sage  describe  training  job  TrainingJobName  job  name  model  name  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  name  print  model  data  primary  container  Image  container  ModelDataUrl  model  data  create  model  response  sage  create  model  ModelName  model  name  ExecutionRol,amazon
eArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  endpoint  config  name  DEMO  Seq2SeqEndpointConfig  strftime  gmtime  print  endpoint  config  name  create  endpoint  config  response  sage  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  import  time  endpoint  name  DEMO  Seq2SeqEndpoint  strftime  gmtime  print  endpoint  name  create  endpoint  response  sage  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  print  create  endpoint  response  EndpointArn  resp  sage  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  wait  until  the  status  has  changed  sage  get  waiter  endpoint  in  service  wai,amazon
t  EndpointName  endpoint  name  print  the  status  of  the  endpoint  endpoint  response  sage  describe  endpoint  EndpointName  endpoint  name  status  endpoint  response  EndpointStatus  print  Endpoint  creation  ended  with  EndpointStatus  format  status  if  status  InService  raise  Exception  Endpoint  creation  failed  runtime  boto3  client  service  name  runtime  sagemaker  sentences  you  are  so  good  can  you  drive  car  want  to  watch  movie  payload  instances  for  sent  in  sentences  payload  instances  append  data  sent  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  json  Body  json  dumps  payload  response  response  Body  read  decode  utf  response  json  loads  response  print  response  sentence  can  you  drive  car  payload  instances  data  sentence  configuration  attention  matrix  true  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  json  Body  json  dumps  payload  response  re,amazon
sponse  Body  read  decode  utf  response  json  loads  response  predictions  source  sentence  target  response  target  attention  matrix  np  array  response  matrix  print  Source  nTarget  source  target  Define  function  for  plotting  the  attentioan  matrix  def  plot  matrix  attention  matrix  target  source  source  tokens  source  split  target  tokens  target  split  assert  attention  matrix  shape  len  target  tokens  plt  imshow  attention  matrix  transpose  interpolation  nearest  cmap  Greys  plt  xlabel  target  plt  ylabel  source  plt  gca  set  xticks  for  in  range  len  target  tokens  plt  gca  set  yticks  for  in  range  len  source  tokens  plt  gca  set  xticklabels  target  tokens  plt  gca  set  yticklabels  source  tokens  plt  tight  layout  plot  matrix  attention  matrix  target  source  import  io  import  tempfile  from  record  pb2  import  Record  from  create  vocab  proto  import  vocab  from  json  reverse  vocab  write  recordio  list  to  record  bytes  read  n,amazon
ext  source  vocab  from  json  vocab  src  json  target  vocab  from  json  vocab  trg  json  source  rev  reverse  vocab  source  target  rev  reverse  vocab  target  sentences  this  is  so  cool  am  having  dinner  am  sitting  in  an  aeroplane  come  let  us  go  for  long  drive  Convert  strings  to  integers  using  source  vocab  mapping  Out  of  vocabulary  strings  are  mapped  to  the  mapping  for  unk  sentences  source  get  token  for  token  in  sentence  split  for  sentence  in  sentences  io  BytesIO  for  sentence  in  sentences  record  list  to  record  bytes  sentence  write  recordio  record  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  recordio  protobuf  Body  getvalue  response  response  Body  read  def  parse  proto  response  received  bytes  output  file  tempfile  NamedTemporaryFile  output  file  write  received  bytes  output  file  flush  target  sentences  with  open  output  file  name  rb  as  datum  next  record  True  ,amazon
while  next  record  next  record  read  next  datum  if  next  record  rec  Record  rec  ParseFromString  next  record  target  list  rec  features  target  int32  tensor  values  target  sentences  append  target  else  break  return  target  sentencestargets  parse  proto  response  response  resp  join  target  rev  get  token  unk  for  token  in  sentence  for  sentence  in  targets  print  resp  sage  delete  endpoint  EndpointName  endpoint  name  ,amazon
import  os  import  boto3  import  time  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  Now  let  define  the  S3  bucket  we  ll  used  for  the  remainder  of  this  example  bucket  your  s3  bucket  name  here  enter  your  s3  bucket  where  you  will  copy  data  and  model  artificats  prefix  sagemaker  DEMO  xgboost  place  to  upload  training  files  within  the  bucketimport  numpy  as  np  For  matrix  operations  and  numerical  processing  import  pandas  as  pd  For  munging  tabular  data  import  sklearn  as  sk  For  access  to  variety  of  machine  learning  models  import  matplotlib  pyplot  as  plt  For  charts  and  visualizations  from  IPython  display  import  Image  For  displaying  images  in  the  notebook  from  IPython  display  import  display  For  displaying  outputs  in  the  notebook  from  sklearn  datasets  import  dump  svmlight  file  For  outputting  data  to  libsvm  format  for  xgboost  from  time  import  gmtime  strftime  ,amazon
For  labeling  SageMaker  models  endpoints  etc  import  sys  For  writing  outputs  to  notebook  import  math  For  ceiling  function  import  json  For  parsing  hosting  output  import  io  For  working  with  stream  data  import  sagemaker  amazon  common  as  smac  For  protobuf  data  format  read  the  data  data  pd  read  csv  https  archive  ics  uci  edu  ml  machine  learning  databases  adult  adult  data  header  None  read  test  data  data  test  pd  read  csv  https  archive  ics  uci  edu  ml  machine  learning  databases  adult  adult  test  header  None  skiprows  set  column  names  data  columns  age  workclass  fnlwgt  education  education  num  marital  status  occupation  relationship  race  sex  capital  gain  capital  loss  hours  per  week  native  country  IncomeGroup  data  test  columns  age  workclass  fnlwgt  education  education  num  marital  status  occupation  relationship  race  sex  capital  gain  capital  loss  hours  per  week  native  country  IncomeGroup  set  dis,amazon
play  options  pd  set  option  display  max  columns  100  Make  sure  we  can  see  all  of  the  columns  pd  set  option  display  max  rows  Keep  the  output  on  one  page  disply  data  display  data  display  data  test  display  positive  and  negative  counts  display  data  iloc  14  value  counts  display  data  test  iloc  14  value  counts  Combine  the  two  datasets  to  convert  the  categorical  values  to  binary  indicators  data  combined  pd  concat  data  data  test  convert  the  categorical  variables  to  binary  indicators  data  combined  bin  pd  get  dummies  data  combined  prefix  workclass  education  marital  status  occupation  relationship  race  sex  native  country  IncomeGroup  drop  first  True  combine  the  income  50k  indicators  Income  50k  data  combined  bin  iloc  101  data  combined  bin  iloc  102  make  the  income  indicator  as  first  column  data  combined  bin  pd  concat  Income  50k  data  combined  bin  iloc  100  axis  Post  conversion  to  binary ,amazon
 split  the  data  sets  separately  data  bin  data  combined  bin  iloc  data  shape  data  test  bin  data  combined  bin  iloc  data  shape  display  the  data  sets  post  conversion  to  binary  indicators  display  data  bin  display  data  test  bin  count  number  of  positives  and  negatives  display  data  bin  iloc  value  counts  display  data  test  bin  iloc  value  counts  Split  the  data  randomly  as  80  for  training  and  remaining  20  and  save  them  locally  train  list  np  random  rand  len  data  bin  data  train  data  bin  train  list  data  val  data  bin  train  list  data  train  to  csv  formatted  train  csv  sep  header  False  index  False  save  training  data  data  val  to  csv  formatted  val  csv  sep  header  False  index  False  save  validation  data  data  test  bin  to  csv  formatted  test  csv  sep  header  False  index  False  save  test  data  train  file  formatted  train  csv  val  file  formatted  val  csv  boto3  Session  resource  s3  Bucket  bucket  O,amazon
bject  os  path  join  prefix  train  train  file  upload  file  train  file  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  val  val  file  upload  file  val  file  xgboost  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  xgboost  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  xgboost  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  xgboost  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  xgboost  latest  import  boto3  from  time  import  gmtime  strftime  job  name  DEMO  xgboost  single  censusincome  strftime  gmtime  print  Training  job  job  name  create  training  params  AlgorithmSpecification  TrainingImage  xgboost  containers  boto3  Session  region  name  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  s3  single  xgboost  format  bucket  prefix  ResourceConfig  InstanceCount  InstanceType  ml  m4  4xlarge  VolumeSizeInGB  20  TrainingJobName  job  name  Hyp,amazon
erParameters  max  depth  eta  gamma  min  child  weight  silent  objective  binary  logistic  eval  metric  auc  num  round  20  StoppingCondition  MaxRuntimeInSeconds  60  60  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  S3DataDistributionType  FullyReplicated  ContentType  csv  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  val  format  bucket  prefix  S3DataDistributionType  FullyReplicated  ContentType  csv  CompressionType  None  time  region  boto3  Session  region  name  sm  boto3  client  sagemaker  sm  create  training  job  create  training  params  status  sm  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  status  sm  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  if  status  Failed  message  sm  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  ,amazon
with  the  following  error  format  message  raise  Exception  Training  job  failed  model  name  job  name  mdl  xgboost  hosting  container  Image  xgboost  containers  boto3  Session  region  name  ModelDataUrl  sm  describe  training  job  TrainingJobName  job  name  ModelArtifacts  S3ModelArtifacts  Environment  this  is  create  model  response  sm  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  xgboost  hosting  container  print  create  model  response  ModelArn  print  sm  describe  training  job  TrainingJobName  job  name  ModelArtifacts  S3ModelArtifacts  from  time  import  gmtime  strftime  endpoint  config  name  DEMO  XGBoostEndpointConfig  strftime  gmtime  print  endpoint  config  name  create  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  InitialVariantWeight  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  Conf,amazon
ig  Arn  create  endpoint  config  response  EndpointConfigArn  time  import  time  endpoint  name  DEMO  XGBoostEndpoint  strftime  gmtime  print  endpoint  name  create  endpoint  response  sm  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  print  create  endpoint  response  EndpointArn  resp  sm  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  while  status  Creating  time  sleep  60  resp  sm  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  print  Arn  resp  EndpointArn  print  Status  status  runtime  boto3  client  runtime  sagemaker  Simple  function  to  create  csv  from  our  numpy  array  def  np2csv  arr  csv  io  BytesIO  np  savetxt  csv  arr  delimiter  fmt  return  csv  getvalue  decode  rstrip  Function  to  generate  prediction  through  sample  data  def  do  predict  data  endpoint  name  content  type  payload  np2csv  data  response  r,amazon
untime  invoke  endpoint  EndpointName  endpoint  name  ContentType  content  type  Body  payload  result  response  Body  read  result  result  decode  utf  result  result  split  preds  float  num  for  num  in  result  return  preds  Function  to  iterate  through  larger  data  set  and  generate  batch  predictions  def  batch  predict  data  batch  size  endpoint  name  content  type  items  len  data  arrs  for  offset  in  range  items  batch  size  if  offset  batch  size  items  datav  data  iloc  offset  offset  batch  size  as  matrix  results  do  predict  datav  endpoint  name  content  type  arrs  extend  results  else  datav  data  iloc  offset  items  as  matrix  arrs  extend  do  predict  datav  endpoint  name  content  type  sys  stdout  write  return  arrs  read  the  saved  data  for  scoring  data  train  pd  read  csv  formatted  train  csv  sep  header  None  data  test  pd  read  csv  formatted  test  csv  sep  header  None  data  val  pd  read  csv  formatted  val  csv  sep  header  ,amazon
None  preds  train  xgb  batch  predict  data  train  iloc  1000  endpoint  name  text  csv  preds  val  xgb  batch  predict  data  val  iloc  1000  endpoint  name  text  csv  preds  test  xgb  batch  predict  data  test  iloc  1000  endpoint  name  text  csv  from  sklearn  metrics  import  roc  auc  score  train  labels  data  train  iloc  val  labels  data  val  iloc  test  labels  data  test  iloc  print  Training  AUC  roc  auc  score  train  labels  preds  train  xgb  9161  print  Validation  AUC  roc  auc  score  val  labels  preds  val  xgb  9065  print  Test  AUC  roc  auc  score  test  labels  preds  test  xgb  9112  prefix  sagemaker  DEMO  linear  subfolder  inside  the  data  bucket  to  be  used  for  Linear  Learner  data  train  pd  read  csv  formatted  train  csv  sep  header  None  data  test  pd  read  csv  formatted  test  csv  sep  header  None  data  val  pd  read  csv  formatted  val  csv  sep  header  None  train  data  train  iloc  as  matrix  train  data  train  iloc  as  matrix  va,amazon
l  data  val  iloc  as  matrix  val  data  val  iloc  as  matrix  test  data  test  iloc  as  matrix  test  data  test  iloc  as  matrix  train  file  linear  train  data  io  BytesIO  smac  write  numpy  to  dense  tensor  train  astype  float32  train  astype  float32  seek  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  train  file  upload  fileobj  validation  file  linear  validation  data  io  BytesIO  smac  write  numpy  to  dense  tensor  val  astype  float32  val  astype  float32  seek  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  train  file  upload  fileobj  linear  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  linear  learner  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  linear  learner  latest  linea,amazon
r  job  DEMO  linear  time  strftime  time  gmtime  print  Job  name  is  linear  job  linear  training  params  RoleArn  role  TrainingJobName  linear  job  AlgorithmSpecification  TrainingImage  linear  containers  boto3  Session  region  name  TrainingInputMode  File  ResourceConfig  InstanceCount  InstanceType  ml  c4  2xlarge  VolumeSizeInGB  10  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  S3DataDistributionType  ShardedByS3Key  CompressionType  None  RecordWrapperType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  prefix  S3DataDistributionType  FullyReplicated  CompressionType  None  RecordWrapperType  None  OutputDataConfig  S3OutputPath  s3  format  bucket  prefix  HyperParameters  feature  dim  100  mini  batch  size  100  predictor  type  binary  classifier  epochs  10  num  models  32  loss  logistic  StoppingCondition  MaxRuntimeInSeconds  60  ,amazon
60  print  linear  job  time  region  boto3  Session  region  name  sm  boto3  client  sagemaker  sm  create  training  job  linear  training  params  status  sm  describe  training  job  TrainingJobName  linear  job  TrainingJobStatus  print  status  sm  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  linear  job  if  status  Failed  message  sm  describe  training  job  TrainingJobName  linear  job  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  linear  hosting  container  Image  linear  containers  boto3  Session  region  name  ModelDataUrl  sm  describe  training  job  TrainingJobName  linear  job  ModelArtifacts  S3ModelArtifacts  174872318107  dkr  ecr  us  west  amazonaws  com  create  model  response  sm  create  model  ModelName  linear  job  ExecutionRoleArn  role  PrimaryContainer  linear  hosting  container  print  create  model  response  ModelArn  linear  endpoint  config  DEMO  linear  end,amazon
point  config  time  strftime  time  gmtime  print  linear  endpoint  config  create  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  linear  endpoint  config  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  linear  job  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  linear  endpoint  DEMO  linear  endpoint  time  strftime  time  gmtime  print  linear  endpoint  create  endpoint  response  sm  create  endpoint  EndpointName  linear  endpoint  EndpointConfigName  linear  endpoint  config  print  create  endpoint  response  EndpointArn  resp  sm  describe  endpoint  EndpointName  linear  endpoint  status  resp  EndpointStatus  print  Status  status  sm  get  waiter  endpoint  in  service  wait  EndpointName  linear  endpoint  resp  sm  describe  endpoint  EndpointName  linear  endpoint  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Status  status  if  status,amazon
  InService  raise  Exception  Endpoint  creation  did  not  succeed  def  np2csv  arr  csv  io  BytesIO  np  savetxt  csv  arr  delimiter  fmt  return  csv  getvalue  decode  rstrip  Function  to  generate  prediction  through  sample  data  def  do  predict  linear  data  endpoint  name  content  type  payload  np2csv  data  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  content  type  Body  payload  result  json  loads  response  Body  read  decode  preds  score  for  in  result  predictions  return  preds  Function  to  iterate  through  larger  data  set  and  generate  batch  predictions  def  batch  predict  linear  data  batch  size  endpoint  name  content  type  items  len  data  arrs  for  offset  in  range  items  batch  size  if  offset  batch  size  items  datav  data  iloc  offset  offset  batch  size  as  matrix  results  do  predict  linear  datav  endpoint  name  content  type  arrs  extend  results  else  datav  data  iloc  offset  items  as  matrix  arrs  e,amazon
xtend  do  predict  linear  datav  endpoint  name  content  type  sys  stdout  write  return  arrs  Predict  on  Training  Data  preds  train  lin  batch  predict  linear  data  train  iloc  100  linear  endpoint  text  csv  Predict  on  Validation  Data  preds  val  lin  batch  predict  linear  data  val  iloc  100  linear  endpoint  text  csv  Predict  on  Test  Data  preds  test  lin  batch  predict  linear  data  test  iloc  100  linear  endpoint  text  csv  print  Training  AUC  roc  auc  score  train  labels  preds  train  lin  9091  print  Validation  AUC  roc  auc  score  val  labels  preds  val  lin  8998  print  Test  AUC  roc  auc  score  test  labels  preds  test  lin  9033  ens  train  np  array  preds  train  xgb  np  array  preds  train  lin  ens  val  np  array  preds  val  xgb  np  array  preds  val  lin  ens  test  np  array  preds  test  xgb  np  array  preds  test  lin  Print  AUC  of  the  combined  model  print  Train  AUC  Xgboost  round  roc  auc  score  train  labels  preds  train  xg,amazon
b  print  Train  AUC  Linear  round  roc  auc  score  train  labels  preds  train  lin  print  Train  AUC  Ensemble  round  roc  auc  score  train  labels  ens  train  print  print  Validation  AUC  Xgboost  round  roc  auc  score  val  labels  preds  val  xgb  print  Validation  AUC  Linear  round  roc  auc  score  val  labels  preds  val  lin  print  Validation  AUC  Ensemble  round  roc  auc  score  val  labels  ens  val  print  print  Test  AUC  Xgboost  round  roc  auc  score  test  labels  preds  test  xgb  print  Test  AUC  Linear  round  roc  auc  score  test  labels  preds  test  lin  print  Test  AUC  Ensemble  round  roc  auc  score  test  labels  ens  test  final  pd  concat  data  test  iloc  pd  DataFrame  ens  test  axis  final  to  csv  Xgboost  linear  ensemble  prediction  csv  sep  header  False  index  False  sm  delete  endpoint  EndpointName  endpoint  name  sm  delete  endpoint  EndpointName  linear  endpoint  ,amazon
import  numpy  as  np  import  pandas  as  pd  from  sklearn  cluster  import  KMeans  import  seaborn  as  sns  import  matplotlib  pyplot  as  pltiris  pd  read  csv  iris  csv  iris  head  iris  info  iris  iloc  values  pd  Categorical  iris  Species  codeskmeans  KMeans  clusters  kmeans  fit  iris  cluster  kmeans  labels  print  kmeans  cluster  centers  sns  lmplot  data  iris  SepalLengthCm  SepalWidthCm  fit  reg  False  hue  cluster  plt  show  iris  cluster  kmeans  labels  facet  plot  sns  FacetGrid  iris  col  cluster  hue  cluster  facet  plot  map  plt  scatter  SepalLengthCm  SepalWidthCm  plt  show  def  elbow  analysis  range  clusters  import  numpy  as  np  from  sklearn  cluster  import  KMeans  import  matplotlib  pyplot  as  plt  from  scipy  spatial  distance  import  cdist  means  determine  distortions  for  in  range  clusters  kmeanModel  KMeans  clusters  fit  kmeanModel  fit  distortions  append  sum  np  min  cdist  kmeanModel  cluster  centers  euclidean  axis  shape  Plot  t,amazon
he  elbow  plt  plot  range  clusters  distortions  bx  color  darkblue  plt  xlabel  plt  ylabel  Distortion  plt  title  The  Elbow  Method  showing  the  optimal  plt  show  elbow  analysis  range  10  def  silhouette  analysis  range  clusters  from  sklearn  cluster  import  KMeans  from  sklearn  metrics  import  silhouette  samples  silhouette  score  import  matplotlib  pyplot  as  plt  import  matplotlib  cm  as  cm  import  numpy  as  np  for  clusters  in  range  clusters  Create  subplot  with  row  and  columns  fig  ax1  ax2  plt  subplots  fig  set  size  inches  12  The  1st  subplot  is  the  silhouette  plot  The  silhouette  coefficient  can  range  from  ax1  set  xlim  The  clusters  10  is  for  inserting  blank  space  between  silhouette  plots  of  individual  clusters  to  demarcate  them  clearly  ax1  set  ylim  len  clusters  10  Initialize  the  clusterer  with  clusters  value  and  random  generator  seed  of  10  for  reproducibility  clusterer  KMeans  clusters  clusters  ran,amazon
dom  state  10  cluster  labels  clusterer  fit  predict  The  silhouette  score  gives  the  average  value  for  all  the  samples  This  gives  perspective  into  the  density  and  separation  of  the  formed  clusters  silhouette  avg  silhouette  score  cluster  labels  print  For  clusters  clusters  The  average  silhouette  score  is  silhouette  avg  Compute  the  silhouette  scores  for  each  sample  sample  silhouette  values  silhouette  samples  cluster  labels  lower  10  for  in  range  clusters  Aggregate  the  silhouette  scores  for  samples  belonging  to  cluster  and  sort  them  ith  cluster  silhouette  values  sample  silhouette  values  cluster  labels  ith  cluster  silhouette  values  sort  size  cluster  ith  cluster  silhouette  values  shape  upper  lower  size  cluster  color  cm  spectral  float  clusters  ax1  fill  betweenx  np  arange  lower  upper  ith  cluster  silhouette  values  facecolor  color  edgecolor  color  alpha  Label  the  silhouette  plots  with  their  clus,amazon
ter  numbers  at  the  middle  ax1  text  05  lower  size  cluster  str  Compute  the  new  lower  for  next  plot  lower  upper  10  10  for  the  samples  ax1  set  title  The  silhouette  plot  for  the  various  clusters  ax1  set  xlabel  The  silhouette  coefficient  values  ax1  set  ylabel  Cluster  label  The  vertical  line  for  average  silhouette  score  of  all  the  values  ax1  axvline  silhouette  avg  color  red  linestyle  ax1  set  yticks  Clear  the  yaxis  labels  ticks  ax1  set  xticks  2nd  Plot  showing  the  actual  clusters  formed  colors  cm  spectral  cluster  labels  astype  float  clusters  ax2  scatter  marker  30  lw  alpha  colors  edgecolor  Labeling  the  clusters  centers  clusterer  cluster  centers  Draw  white  circles  at  cluster  centers  ax2  scatter  centers  centers  marker  white  alpha  200  edgecolor  for  in  enumerate  centers  ax2  scatter  marker  alpha  50  edgecolor  ax2  set  title  The  visualization  of  the  clustered  data  ax2  set  xlabel  Featur,amazon
e  space  for  the  1st  feature  ax2  set  ylabel  Feature  space  for  the  2nd  feature  plt  suptitle  Silhouette  analysis  for  KMeans  clustering  on  sample  data  with  clusters  clusters  fontsize  14  fontweight  bold  plt  show  silhouette  analysis  list  range  10  ,amazon
import  urllib  request  urllib  request  urlretrieve  https  archive  ics  uci  edu  ml  machine  learning  databases  00352  Online  20Retail  xlsx  online  retail  xlsx  from  IPython  core  display  import  display  import  pandas  as  pd  df  pd  read  excel  online  retail  xlsx  parse  dates  InvoiceDate  display  df  del  df  InvoiceNo  df  Description  df  CustomerID  top  five  code  df  StockCode  value  counts  head  index  top  five  record  df  df  StockCode  isin  top  five  code  top  five  record  set  index  top  five  record  InvoiceDate  drop  True  inplace  True  del  top  five  record  InvoiceDate  display  top  five  record  head  top  five  record  groupby  stock  qty  top  five  record  groupby  StockCode  Country  resample  sum  Quantity  price  top  five  record  groupby  StockCode  Country  resample  mean  UnitPrice  resampled  pd  concat  qty  price  axis  join  inner  display  resampled  head  import  numpy  as  np  resampled  UnitPrice  resampled  groupby  StockCode  Country  Un,amazon
itPrice  apply  lambda  fillna  mode  display  resampled  head  from  sklearn  preprocessing  import  StandardScaler  scaler  StandardScaler  scaled  scaler  fit  transform  resampled  loc  Quantity  UnitPrice  values  resampled  Quantity  scaled  resampled  UnitPrice  scaled  stockcode  arr  resampled  index  levels  country  arr  resampled  index  levels  json  data  for  stock  index  in  range  len  stockcode  arr  for  country  index  in  range  len  country  arr  one  data  try  record  resampled  loc  stockcode  arr  stock  index  country  arr  country  index  one  data  start  str  record  index  one  data  target  record  Quantity  values  tolist  one  data  cat  stock  index  country  index  one  data  dynamic  feat  record  UnitPrice  values  tolist  json  data  append  one  data  except  pass  import  json  with  open  training  data  json  wb  as  fp  for  in  json  data  fp  write  json  dumps  encode  utf  fp  write  encode  utf  import  sagemaker  from  sagemaker  import  get  execution  role ,amazon
 sagemaker  session  sagemaker  Session  role  get  execution  role  train  input  sagemaker  session  upload  data  path  training  data  json  key  prefix  deepar  retail  forecast  region  ap  northeast  image  name  sagemaker  amazon  amazon  estimator  get  image  uri  region  forecasting  deepar  latest  estimator  sagemaker  estimator  Estimator  sagemaker  session  sagemaker  session  image  name  image  name  role  role  train  instance  count  train  instance  type  ml  c4  2xlarge  base  job  name  deepar  retail  forecast  hyperparameters  time  freq  epochs  400  early  stopping  patience  40  mini  batch  size  64  learning  rate  5E  context  length  168  prediction  length  72  estimator  set  hyperparameters  hyperparameters  estimator  fit  inputs  train  train  input  wait  True  instance  type  ml  m4  xlarge  predictor  estimator  deploy  initial  instance  count  instance  type  instance  type  instances  json  data  104  Pick  up  the  first  168  past  sales  and  168  past  prices  72,amazon
  future  prices  data  start  instances  start  target  instances  target  168  cat  instances  cat  dynamic  feat  instances  dynamic  feat  168  72  configuration  num  samples  100  output  types  samples  quantiles  http  request  data  instances  data  configuration  configuration  byte  json  predictor  predict  json  dumps  http  request  data  encode  utf  result  json  loads  byte  json  gt  instances  target  168  prediction  result  predictions  samples  ,amazon
Lab  10  MNIST  and  Dropout  SELU  implementation  from  https  github  com  bioinf  jku  SNNs  blob  master  selu  py  import  tensorflow  as  tf  import  random  import  matplotlib  pyplot  as  plt  coding  utf  Tensorflow  Implementation  of  the  Scaled  ELU  function  and  Dropout  import  numbers  from  tensorflow  contrib  import  layers  from  tensorflow  python  framework  import  ops  from  tensorflow  python  framework  import  tensor  shape  from  tensorflow  python  framework  import  tensor  util  from  tensorflow  python  ops  import  math  ops  from  tensorflow  python  ops  import  random  ops  from  tensorflow  python  ops  import  array  ops  from  tensorflow  python  layers  import  utils  from  tensorflow  examples  tutorials  mnist  import  input  data  tf  set  random  seed  777  reproducibilitydef  selu  with  ops  name  scope  elu  as  scope  alpha  6732632423543772848170429916717  scale  0507009873554804934193349852946  return  scale  tf  where  alpha  tf  nn  elu  def  dropout  sel,ibm
u  keep  prob  alpha  7580993408473766  fixedPointMean  fixedPointVar  noise  shape  None  seed  None  name  None  training  False  Dropout  to  value  with  rescaling  def  dropout  selu  impl  rate  alpha  noise  shape  seed  name  keep  prob  rate  ops  convert  to  tensor  name  if  isinstance  keep  prob  numbers  Real  and  not  keep  prob  raise  ValueError  keep  prob  must  be  scalar  tensor  or  float  in  the  range  got  keep  prob  keep  prob  ops  convert  to  tensor  keep  prob  dtype  dtype  name  keep  prob  keep  prob  get  shape  assert  is  compatible  with  tensor  shape  scalar  alpha  ops  convert  to  tensor  alpha  dtype  dtype  name  alpha  keep  prob  get  shape  assert  is  compatible  with  tensor  shape  scalar  if  tensor  util  constant  value  keep  prob  return  noise  shape  noise  shape  if  noise  shape  is  not  None  else  array  ops  shape  random  tensor  keep  prob  random  tensor  random  ops  random  uniform  noise  shape  seed  seed  dtype  dtype  binary  tensor  ,ibm
math  ops  floor  random  tensor  ret  binary  tensor  alpha  binary  tensor  tf  sqrt  fixedPointVar  keep  prob  keep  prob  tf  pow  alpha  fixedPointMean  fixedPointVar  fixedPointMean  keep  prob  fixedPointMean  keep  prob  alpha  ret  ret  ret  set  shape  get  shape  return  ret  with  ops  name  scope  name  dropout  as  name  return  utils  smart  cond  training  lambda  dropout  selu  impl  keep  prob  alpha  noise  shape  seed  name  lambda  array  ops  identity  parameters  learning  rate  001  training  epochs  50  batch  size  100  input  place  holders  tf  placeholder  tf  float32  None  784  tf  placeholder  tf  float32  None  10  dropout  keep  prob  rate  on  training  but  should  be  for  testing  keep  prob  tf  placeholder  tf  float32  weights  bias  for  nn  layers  http  stackoverflow  com  questions  33640581  how  to  do  xavier  initialization  on  tensorflow  W1  tf  get  variable  W1  shape  784  512  initializer  tf  contrib  layers  xavier  initializer  b1  tf  Variable  tf  ,ibm
random  normal  512  L1  selu  tf  matmul  W1  b1  L1  dropout  selu  L1  keep  prob  keep  prob  W2  tf  get  variable  W2  shape  512  512  initializer  tf  contrib  layers  xavier  initializer  b2  tf  Variable  tf  random  normal  512  L2  selu  tf  matmul  L1  W2  b2  L2  dropout  selu  L2  keep  prob  keep  prob  W3  tf  get  variable  W3  shape  512  512  initializer  tf  contrib  layers  xavier  initializer  b3  tf  Variable  tf  random  normal  512  L3  selu  tf  matmul  L2  W3  b3  L3  dropout  selu  L3  keep  prob  keep  prob  W4  tf  get  variable  W4  shape  512  512  initializer  tf  contrib  layers  xavier  initializer  b4  tf  Variable  tf  random  normal  512  L4  selu  tf  matmul  L3  W4  b4  L4  dropout  selu  L4  keep  prob  keep  prob  W5  tf  get  variable  W5  shape  512  10  initializer  tf  contrib  layers  xavier  initializer  b5  tf  Variable  tf  random  normal  10  hypothesis  tf  matmul  L4  W5  b5  define  cost  loss  optimizer  cost  tf  reduce  mean  tf  nn  softmax  cross  en,ibm
tropy  with  logits  logits  hypothesis  labels  optimizer  tf  train  AdamOptimizer  learning  rate  learning  rate  minimize  cost  initialize  sess  tf  Session  sess  run  tf  global  variables  initializer  mnist  input  data  read  data  sets  MNIST  data  one  hot  True  Check  out  https  www  tensorflow  org  get  started  mnist  beginners  for  more  information  about  the  mnist  dataset  train  my  model  for  epoch  in  range  training  epochs  avg  cost  total  batch  int  mnist  train  num  examples  batch  size  for  in  range  total  batch  batch  xs  batch  ys  mnist  train  next  batch  batch  size  feed  dict  batch  xs  batch  ys  keep  prob  sess  run  cost  optimizer  feed  dict  feed  dict  avg  cost  total  batch  print  Epoch  04d  epoch  cost  9f  format  avg  cost  print  Learning  Finished  Test  model  and  check  accuracy  correct  prediction  tf  equal  tf  argmax  hypothesis  tf  argmax  accuracy  tf  reduce  mean  tf  cast  correct  prediction  tf  float32  print  Accuracy  ,ibm
sess  run  accuracy  feed  dict  mnist  test  images  mnist  test  labels  keep  prob  Get  one  and  predict  random  randint  mnist  test  num  examples  print  Label  sess  run  tf  argmax  mnist  test  labels  print  Prediction  sess  run  tf  argmax  hypothesis  feed  dict  mnist  test  images  keep  prob  plt  imshow  mnist  test  images  reshape  28  28  cmap  Greys  interpolation  nearest  plt  show  Epoch  0001  cost  447322626  Epoch  0002  cost  157285590  Epoch  0003  cost  121884535  Epoch  0004  cost  098128681  Epoch  0005  cost  082901778  Epoch  0006  cost  075337573  Epoch  0007  cost  069752543  Epoch  0008  cost  060884363  Epoch  0009  cost  055276413  Epoch  0010  cost  054631256  Epoch  0011  cost  049675195  Epoch  0012  cost  049125314  Epoch  0013  cost  047231930  Epoch  0014  cost  041290121  Epoch  0015  cost  043621063  Learning  Finished  Accuracy  9804  ,ibm
from  sagemaker  import  get  execution  role  Bucket  location  to  save  your  custom  code  in  tar  gz  format  custom  code  upload  location  s3  bucket  name  customcode  mxnet  Bucket  location  where  results  of  model  training  are  saved  model  artifacts  location  s3  bucket  name  artifacts  IAM  execution  role  that  gives  SageMaker  access  to  resources  in  your  AWS  account  We  can  use  the  SageMaker  Python  SDK  to  get  the  role  from  our  notebook  environment  role  get  execution  role  cat  mnist  pyfrom  sagemaker  mxnet  import  MXNet  mnist  estimator  MXNet  entry  point  mnist  py  role  role  output  path  model  artifacts  location  code  location  custom  code  upload  location  train  instance  count  train  instance  type  ml  m4  xlarge  hyperparameters  learning  rate  time  import  boto3  region  boto3  Session  region  name  train  data  location  s3  sagemaker  sample  data  mxnet  mnist  train  format  region  test  data  location  s3  sagemaker  sample  dat,amazon
a  mxnet  mnist  test  format  region  mnist  estimator  fit  train  train  data  location  test  test  data  location  time  predictor  mnist  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  IPython  display  import  HTML  HTML  open  input  html  read  response  predictor  predict  data  print  Raw  prediction  result  print  response  labeled  predictions  list  zip  range  10  response  print  Labeled  predictions  print  labeled  predictions  labeled  predictions  sort  key  lambda  label  and  prob  label  and  prob  print  Most  likely  answer  format  labeled  predictions  print  Endpoint  name  predictor  endpoint  import  sagemaker  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
import  sklearn  from  sklearn  datasets  import  fetch  mldata  mnist  fetch  mldata  MNIST  original  mnistX  mnist  data  mnist  target  shapey  shape  matplotlib  inline  import  matplotlib  import  matplotlib  pyplot  as  plt  some  digit  20000  some  digit  image  some  digit  reshape  28  28  plt  imshow  some  digit  image  cmap  matplotlib  cm  binary  interpolation  nearest  plt  axis  off  plt  show  20000  train  test  train  test  60000  60000  60000  60000  import  numpy  as  np  shuffle  index  np  random  permutation  60000  train  train  train  shuffle  index  train  shuffle  index  train  train  True  for  threes  false  for  the  others  test  test  from  sklearn  linear  model  import  SGDClassifier  sgd  clf  SGDClassifier  random  state  42  sgd  clf  fit  train  train  sgd  clf  predict  some  digit  from  sklearn  model  selection  import  cross  val  score  cross  val  score  sgd  clf  train  train  cv  scoring  accuracy  from  sklearn  model  selection  import  StratifiedKFold  from,ibm
  sklearn  base  import  clone  skfolds  StratifiedKFold  splits  random  state  42  for  train  index  test  index  in  skfolds  split  train  train  clone  clf  clone  sgd  clf  train  folds  train  train  index  train  folds  train  train  index  test  folds  train  test  index  test  folds  train  test  index  clone  clf  fit  train  folds  train  folds  pred  clone  clf  predict  test  folds  correct  sum  pred  test  folds  print  correct  len  pred  from  sklearn  base  import  BaseEstimator  class  Never3Classifier  BaseEstimator  def  fit  self  None  pass  def  predict  self  return  np  zeros  len  dtype  bool  never  clf  Never3Classifier  cross  val  score  never  clf  train  train  cv  scoring  accuracy  from  sklearn  model  selection  import  cross  val  predict  train  pred  cross  val  predict  sgd  clf  train  train  cv  from  sklearn  metrics  import  confusion  matrix  confusion  matrix  train  train  pred  train  perfect  predictions  train  confusion  matrix  train  train  perfect  pred,ibm
ictions  from  sklearn  metrics  import  precision  score  recall  score  precision  score  train  train  pred  recall  score  train  train  pred  from  sklearn  metrics  import  f1  score  f1  score  train  train  pred  pip  install  watson  machine  learning  client  upgradefrom  watson  machine  learning  client  import  WatsonMachineLearningAPIClient  wml  credentials  url  https  us  south  ml  cloud  ibm  com  username  5ac9555e  631b  486c  b484  7d6e556c97e3  password  78c4c71e  5eae  4ccd  926d  d8a40c264d18  instance  id  76075c55  c628  45c5  9de4  143fbd42238d  client  WatsonMachineLearningAPIClient  wml  credentials  instance  details  client  service  instance  get  details  print  instance  details  ,ibm
from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  bucket  name  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  30  This  is  format  train  set  30  from  sagemaker  import  KMeans  data  location  s3  kmeans  highlevel  example  data  format  bucket  output  location  s3  kmeans  example  output  format  bucket  print  training  data  will  be  uploaded  to  format  data  location  print  training  artifacts  will  be  uploaded  to  f,amazon
ormat  output  location  kmeans  KMeans  role  role  train  instance  count  train  instance  type  ml  c4  8xlarge  output  path  output  location  10  data  location  data  location  time  kmeans  fit  kmeans  record  set  train  set  time  kmeans  predictor  kmeans  deploy  initial  instance  count  instance  type  ml  m4  xlarge  result  kmeans  predictor  predict  train  set  30  31  print  result  time  result  kmeans  predictor  predict  valid  set  100  clusters  label  closest  cluster  float32  tensor  values  for  in  result  for  cluster  in  range  10  print  nCluster  format  int  cluster  digits  img  for  img  in  zip  clusters  valid  set  if  int  cluster  height  len  digits  width  plt  rcParams  figure  figsize  width  height  subplots  plt  subplots  height  width  subplots  numpy  ndarray  flatten  subplots  for  subplot  image  in  zip  subplots  digits  show  digit  image  subplot  subplot  for  subplot  in  subplots  len  digits  subplot  axis  off  plt  show  print  kmeans  predicto,amazon
r  endpoint  import  sagemaker  sagemaker  Session  delete  endpoint  kmeans  predictor  endpoint  ,amazon
bin  bash  setup  shimport  os  import  subprocess  import  boto3  import  sagemaker  from  sagemaker  mxnet  import  MXNet  from  mxnet  import  gluon  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  instance  type  local  if  subprocess  call  nvidia  smi  Set  type  to  GPU  if  one  is  present  instance  type  local  gpu  print  Instance  type  instance  type  role  get  execution  role  gluon  data  vision  MNIST  data  train  train  True  gluon  data  vision  MNIST  data  test  train  False  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  mnist  cat  mnist  py  MXNet  mnist  py  role  role  train  instance  count  train  instance  type  instance  type  framework  version  hyperparameters  batch  size  100  epochs  20  learning  rate  momentum  log  interval  100  fit  inputs  predictor  deploy  initial  instance  count  instance  type  instance  type  from  IPython  display  import  HTML  HTML  open  input  html  read  response  predictor ,amazon
 predict  data  print  int  response  delete  endpoint  ,amazon
Lab  Linear  Regression  import  tensorflow  as  tf  tf  set  random  seed  777  for  reproducibility  import  matplotlib  pyplot  as  plt  import  numpy  as  np  matplotlib  inline  and  data  train  train  Try  to  find  values  for  and  to  compute  data  data  We  know  that  should  be  and  should  be  But  let  TensorFlow  figure  it  out  tf  Variable  tf  random  normal  name  weight  tf  Variable  tf  random  normal  name  bias  Our  hypothesis  XW  hypothesis  train  cost  loss  function  cost  tf  reduce  mean  tf  square  hypothesis  train  Minimize  optimizer  tf  train  GradientDescentOptimizer  learning  rate  01  train  optimizer  minimize  cost  Launch  the  graph  in  session  sess  tf  Session  y1  y2  y3  y4  Initializes  global  variables  in  the  graph  sess  run  tf  global  variables  initializer  for  step  in  range  2500  val  cost  val  val  hypor  val  sess  run  train  cost  hypothesis  append  step  y1  append  val  y2  append  cost  val  y3  append  val  y4  append  hypor  v,ibm
al  if  step  50  print  step  sess  run  cost  sess  run  sess  run  plt  plot  y1  label  plt  plot  y2  label  cost  plt  plot  y3  label  plt  plot  y4  label  plt  plot  y1  y2  label  cost  plt  legend  shadow  True  fancybox  True  loc  upper  right  plt  ylim  plt  show  import  matplotlib  pyplot  as  plt  import  numpy  as  np  np  arange  10  value  for  value  in  np  sin  value  for  value  in  np  cos  value  for  value  in  fig  plt  figure  fig  set  size  inches  10  plt  plot  label  sin  linestyle  dotted  plt  plot  label  cos  plt  title  sine  cosine  plt  xlabel  Label  plt  ylabel  label  plt  text  sine  plt  annotate  Co  sine  xy  arrowprops  dict  facecolor  black  shrink  05  xytext  plt  legend  shadow  True  fancybox  True  loc  lower  right  plt  grid  True  ls  dotted  black  lw  plt  xlim  10  plt  ylim  20  fig  savefig  test  jpg  plt  show  ,ibm
conda  install  yes  name  JupyterSystemEnv  channel  essentials  ,amazon
from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  angiemachinelearning  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  30  This  is  format  train  set  30  from  sagemaker  import  get  execution  role  Bucket  location  to  save  your  custom  code  in  tar  gz  format  custom  code  upload  location  s3  angiemachinelearning  mxnet  code  output  Bucket  location  where  results  of  model  training  are  saved  model  artifacts  lo,amazon
cation  s3  angiemachinelearning  model  result  IAM  execution  role  that  gives  SageMaker  access  to  resources  in  your  AWS  account  We  can  use  the  SageMaker  Python  SDK  to  get  the  role  from  our  notebook  environment  role  get  execution  role  print  training  data  will  be  uploaded  to  format  output  location  print  training  artifacts  will  be  uploaded  to  format  model  location  cat  mnist  pyfrom  sagemaker  mxnet  import  MXNet  mnist  estimator  MXNet  entry  point  mnist  py  role  role  output  path  model  artifacts  location  code  location  custom  code  upload  location  train  instance  count  train  instance  type  ml  m4  xlarge  hyperparameters  learning  rate  time  import  boto3  region  boto3  Session  region  name  train  data  location  s3  sagemaker  sample  data  mxnet  mnist  train  format  region  test  data  location  s3  sagemaker  sample  data  mxnet  mnist  test  format  region  mnist  estimator  fit  train  train  data  location  test  test  data  ,amazon
location  time  predictor  mnist  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  IPython  display  import  HTML  HTML  open  input  html  read  response  predictor  predict  data  print  Raw  prediction  result  print  response  labeled  predictions  list  zip  range  10  response  print  Labeled  predictions  print  labeled  predictions  labeled  predictions  sort  key  lambda  label  and  prob  label  and  prob  print  Most  likely  answer  format  labeled  predictions  print  kmeans  predictor  endpoint  import  sagemaker  sagemaker  Session  delete  endpoint  kmeans  predictor  endpoint  ,amazon
from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  bucket  name  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  30  This  is  format  train  set  30  time  from  sagemaker  amazon  common  import  write  numpy  to  dense  tensor  import  io  import  boto3  data  key  kmeans  lowlevel  example  data  data  location  s3  format  bucket  data  key  print  training  data  will  be  uploaded  to  format  data  location  Convert  the  trainin,amazon
g  data  into  the  format  required  by  the  SageMaker  KMeans  algorithm  buf  io  BytesIO  write  numpy  to  dense  tensor  buf  train  set  train  set  buf  seek  boto3  resource  s3  Bucket  bucket  Object  data  key  upload  fileobj  buf  time  import  boto3  from  time  import  gmtime  strftime  job  name  kmeans  lowlevel  strftime  gmtime  print  Training  job  job  name  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  image  get  image  uri  boto3  Session  region  name  kmeans  output  location  s3  kmeans  example  output  format  bucket  print  training  artifacts  will  be  uploaded  to  format  output  location  create  training  params  AlgorithmSpecification  TrainingImage  image  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  output  location  ResourceConfig  InstanceCount  InstanceType  ml  c4  8xlarge  VolumeSizeInGB  50  TrainingJobName  job  name  HyperParameters  10  feature  dim  784  mini  batch  size  500  force  dense  True  StoppingCon,amazon
dition  MaxRuntimeInSeconds  60  60  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  data  location  S3DataDistributionType  FullyReplicated  CompressionType  None  RecordWrapperType  None  sagemaker  boto3  client  sagemaker  sagemaker  create  training  job  create  training  params  status  sagemaker  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  status  try  sagemaker  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  finally  status  sagemaker  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  Training  job  ended  with  status  status  if  status  Failed  message  sagemaker  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  time  import  boto3  from  time  import  gmtime  strftime  model  name  job  name  print  model  name  info  sagem,amazon
aker  describe  training  job  TrainingJobName  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  primary  container  Image  image  ModelDataUrl  model  data  create  model  response  sagemaker  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  endpoint  config  name  KMeansEndpointConfig  strftime  gmtime  print  endpoint  config  name  create  endpoint  config  response  sagemaker  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  import  time  endpoint  name  KMeansEndpoint  strftime  gmtime  print  endpoint  name  create  endpoint  response  sagemaker  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config,amazon
  name  print  create  endpoint  response  EndpointArn  resp  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  try  sagemaker  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  finally  resp  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Create  endpoint  ended  with  status  status  if  status  InService  message  sagemaker  describe  endpoint  EndpointName  endpoint  name  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Endpoint  creation  did  not  succeed  Simple  function  to  create  csv  from  our  numpy  array  def  np2csv  arr  csv  io  BytesIO  numpy  savetxt  csv  arr  delimiter  fmt  return  csv  getvalue  decode  rstrip  runtime  boto3  Session  client  runtime  sagemaker  import  json  payload  np2csv  train  set  30  31  response  runtime  invoke  endpoint  EndpointName  e,amazon
ndpoint  name  ContentType  text  csv  Body  payload  result  json  loads  response  Body  read  decode  print  result  time  payload  np2csv  valid  set  100  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  text  csv  Body  payload  result  json  loads  response  Body  read  decode  clusters  closest  cluster  for  in  result  predictions  for  cluster  in  range  10  print  nCluster  format  int  cluster  digits  img  for  img  in  zip  clusters  valid  set  if  int  cluster  height  len  digits  width  plt  rcParams  figure  figsize  width  height  subplots  plt  subplots  height  width  subplots  numpy  ndarray  flatten  subplots  for  subplot  image  in  zip  subplots  digits  show  digit  image  subplot  subplot  for  subplot  in  subplots  len  digits  subplot  axis  off  plt  show  sagemaker  delete  endpoint  EndpointName  endpoint  name  ,amazon
matplotlib  inline  import  os  import  time  from  time  import  gmtime  strftime  import  numpy  as  np  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  from  sklearn  datasets  import  load  boston  import  sklearn  model  selectionimport  sagemaker  from  sagemaker  import  get  execution  role  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  This  is  an  object  that  represents  the  SageMaker  session  that  we  are  currently  operating  in  This  object  contains  some  useful  information  that  we  will  need  to  access  later  such  as  our  region  session  sagemaker  Session  This  is  an  object  that  represents  the  IAM  role  that  we  are  currently  assigned  When  we  construct  and  launch  the  training  job  later  we  will  need  to  tell  it  what  IAM  role  it  should  have  Since  our  use  case  is  relatively  simple  we  will  simply  assign  the  training  job  the  role  we  currently  have  role  get  execution  role  boston  load  boston,amazon
  First  we  package  up  the  input  data  and  the  target  variable  the  median  value  as  pandas  dataframes  This  will  make  saving  the  data  to  file  little  easier  later  on  bos  pd  pd  DataFrame  boston  data  columns  boston  feature  names  bos  pd  pd  DataFrame  boston  target  We  split  the  dataset  into  training  and  testing  sets  train  test  train  test  sklearn  model  selection  train  test  split  bos  pd  bos  pd  test  size  33  Then  we  split  the  training  set  further  into  training  and  validation  sets  train  val  train  val  sklearn  model  selection  train  test  split  train  train  test  size  33  This  is  our  local  data  directory  We  need  to  make  sure  that  it  exists  data  dir  data  boston  if  not  os  path  exists  data  dir  os  makedirs  data  dir  We  use  pandas  to  save  our  test  train  and  validation  data  to  csv  files  Note  that  we  make  sure  not  to  include  header  information  or  an  index  as  this  is  required  by  the ,amazon
 built  in  algorithms  provided  by  Amazon  Also  for  the  train  and  validation  data  it  is  assumed  that  the  first  entry  in  each  row  is  the  target  variable  test  to  csv  os  path  join  data  dir  test  csv  header  False  index  False  pd  concat  val  val  axis  to  csv  os  path  join  data  dir  validation  csv  header  False  index  False  pd  concat  train  train  axis  to  csv  os  path  join  data  dir  train  csv  header  False  index  False  prefix  boston  xgboost  LL  test  location  session  upload  data  os  path  join  data  dir  test  csv  key  prefix  prefix  val  location  session  upload  data  os  path  join  data  dir  validation  csv  key  prefix  prefix  train  location  session  upload  data  os  path  join  data  dir  train  csv  key  prefix  prefix  We  will  need  to  know  the  name  of  the  container  that  we  want  to  use  for  training  SageMaker  provides  nice  utility  method  to  construct  this  for  us  container  get  image  uri  session  boto  reg,amazon
ion  name  xgboost  We  now  specify  the  parameters  we  wish  to  use  for  our  training  job  training  params  We  need  to  specify  the  permissions  that  this  training  job  will  have  For  our  purposes  we  can  use  the  same  permissions  that  our  current  SageMaker  session  has  training  params  RoleArn  role  Here  we  describe  the  algorithm  we  wish  to  use  The  most  important  part  is  the  container  which  contains  the  training  code  training  params  AlgorithmSpecification  TrainingImage  container  TrainingInputMode  File  We  also  need  to  say  where  we  would  like  the  resulting  model  artifacst  stored  training  params  OutputDataConfig  S3OutputPath  s3  session  default  bucket  prefix  output  We  also  need  to  set  some  parameters  for  the  training  job  itself  Namely  we  need  to  describe  what  sort  of  compute  instance  we  wish  to  use  along  with  stopping  condition  to  handle  the  case  that  there  is  some  sort  of  error  and  the  t,amazon
raining  script  doesn  terminate  training  params  ResourceConfig  InstanceCount  InstanceType  ml  m4  xlarge  VolumeSizeInGB  training  params  StoppingCondition  MaxRuntimeInSeconds  86400  Next  we  set  the  algorithm  specific  hyperparameters  You  may  wish  to  change  these  to  see  what  effect  there  is  on  the  resulting  model  training  params  StaticHyperParameters  gamma  subsample  objective  reg  linear  early  stopping  rounds  10  num  round  200  Now  we  need  to  tell  SageMaker  where  the  data  should  be  retrieved  from  training  params  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  train  location  S3DataDistributionType  FullyReplicated  ContentType  csv  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  val  location  S3DataDistributionType  FullyReplicated  ContentType  csv  CompressionType  None  We  need  to  construct  dictionary  which  specifies  the  tuning  job  we  ,amazon
want  SageMaker  to  perform  tuning  job  config  First  we  specify  which  hyperparameters  we  want  SageMaker  to  be  able  to  vary  and  we  specify  the  type  and  range  of  the  hyperparameters  ParameterRanges  CategoricalParameterRanges  ContinuousParameterRanges  MaxValue  MinValue  05  Name  eta  IntegerParameterRanges  MaxValue  12  MinValue  Name  max  depth  MaxValue  MinValue  Name  min  child  weight  We  also  need  to  specify  how  many  models  should  be  fit  and  how  many  can  be  fit  in  parallel  ResourceLimits  MaxNumberOfTrainingJobs  20  MaxParallelTrainingJobs  Here  we  specify  how  SageMaker  should  update  the  hyperparameters  as  new  models  are  fit  Strategy  Bayesian  And  lastly  we  need  to  specify  how  we  like  to  determine  which  models  are  better  or  worse  HyperParameterTuningJobObjective  MetricName  validation  rmse  Type  Minimize  First  we  need  to  choose  name  for  the  job  This  is  useful  for  if  we  want  to  recall  information  ab,amazon
out  our  tuning  job  at  later  date  Note  that  SageMaker  requires  tuning  job  name  and  that  the  name  needs  to  be  unique  which  we  accomplish  by  appending  the  current  timestamp  tuning  job  name  tuning  job  strftime  gmtime  And  now  we  ask  SageMaker  to  create  and  execute  the  training  job  session  sagemaker  client  create  hyper  parameter  tuning  job  HyperParameterTuningJobName  tuning  job  name  HyperParameterTuningJobConfig  tuning  job  config  TrainingJobDefinition  training  params  session  wait  for  tuning  job  tuning  job  name  tuning  job  info  session  sagemaker  client  describe  hyper  parameter  tuning  job  HyperParameterTuningJobName  tuning  job  name  We  begin  by  asking  SageMaker  to  describe  for  us  the  results  of  the  best  training  job  The  data  structure  returned  contains  lot  more  information  than  we  currently  need  try  checking  it  out  yourself  in  more  detail  best  training  job  name  tuning  job  info  BestTraini,amazon
ngJob  TrainingJobName  training  job  info  session  sagemaker  client  describe  training  job  TrainingJobName  best  training  job  name  model  artifacts  training  job  info  ModelArtifacts  S3ModelArtifacts  Just  like  when  we  created  training  job  the  model  name  must  be  unique  model  name  best  training  job  name  model  We  also  need  to  tell  SageMaker  which  container  should  be  used  for  inference  and  where  it  should  retrieve  the  model  artifacts  from  In  our  case  the  xgboost  container  that  we  used  for  training  can  also  be  used  for  inference  primary  container  Image  container  ModelDataUrl  model  artifacts  And  lastly  we  construct  the  SageMaker  model  model  info  session  sagemaker  client  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  Just  like  in  each  of  the  previous  steps  we  need  to  make  sure  to  name  our  job  and  the  name  should  be  unique  transform  job  name  bosto,amazon
n  xgboost  batch  transform  strftime  gmtime  Now  we  construct  the  data  structure  which  will  describe  the  batch  transform  job  transform  request  TransformJobName  transform  job  name  This  is  the  name  of  the  model  that  we  created  earlier  ModelName  model  name  This  describes  how  many  compute  instances  should  be  used  at  once  If  you  happen  to  be  doing  very  large  batch  transform  job  it  may  be  worth  running  multiple  compute  instances  at  once  MaxConcurrentTransforms  This  says  how  big  each  individual  request  sent  to  the  model  should  be  at  most  One  of  the  things  that  SageMaker  does  in  the  background  is  to  split  our  data  up  into  chunks  so  that  each  chunks  stays  under  this  size  limit  MaxPayloadInMB  Sometimes  we  may  want  to  send  only  single  sample  to  our  endpoint  at  time  however  in  this  case  each  of  the  chunks  that  we  send  should  contain  multiple  samples  of  our  input  data  BatchStrate,amazon
gy  MultiRecord  This  next  object  describes  where  the  output  data  should  be  stored  Some  of  the  more  advanced  options  which  we  don  cover  here  also  describe  how  SageMaker  should  collect  output  from  various  batches  TransformOutput  S3OutputPath  s3  batch  bransform  format  session  default  bucket  prefix  Here  we  describe  our  input  data  Of  course  we  need  to  tell  SageMaker  where  on  S3  our  input  data  is  stored  in  addition  we  need  to  detail  the  characteristics  of  our  input  data  In  particular  since  SageMaker  may  need  to  split  our  data  up  into  chunks  it  needs  to  know  how  the  individual  samples  in  our  data  file  appear  In  our  case  each  line  is  its  own  sample  and  so  we  set  the  split  type  to  line  We  also  need  to  tell  SageMaker  what  type  of  data  is  being  sent  in  this  case  csv  so  that  it  can  properly  serialize  the  data  TransformInput  ContentType  text  csv  SplitType  Line  DataSource  S,amazon
3DataSource  S3DataType  S3Prefix  S3Uri  test  location  And  lastly  we  tell  SageMaker  what  sort  of  compute  instance  we  would  like  it  to  use  TransformResources  InstanceType  ml  m4  xlarge  InstanceCount  transform  response  session  sagemaker  client  create  transform  job  transform  request  transform  desc  session  wait  for  transform  job  transform  job  name  transform  output  s3  batch  bransform  format  session  default  bucket  prefix  aws  s3  cp  recursive  transform  output  data  dirY  pred  pd  read  csv  os  path  join  data  dir  test  csv  out  header  None  plt  scatter  test  pred  plt  xlabel  Median  Price  plt  ylabel  Predicted  Price  plt  title  Median  Price  vs  Predicted  Price  First  we  will  remove  all  of  the  files  contained  in  the  data  dir  directory  rm  data  dir  And  then  we  delete  the  directory  itself  rmdir  data  dir  ,amazon
import  plotly  offline  as  py  from  plotly  graph  objs  import  Data  Contour  py  init  notebook  mode  connected  True  data  Data  Contour  10  10  625  12  15  625  20  625  25  125  11  25  15  625  125  125  12  625  25  125  25  10  625  625  625  10  py  iplot  data  show  link  False  ,microsoft
import  os  import  boto3  import  sagemaker  from  sagemaker  mxnet  import  MXNet  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  bash  mkdir  data  curl  https  raw  githubusercontent  com  saurabh3949  Text  Classification  Datasets  master  stsa  binary  phrases  train  data  train  curl  https  raw  githubusercontent  com  saurabh3949  Text  Classification  Datasets  master  stsa  binary  test  data  test  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  DEMO  sentiment  cat  sentiment  py  MXNet  sentiment  py  role  role  train  instance  count  train  instance  type  ml  c4  2xlarge  hyperparameters  batch  size  epochs  learning  rate  01  embedding  size  50  log  interval  1000  fit  inputs  predictor  deploy  initial  instance  count  instance  type  ml  c4  xlarge  data  this  movie  was  extremely  good  the  plot  was  very  boring  this  film  is  so  slick  superficial  and  trend  hoppy  just  could ,amazon
 not  watch  it  till  the  end  the  movie  was  so  enthralling  response  predictor  predict  data  print  responsesagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
import  os  import  boto3  import  sagemaker  from  sagemaker  mxnet  import  MXNet  from  mxnet  import  gluon  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  from  cifar10  utils  import  download  training  data  download  training  data  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  DEMO  gluon  cifar10  print  input  spec  in  this  case  just  an  S3  path  format  inputs  cat  cifar10  py  MXNet  cifar10  py  role  role  train  instance  count  train  instance  type  ml  p2  xlarge  hyperparameters  batch  size  128  epochs  50  learning  rate  momentum  fit  inputs  predictor  deploy  initial  instance  count  instance  type  ml  m4  xlarge  load  the  CIFAR10  samples  and  convert  them  into  format  we  can  use  with  the  prediction  endpoint  from  cifar10  utils  import  read  images  filenames  images  airplane1  png  images  automobile1  png  images  bird1  png  images  cat1  png  images  deer1  pn,amazon
g  images  dog1  png  images  frog1  png  images  horse1  png  images  ship1  png  images  truck1  png  image  data  read  images  filenames  for  img  in  enumerate  image  data  response  predictor  predict  img  print  image  class  format  int  response  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
import  boto3  import  sagemaker  import  os  region  boto3  Session  region  name  sage  client  boto3  Session  client  sagemaker  tuning  job  name  xgboost  180614  1835  run  this  cell  to  check  current  status  of  hyperparameter  tuning  job  tuning  job  result  sage  client  describe  hyper  parameter  tuning  job  HyperParameterTuningJobName  tuning  job  name  status  tuning  job  result  HyperParameterTuningJobStatus  if  status  Completed  print  Reminder  the  tuning  job  has  not  been  completed  is  minimize  tuning  job  result  HyperParameterTuningJobConfig  HyperParameterTuningJobObjective  Type  Maximize  objective  name  tuning  job  result  HyperParameterTuningJobConfig  HyperParameterTuningJobObjective  MetricName  from  pprint  import  pprint  if  tuning  job  result  get  BestTrainingJob  None  print  Best  model  found  so  far  pprint  tuning  job  result  BestTrainingJob  else  print  No  training  jobs  have  reported  results  yet  import  pandas  as  pd  tuner  sagemaker  H,amazon
yperparameterTuningJobAnalytics  tuning  job  name  full  df  tuner  dataframe  if  len  full  df  df  full  df  full  df  FinalObjectiveValue  float  inf  if  len  df  df  df  sort  values  FinalObjectiveValue  ascending  is  minimize  print  Number  of  training  jobs  with  valid  objective  len  df  print  lowest  min  df  FinalObjectiveValue  highest  max  df  FinalObjectiveValue  pd  set  option  display  max  colwidth  Don  truncate  TrainingJobName  else  print  No  training  jobs  have  reported  valid  results  yet  dfimport  bokeh  import  bokeh  io  bokeh  io  output  notebook  from  bokeh  plotting  import  figure  show  from  bokeh  models  import  HoverTool  class  HoverHelper  def  init  self  tuning  analytics  self  tuner  tuning  analytics  def  hovertool  self  tooltips  FinalObjectiveValue  FinalObjectiveValue  TrainingJobName  TrainingJobName  for  in  self  tuner  tuning  ranges  keys  tooltips  append  ht  HoverTool  tooltips  tooltips  return  ht  def  tools  self  standard  tools  pa,amazon
n  crosshair  wheel  zoom  zoom  in  zoom  out  undo  reset  return  self  hovertool  standard  tools  hover  HoverHelper  tuner  figure  plot  width  900  plot  height  400  tools  hover  tools  axis  type  datetime  circle  source  df  TrainingStartTime  FinalObjectiveValue  show  ranges  tuner  tuning  ranges  figures  for  hp  name  hp  range  in  ranges  items  categorical  args  if  hp  range  get  Values  This  is  marked  as  categorical  Check  if  all  options  are  actually  numbers  def  is  num  try  float  return  except  return  vals  hp  range  Values  if  sum  is  num  for  in  vals  len  vals  Bokeh  has  issues  plotting  categorical  range  that  actually  numeric  so  plot  as  numeric  print  Hyperparameter  is  tuned  as  categorical  but  all  values  are  numeric  hp  name  else  Set  up  extra  options  for  plotting  categoricals  bit  tricky  when  they  re  actually  numbers  categorical  args  range  vals  Now  plot  it  figure  plot  width  500  plot  height  500  title  Objecti,amazon
ve  vs  hp  name  tools  hover  tools  axis  label  hp  name  axis  label  objective  name  categorical  args  circle  source  df  hp  name  FinalObjectiveValue  figures  append  show  bokeh  layouts  Column  figures  ,amazon
import  boto3  import  io  import  matplotlib  pyplot  as  plt  import  numpy  as  np  import  os  import  pandas  as  pd  import  sagemaker  import  sagemaker  amazon  common  as  smac  from  sagemaker  import  get  execution  role  from  sagemaker  predictor  import  csv  serializer  json  deserializer  Set  data  locations  bucket  your  s3  bucket  here  replace  this  with  your  own  bucket  prefix  sagemaker  DEMO  linear  learner  loss  weights  replace  this  with  your  own  prefix  s3  train  key  train  recordio  pb  data  format  prefix  s3  train  path  os  path  join  s3  bucket  s3  train  key  local  raw  data  creditcard  csv  zip  role  get  execution  role  Confirm  access  to  s3  bucket  for  obj  in  boto3  resource  s3  Bucket  bucket  objects  all  print  obj  key  Read  the  data  shuffle  and  split  into  train  and  test  sets  separating  the  labels  last  column  from  the  features  raw  data  pd  read  csv  local  raw  data  as  matrix  np  random  seed  np  random  shuffle  ,amazon
raw  data  train  size  int  raw  data  shape  train  features  raw  data  train  size  train  labels  raw  data  train  size  test  features  raw  data  train  size  test  labels  raw  data  train  size  Convert  the  processed  training  data  to  protobuf  and  write  to  S3  for  linear  learner  vectors  np  array  tolist  for  in  train  features  astype  float32  labels  np  array  tolist  for  in  train  labels  astype  float32  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  vectors  labels  buf  seek  boto3  resource  s3  Bucket  bucket  Object  s3  train  key  upload  fileobj  buf  def  predictor  from  hyperparams  s3  train  data  hyperparams  output  path  Create  an  Estimator  from  the  given  hyperparams  fit  to  training  data  and  return  deployed  predictor  specify  algorithm  containers  and  instantiate  an  Estimator  with  given  hyperparams  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  linear  learner  latest  us  east  382416733822  dkr  e,amazon
cr  us  east  amazonaws  com  linear  learner  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  linear  learner  latest  linear  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  role  train  instance  count  train  instance  type  ml  m4  xlarge  output  path  output  path  sagemaker  session  sagemaker  Session  linear  set  hyperparameters  hyperparams  train  model  linear  fit  train  s3  train  data  deploy  predictor  linear  predictor  linear  deploy  initial  instance  count  instance  type  ml  m4  xlarge  linear  predictor  content  type  text  csv  linear  predictor  serializer  csv  serializer  linear  predictor  deserializer  json  deserializer  return  linear  predictordef  evaluate  linear  predictor  test  features  test  labels  model  name  verbose  True  Evaluate  model  on  test  set  given  the  prediction  endpoint  Return  binary  classification  metrics  spl,amazon
it  the  test  data  set  into  100  batches  and  evaluate  using  prediction  endpoint  prediction  batches  linear  predictor  predict  batch  predictions  for  batch  in  np  array  split  test  features  100  parse  raw  predictions  json  to  exctract  predicted  label  test  preds  np  concatenate  np  array  predicted  label  for  in  batch  for  batch  in  prediction  batches  calculate  true  positives  false  positives  true  negatives  false  negatives  tp  np  logical  and  test  labels  test  preds  sum  fp  np  logical  and  test  labels  test  preds  sum  tn  np  logical  and  test  labels  test  preds  sum  fn  np  logical  and  test  labels  test  preds  sum  calculate  binary  classification  metrics  recall  tp  tp  fn  precision  tp  tp  fp  accuracy  tp  tn  tp  fp  tn  fn  f1  precision  recall  precision  recall  if  verbose  print  pd  crosstab  test  labels  test  preds  rownames  actuals  colnames  predictions  print  11  3f  format  Recall  recall  print  11  3f  format  Precision ,amazon
 precision  print  11  3f  format  Accuracy  accuracy  print  11  3f  format  F1  f1  return  TP  tp  FP  fp  FN  fn  TN  tn  Precision  precision  Recall  recall  Accuracy  accuracy  F1  f1  Model  model  name  def  delete  endpoint  predictor  try  boto3  client  sagemaker  delete  endpoint  EndpointName  predictor  endpoint  print  Deleted  format  predictor  endpoint  except  print  Already  deleted  format  predictor  endpoint  Training  binary  classifier  with  default  settings  logistic  regression  defaults  hyperparams  feature  dim  30  predictor  type  binary  classifier  epochs  40  defaults  output  path  s3  defaults  output  format  bucket  prefix  defaults  predictor  predictor  from  hyperparams  s3  train  path  defaults  hyperparams  defaults  output  path  Training  binary  classifier  with  automated  threshold  tuning  autothresh  hyperparams  feature  dim  30  predictor  type  binary  classifier  binary  classifier  model  selection  criteria  precision  at  target  recall  target  re,amazon
call  epochs  40  autothresh  output  path  s3  autothresh  output  format  bucket  prefix  autothresh  predictor  predictor  from  hyperparams  s3  train  path  autothresh  hyperparams  autothresh  output  path  Training  binary  classifier  with  class  weights  and  automated  threshold  tuning  class  weights  hyperparams  feature  dim  30  predictor  type  binary  classifier  binary  classifier  model  selection  criteria  precision  at  target  recall  target  recall  positive  example  weight  mult  balanced  epochs  40  class  weights  output  path  s3  class  weights  output  format  bucket  prefix  class  weights  predictor  predictor  from  hyperparams  s3  train  path  class  weights  hyperparams  class  weights  output  path  Training  binary  classifier  with  hinge  loss  and  automated  threshold  tuning  svm  hyperparams  feature  dim  30  predictor  type  binary  classifier  loss  hinge  loss  binary  classifier  model  selection  criteria  precision  at  target  recall  target  recall  epoc,amazon
hs  40  svm  output  path  s3  svm  output  format  bucket  prefix  svm  predictor  predictor  from  hyperparams  s3  train  path  svm  hyperparams  svm  output  path  Training  binary  classifier  with  hinge  loss  balanced  class  weights  and  automated  threshold  tuning  svm  balanced  hyperparams  feature  dim  30  predictor  type  binary  classifier  loss  hinge  loss  binary  classifier  model  selection  criteria  precision  at  target  recall  target  recall  positive  example  weight  mult  balanced  epochs  40  svm  balanced  output  path  s3  svm  balanced  output  format  bucket  prefix  svm  balanced  predictor  predictor  from  hyperparams  s3  train  path  svm  balanced  hyperparams  svm  balanced  output  path  Evaluate  the  trained  models  predictors  Logistic  defaults  predictor  Logistic  with  auto  threshold  autothresh  predictor  Logistic  with  class  weights  class  weights  predictor  Hinge  with  auto  threshold  svm  predictor  Hinge  with  class  weights  svm  balanced  pred,amazon
ictor  metrics  key  evaluate  predictor  test  features  test  labels  key  False  for  key  predictor  in  predictors  items  pd  set  option  display  float  format  lambda  3f  display  pd  DataFrame  list  metrics  values  loc  Model  Recall  Precision  Accuracy  F1  for  predictor  in  defaults  predictor  autothresh  predictor  class  weights  predictor  svm  predictor  svm  balanced  predictor  delete  endpoint  predictor  ,amazon
import  os  import  sagemaker  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  DEMO  abalone  cat  abalone  py  from  sagemaker  tensorflow  import  TensorFlow  abalone  estimator  TensorFlow  entry  point  abalone  py  role  role  training  steps  100  evaluation  steps  100  hyperparameters  learning  rate  001  train  instance  count  train  instance  type  ml  c4  xlarge  abalone  estimator  fit  inputs  abalone  predictor  abalone  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  import  tensorflow  as  tf  import  numpy  as  np  prediction  set  tf  contrib  learn  datasets  base  load  csv  without  header  filename  os  path  join  data  abalone  predict  csv  target  dtype  np  int  features  dtype  np  float32  data  prediction  set  data  tensor  proto  tf  make  tensor  proto  values  np  asarray  data  shape  len  data  dtype  tf  ,amazon
float32  abalone  predictor  predict  tensor  proto  sagemaker  Session  delete  endpoint  abalone  predictor  endpoint  ,amazon
import  tensorflow  as  tf  import  matplotlib  pyplot  as  plt  tf  set  random  seed  777  for  reproducibility  tf  placeholder  tf  float32  Our  hypothesis  for  linear  model  hypothesis  cost  loss  function  cost  tf  reduce  mean  tf  square  hypothesis  Launch  the  graph  in  session  sess  tf  Session  Initializes  global  variables  in  the  graph  sess  run  tf  global  variables  initializer  Variables  for  plotting  cost  function  history  cost  history  for  in  range  30  50  curr  curr  cost  sess  run  cost  feed  dict  curr  history  append  curr  cost  history  append  curr  cost  Show  the  cost  function  plt  plot  history  cost  history  plt  show  ,ibm
import  sagemaker  bucket  sagemaker  Session  default  bucket  prefix  radix  mnist  fashion  tutorial  role  sagemaker  get  execution  role  import  boto3  from  time  import  gmtime  strftime  from  sagemaker  tensorflow  import  TensorFlow  from  sagemaker  tuner  import  IntegerParameter  CategoricalParameter  ContinuousParameter  HyperparameterTuner  cat  cnn  fashion  mnist  py  model  artifacts  location  s3  sagemaker  eu  central  959924085179  radix  mnist  fashion  tutorial  artifacts  custom  code  location  s3  sagemaker  eu  central  959924085179  radix  mnist  fashion  tutorial  artifacts  estimator  TensorFlow  entry  point  cnn  fashion  mnist  py  role  role  input  mode  Pipe  training  steps  30000  evaluation  steps  100  train  instance  count  train  instance  type  ml  p2  xlarge  base  job  name  radix  mnist  fashion  hyperparameters  nw  depth  optimizer  type  adam  dropout  rate  learning  rate  0001  train  data  s3  sagemaker  eu  central  959924085179  radix  mnist  fashion  ,amazon
tutorial  data  mnist  train  tfrecords  eval  data  s3  sagemaker  eu  central  959924085179  radix  mnist  fashion  tutorial  data  mnist  validation  tfrecords  estimator  fit  train  train  data  eval  eval  data  predictor  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  labels  lookup  shirt  top  Trouser  Pullover  Dress  Coat  Sandal  Shirt  Sneaker  Bag  Ankle  boot  import  numpy  as  np  import  json  import  pandas  as  pd  random  image  data  np  random  rand  28  28  prediction  predictor  predict  random  image  data  predicted  class  prediction  outputs  classes  int64  val  class  probabilities  prediction  outputs  probabilities  float  val  pd  DataFrame  selected  if  predicted  class  else  for  in  range  10  label  labels  lookup  probability  class  probabilities  import  tensorflow  as  tf  import  matplotlib  pyplot  as  pltfashion  mnist  tf  keras  datasets  fashion  mnist  train  images  train  labels  test  images  test  labels  fashion  mnist  load,amazon
  data  def  predict  images  for  image  in  images  prediction  predictor  predict  image  predicted  class  prediction  outputs  classes  int64  val  class  probabilities  prediction  outputs  probabilities  float  val  yield  class  probabilitiesdef  plot  image  predictions  array  true  label  img  predictions  array  true  label  img  predictions  array  true  label  img  plt  grid  False  plt  xticks  plt  yticks  plt  imshow  img  cmap  plt  cm  binary  predicted  label  np  argmax  predictions  array  if  predicted  label  true  label  color  blue  else  color  red  plt  xlabel  0f  format  labels  lookup  predicted  label  100  np  max  predictions  array  labels  lookup  true  label  color  color  def  plot  value  array  predictions  array  true  label  predictions  array  true  label  predictions  array  true  label  plt  grid  False  plt  xticks  plt  yticks  thisplot  plt  bar  range  10  predictions  array  color  777777  plt  ylim  predicted  label  np  argmax  predictions  array  thisplot  ,amazon
predicted  label  set  color  red  thisplot  true  label  set  color  blue  predictions  list  predict  test  images  num  rows  num  cols  num  images  num  rows  num  cols  plt  figure  figsize  num  cols  num  rows  for  in  range  num  images  plt  subplot  num  rows  num  cols  plot  image  predictions  test  labels  test  images  plt  subplot  num  rows  num  cols  plot  value  array  predictions  test  labels  ,amazon
from  sagemaker  import  get  execution  role  role  get  execution  role  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  30  This  is  format  train  set  30  time  import  io  import  boto3  from  sagemaker  amazon  common  import  write  numpy  to  dense  tensor  bucket  batch  transform  bucket  URL  s3  bucket  trainValTest  folder  KMexample  data  training  data  key  trainValTest  folder  train  data  training  data  URL  s3  bucket  training  data  key  train,amazon
ing  data  folder  s3  bucket  trainValTest  folder  model  URL  bucket  URL  KMexample  model  print  training  data  will  be  uploaded  to  format  training  data  URL  Convert  the  training  data  into  the  format  required  by  the  SageMaker  KMeans  algorithm  buf  io  BytesIO  write  numpy  to  dense  tensor  buf  train  set  train  set  buf  seek  boto3  resource  s3  Bucket  bucket  Object  training  data  key  upload  fileobj  buf  print  training  data  folder  from  sagemaker  import  KMeans  kmeans  KMeans  role  role  train  instance  count  train  instance  type  ml  c4  8xlarge  output  path  model  URL  10  data  location  training  data  URL  time  kmeans  fit  kmeans  record  set  train  set  kmeans  latest  training  job  job  name  modelURL  output  model  tar  gz  format  model  URL  kmeans  latest  training  job  job  name  modelURLsagemaker  boto3  client  sagemaker  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  image  get  image  uri  boto3  Session  region  ,amazon
name  kmeans  kmeans  hosting  container  Image  image  ModelDataUrl  modelURL  kmeans  hosting  containercreate  model  response  sagemaker  create  model  ModelName  kmeans  latest  training  job  job  name  ExecutionRoleArn  role  PrimaryContainer  kmeans  hosting  container  import  sagemaker  model  name  kmeans  latest  training  job  job  name  bucket  batch  transform  validation  file  validation  data  csv  validation  key  KMexample  data  validation  file  prediction  folder  KMexample  prediction  prediction  key  prediction  folder  validation  file  out  Convert  the  validation  set  numpy  array  to  csv  file  and  upload  to  s3  numpy  savetxt  validation  file  valid  set  delimiter  fmt  s3  client  boto3  client  s3  s3  client  upload  file  validation  file  bucket  validation  key  input  URL  s3  format  bucket  validation  key  output  URL  s3  format  bucket  prediction  folder  print  input  URL  print  output  URL  Initialize  the  transformer  object  transformer  sagemaker  tr,amazon
ansformer  Transformer  base  transform  job  name  Batch  Transform  model  name  model  name  instance  count  instance  type  ml  c4  xlarge  output  path  output  URL  time  To  start  transform  job  transformer  transform  input  URL  content  type  text  csv  split  type  Line  Then  wait  until  transform  job  is  completed  transformer  wait  To  fetch  validation  result  s3  client  download  file  bucket  prediction  key  valid  result  with  open  valid  result  as  results  readlines  print  Sample  transform  result  format  results  ,amazon
import  threading  import  boto3  import  json  import  numpy  as  np  import  time  import  math  from  multiprocessing  pool  import  ThreadPool  from  sklearn  import  datasetssm  boto3  client  sagemaker  runtime  codepipeline  boto3  client  codepipeline  endpoint  name  mlops  iris  model  prd  pipeline  name  mlops  iris  model  iris  datasets  load  iris  dataset  np  insert  iris  data  iris  target  axis  def  get  env  jobid  env  response  codepipeline  get  pipeline  state  name  pipeline  name  for  stage  in  response  stageStates  if  stage  stageName  Deploy  env  capitalize  for  action  in  stage  actionStates  if  action  actionName  DeployModel  env  capitalize  return  stage  latestExecution  pipelineExecutionId  def  predict  payload  payload  payload  payload  payload  response  elapsed  time  time  time  resp  sm  invoke  endpoint  EndpointName  endpoint  name  env  Body  json  dumps  algorithm  logistic  payload  elapsed  time  time  time  elapsed  time  resp  json  loads  resp  Body,amazon
  read  response  append  resp  iris  id  elapsed  time  elapsed  time  time  time  resp  sm  invoke  endpoint  EndpointName  endpoint  name  env  Body  json  dumps  algorithm  random  forest  payload  elapsed  time  time  time  elapsed  time  resp  json  loads  resp  Body  read  response  append  resp  iris  id  elapsed  time  return  responsedef  run  test  max  threads  max  requests  num  batches  math  ceil  max  requests  len  dataset  requests  for  in  range  num  batches  batch  dataset  copy  np  random  shuffle  batch  requests  batch  tolist  len  requests  pool  ThreadPool  max  threads  result  pool  map  predict  requests  pool  close  pool  join  correct  logistic  correct  random  forest  elapsedtime  logistic  elapsedtime  random  forest  for  in  result  correct  logistic  correct  random  forest  elapsedtime  logistic  elapsedtime  random  forest  print  Score  logistic  format  correct  logistic  len  result  print  Score  random  forest  format  correct  random  forest  len  result  prin,amazon
t  Elapsed  time  logistic  format  elapsedtime  logistic  print  Elapsed  time  random  forest  format  elapsedtime  random  forest  env  get  env  jobid  prd  time  print  Starting  test  run  test  10  1000  time  print  Starting  test  run  test  100  10000  time  print  Starting  test  run  test  150  100000  ,amazon
print  test  ,ibm
Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  sagemaker  periscopedata  demo  nyc  data  key  enhancedtotodataset  csv  data  location  s3  format  bucket  data  key  set  prefix  for  this  instance  please  input  your  name  in  the  following  set  of  square  brackets  making  sure  to  use  appropriate  directory  characters  prefix  sagemaker  your  name  here  xgboost  batch  dm  import  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  from  io  import  BytesIO  import  os  import  time  import  json  import  sagemaker  amazon  common  as  smac  import  sagemaker  from  sagemaker  predictor  import  csv  serializer  json  deserializer  read  the  csv  from  S3  df  pd  read  csv  data  location  display  the  first  records  to  verify  the  import  df  head  some  of  the  categorical  variables  are  currently  encoded  as  numeric  The  number  of  categories  is  low  and  can  easily  be ,amazon
 one  hot  encoded  using  get  dummy  categorical  columns  max  dog  size  min  dog  size  requester  gender  provider  gender  experience  continuous  walk  count  dog  count  requester  fee  previous  client  count  price  per  walk  provider  fee  percent  morning  walks  percent  afternoon  walks  percent  evening  walks  df  pd  get  dummies  df  columns  max  dog  size  min  dog  size  requester  gender  provider  gender  experience  verify  that  the  one  hot  encoding  creation  of  boolean  for  each  categorical  variable  succeeded  df  head  train  data  validation  data  test  data  np  split  df  sample  frac  random  state  1729  int  len  df  int  len  df  pd  concat  train  data  lifetime  revenue  train  data  drop  lifetime  revenue  axis  axis  to  csv  train  csv  index  False  header  False  pd  concat  validation  data  lifetime  revenue  validation  data  drop  lifetime  revenue  axis  axis  to  csv  validation  csv  index  False  header  False  test  data  drop  lifetime  revenue  ,amazon
axis  to  csv  test  csv  index  False  header  False  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  train  csv  upload  file  train  csv  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  validation  csv  upload  file  validation  csv  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  test  test  csv  upload  file  test  csv  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  xgboost  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  xgboost  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  xgboost  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  xgboost  latest  ap  northeast  501404015308  dkr  ecr  ap  northeast  amazonaws  com  xgboost  latest  ap  northeast  306986355934  dkr  ecr  ap  northeast  amazonaws  com  xgboost  latest  s3  input  train  sagemaker  s3  input  s3  data  s3  train  format  bucket  prefix  content  ,amazon
type  csv  s3  input  validation  sagemaker  s3  input  s3  data  s3  validation  format  bucket  prefix  content  type  csv  sess  sagemaker  Session  xgb  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  role  train  instance  count  train  instance  type  ml  m4  xlarge  output  path  s3  output  format  bucket  prefix  sagemaker  session  sess  xgb  set  hyperparameters  max  depth  eta  gamma  min  child  weight  subsample  silent  objective  reg  linear  use  linear  regression  to  create  continuous  output  num  round  100  xgb  fit  train  s3  input  train  validation  s3  input  validation  xgb  predictor  xgb  deploy  initial  instance  count  instance  type  ml  m4  xlarge  time  from  time  import  gmtime  strftime  input  prefix  prefix  test  csv  file  test  csv  input  data  s3  format  bucket  input  prefix  output  prefix  prefix  xgboost  batch  test  output  output  data  s3  format  bucket  output  prefix  Important  Update  this  value  with  the  model  name ,amazon
 from  the  output  of  the  hosting  step  model  name  xgboost  2018  07  17  09  08  32  655  job  name  model  name  batch  job  name  xgboost  batch  strftime  gmtime  batch  boto3  client  sagemaker  create  params  TransformJobName  batch  job  name  ModelName  model  name  MaxConcurrentTransforms  BatchStrategy  MultiRecord  TransformInput  ContentType  text  csv  SplitType  Line  CompressionType  None  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  input  data  TransformOutput  S3OutputPath  output  data  AssembleWith  Line  TransformResources  InstanceCount  InstanceType  ml  m4  xlarge  print  Job  name  is  job  name  batch  create  transform  job  create  params  import  time  def  describe  job  name  batch  describe  transform  job  TransformJobName  job  name  pop  ResponseMetadata  return  def  wait  for  job  name  sleep  time  30  while  True  desc  describe  job  name  print  Status  format  desc  TransformJobStatus  if  desc  TransformJobStatus  InProgress  break  time  sleep  sle,amazon
ep  time  return  desc  time  import  yaml  desc  wait  for  batch  job  name  print  print  yaml  dump  desc  default  flow  style  False  part  file  csv  file  out  boto3  resource  s3  Bucket  bucket  Object  format  output  prefix  part  file  download  file  part  file  import  pandas  as  pd  predictions  pd  read  csv  part  file  header  None  predictions  columns  predictions  predictions  head  xgb  predictor  content  type  text  csv  xgb  predictor  serializer  csv  serializerdef  predict  data  rows  500  split  array  np  array  split  data  int  data  shape  float  rows  predictions  for  array  in  split  array  predictions  join  predictions  xgb  predictor  predict  array  decode  utf  return  np  fromstring  predictions  sep  export  test  predictions  test  data  with  pred  test  data  test  data  with  pred  insert  predictions  predictions  test  data  with  pred  head  test  data  with  pred  to  csv  toto  test  predictions  csv  index  False  header  True  export  train  data  with ,amazon
 predictiona  train  predictions  predict  train  data  drop  lifetime  revenue  axis  values  train  data  with  pred  train  data  train  data  with  pred  insert  predictions  train  predictions  train  data  with  pred  head  train  data  with  pred  to  csv  toto  train  predictions  csv  index  False  header  True  def  rmse  predictions  actuals  rmse  predictions  actuals  mean  return  rmse  rmse  predictions  np  round  predictions  predictions  actuals  test  data  lifetime  revenue  import  seaborn  as  sns  import  matplotlib  pyplot  as  plt  import  numpy  as  np  from  scipy  import  stats  plt  figure  figsize  plt  gca  set  aspect  equal  adjustable  box  max  lim  max  int  np  max  np  round  predictions  predictions  int  np  max  test  data  lifetime  revenue  np  linspace  max  lim  10  plt  plot  linewidth  linestyle  alpha  label  Actual  regression  part  sns  regplot  np  round  predictions  predictions  test  data  lifetime  revenue  color  purple  label  Prediction  plt  xlabel  ,amazon
Predictions  plt  ylabel  Actual  plt  title  Predictive  vs  Actual  Lifetime  Revenue  plt  legend  plt  show  import  seaborn  as  sns  plt  figure  figsize  sns  residplot  np  round  predictions  predictions  test  data  lifetime  revenue  color  purple  plt  xlabel  LTV  plt  ylabel  Residual  plt  title  Residual  Plot  sagemaker  Session  delete  endpoint  xgb  predictor  endpoint  ,amazon
conda  install  scipy  matplotlib  inline  import  os  re  tarfile  import  boto3  import  matplotlib  pyplot  as  plt  import  mxnet  as  mx  import  numpy  as  np  np  set  printoptions  precision  suppress  True  some  helpful  utility  functions  are  defined  in  the  Python  module  generate  example  data  located  in  the  same  directory  as  this  notebook  from  generate  example  data  import  generate  griffiths  data  match  estimated  topics  plot  lda  plot  lda  topics  accessing  the  SageMaker  Python  SDK  import  sagemaker  from  sagemaker  amazon  common  import  numpy  to  record  serializer  from  sagemaker  predictor  import  csv  serializer  json  deserializerfrom  sagemaker  import  get  execution  role  role  get  execution  role  bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  lda  science  print  Training  input  output  will  be  stored  in  format  bucket  prefix  print  nIAM  Role  format  role  print  Generating  example  data  num  documents  6000  known  alph,amazon
a  known  beta  documents  topic  mixtures  generate  griffiths  data  num  documents  num  documents  num  topics  10  num  topics  vocabulary  size  known  beta  shape  separate  the  generated  data  into  training  and  tests  subsets  num  documents  training  int  num  documents  num  documents  test  num  documents  num  documents  training  documents  training  documents  num  documents  training  documents  test  documents  num  documents  training  topic  mixtures  training  topic  mixtures  num  documents  training  topic  mixtures  test  topic  mixtures  num  documents  training  print  documents  training  shape  format  documents  training  shape  print  documents  test  shape  format  documents  test  shape  print  First  training  document  format  documents  training  print  nVocabulary  size  format  vocabulary  size  print  Length  of  first  document  format  documents  training  sum  average  document  length  documents  sum  axis  mean  print  Observed  average  document  length  format ,amazon
 average  document  length  print  First  topic  format  known  beta  print  nTopic  word  probability  matrix  beta  shape  num  topics  vocabulary  size  format  known  beta  shape  print  nSum  of  elements  of  first  topic  format  known  beta  sum  print  Topic  format  known  beta  print  Topic  format  known  beta  matplotlib  inline  fig  plot  lda  documents  training  nrows  ncols  cmap  gray  with  colorbar  True  fig  suptitle  Document  Word  Counts  fig  set  dpi  160  matplotlib  inline  fig  plot  lda  known  beta  nrows  ncols  10  fig  suptitle  Known  beta  Topic  Word  Probability  Distributions  fig  set  dpi  160  fig  set  figheight  print  First  training  document  format  documents  training  print  nVocabulary  size  format  vocabulary  size  print  Length  of  first  document  format  documents  training  sum  print  First  training  document  topic  mixture  format  topic  mixtures  training  print  nNumber  of  topics  format  num  topics  print  sum  theta  format  topic  mixtu,amazon
res  training  sum  matplotlib  inline  fig  ax1  ax2  plt  subplots  ax1  matshow  documents  reshape  cmap  gray  ax1  set  title  Document  fontsize  20  ax1  set  xticks  ax1  set  yticks  cax2  ax2  matshow  topic  mixtures  reshape  cmap  Reds  vmin  vmax  cbar  fig  colorbar  cax2  orientation  horizontal  ax2  set  title  theta  Topic  Mixture  fontsize  20  ax2  set  xticks  ax2  set  yticks  fig  set  dpi  100  matplotlib  inline  pot  fig  plot  lda  known  beta  nrows  ncols  10  fig  suptitle  Known  beta  Topic  Word  Probability  Distributions  fig  set  dpi  160  fig  set  figheight  matplotlib  inline  fig  plot  lda  topics  documents  training  topic  mixtures  topic  mixtures  fig  suptitle  theta  Documents  with  Known  Topic  Mixtures  fig  set  dpi  160  convert  documents  training  to  Protobuf  RecordIO  format  recordio  protobuf  serializer  numpy  to  record  serializer  fbuffer  recordio  protobuf  serializer  documents  training  upload  to  S3  in  bucket  prefix  train  fname,amazon
  lda  data  s3  object  os  path  join  prefix  train  fname  boto3  Session  resource  s3  Bucket  bucket  Object  s3  object  upload  fileobj  fbuffer  s3  train  data  s3  format  bucket  s3  object  print  Uploaded  data  to  S3  format  s3  train  data  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  region  name  boto3  Session  region  name  container  get  image  uri  boto3  Session  region  name  lda  print  Using  SageMaker  LDA  container  format  container  region  name  session  sagemaker  Session  specify  general  training  job  information  lda  sagemaker  estimator  Estimator  container  role  output  path  s3  output  format  bucket  prefix  train  instance  count  train  instance  type  ml  c4  2xlarge  sagemaker  session  session  set  algorithm  specific  hyperparameters  lda  set  hyperparameters  num  topics  num  topics  feature  dim  vocabulary  size  mini  batch  size  num  documents  training  alpha0  run  the  training  job  on  input  data  stored  in  S3  ld,amazon
a  fit  train  s3  train  data  print  Training  job  name  format  lda  latest  training  job  job  name  download  and  extract  the  model  file  from  S3  job  name  lda  latest  training  job  job  name  model  fname  model  tar  gz  model  object  os  path  join  prefix  output  job  name  output  model  fname  boto3  Session  resource  s3  Bucket  bucket  Object  model  object  download  file  fname  with  tarfile  open  fname  as  tar  tar  extractall  print  Downloaded  and  extracted  model  tarball  format  model  object  obtain  the  model  file  model  list  fname  for  fname  in  os  listdir  if  fname  startswith  model  model  fname  model  list  print  Found  model  file  format  model  fname  get  the  model  from  the  model  file  and  store  in  Numpy  arrays  alpha  beta  mx  ndarray  load  model  fname  learned  alpha  permuted  alpha  asnumpy  learned  beta  permuted  beta  asnumpy  print  nLearned  alpha  shape  format  learned  alpha  permuted  shape  print  Learned  beta  shape  for,amazon
mat  learned  beta  permuted  shape  permutation  learned  beta  match  estimated  topics  known  beta  learned  beta  permuted  learned  alpha  learned  alpha  permuted  permutation  fig  plot  lda  np  vstack  known  beta  learned  beta  10  fig  set  dpi  160  fig  suptitle  Known  vs  Found  Topic  Word  Probability  Distributions  fig  set  figheight  beta  error  np  linalg  norm  known  beta  learned  beta  alpha  error  np  linalg  norm  known  alpha  learned  alpha  print  L1  error  beta  format  beta  error  print  L1  error  alpha  format  alpha  error  lda  inference  lda  deploy  initial  instance  count  instance  type  ml  m4  xlarge  LDA  inference  may  work  better  at  scale  on  ml  c4  instances  print  Endpoint  name  format  lda  inference  endpoint  lda  inference  content  type  text  csv  lda  inference  serializer  csv  serializer  lda  inference  deserializer  json  deserializerresults  lda  inference  predict  documents  test  12  print  results  inferred  topic  mixtures  permut,amazon
ed  np  array  prediction  topic  mixture  for  prediction  in  results  predictions  print  Inferred  topic  mixtures  permuted  format  inferred  topic  mixtures  permuted  inferred  topic  mixtures  inferred  topic  mixtures  permuted  permutation  print  Inferred  topic  mixtures  format  inferred  topic  mixtures  matplotlib  inline  create  array  of  bar  plots  width  np  arange  10  nrows  ncols  fig  ax  plt  subplots  nrows  ncols  sharey  True  for  in  range  nrows  for  in  range  ncols  index  ncols  ax  bar  topic  mixtures  test  index  width  color  C0  ax  bar  width  inferred  topic  mixtures  index  width  color  C1  ax  set  xticks  range  num  topics  ax  set  yticks  np  linspace  ax  grid  which  major  axis  ax  set  ylim  ax  set  xticklabels  if  nrows  ax  set  xticklabels  range  num  topics  fontsize  if  ax  set  yticklabels  fontsize  fig  suptitle  Known  vs  Inferred  Topic  Mixtures  ax  super  fig  add  subplot  111  frameon  False  ax  super  tick  params  labelcolor  non,amazon
e  top  off  bottom  off  left  off  right  off  ax  super  grid  False  ax  super  set  xlabel  Topic  Index  ax  super  set  ylabel  Topic  Probability  fig  set  dpi  160  time  create  payload  containing  all  of  the  test  documents  and  run  inference  again  TRY  THIS  try  switching  between  the  test  data  set  and  subset  of  the  training  data  set  It  is  likely  that  LDA  inference  will  perform  better  against  the  training  set  than  the  holdout  test  set  payload  documents  documents  test  Example  known  topic  mixtures  topic  mixtures  test  Example  payload  documents  documents  training  600  Example  known  topic  mixtures  topic  mixtures  training  600  Example  print  Invoking  endpoint  results  lda  inference  predict  payload  documents  inferred  topic  mixtures  permuted  np  array  prediction  topic  mixture  for  prediction  in  results  predictions  inferred  topic  mixtures  inferred  topic  mixtures  permuted  permutation  print  known  topics  mixtures  sh,amazon
ape  format  known  topic  mixtures  shape  print  inferred  topics  mixtures  test  shape  format  inferred  topic  mixtures  shape  matplotlib  inline  l1  errors  np  linalg  norm  inferred  topic  mixtures  known  topic  mixtures  axis  plot  the  error  freqency  fig  ax  frequency  plt  subplots  bins  np  linspace  40  weights  np  ones  like  l1  errors  len  l1  errors  freq  bins  ax  frequency  hist  l1  errors  bins  50  weights  weights  color  C0  ax  frequency  set  xlabel  L1  Error  ax  frequency  set  ylabel  Frequency  color  C0  plot  the  cumulative  error  shift  bins  bins  bins  shift  ax  cumulative  ax  frequency  twinx  cumulative  np  cumsum  freq  sum  freq  ax  cumulative  plot  cumulative  marker  color  C1  ax  cumulative  set  ylabel  Cumulative  Frequency  color  C1  align  grids  and  show  freq  ticks  np  linspace  freq  max  freq  ticklabels  np  round  100  freq  ticks  100  ax  frequency  set  yticks  freq  ticks  ax  frequency  set  yticklabels  freq  ticklabels  ax  c,amazon
umulative  set  yticks  np  linspace  ax  cumulative  grid  which  major  axis  ax  cumulative  set  ylim  fig  suptitle  Topic  Mixutre  L1  Errors  fig  set  dpi  110  good  idx  l1  errors  05  good  documents  payload  documents  good  idx  good  topic  mixtures  inferred  topic  mixtures  good  idx  poor  idx  l1  errors  poor  documents  payload  documents  poor  idx  poor  topic  mixtures  inferred  topic  mixtures  poor  idx  matplotlib  inline  fig  plot  lda  topics  good  documents  topic  mixtures  good  topic  mixtures  fig  suptitle  Documents  With  Accurate  Inferred  Topic  Mixtures  fig  set  dpi  120  matplotlib  inline  fig  plot  lda  topics  poor  documents  topic  mixtures  poor  topic  mixtures  fig  suptitle  Documents  With  Inaccurate  Inferred  Topic  Mixtures  fig  set  dpi  120  sagemaker  Session  delete  endpoint  lda  inference  endpoint  ,amazon
time  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  bucket  name  customize  to  your  bucket  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  image  classification  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  image  classification  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  image  classification  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  image  classification  latest  training  image  containers  boto3  Session  region  name  import  os  import  urllib  request  import  boto3  def  download  url  filename  url  split  if  not  os  path  exists  filename  urllib  request  urlretrieve  url  filename  def  upload  to  s3  channel  file  s3  boto3  resource  s3  data  open  file  rb  key  channel  file  s3  Bucket  bucket  put  object  Key  key  Body  data  caltech  256  download  http  data  mxnet  io  data  caltech  256  caltech  256  60  train  rec ,amazon
 upload  to  s3  train  caltech  256  60  train  rec  download  http  data  mxnet  io  data  caltech  256  caltech  256  60  val  rec  upload  to  s3  validation  caltech  256  60  val  rec  The  algorithm  supports  multiple  network  depth  number  of  layers  They  are  18  34  50  101  152  and  200  For  this  training  we  will  use  18  layers  num  layers  18  we  need  to  specify  the  input  image  shape  for  the  training  data  image  shape  224  224  we  also  need  to  specify  the  number  of  training  samples  in  the  training  set  for  caltech  it  is  15420  num  training  samples  15420  specify  the  number  of  output  classes  num  classes  257  batch  size  for  training  mini  batch  size  64  number  of  epochs  epochs  learning  rate  learning  rate  01  time  import  time  import  boto3  from  time  import  gmtime  strftime  s3  boto3  client  s3  create  unique  job  name  job  name  prefix  DEMO  imageclassification  timestamp  time  strftime  time  gmtime  job  name  job  na,amazon
me  prefix  timestamp  training  params  specify  the  training  docker  image  AlgorithmSpecification  TrainingImage  training  image  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  s3  output  format  bucket  job  name  prefix  ResourceConfig  InstanceCount  InstanceType  ml  p2  xlarge  VolumeSizeInGB  50  TrainingJobName  job  name  HyperParameters  image  shape  image  shape  num  layers  str  num  layers  num  training  samples  str  num  training  samples  num  classes  str  num  classes  mini  batch  size  str  mini  batch  size  epochs  str  epochs  learning  rate  str  learning  rate  StoppingCondition  MaxRuntimeInSeconds  360000  Training  data  should  be  inside  subdirectory  called  train  Validation  data  should  be  inside  subdirectory  called  validation  The  algorithm  currently  only  supports  fullyreplicated  model  where  data  is  copied  onto  each  machine  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  tra,amazon
in  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  recordio  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  recordio  CompressionType  None  print  Training  job  name  format  job  name  print  nInput  Data  Location  format  training  params  InputDataConfig  DataSource  S3DataSource  create  the  Amazon  SageMaker  training  job  sagemaker  boto3  client  service  name  sagemaker  sagemaker  create  training  job  training  params  confirm  that  the  training  job  has  started  status  sagemaker  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  Training  job  current  status  format  status  try  wait  for  the  job  to  finish  and  report  the  ending  status  sagemaker  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  training  info  sagemaker  des,amazon
cribe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  except  print  Training  failed  to  start  if  exception  is  raised  that  means  it  has  failed  message  sagemaker  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  time  import  boto3  from  time  import  gmtime  strftime  sage  boto3  Session  client  service  name  sagemaker  model  name  DEMO  full  image  classification  model  print  model  name  info  sage  describe  training  job  TrainingJobName  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  data  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  image  classification  latest  us  east  81128,amazon
4229777  dkr  ecr  us  east  amazonaws  com  image  classification  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  image  classification  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  image  classification  latest  hosting  image  containers  boto3  Session  region  name  primary  container  Image  hosting  image  ModelDataUrl  model  data  create  model  response  sage  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  timestamp  time  strftime  time  gmtime  endpoint  config  name  job  name  prefix  epc  timestamp  endpoint  config  response  sage  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  configuration  name  format  endpoint  config  name  print  Endpoint  configuration ,amazon
 arn  format  endpoint  config  response  EndpointConfigArn  time  import  time  timestamp  time  strftime  time  gmtime  endpoint  name  job  name  prefix  ep  timestamp  print  Endpoint  name  format  endpoint  name  endpoint  params  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  endpoint  response  sagemaker  create  endpoint  endpoint  params  print  EndpointArn  format  endpoint  response  EndpointArn  get  the  status  of  the  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  response  EndpointStatus  print  EndpointStatus  format  status  wait  until  the  status  has  changed  sagemaker  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  print  the  status  of  the  endpoint  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  endpoint  response  EndpointStatus  print  Endpoint  creation  ended  with  EndpointStatus  format  status  if  status  InService  raise  Exception  Endp,amazon
oint  creation  failed  import  boto3  runtime  boto3  Session  client  service  name  runtime  sagemaker  wget  tmp  test  jpg  http  www  vision  caltech  edu  Image  Datasets  Caltech256  images  008  bathtub  008  0007  jpg  file  name  tmp  test  jpg  test  image  from  IPython  display  import  Image  Image  file  name  import  json  import  numpy  as  np  with  open  file  name  rb  as  payload  read  payload  bytearray  payload  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  image  Body  payload  result  response  Body  read  result  will  be  in  json  format  and  convert  it  to  ndarray  result  json  loads  result  the  result  will  output  the  probabilities  for  all  classes  find  the  class  with  maximum  probability  and  print  the  class  index  index  np  argmax  result  object  categories  ak47  american  flag  backpack  baseball  bat  baseball  glove  basketball  hoop  bat  bathtub  bear  beer  mug  billiards  binoculars  birdbath  blimp ,amazon
 bonsai  101  boom  box  bowling  ball  bowling  pin  boxing  glove  brain  101  breadmaker  buddha  101  bulldozer  butterfly  cactus  cake  calculator  camel  cannon  canoe  car  tire  cartman  cd  centipede  cereal  box  chandelier  101  chess  board  chimp  chopsticks  cockroach  coffee  mug  coffin  coin  comet  computer  keyboard  computer  monitor  computer  mouse  conch  cormorant  covered  wagon  cowboy  hat  crab  101  desk  globe  diamond  ring  dice  dog  dolphin  101  doorknob  drinking  straw  duck  dumb  bell  eiffel  tower  electric  guitar  101  elephant  101  elk  ewer  101  eyeglasses  fern  fighter  jet  fire  extinguisher  fire  hydrant  fire  truck  fireworks  flashlight  floppy  disk  football  helmet  french  horn  fried  egg  frisbee  frog  frying  pan  galaxy  gas  pump  giraffe  goat  golden  gate  bridge  goldfish  golf  ball  goose  gorilla  grand  piano  101  grapes  grasshopper  guitar  pick  hamburger  hammock  harmonica  harp  harpsichord  hawksbill  101  head  phones  helicop,amazon
ter  101  hibiscus  homer  simpson  horse  horseshoe  crab  hot  air  balloon  hot  dog  hot  tub  hourglass  house  fly  human  skeleton  hummingbird  ibis  101  ice  cream  cone  iguana  ipod  iris  jesus  christ  joy  stick  kangaroo  101  kayak  ketch  101  killer  whale  knife  ladder  laptop  101  lathe  leopards  101  license  plate  lightbulb  light  house  lightning  llama  101  mailbox  mandolin  mars  mattress  megaphone  menorah  101  microscope  microwave  minaret  minotaur  motorbikes  101  mountain  bike  mushroom  mussels  necktie  octopus  ostrich  owl  palm  pilot  palm  tree  paperclip  paper  shredder  pci  card  penguin  people  pez  dispenser  photocopier  picnic  table  playing  card  porcupine  pram  praying  mantis  pyramid  raccoon  radio  telescope  rainbow  refrigerator  revolver  101  rifle  rotary  phone  roulette  wheel  saddle  saturn  school  bus  scorpion  101  screwdriver  segway  self  propelled  lawn  mower  sextant  sheet  music  skateboard  skunk  skyscraper  smokestack ,amazon
 snail  snake  sneaker  snowmobile  soccer  ball  socks  soda  can  spaghetti  speed  boat  spider  spoon  stained  glass  starfish  101  steering  wheel  stirrups  sunflower  101  superman  sushi  swan  swiss  army  knife  sword  syringe  tambourine  teapot  teddy  bear  teepee  telephone  box  tennis  ball  tennis  court  tennis  racket  theodolite  toaster  tomato  tombstone  top  hat  touring  bike  tower  pisa  traffic  light  treadmill  triceratops  tricycle  trilobite  101  tripod  shirt  tuning  fork  tweezer  umbrella  101  unicorn  vcr  video  projector  washing  machine  watch  101  waterfall  watermelon  welding  mask  wheelbarrow  windmill  wine  bottle  xylophone  yarmulke  yo  yo  zebra  airplanes  101  car  side  101  faces  easy  101  greyhound  tennis  shoes  toad  clutter  print  Result  label  object  categories  index  probability  str  result  index  sage  delete  endpoint  EndpointName  endpoint  name  ,amazon
import  os  import  sagemaker  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  DEMO  abalone  cat  abalone  py  from  sagemaker  tensorflow  import  TensorFlow  abalone  estimator  TensorFlow  entry  point  abalone  py  role  role  framework  version  training  steps  100  evaluation  steps  100  hyperparameters  learning  rate  001  train  instance  count  train  instance  type  ml  c4  xlarge  abalone  estimator  fit  inputs  abalone  predictor  abalone  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  import  tensorflow  as  tf  import  numpy  as  np  prediction  set  tf  contrib  learn  datasets  base  load  csv  without  header  filename  os  path  join  data  abalone  predict  csv  target  dtype  np  int  features  dtype  np  float32  data  prediction  set  data  tensor  proto  tf  make  tensor  proto  values  np  asarray  data  shape  le,amazon
n  data  dtype  tf  float32  abalone  predictor  predict  tensor  proto  sagemaker  Session  delete  endpoint  abalone  predictor  endpoint  ,amazon
if  require  mFilter  install  packages  mFilter  library  mFilter  data  unemp  opar  par  no  readonly  TRUE  unemp  hp  hpfilter  unemp  plot  unemp  hp  unemp  hp1  hpfilter  unemp  drift  TRUE  unemp  hp2  hpfilter  unemp  freq  800  drift  TRUE  unemp  hp3  hpfilter  unemp  freq  12  type  frequency  drift  TRUE  unemp  hp4  hpfilter  unemp  freq  52  type  frequency  drift  TRUE  par  mfrow  mar  cex  plot  unemp  hp1  ylim  13  main  Hodrick  Prescott  filter  of  unemployment  Trend  drift  TRUE  col  ylab  lines  unemp  hp1  trend  col  lines  unemp  hp2  trend  col  lines  unemp  hp3  trend  col  lines  unemp  hp4  trend  col  legend  topleft  legend  series  lambda  1600  lambda  800  freq  12  freq  52  col  lty  rep  ncol  cycle  plot  unemp  hp1  cycle  main  Hodrick  Prescott  filter  of  unemployment  Cycle  drift  TRUE  col  ylab  ylim  range  unemp  hp4  cycle  na  rm  TRUE  lines  unemp  hp2  cycle  col  lines  unemp  hp3  cycle  col  lines  unemp  hp4  cycle  col  legend  topleft  legend ,microsoft
 lambda  1600  lambda  800  freq  12  freq  52  col  lty  rep  ncol  par  opar  ,microsoft
import  json  import  math  from  os  import  path  import  pandas  as  pdCPU  DF  PATH  path  join  path  curdir  dftoformat  csv  cpu  use  df  pd  read  csv  CPU  DF  PATH  cpu  use  df  head  cpu  use  timeseries  vm  index  range  cpu  use  df  VM  unique  for  vm  in  vm  index  range  vm  series  cpu  use  df  cpu  use  df  VM  vm  some  data  is  invalid  and  thus  needs  to  be  set  to  something  it  could  be  the  previous  num  for  normalization  or  removed  target  list  for  num  in  vm  series  target  if  math  isnan  num  False  target  list  append  float  num  else  target  list  append  cpu  use  timeseries  append  start  vm  series  Timestamp  tolist  the  starting  timestamp  target  target  list  these  need  to  be  converted  to  floats  for  json  cat  vm  if  we  used  categories  we  could  do  this  with  open  cpu  use  timeseries  json  as  new  file  new  file  write  json  dumps  cpu  use  timeseries  ,amazon
import  os  import  sagemaker  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  DEMO  abalone  cat  abalone  py  from  sagemaker  tensorflow  import  TensorFlow  abalone  estimator  TensorFlow  entry  point  abalone  py  role  role  training  steps  100  evaluation  steps  100  hyperparameters  learning  rate  001  train  instance  count  train  instance  type  ml  c4  xlarge  abalone  estimator  fit  inputs  abalone  predictor  abalone  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  import  tensorflow  as  tf  import  numpy  as  np  prediction  set  tf  contrib  learn  datasets  base  load  csv  without  header  filename  os  path  join  data  abalone  predict  csv  target  dtype  np  int  features  dtype  np  float32  data  prediction  set  data  tensor  proto  tf  make  tensor  proto  values  np  asarray  data  shape  len  data  dtype  tf  ,amazon
float32  abalone  predictor  predict  tensor  proto  sagemaker  Session  delete  endpoint  abalone  predictor  endpoint  ,amazon
cat  container  Dockerfile  sh  The  name  of  our  algorithm  algorithm  name  decision  trees  sample  cd  container  chmod  decision  trees  train  chmod  decision  trees  serve  account  aws  sts  get  caller  identity  query  Account  output  text  Get  the  region  defined  in  the  current  configuration  default  to  us  west  if  none  defined  region  aws  configure  get  region  region  region  us  west  fullname  account  dkr  ecr  region  amazonaws  com  algorithm  name  latest  If  the  repository  doesn  exist  in  ECR  create  it  aws  ecr  describe  repositories  repository  names  algorithm  name  dev  null  if  ne  then  aws  ecr  create  repository  repository  name  algorithm  name  dev  null  fi  Get  the  login  command  from  ECR  and  execute  it  directly  aws  ecr  get  login  region  region  no  include  email  Build  the  docker  image  locally  with  the  image  name  and  then  push  it  to  ECR  with  the  full  name  On  SageMaker  Notebook  Instance  the  docker  daemon  may ,amazon
 need  to  be  restarted  in  order  to  detect  your  network  configuration  correctly  This  is  known  issue  if  home  ec2  user  SageMaker  then  sudo  service  docker  restart  fi  docker  build  algorithm  name  docker  tag  algorithm  name  fullname  docker  push  fullname  S3  prefix  prefix  scikit  byo  iris  Define  IAM  role  import  boto3  import  re  import  os  import  numpy  as  np  import  pandas  as  pd  from  sagemaker  import  get  execution  role  role  get  execution  role  roleimport  sagemaker  as  sage  from  time  import  gmtime  strftime  sess  sage  Session  WORK  DIRECTORY  data  data  location  sess  upload  data  WORK  DIRECTORY  key  prefix  prefix  account  sess  boto  session  client  sts  get  caller  identity  Account  region  sess  boto  session  region  name  image  dkr  ecr  amazonaws  com  decision  trees  sample  format  account  region  tree  sage  estimator  Estimator  image  role  ml  c4  2xlarge  output  path  s3  output  format  sess  default  bucket  sagemaker ,amazon
 session  sess  tree  fit  data  location  from  sagemaker  predictor  import  csv  serializer  predictor  tree  deploy  ml  c4  xlarge  serializer  csv  serializer  shape  pd  read  csv  data  iris  csv  header  None  import  itertools  50  for  in  range  40  for  in  range  10  indices  for  in  itertools  product  test  data  shape  iloc  indices  test  test  data  iloc  test  test  data  iloc  print  predictor  predict  test  values  decode  utf  sess  delete  endpoint  predictor  endpoint  ,amazon
import  os  import  tensorflow  as  tf  import  numpy  as  npimport  glob  import  imghdr  datadir  os  path  expanduser  catsanddogs  cat  files  glob  glob  os  path  join  datadir  PetImages  Cat  jpg  dog  files  glob  glob  os  path  join  datadir  PetImages  Dog  jpg  Limit  the  data  set  to  make  the  notebook  execute  quickly  cat  files  cat  files  64  dog  files  dog  files  64  The  data  set  has  few  images  that  are  not  jpeg  Remove  them  cat  files  for  in  cat  files  if  imghdr  what  jpeg  dog  files  for  in  dog  files  if  imghdr  what  jpeg  if  not  len  cat  files  or  not  len  dog  files  print  Please  download  the  Kaggle  Cats  and  Dogs  dataset  form  https  www  microsoft  com  en  us  download  details  aspx  id  54765  and  extract  the  zip  to  datadir  raise  ValueError  Data  not  found  else  print  cat  files  print  dog  files  constructing  numpy  array  as  labels  image  paths  cat  files  dog  files  total  files  len  cat  files  len  dog  files  label,microsoft
s  np  zeros  total  files  labels  len  cat  files  Input  images  as  two  dimensional  tensor  containing  an  arbitrary  number  of  images  represented  strings  import  azureml  contrib  brainwave  models  utils  as  utils  in  images  tf  placeholder  tf  string  image  tensors  utils  preprocess  array  in  images  print  image  tensors  shape  from  azureml  contrib  brainwave  models  import  QuantizedResnet152  model  path  os  path  expanduser  models  bwmodel  QuantizedResnet152  model  path  is  frozen  True  print  bwmodel  version  features  bwmodel  import  graph  def  input  tensor  image  tensors  from  tqdm  import  tqdm  def  chunks  Yield  successive  sized  chunks  from  for  in  range  len  yield  def  read  files  files  contents  for  path  in  files  with  open  path  rb  as  contents  append  read  return  contents  feature  list  with  tf  Session  as  sess  for  chunk  in  tqdm  chunks  image  paths  contents  read  files  chunk  result  sess  run  features  feed  dict  in  image,microsoft
s  contents  feature  list  extend  result  feature  results  np  array  feature  list  print  feature  results  shape  from  keras  models  import  Sequential  from  keras  layers  import  Dropout  Dense  Flatten  from  keras  import  optimizers  FC  SIZE  1024  NUM  CLASSES  model  Sequential  model  add  Dropout  input  shape  2048  model  add  Dense  FC  SIZE  activation  relu  input  dim  2048  model  add  Flatten  model  add  Dense  NUM  CLASSES  activation  sigmoid  input  dim  FC  SIZE  model  compile  optimizer  optimizers  SGD  lr  1e  momentum  loss  binary  crossentropy  metrics  accuracy  from  sklearn  model  selection  import  train  test  split  onehot  labels  np  array  if  else  for  in  labels  train  test  train  test  train  test  split  feature  results  onehot  labels  random  state  42  shuffle  True  print  train  shape  test  shape  train  shape  test  shape  model  fit  train  train  epochs  16  batch  size  32  from  numpy  import  argmax  probs  model  predict  test  prob  max  n,microsoft
p  argmax  probs  test  max  np  argmax  test  print  prob  max  print  test  max  from  sklearn  metrics  import  confusion  matrix  roc  auc  score  accuracy  score  precision  score  recall  score  f1  score  import  itertools  import  matplotlib  from  matplotlib  import  pyplot  as  plt  compute  bunch  of  classification  metrics  def  classification  metrics  true  pred  prob  cm  dict  cm  dict  Accuracy  accuracy  score  true  pred  cm  dict  Precision  precision  score  true  pred  cm  dict  Recall  recall  score  true  pred  cm  dict  F1  f1  score  true  pred  cm  dict  AUC  roc  auc  score  true  prob  cm  dict  Confusion  Matrix  confusion  matrix  true  pred  tolist  return  cm  dict  def  plot  confusion  matrix  cm  classes  normalize  False  title  Confusion  matrix  cmap  plt  cm  Blues  Plots  confusion  matrix  Source  http  scikit  learn  org  stable  auto  examples  model  selection  plot  confusion  matrix  html  New  BSD  License  see  appendix  cm  max  cm  max  cm  min  cm  min  if ,microsoft
 cm  min  cm  min  if  normalize  cm  cm  astype  float  cm  sum  axis  np  newaxis  cm  max  plt  imshow  cm  interpolation  nearest  cmap  cmap  plt  title  title  plt  colorbar  tick  marks  np  arange  len  classes  plt  xticks  tick  marks  classes  rotation  45  plt  yticks  tick  marks  classes  thresh  cm  max  plt  clim  cm  min  cm  max  for  in  itertools  product  range  cm  shape  range  cm  shape  plt  text  round  cm  round  to  decimals  if  they  are  float  horizontalalignment  center  color  white  if  cm  thresh  else  black  plt  ylabel  True  label  plt  xlabel  Predicted  label  plt  show  cm  dict  classification  metrics  test  max  prob  max  probs  for  in  cm  dict  print  cm  dict  cm  np  asarray  cm  dict  Confusion  Matrix  plot  confusion  matrix  cm  fail  pass  normalize  False  from  azureml  contrib  brainwave  pipeline  import  ModelDefinition  TensorflowStage  BrainWaveStage  KerasStage  model  def  ModelDefinition  model  def  pipeline  append  TensorflowStage  tf  Sess,microsoft
ion  in  images  image  tensors  model  def  pipeline  append  BrainWaveStage  tf  Session  bwmodel  model  def  pipeline  append  KerasStage  model  model  def  path  os  path  join  datadir  save  model  def  model  def  save  model  def  path  print  model  def  path  from  azureml  core  model  import  Model  from  azureml  core  import  Workspace  ws  Workspace  from  config  print  ws  name  ws  resource  group  ws  location  ws  subscription  id  sep  model  name  catsanddogs  model  service  name  modelbuild  service  registered  model  Model  register  ws  model  def  path  model  name  from  azureml  core  webservice  import  Webservice  from  azureml  exceptions  import  WebserviceException  from  azureml  contrib  brainwave  import  BrainwaveWebservice  BrainwaveImage  try  service  Webservice  ws  service  name  except  WebserviceException  image  config  BrainwaveImage  image  configuration  deployment  config  BrainwaveWebservice  deploy  configuration  service  Webservice  deploy  from  model ,microsoft
 ws  service  name  registered  model  image  config  deployment  config  service  wait  for  deployment  true  print  service  ipAddress  str  service  port  from  azureml  contrib  brainwave  client  import  PredictionClient  client  PredictionClient  service  ipAddress  service  port  Specify  an  image  to  classify  print  CATS  for  image  file  in  cat  files  results  client  score  image  image  file  result  CORRECT  if  results  results  else  WRONG  print  result  str  results  print  DOGS  for  image  file  in  dog  files  results  client  score  image  image  file  result  CORRECT  if  results  results  else  WRONG  print  result  str  results  service  delete  registered  model  delete  ,microsoft
import  os  import  sagemaker  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  import  utils  utils  cifar10  download  inputs  sagemaker  session  upload  data  path  tmp  cifar10  data  key  prefix  data  DEMO  cifar10  from  sagemaker  tensorflow  import  TensorFlow  source  dir  os  path  join  os  getcwd  source  dir  estimator  TensorFlow  entry  point  resnet  cifar  10  py  source  dir  source  dir  role  role  hyperparameters  throttle  secs  30  training  steps  1000  evaluation  steps  100  train  instance  count  train  instance  type  ml  c4  xlarge  base  job  name  tensorboard  example  estimator  fit  inputs  run  tensorboard  locally  True  predictor  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  import  numpy  as  np  random  image  data  np  random  rand  32  32  predictor  predict  random  image  data  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
import  sagemaker  bucket  sagemaker  Session  default  bucket  prefix  radix  mnist  fashion  tutorial  role  sagemaker  get  execution  role  import  boto3  from  time  import  gmtime  strftime  from  sagemaker  tensorflow  import  TensorFlow  from  sagemaker  tuner  import  IntegerParameter  CategoricalParameter  ContinuousParameter  HyperparameterTuner  cat  cnn  fashion  mnist  py  model  artifacts  location  s3  sagemaker  eu  central  959924085179  radix  mnist  fashion  tutorial  artifacts  custom  code  location  s3  sagemaker  eu  central  959924085179  radix  mnist  fashion  tutorial  artifacts  estimator  TensorFlow  entry  point  cnn  fashion  mnist  py  role  role  input  mode  Pipe  training  steps  30000  evaluation  steps  100  train  instance  count  train  instance  type  ml  p2  xlarge  base  job  name  radix  mnist  fashion  hyperparameters  nw  depth  optimizer  type  adam  dropout  rate  learning  rate  0001  train  data  s3  sagemaker  eu  central  959924085179  radix  mnist  fashion  ,amazon
tutorial  data  mnist  train  tfrecords  eval  data  s3  sagemaker  eu  central  959924085179  radix  mnist  fashion  tutorial  data  mnist  validation  tfrecords  estimator  fit  train  train  data  eval  eval  data  predictor  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  labels  lookup  shirt  top  Trouser  Pullover  Dress  Coat  Sandal  Shirt  Sneaker  Bag  Ankle  boot  import  numpy  as  np  import  json  import  pandas  as  pd  random  image  data  np  random  rand  28  28  prediction  predictor  predict  random  image  data  predicted  class  prediction  outputs  classes  int64  val  class  probabilities  prediction  outputs  probabilities  float  val  pd  DataFrame  selected  if  predicted  class  else  for  in  range  10  label  labels  lookup  probability  class  probabilities  import  tensorflow  as  tf  import  matplotlib  pyplot  as  pltfashion  mnist  tf  keras  datasets  fashion  mnist  train  images  train  labels  test  images  test  labels  fashion  mnist  load,amazon
  data  def  predict  images  for  image  in  images  prediction  predictor  predict  image  predicted  class  prediction  outputs  classes  int64  val  class  probabilities  prediction  outputs  probabilities  float  val  yield  class  probabilitiesdef  plot  image  predictions  array  true  label  img  predictions  array  true  label  img  predictions  array  true  label  img  plt  grid  False  plt  xticks  plt  yticks  plt  imshow  img  cmap  plt  cm  binary  predicted  label  np  argmax  predictions  array  if  predicted  label  true  label  color  blue  else  color  red  plt  xlabel  0f  format  labels  lookup  predicted  label  100  np  max  predictions  array  labels  lookup  true  label  color  color  def  plot  value  array  predictions  array  true  label  predictions  array  true  label  predictions  array  true  label  plt  grid  False  plt  xticks  plt  yticks  thisplot  plt  bar  range  10  predictions  array  color  777777  plt  ylim  predicted  label  np  argmax  predictions  array  thisplot  ,amazon
predicted  label  set  color  red  thisplot  true  label  set  color  blue  predictions  list  predict  test  images  num  rows  num  cols  num  images  num  rows  num  cols  plt  figure  figsize  num  cols  num  rows  for  in  range  num  images  plt  subplot  num  rows  num  cols  plot  image  predictions  test  labels  test  images  plt  subplot  num  rows  num  cols  plot  value  array  predictions  test  labels  ,amazon
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  gluon  recsys  import  sagemaker  role  sagemaker  get  execution  role  import  os  import  mxnet  as  mx  from  mxnet  import  gluon  nd  ndarray  from  mxnet  metric  import  MSE  import  pandas  as  pd  import  numpy  as  np  import  sagemaker  from  sagemaker  mxnet  import  MXNet  import  boto3  import  json  import  matplotlib  pyplot  as  plt  mkdir  tmp  recsys  aws  s3  cp  s3  amazon  reviews  pds  tsv  amazon  reviews  us  Digital  Video  Download  v1  00  tsv  gz  tmp  recsys  df  pd  read  csv  tmp  recsys  amazon  reviews  us  Digital  Video  Download  v1  00  tsv  gz  delimiter  error  bad  lines  False  df  head  df  df  customer  id  product  id  star  rating  product  title  customers  df  customer  id  value  counts  products  df  product  id  value  counts  quantiles  01  02  03  04  05  25  75  95  96  97  98  99  print  customers  customers  quantile  quantiles  print  products  products  quantile  quantiles  customers  cust,amazon
omers  customers  products  products  products  10  reduced  df  df  merge  pd  DataFrame  customer  id  customers  index  merge  pd  DataFrame  product  id  products  index  customers  reduced  df  customer  id  value  counts  products  reduced  df  product  id  value  counts  customer  index  pd  DataFrame  customer  id  customers  index  user  np  arange  customers  shape  product  index  pd  DataFrame  product  id  products  index  item  np  arange  products  shape  reduced  df  reduced  df  merge  customer  index  merge  product  index  reduced  df  head  test  df  reduced  df  groupby  customer  id  last  reset  index  train  df  reduced  df  merge  test  df  customer  id  product  id  on  customer  id  product  id  how  outer  indicator  True  train  df  train  df  train  df  merge  left  only  class  SparseMatrixDataset  gluon  data  Dataset  def  init  self  data  label  assert  data  shape  len  label  self  data  data  self  label  label  if  isinstance  label  ndarray  NDArray  and  len  label  sh,amazon
ape  self  label  label  asnumpy  else  self  label  label  def  getitem  self  idx  return  self  data  idx  self  data  idx  self  label  idx  def  len  self  return  self  data  shape  batch  size  40  train  iter  gluon  data  DataLoader  SparseMatrixDataset  nd  array  train  df  user  item  values  dtype  np  float32  nd  array  train  df  star  rating  values  dtype  np  float32  shuffle  True  batch  size  batch  size  test  iter  gluon  data  DataLoader  SparseMatrixDataset  nd  array  test  df  user  item  values  dtype  np  float32  nd  array  test  df  star  rating  values  dtype  np  float32  shuffle  True  batch  size  batch  size  class  MFBlock  gluon  HybridBlock  def  init  self  max  users  max  items  num  emb  dropout  super  MFBlock  self  init  self  max  users  max  users  self  max  items  max  items  self  dropout  dropout  self  num  emb  num  emb  with  self  name  scope  self  user  embeddings  gluon  nn  Embedding  max  users  num  emb  self  item  embeddings  gluon  nn  Embeddin,amazon
g  max  items  num  emb  self  dropout  gluon  nn  Dropout  dropout  self  dense  gluon  nn  Dense  num  emb  activation  relu  def  hybrid  forward  self  users  items  self  user  embeddings  users  self  dense  self  item  embeddings  items  self  dense  predictions  self  dropout  self  dropout  predictions  sum  predictions  axis  return  predictionsnum  embeddings  64  net  MFBlock  max  users  customer  index  shape  max  items  product  index  shape  num  emb  num  embeddings  dropout  net  collect  params  Initialize  network  parameters  ctx  mx  gpu  net  collect  params  initialize  mx  init  Xavier  magnitude  24  ctx  ctx  force  reinit  True  net  hybridize  Set  optimization  parameters  opt  sgd  lr  02  momentum  wd  trainer  gluon  Trainer  net  collect  params  opt  learning  rate  lr  wd  wd  momentum  momentum  def  execute  train  iter  test  iter  net  epochs  ctx  loss  function  gluon  loss  L2Loss  for  in  range  epochs  print  epoch  format  for  user  item  label  in  enumerate  ,amazon
train  iter  try  user  user  as  in  context  ctx  reshape  batch  size  item  item  as  in  context  ctx  reshape  batch  size  label  label  as  in  context  ctx  reshape  batch  size  with  mx  autograd  record  output  net  user  item  loss  loss  function  output  label  loss  backward  trainer  step  batch  size  except  pass  print  EPOCH  MSE  ON  TRAINING  and  TEST  format  eval  net  train  iter  net  ctx  loss  function  eval  net  test  iter  net  ctx  loss  function  print  end  of  training  return  netdef  eval  net  data  net  ctx  loss  function  acc  MSE  for  user  item  label  in  enumerate  data  try  user  user  as  in  context  ctx  reshape  batch  size  item  item  as  in  context  ctx  reshape  batch  size  label  label  as  in  context  ctx  reshape  batch  size  predictions  net  user  item  reshape  batch  size  acc  update  preds  predictions  labels  label  except  pass  return  acc  get  time  epochs  trained  net  execute  train  iter  test  iter  net  epochs  ctx  product  i,amazon
ndex  u6  predictions  trained  net  nd  array  product  index  shape  as  in  context  ctx  nd  array  product  index  item  values  as  in  context  ctx  asnumpy  product  index  sort  values  u6  predictions  ascending  False  product  index  u7  predictions  trained  net  nd  array  product  index  shape  as  in  context  ctx  nd  array  product  index  item  values  as  in  context  ctx  asnumpy  product  index  sort  values  u7  predictions  ascending  False  product  index  u6  predictions  u7  predictions  plot  scatter  u6  predictions  u7  predictions  plt  show  cat  recommender  py  time  import  recommender  local  test  net  local  customer  index  local  product  index  recommender  train  train  tmp  recsys  num  embeddings  64  opt  sgd  lr  02  momentum  wd  epochs  local  boto3  client  s3  copy  Bucket  amazon  reviews  pds  Key  tsv  amazon  reviews  us  Digital  Video  Download  v1  00  tsv  gz  bucket  prefix  train  amazon  reviews  us  Digital  Video  Download  v1  00  tsv  gz  MXNet ,amazon
 recommender  py  py  version  py3  role  role  train  instance  count  train  instance  type  ml  p2  xlarge  output  path  s3  output  format  bucket  prefix  hyperparameters  num  embeddings  512  opt  opt  lr  lr  momentum  momentum  wd  wd  epochs  10  fit  train  s3  train  format  bucket  prefix  predictor  deploy  initial  instance  count  instance  type  ml  m4  xlarge  predictor  serializer  Nonepredictor  predict  json  dumps  customer  id  customer  index  customer  index  user  customer  id  values  tolist  product  id  B00KH1O9HW  B00M5KODWO  print  Naive  MSE  np  mean  test  df  star  rating  np  mean  train  df  star  rating  test  preds  for  array  in  np  array  split  test  df  customer  id  product  id  values  40  test  preds  predictor  predict  json  dumps  customer  id  array  tolist  product  id  array  tolist  test  preds  np  array  test  preds  print  MSE  np  mean  test  df  star  rating  test  preds  reduced  df  reduced  df  user  sort  values  star  rating  item  ascending  F,amazon
alse  True  predictions  for  array  in  np  array  split  product  index  product  id  values  40  predictions  predictor  predict  json  dumps  customer  id  customer  index  customer  index  user  customer  id  values  tolist  array  shape  product  id  array  tolist  predictions  pd  DataFrame  product  id  product  index  product  id  prediction  predictions  titles  reduced  df  groupby  product  id  product  title  last  reset  index  predictions  titles  predictions  merge  titles  predictions  titles  sort  values  prediction  product  id  ascending  False  True  predictions  user7  for  array  in  np  array  split  product  index  product  id  values  40  predictions  user7  predictor  predict  json  dumps  customer  id  customer  index  customer  index  user  customer  id  values  tolist  array  shape  product  id  array  tolist  plt  scatter  predictions  prediction  np  array  predictions  user7  plt  show  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
Download  data  from  S3  Enter  the  location  of  the  input  dataset  uploaded  from  kaggle  Say  s3  mybucket  PaySimFraudDetection  PS  20174392719  1491204439457  log  csv  zip  s3  source  data  s3  s3  path  Results  bucket  enter  your  bucket  to  hold  results  prefix  DemoPaySimFraudDetection  bucket  prefix  format  bucket  prefix  tmpdir  tmpDemoPaySimFraudDetection  This  is  to  ensure  that  the  right  libraries  are  installed  pip  install  requirements  txt  mkdir  tmpdirimport  os  local  input  zip  os  path  join  tmpdir  paysim  zip  aws  s3  cp  s3  source  data  local  input  zip  unzip  local  input  zip  tmpdirimport  pandas  as  pd  import  matplotlib  pyplot  as  plt  import  seaborn  as  snsdata  pd  read  csv  os  path  join  tmpdir  PS  20174392719  1491204439457  log  csv  data  head  10  data  describe  data  hist  bins  50  figsize  15  15  color  green  plt  show  data  nameOrig  value  counts  hist  bins  500  figsize  15  color  red  plt  show  data  nameOrig  value  c,amazon
ounts  describe  data  nameDest  value  counts  hist  bins  500  figsize  15  color  green  plt  show  data  sort  values  by  step  nameOrig  head  20  data  isFraud  value  counts  plot  pie  autopct  2f  figsize  colors  green  cyan  explode  plt  title  Class  Distribution  plt  tight  layout  fig  ax  plt  subplots  figsize  15  ax  set  title  Fraudent  Records  correlation  sns  heatmap  data  query  isFraud  drop  isFraud  isFlaggedFraud  corr  cmap  OrRd  ax  ax  ax  set  title  Non  fraudent  Records  correlation  sns  heatmap  data  query  isFraud  drop  isFraud  isFlaggedFraud  corr  cmap  OrRd  ax  ax  plt  show  data  clean  data  data  clean  data  clean  drop  newbalanceOrig  newbalanceDest  isFlaggedFraud  data  clean  isMerchantTransOrig  data  clean  nameOrig  str  startswith  astype  int  data  clean  isMerchantTransDest  data  clean  nameDest  str  startswith  astype  int  data  clean  isMerchantTrans  data  clean  isMerchantTransOrig  data  clean  isMerchantTransDest  fig  ax  plt  subpl,amazon
ots  figsize  15  ax  set  title  Fraudent  Records  correlation  after  clean  sns  heatmap  data  clean  query  isFraud  drop  isFraud  corr  cmap  OrRd  ax  ax  ax  set  title  Non  fraudent  Records  correlation  after  clean  sns  heatmap  data  clean  query  isFraud  drop  isFraud  corr  cmap  OrRd  ax  ax  plt  show  cols  data  clean  drop  isFraud  columns  tolist  cols  insert  isFraud  data  clean  data  clean  cols  data  clean  head  data  clean  data  clean  drop  nameOrig  nameDest  data  clean  pd  get  dummies  data  clean  prefix  transaction  type  columns  type  data  clean  head  from  sklearn  model  selection  import  train  test  split  learning  curve  train  val  test  train  test  split  data  clean  test  size  random  state  777  train  validation  train  test  split  train  val  test  size  random  state  777  fig  ax  plt  subplots  figsize  15  train  isFraud  value  counts  plot  pie  autopct  2f  ax  ax  colors  green  cyan  explode  ax  set  title  Train  fraud  distribution,amazon
  records  format  train  shape  test  isFraud  value  counts  plot  pie  autopct  2f  ax  ax  colors  green  cyan  explode  ax  set  title  Test  fraud  distribution  records  format  test  shape  validation  isFraud  value  counts  plot  pie  autopct  2f  ax  ax  colors  green  cyan  explode  ax  set  title  Validation  fraud  distribution  records  format  validation  shape  plt  show  import  os  trainfile  os  path  join  tmpdir  train  paysim  csv  testfile  os  path  join  tmpdir  test  paysim  csv  validationfile  os  path  join  tmpdir  validation  paysim  csv  train  to  csv  path  or  buf  trainfile  sep  na  rep  header  False  index  False  mode  encoding  UTF  quotechar  line  terminator  decimal  test  to  csv  path  or  buf  testfile  sep  na  rep  header  False  index  False  mode  encoding  UTF  quotechar  line  terminator  decimal  validation  to  csv  path  or  buf  validationfile  sep  na  rep  header  False  index  False  mode  encoding  UTF  quotechar  line  terminator  decimal  head  t,amazon
rainfiles3train  s3  train  format  bucket  prefix  train  txt  s3validation  s3  validation  format  bucket  prefix  validation  txt  print  trainfile  aws  s3  cp  trainfile  s3train  aws  s3  cp  validationfile  s3validation  import  boto3  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  boto3  Session  region  name  xgboost  from  sagemaker  import  get  execution  role  role  get  execution  role  time  import  boto3  from  time  import  gmtime  strftime  job  name  Fraud  xgboost  classification  strftime  gmtime  print  Training  job  job  name  Ensure  that  the  training  and  validation  data  folders  generated  above  are  reflected  in  the  InputDataConfig  parameter  below  create  training  params  AlgorithmSpecification  TrainingImage  container  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  s3  single  xgboost  format  bucket  prefix  ResourceConfig  InstanceCount  InstanceType  ml  m4  4xlarge  VolumeSizeInGB  Traini,amazon
ngJobName  job  name  HyperParameters  max  depth  eta  gamma  min  child  weight  subsample  silent  objective  reg  logistic  num  round  50  eval  metric  auc  StoppingCondition  MaxRuntimeInSeconds  3600  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3train  S3DataDistributionType  FullyReplicated  ContentType  csv  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3validation  S3DataDistributionType  FullyReplicated  ContentType  csv  CompressionType  None  client  boto3  client  sagemaker  client  create  training  job  create  training  params  import  time  status  client  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  status  while  status  Completed  and  status  Failed  time  sleep  60  status  client  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  status  time  import  boto3  from  time  import  gmtime  strftime  model  name  job  name,amazon
  model  print  model  name  info  client  describe  training  job  TrainingJobName  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  data  primary  container  Image  container  ModelDataUrl  model  data  create  model  response  client  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  import  os  batchfileinput  os  path  join  tmpdir  batchvalidation  csv  batchfileresults  os  path  join  tmpdir  batchvalidation  results  csv  time  import  json  from  itertools  import  islice  import  math  import  struct  file  name  testfile  with  open  file  name  as  lines  readlines  input  records  join  strip  split  for  in  lines  labels  int  split  for  in  lines  with  open  batchfileinput  as  writelines  format  item  for  item  in  input  records  time  import  boto3  import  sagemaker  import  json  fmttime  strftime  gmtime  input  key  file  batchvalidation  csv  input  batch  key ,amazon
 batchTransform  input  format  prefix  fmttime  input  key  file  input  location  s3  format  bucket  input  batch  key  output  batch  key  batchTransform  output  format  prefix  fmttime  output  location  s3  format  bucket  output  batch  key  s3  client  boto3  client  s3  s3  client  upload  file  batchfileinput  bucket  input  batch  key  Initialize  the  transformer  object  transformer  sagemaker  transformer  Transformer  base  transform  job  name  Batch  Transform  model  name  model  name  instance  count  instance  type  ml  c4  xlarge  output  path  output  location  To  start  transform  job  transformer  transform  input  location  content  type  text  csv  split  type  Line  Then  wait  until  transform  job  is  completed  transformer  wait  To  fetch  validation  result  outputkey  out  format  output  batch  key  input  key  file  print  outputkey  s3  client  download  file  bucket  outputkey  batchfileresults  with  open  batchfileresults  as  results  readlines  predicted  float  for,amazon
  in  results  print  Sample  transform  result  format  results  import  sklearn  micro  score  sklearn  metrics  average  precision  score  labels  predicted  average  micro  sample  weight  None  print  AUC  under  precision  recall  curve  is  format  micro  score  from  time  import  gmtime  strftime  endpoint  config  name  DEMO  XGBoostEndpointConfig  strftime  gmtime  print  endpoint  config  name  create  endpoint  config  response  client  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialVariantWeight  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  import  time  endpoint  name  DEMO  XGBoostEndpoint  strftime  gmtime  print  endpoint  name  create  endpoint  response  client  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  print  create  endpoint  response  EndpointA,amazon
rn  resp  client  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  while  status  Creating  time  sleep  60  resp  client  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  print  Arn  resp  EndpointArn  print  Status  status  runtime  client  boto3  client  runtime  sagemaker  time  import  json  from  itertools  import  islice  import  math  import  struct  file  name  testfile  with  open  file  name  as  lines  readlines  input  records  join  strip  split  for  in  lines  labels  int  split  for  in  lines  def  chunks  Yield  successive  sized  chunks  from  for  in  range  len  yield  time  predicted  for  record  chunks  in  chunks  input  records  10000  formatted  join  record  chunks  response  runtime  client  invoke  endpoint  EndpointName  endpoint  name  ContentType  text  csv  Body  formatted  encode  utf  result  response  Body  read  result  result  decode  utf  predicted  extend  float,amazon
  for  in  result  split  print  Predicted  out  of  so  far  format  len  predicted  len  input  records  import  sklearn  macro  score  sklearn  metrics  average  precision  score  labels  predicted  average  macro  sample  weight  None  print  The  AUC  under  precision  recall  curve  is  format  micro  score  macro  score  confidence  threshold  5confusion  matrix  sklearn  metrics  confusion  matrix  labels  pd  DataFrame  predicted  confidence  threshold  labels  sample  weight  None  confusion  matriximport  seaborn  as  sn  df  cm  pd  DataFrame  confusion  matrix  index  Fraud  Non  Fraud  columns  Fraud  Non  Fraud  sn  set  font  scale  for  label  size  sn  heatmap  df  cm  annot  True  annot  kws  size  16  fmt  cmap  tab10  plt  show  from  sklearn  metrics  import  precision  recall  fscore  support  precision  recall  fscore  support  labels  pd  DataFrame  predicted  confidence  threshold  average  None  client  delete  endpoint  EndpointName  endpoint  name  rm  rf  tmpdir  ,amazon
time  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  prabhat00  public  customize  to  your  bucket  containers  us  west  107995894928  dkr  ecr  us  west  amazonaws  com  object  detection  training  image  containers  boto3  Session  region  name  For  this  training  we  will  run  it  for  10  minutes  so  as  to  have  demo  of  it  max  run  time  600  time  import  time  import  boto3  from  time  import  gmtime  strftime  s3  boto3  client  s3  create  unique  job  name  job  name  prefix  object  detection  notebook  timestamp  time  strftime  time  gmtime  job  name  job  name  prefix  timestamp  training  params  specify  the  training  docker  image  AlgorithmSpecification  TrainingImage  training  image  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  s3  output  format  bucket  job  name  prefix  ResourceConfig  InstanceCount  InstanceType  ml  p3  2xlarge  VolumeSizeInGB  50  TrainingJobName  job  name  Hype,amazon
rParameters  max  run  time  str  max  run  time  after  this  time  training  job  will  terminate  itself  StoppingCondition  MaxRuntimeInSeconds  20  60  20  minutes  After  this  sagemaker  will  stop  training  Training  data  should  be  inside  subdirectory  called  train  Validation  data  should  be  inside  subdirectory  called  validation  The  algorithm  currently  only  supports  fullyreplicated  model  where  data  is  copied  onto  each  machine  InputDataConfig  ChannelName  training  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  pet  detection  data  tf  record  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  recordio  CompressionType  None  print  Training  job  name  format  job  name  print  nInput  Data  Location  format  training  params  InputDataConfig  DataSource  S3DataSource  create  the  Amazon  SageMaker  training  job  sagemaker  boto3  client  service  name  sagemaker  sagemaker  create  training  job  training  params  confirm  th,amazon
at  the  training  job  has  started  status  sagemaker  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  Training  job  current  status  format  status  try  wait  for  the  job  to  finish  and  report  the  ending  status  sagemaker  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  except  print  Training  failed  to  start  if  exception  is  raised  that  means  it  has  failed  message  sagemaker  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  ,amazon
time  import  os  import  boto3  import  re  import  copy  import  time  from  time  import  gmtime  strftime  from  sagemaker  import  get  execution  role  role  get  execution  role  region  boto3  Session  region  name  bucket  bucket  name  put  your  s3  bucket  name  here  and  create  s3  bucket  prefix  sagemaker  DEMO  xgboost  multiclass  classification  customize  to  your  bucket  where  you  have  stored  the  data  bucket  path  https  s3  amazonaws  com  format  region  bucket  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  gzip  open  mnist  pkl  gz  rb  train  set  valid  set  test  set  pickle  load  encoding  latin1  close  time  import  struct  import  io  import  boto3  def  to  libsvm  labels  values  write  bytes  join  format  label  join  format  el  for  el  in  enumerate  vec  for  label  vec  in  zip  labels  values  utf  return  def  write  to  s3  fo,amazon
bj  bucket  key  return  boto3  Session  resource  s3  Bucket  bucket  Object  key  upload  fileobj  fobj  def  get  dataset  import  pickle  import  gzip  with  gzip  open  mnist  pkl  gz  rb  as  pickle  Unpickler  encoding  latin1  return  load  def  upload  to  s3  partition  name  partition  labels  tolist  for  in  partition  vectors  tolist  for  in  partition  num  partition  partition  file  into  parts  partition  bound  int  len  labels  num  partition  for  in  range  num  partition  io  BytesIO  to  libsvm  labels  partition  bound  partition  bound  vectors  partition  bound  partition  bound  seek  key  examples  format  prefix  partition  name  str  url  s3n  format  bucket  key  print  Writing  to  format  url  write  to  s3  bucket  key  print  Done  writing  to  format  url  def  download  from  s3  partition  name  number  filename  key  examples  format  prefix  partition  name  number  url  s3n  format  bucket  key  print  Reading  from  format  url  s3  boto3  resource  s3  s3  Bucket  ,amazon
bucket  download  file  key  filename  try  s3  Bucket  bucket  download  file  key  mnist  local  test  except  botocore  exceptions  ClientError  as  if  response  Error  Code  404  print  The  object  does  not  exist  at  format  url  else  raise  def  convert  data  train  set  valid  set  test  set  get  dataset  partitions  train  train  set  validation  valid  set  test  test  set  for  partition  name  partition  in  partitions  print  format  partition  name  partition  shape  partition  shape  upload  to  s3  partition  name  partition  time  convert  data  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  xgboost  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  xgboost  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  xgboost  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  xgboost  latest  container  containers  boto3  Session  region  name  Ensure  that  the  train  and  validation  data  folders  generated  above  ,amazon
are  reflected  in  the  InputDataConfig  parameter  below  common  training  params  AlgorithmSpecification  TrainingImage  container  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  bucket  path  prefix  xgboost  ResourceConfig  InstanceCount  InstanceType  ml  m4  10xlarge  VolumeSizeInGB  HyperParameters  max  depth  eta  gamma  min  child  weight  silent  objective  multi  softmax  num  class  10  num  round  10  StoppingCondition  MaxRuntimeInSeconds  86400  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  bucket  path  prefix  train  S3DataDistributionType  FullyReplicated  ContentType  libsvm  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  bucket  path  prefix  validation  S3DataDistributionType  FullyReplicated  ContentType  libsvm  CompressionType  None  single  machine  job  params  single  machine  job  name  DEMO  xgboost  classification  strftime  gmtime  print  Job  name  i,amazon
s  single  machine  job  name  single  machine  job  params  copy  deepcopy  common  training  params  single  machine  job  params  TrainingJobName  single  machine  job  name  single  machine  job  params  OutputDataConfig  S3OutputPath  bucket  path  prefix  xgboost  single  single  machine  job  params  ResourceConfig  InstanceCount  distributed  job  params  distributed  job  name  DEMO  xgboost  distrib  classification  strftime  gmtime  print  Job  name  is  distributed  job  name  distributed  job  params  copy  deepcopy  common  training  params  distributed  job  params  TrainingJobName  distributed  job  name  distributed  job  params  OutputDataConfig  S3OutputPath  bucket  path  prefix  xgboost  distributed  number  of  instances  used  for  training  distributed  job  params  ResourceConfig  InstanceCount  no  more  than  if  there  are  total  partition  files  generated  above  data  distribution  type  for  train  channel  distributed  job  params  InputDataConfig  DataSource  S3DataSource  S,amazon
3DataDistributionType  ShardedByS3Key  data  distribution  type  for  validation  channel  distributed  job  params  InputDataConfig  DataSource  S3DataSource  S3DataDistributionType  ShardedByS3Key  time  region  boto3  Session  region  name  sm  boto3  Session  client  sagemaker  sm  create  training  job  single  machine  job  params  sm  create  training  job  distributed  job  params  status  sm  describe  training  job  TrainingJobName  distributed  job  name  TrainingJobStatus  print  status  sm  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  distributed  job  name  status  sm  describe  training  job  TrainingJobName  distributed  job  name  TrainingJobStatus  print  Training  job  ended  with  status  status  if  status  Failed  message  sm  describe  training  job  TrainingJobName  distributed  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  print  Single  Machine  sm  describe  trai,amazon
ning  job  TrainingJobName  single  machine  job  name  TrainingJobStatus  print  Distributed  sm  describe  training  job  TrainingJobName  distributed  job  name  TrainingJobStatus  time  import  boto3  from  time  import  gmtime  strftime  model  name  distributed  job  name  mod  print  model  name  info  sm  describe  training  job  TrainingJobName  distributed  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  data  primary  container  Image  container  ModelDataUrl  model  data  create  model  response  sm  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  endpoint  config  name  DEMO  XGBoostEndpointConfig  strftime  gmtime  print  endpoint  config  name  create  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialVariantWeight  InitialInst,amazon
anceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  import  time  endpoint  name  DEMO  XGBoostEndpoint  strftime  gmtime  print  endpoint  name  create  endpoint  response  sm  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  print  create  endpoint  response  EndpointArn  resp  sm  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  while  status  Creating  time  sleep  60  resp  sm  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  print  Arn  resp  EndpointArn  print  Status  status  runtime  client  boto3  client  runtime  sagemaker  download  from  s3  test  mnist  local  test  reading  the  first  part  file  within  test  head  mnist  local  test  mnist  single  test  time  import  json  file  name  mnist  single  test  customize  to  your  test  file  mnist  ,amazon
single  test  if  use  the  data  above  with  open  file  name  as  payload  read  response  runtime  client  invoke  endpoint  EndpointName  endpoint  name  ContentType  text  libsvm  Body  payload  result  response  Body  read  decode  ascii  print  Predicted  label  is  format  result  import  sys  def  do  predict  data  endpoint  name  content  type  payload  join  data  response  runtime  client  invoke  endpoint  EndpointName  endpoint  name  ContentType  content  type  Body  payload  result  response  Body  read  decode  ascii  preds  float  num  for  num  in  result  split  return  preds  def  batch  predict  data  batch  size  endpoint  name  content  type  items  len  data  arrs  for  offset  in  range  items  batch  size  arrs  extend  do  predict  data  offset  min  offset  batch  size  items  endpoint  name  content  type  sys  stdout  write  return  arrs  time  import  json  file  name  mnist  local  test  with  open  file  name  as  payload  read  strip  labels  float  line  split  for  line ,amazon
 in  payload  split  test  data  payload  split  preds  batch  predict  test  data  100  endpoint  name  text  libsvm  print  nerror  rate  sum  for  in  range  len  preds  if  preds  labels  float  len  preds  preds  10  labels  10  import  numpy  def  error  rate  predictions  labels  Return  the  error  rate  and  confusions  correct  numpy  sum  predictions  labels  total  predictions  shape  error  100  100  float  correct  float  total  confusions  numpy  zeros  10  10  numpy  int32  bundled  zip  predictions  labels  for  predicted  actual  in  bundled  confusions  int  predicted  int  actual  return  error  confusionsimport  matplotlib  pyplot  as  plt  matplotlib  inline  NUM  LABELS  10  change  it  according  to  num  class  in  your  dataset  test  error  confusions  error  rate  numpy  asarray  preds  numpy  asarray  labels  print  Test  error  1f  test  error  plt  xlabel  Actual  plt  ylabel  Predicted  plt  grid  False  plt  xticks  numpy  arange  NUM  LABELS  plt  yticks  numpy  arange  NUM  ,amazon
LABELS  plt  imshow  confusions  cmap  plt  cm  jet  interpolation  nearest  for  cas  in  enumerate  confusions  for  count  in  enumerate  cas  if  count  xoff  07  len  str  count  plt  text  xoff  int  count  fontsize  color  white  sm  delete  endpoint  EndpointName  endpoint  name  ,amazon
time  import  os  import  boto3  import  re  import  json  from  sagemaker  import  get  execution  role  region  boto3  Session  region  name  role  get  execution  role  bucket  s3  bucket  put  your  s3  bucket  name  here  and  create  s3  bucket  prefix  sagemaker  DEMO  xgboost  byo  bucket  path  https  s3  amazonaws  com  format  region  bucket  customize  to  your  bucket  where  you  have  stored  the  data  conda  install  conda  forge  xgboost  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  gzip  open  mnist  pkl  gz  rb  train  set  valid  set  test  set  pickle  load  encoding  latin1  close  time  import  struct  import  io  import  boto3  def  get  dataset  import  pickle  import  gzip  with  gzip  open  mnist  pkl  gz  rb  as  pickle  Unpickler  encoding  latin1  return  load  train  set  valid  set  test  set  get  dataset  train  train  set  train  train  set  v,amazon
alid  valid  set  valid  valid  set  test  test  set  test  test  set  import  xgboost  as  xgb  import  sklearn  as  sk  bt  xgb  XGBClassifier  max  depth  learning  rate  estimators  10  objective  multi  softmax  Setup  xgboost  model  bt  fit  train  train  Train  it  to  our  data  eval  set  valid  valid  verbose  False  model  file  name  DEMO  local  xgboost  model  bt  Booster  save  model  model  file  name  tar  czvf  model  tar  gz  model  file  namefObj  open  model  tar  gz  rb  key  os  path  join  prefix  model  file  name  model  tar  gz  boto3  Session  resource  s3  Bucket  bucket  Object  key  upload  fileobj  fObj  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  boto3  Session  region  name  xgboost  time  from  time  import  gmtime  strftime  model  name  model  file  name  strftime  gmtime  model  url  https  s3  amazonaws  com  format  region  bucket  key  sm  client  boto3  client  sagemaker  print  model  url  primary  container  Imag,amazon
e  container  ModelDataUrl  model  url  create  model  response2  sm  client  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response2  ModelArn  from  time  import  gmtime  strftime  endpoint  config  name  DEMO  XGBoostEndpointConfig  strftime  gmtime  print  endpoint  config  name  create  endpoint  config  response  sm  client  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  InitialVariantWeight  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  import  time  endpoint  name  DEMO  XGBoostEndpoint  strftime  gmtime  print  endpoint  name  create  endpoint  response  sm  client  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  print  create  endpoint  response  EndpointArn  resp  sm  client  describe  endpoint,amazon
  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  while  status  Creating  time  sleep  60  resp  sm  client  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  print  Arn  resp  EndpointArn  print  Status  status  runtime  client  boto3  client  runtime  sagemaker  import  numpy  as  np  point  test  point  np  expand  dims  point  axis  point  test  np  savetxt  test  point  csv  point  delimiter  time  import  json  file  name  test  point  csv  customize  to  your  test  file  will  be  mnist  single  test  if  use  data  above  with  open  file  name  as  payload  read  strip  response  runtime  client  invoke  endpoint  EndpointName  endpoint  name  ContentType  text  csv  Body  payload  result  response  Body  read  decode  ascii  print  Predicted  Class  Probabilities  format  result  floatArr  np  array  json  loads  result  predictedLabel  np  argmax  floatArr  print  Predicted  Class  Label  format  predictedLabe,amazon
l  print  Actual  Class  Label  format  point  sm  client  delete  endpoint  EndpointName  endpoint  name  ,amazon
import  sys  print  sys  executable  print  sys  version  print  sys  version  info  import  tensorflow  as  tf  tf  VERSIONimport  keras  as  kf  kf  version  ,microsoft
conda  install  scipy  matplotlib  inline  import  os  re  tarfile  import  boto3  import  matplotlib  pyplot  as  plt  import  mxnet  as  mx  import  numpy  as  np  np  set  printoptions  precision  suppress  True  some  helpful  utility  functions  are  defined  in  the  Python  module  generate  example  data  located  in  the  same  directory  as  this  notebook  from  generate  example  data  import  generate  griffiths  data  match  estimated  topics  plot  lda  plot  lda  topics  accessing  the  SageMaker  Python  SDK  import  sagemaker  from  sagemaker  amazon  common  import  numpy  to  record  serializer  from  sagemaker  predictor  import  csv  serializer  json  deserializerfrom  sagemaker  import  get  execution  role  role  get  execution  role  bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  lda  science  print  Training  input  output  will  be  stored  in  format  bucket  prefix  print  nIAM  Role  format  role  print  Generating  example  data  num  documents  6000  known  alph,amazon
a  known  beta  documents  topic  mixtures  generate  griffiths  data  num  documents  num  documents  num  topics  10  num  topics  vocabulary  size  known  beta  shape  separate  the  generated  data  into  training  and  tests  subsets  num  documents  training  int  num  documents  num  documents  test  num  documents  num  documents  training  documents  training  documents  num  documents  training  documents  test  documents  num  documents  training  topic  mixtures  training  topic  mixtures  num  documents  training  topic  mixtures  test  topic  mixtures  num  documents  training  print  documents  training  shape  format  documents  training  shape  print  documents  test  shape  format  documents  test  shape  print  First  training  document  format  documents  training  print  nVocabulary  size  format  vocabulary  size  print  Length  of  first  document  format  documents  training  sum  average  document  length  documents  sum  axis  mean  print  Observed  average  document  length  format ,amazon
 average  document  length  print  First  topic  format  known  beta  print  nTopic  word  probability  matrix  beta  shape  num  topics  vocabulary  size  format  known  beta  shape  print  nSum  of  elements  of  first  topic  format  known  beta  sum  print  Topic  format  known  beta  print  Topic  format  known  beta  matplotlib  inline  fig  plot  lda  documents  training  nrows  ncols  cmap  gray  with  colorbar  True  fig  suptitle  Document  Word  Counts  fig  set  dpi  160  matplotlib  inline  fig  plot  lda  known  beta  nrows  ncols  10  fig  suptitle  Known  beta  Topic  Word  Probability  Distributions  fig  set  dpi  160  fig  set  figheight  print  First  training  document  format  documents  training  print  nVocabulary  size  format  vocabulary  size  print  Length  of  first  document  format  documents  training  sum  print  First  training  document  topic  mixture  format  topic  mixtures  training  print  nNumber  of  topics  format  num  topics  print  sum  theta  format  topic  mixtu,amazon
res  training  sum  matplotlib  inline  fig  ax1  ax2  plt  subplots  ax1  matshow  documents  reshape  cmap  gray  ax1  set  title  Document  fontsize  20  ax1  set  xticks  ax1  set  yticks  cax2  ax2  matshow  topic  mixtures  reshape  cmap  Reds  vmin  vmax  cbar  fig  colorbar  cax2  orientation  horizontal  ax2  set  title  theta  Topic  Mixture  fontsize  20  ax2  set  xticks  ax2  set  yticks  fig  set  dpi  100  matplotlib  inline  pot  fig  plot  lda  known  beta  nrows  ncols  10  fig  suptitle  Known  beta  Topic  Word  Probability  Distributions  fig  set  dpi  160  fig  set  figheight  matplotlib  inline  fig  plot  lda  topics  documents  training  topic  mixtures  topic  mixtures  fig  suptitle  theta  Documents  with  Known  Topic  Mixtures  fig  set  dpi  160  convert  documents  training  to  Protobuf  RecordIO  format  recordio  protobuf  serializer  numpy  to  record  serializer  fbuffer  recordio  protobuf  serializer  documents  training  upload  to  S3  in  bucket  prefix  train  fname,amazon
  lda  data  s3  object  os  path  join  prefix  train  fname  boto3  Session  resource  s3  Bucket  bucket  Object  s3  object  upload  fileobj  fbuffer  s3  train  data  s3  format  bucket  s3  object  print  Uploaded  data  to  S3  format  s3  train  data  containers  us  west  266724342769  dkr  ecr  us  west  amazonaws  com  lda  latest  us  east  766337827248  dkr  ecr  us  east  amazonaws  com  lda  latest  us  east  999911452149  dkr  ecr  us  east  amazonaws  com  lda  latest  eu  west  999678624901  dkr  ecr  eu  west  amazonaws  com  lda  latest  region  name  boto3  Session  region  name  container  containers  region  name  print  Using  SageMaker  LDA  container  format  container  region  name  session  sagemaker  Session  specify  general  training  job  information  lda  sagemaker  estimator  Estimator  container  role  output  path  s3  output  format  bucket  prefix  train  instance  count  train  instance  type  ml  c4  2xlarge  sagemaker  session  session  set  algorithm  specific  hyperp,amazon
arameters  lda  set  hyperparameters  num  topics  num  topics  feature  dim  vocabulary  size  mini  batch  size  num  documents  training  alpha0  run  the  training  job  on  input  data  stored  in  S3  lda  fit  train  s3  train  data  print  Training  job  name  format  lda  latest  training  job  job  name  download  and  extract  the  model  file  from  S3  job  name  lda  latest  training  job  job  name  model  fname  model  tar  gz  model  object  os  path  join  prefix  output  job  name  output  model  fname  boto3  Session  resource  s3  Bucket  bucket  Object  model  object  download  file  fname  with  tarfile  open  fname  as  tar  tar  extractall  print  Downloaded  and  extracted  model  tarball  format  model  object  obtain  the  model  file  model  list  fname  for  fname  in  os  listdir  if  fname  startswith  model  model  fname  model  list  print  Found  model  file  format  model  fname  get  the  model  from  the  model  file  and  store  in  Numpy  arrays  alpha  beta  mx  ndarra,amazon
y  load  model  fname  learned  alpha  permuted  alpha  asnumpy  learned  beta  permuted  beta  asnumpy  print  nLearned  alpha  shape  format  learned  alpha  permuted  shape  print  Learned  beta  shape  format  learned  beta  permuted  shape  permutation  learned  beta  match  estimated  topics  known  beta  learned  beta  permuted  learned  alpha  learned  alpha  permuted  permutation  fig  plot  lda  np  vstack  known  beta  learned  beta  10  fig  set  dpi  160  fig  suptitle  Known  vs  Found  Topic  Word  Probability  Distributions  fig  set  figheight  beta  error  np  linalg  norm  known  beta  learned  beta  alpha  error  np  linalg  norm  known  alpha  learned  alpha  print  L1  error  beta  format  beta  error  print  L1  error  alpha  format  alpha  error  lda  inference  lda  deploy  initial  instance  count  instance  type  ml  m4  xlarge  LDA  inference  may  work  better  at  scale  on  ml  c4  instances  print  Endpoint  name  format  lda  inference  endpoint  lda  inference  content  type ,amazon
 text  csv  lda  inference  serializer  csv  serializer  lda  inference  deserializer  json  deserializerresults  lda  inference  predict  documents  test  12  print  results  inferred  topic  mixtures  permuted  np  array  prediction  topic  mixture  for  prediction  in  results  predictions  print  Inferred  topic  mixtures  permuted  format  inferred  topic  mixtures  permuted  inferred  topic  mixtures  inferred  topic  mixtures  permuted  permutation  print  Inferred  topic  mixtures  format  inferred  topic  mixtures  matplotlib  inline  create  array  of  bar  plots  width  np  arange  10  nrows  ncols  fig  ax  plt  subplots  nrows  ncols  sharey  True  for  in  range  nrows  for  in  range  ncols  index  ncols  ax  bar  topic  mixtures  test  index  width  color  C0  ax  bar  width  inferred  topic  mixtures  index  width  color  C1  ax  set  xticks  range  num  topics  ax  set  yticks  np  linspace  ax  grid  which  major  axis  ax  set  ylim  ax  set  xticklabels  if  nrows  ax  set  xticklabels  r,amazon
ange  num  topics  fontsize  if  ax  set  yticklabels  fontsize  fig  suptitle  Known  vs  Inferred  Topic  Mixtures  ax  super  fig  add  subplot  111  frameon  False  ax  super  tick  params  labelcolor  none  top  off  bottom  off  left  off  right  off  ax  super  grid  False  ax  super  set  xlabel  Topic  Index  ax  super  set  ylabel  Topic  Probability  fig  set  dpi  160  time  create  payload  containing  all  of  the  test  documents  and  run  inference  again  TRY  THIS  try  switching  between  the  test  data  set  and  subset  of  the  training  data  set  It  is  likely  that  LDA  inference  will  perform  better  against  the  training  set  than  the  holdout  test  set  payload  documents  documents  test  Example  known  topic  mixtures  topic  mixtures  test  Example  payload  documents  documents  training  600  Example  known  topic  mixtures  topic  mixtures  training  600  Example  print  Invoking  endpoint  results  lda  inference  predict  payload  documents  inferred  topic  mixt,amazon
ures  permuted  np  array  prediction  topic  mixture  for  prediction  in  results  predictions  inferred  topic  mixtures  inferred  topic  mixtures  permuted  permutation  print  known  topics  mixtures  shape  format  known  topic  mixtures  shape  print  inferred  topics  mixtures  test  shape  format  inferred  topic  mixtures  shape  matplotlib  inline  l1  errors  np  linalg  norm  inferred  topic  mixtures  known  topic  mixtures  axis  plot  the  error  freqency  fig  ax  frequency  plt  subplots  bins  np  linspace  40  weights  np  ones  like  l1  errors  len  l1  errors  freq  bins  ax  frequency  hist  l1  errors  bins  50  weights  weights  color  C0  ax  frequency  set  xlabel  L1  Error  ax  frequency  set  ylabel  Frequency  color  C0  plot  the  cumulative  error  shift  bins  bins  bins  shift  ax  cumulative  ax  frequency  twinx  cumulative  np  cumsum  freq  sum  freq  ax  cumulative  plot  cumulative  marker  color  C1  ax  cumulative  set  ylabel  Cumulative  Frequency  color  C1  ali,amazon
gn  grids  and  show  freq  ticks  np  linspace  freq  max  freq  ticklabels  np  round  100  freq  ticks  100  ax  frequency  set  yticks  freq  ticks  ax  frequency  set  yticklabels  freq  ticklabels  ax  cumulative  set  yticks  np  linspace  ax  cumulative  grid  which  major  axis  ax  cumulative  set  ylim  fig  suptitle  Topic  Mixutre  L1  Errors  fig  set  dpi  110  good  idx  l1  errors  05  good  documents  payload  documents  good  idx  good  topic  mixtures  inferred  topic  mixtures  good  idx  poor  idx  l1  errors  poor  documents  payload  documents  poor  idx  poor  topic  mixtures  inferred  topic  mixtures  poor  idx  matplotlib  inline  fig  plot  lda  topics  good  documents  topic  mixtures  good  topic  mixtures  fig  suptitle  Documents  With  Accurate  Inferred  Topic  Mixtures  fig  set  dpi  120  matplotlib  inline  fig  plot  lda  topics  poor  documents  topic  mixtures  poor  topic  mixtures  fig  suptitle  Documents  With  Inaccurate  Inferred  Topic  Mixtures  fig  set  dpi  ,amazon
120  sagemaker  Session  delete  endpoint  lda  inference  endpoint  ,amazon
Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  import  sagemaker  bucket  sagemaker  Session  default  bucket  prefix  sagemaker  DEMO  xgboost  churn  role  get  execution  role  import  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  import  io  import  os  import  sys  import  time  import  json  from  IPython  display  import  display  from  time  import  strftime  gmtime  import  sagemaker  from  sagemaker  predictor  import  csv  serializer  wget  http  dataminingconsultant  com  DKD2e  data  sets  zip  unzip  DKD2e  data  sets  zipchurn  pd  read  csv  Data  sets  churn  txt  pd  set  option  display  max  columns  500  churn  head  len  churn  Frequency  tables  for  each  categorical  feature  for  column  in  churn  select  dtypes  include  object  columns  display  pd  crosstab  index  churn  column  columns  observations  normalize  columns  Histograms  for  each  numeric  features  display  churn  describe  matplotlib  inli,amazon
ne  hist  churn  hist  bins  30  sharey  True  figsize  10  10  churn  churn  drop  Phone  axis  churn  Area  Code  churn  Area  Code  astype  object  for  column  in  churn  select  dtypes  include  object  columns  if  column  Churn  display  pd  crosstab  index  churn  column  columns  churn  Churn  normalize  columns  for  column  in  churn  select  dtypes  exclude  object  columns  print  column  hist  churn  column  Churn  hist  by  Churn  bins  30  plt  show  display  churn  corr  pd  plotting  scatter  matrix  churn  figsize  12  12  plt  show  churn  churn  drop  Day  Charge  Eve  Charge  Night  Charge  Intl  Charge  axis  model  data  pd  get  dummies  churn  model  data  pd  concat  model  data  Churn  True  model  data  drop  Churn  False  Churn  True  axis  axis  train  data  validation  data  test  data  np  split  model  data  sample  frac  random  state  1729  int  len  model  data  int  len  model  data  train  data  to  csv  train  csv  header  False  index  False  validation  data  to  csv ,amazon
 validation  csv  header  False  index  False  test  data  to  csv  test  csv  header  False  index  False  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  train  csv  upload  file  train  csv  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  validation  csv  upload  file  validation  csv  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  boto3  Session  region  name  xgboost  s3  input  train  sagemaker  s3  input  s3  data  s3  train  format  bucket  prefix  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  s3  validation  format  bucket  prefix  content  type  csv  sess  sagemaker  Session  xgb  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  m4  xlarge  output  path  s3  output  format  bucket  prefix  sagemaker  session  sess  xgb  set  hyperparameters  max  depth  eta  min  child  weight  subsample  sile,amazon
nt  objective  binary  logistic  num  round  70  xgb  fit  train  s3  input  train  validation  s3  input  validation  xgb  predictor  xgb  deploy  initial  instance  count  instance  type  ml  m4  xlarge  xgb  predictor  content  type  text  csv  xgb  predictor  serializer  csv  serializer  xgb  predictor  deserializer  Nonetest  data  head  def  predict  data  rows  500  split  array  np  array  split  data  int  data  shape  float  rows  predictions  for  array  in  split  array  predictions  join  predictions  xgb  predictor  predict  array  decode  utf  return  np  fromstring  predictions  sep  predictions  predict  test  data  as  matrix  pd  crosstab  index  test  data  iloc  columns  np  round  predictions  rownames  actual  colnames  predictions  plt  hist  predictions  plt  show  pd  crosstab  index  test  data  iloc  columns  np  where  predictions  from  sklearn  metrics  import  accuracy  score  f1  score  precision  score  recall  score  classification  report  confusion  matrix  roc  curve  fro,amazon
m  sklearn  metrics  import  precision  recall  curve  average  precision  score  precision  recall  fscore  supportdef  f1  get  label  bin  if  cont  else  for  cont  in  change  the  prob  to  class  output  return  f1  f1  score  bin  def  print  evaluation  metric  true  pred  precision  recall  fscore  support  precision  recall  fscore  support  true  pred  print  Precision  format  precision  print  Recall  format  recall  print  score  format  fscore  print  Support  format  support  return  def  plot  roc  curve  true  prob  fpr  tpr  threshold  roc  curve  true  prob  fig  plt  gcf  fig  set  size  inches  10  plt  title  Receiver  Operating  Characteristic  ROC  plt  plot  fpr  tpr  plt  plot  plt  xlim  plt  ylim  plt  ylabel  True  Positive  Rate  plt  xlabel  False  Positive  Rate  plt  show  returnprint  evaluation  metric  test  data  iloc  np  where  predictions  plot  roc  curve  test  data  iloc  predictions  sagemaker  Session  delete  endpoint  xgb  predictor  endpoint  ,amazon
bin  bash  setup  shimport  os  import  subprocess  import  boto3  import  sagemaker  from  sagemaker  mxnet  import  MXNet  from  mxnet  import  gluon  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  instance  type  local  if  subprocess  call  nvidia  smi  Set  type  to  GPU  if  one  is  present  instance  type  local  gpu  print  Instance  type  instance  type  role  get  execution  role  gluon  data  vision  MNIST  data  train  train  True  gluon  data  vision  MNIST  data  test  train  False  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  mnist  cat  mnist  py  MXNet  mnist  py  role  role  train  instance  count  train  instance  type  instance  type  hyperparameters  batch  size  100  epochs  learning  rate  momentum  log  interval  100  fit  inputs  predictor  deploy  initial  instance  count  instance  type  instance  type  from  IPython  display  import  HTML  HTML  open  input  html  read  response  predictor  predict  data  print  i,amazon
nt  response  delete  endpoint  ,amazon
hidden  cell  The  project  token  is  an  authorization  token  that  is  used  to  access  project  resources  like  data  sources  connections  and  used  by  platform  APIs  project  id  project  access  token  training  bucket  project  id  training  results  bucket  project  id  results  cos  connection  name  COS  Images  import  keras  from  keras  datasets  import  mnist  import  pickle  numpy  from  sklearn  model  selection  import  train  test  split  The  data  split  between  train  and  test  sets  test  test  mnist  load  data  Make  additional  split  between  training  and  validation  sets  train  valid  train  valid  train  test  split  test  size  166  random  state  42  with  open  mnist  tf  train  pkl  wb  as  pickle  dump  train  train  protocol  pickle  HIGHEST  PROTOCOL  with  open  mnist  tf  valid  pkl  wb  as  pickle  dump  valid  valid  protocol  pickle  HIGHEST  PROTOCOL  with  open  mnist  tf  test  pkl  wb  as  pickle  dump  test  test  protocol  pickle  HIGHEST  PROTOCOL  ls,ibm
  lhfrom  project  lib  import  Project  project  Project  project  id  project  id  project  access  token  project  access  token  pc  project  project  context  cos  asset  id  asset  id  for  in  project  get  assets  if  name  cos  connection  name  and  type  connection  cos  creds  project  get  connection  id  cos  asset  id  pip  install  quiet  ibm  cos  sdkimport  ibm  boto3  from  ibm  botocore  client  import  Config  api  key  cos  creds  api  key  service  instance  id  cos  creds  resource  instance  id  if  Not  cos  creds  iam  url  startswith  https  cos  creds  iam  url  https  cos  creds  iam  url  auth  endpoint  cos  creds  iam  url  if  Not  cos  creds  url  startswith  https  cos  creds  url  https  cos  creds  url  service  endpoint  cos  creds  url  cos  ibm  boto3  client  s3  ibm  api  key  id  api  key  ibm  service  instance  id  service  instance  id  ibm  auth  endpoint  auth  endpoint  config  Config  signature  version  oauth  endpoint  url  service  endpoint  Create  bucket,ibm
  if  it  doesn  exist  for  bucket  in  training  bucket  results  bucket  try  cos  create  bucket  Bucket  bucket  print  Created  bucket  bucket  except  print  Bucket  already  exist  bucket  print  for  in  mnist  tf  train  pkl  mnist  tf  valid  pkl  mnist  tf  test  pkl  with  open  rb  as  data  cos  upload  fileobj  data  training  bucket  print  Uploaded  to  format  training  bucket  ,ibm
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  ntm  synthetic  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  numpy  as  np  from  generate  example  data  import  generate  griffiths  data  plot  topic  data  import  io  import  os  import  time  import  json  import  sys  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  from  IPython  display  import  display  import  scipy  import  sagemaker  import  sagemaker  amazon  common  as  smac  from  sagemaker  predictor  import  csv  serializer  json  deserializer  generate  the  sample  data  num  documents  5000  num  topics  vocabulary  size  25  known  alpha  known  beta  documents  topic  mixtures  generate  griffiths  data  num  documents  num  documents  num  topics  num  topics  vocabulary  size  vocabulary  size  separate  the  generated  data  into  training  and  tests  subsets  num  documents  training  int  num  documents  num  documents  tes,amazon
t  num  documents  num  documents  training  documents  training  documents  num  documents  training  documents  test  documents  num  documents  training  topic  mixtures  training  topic  mixtures  num  documents  training  topic  mixtures  test  topic  mixtures  num  documents  training  data  training  documents  training  np  zeros  num  documents  training  data  test  documents  test  np  zeros  num  documents  test  print  First  training  document  format  documents  print  nVocabulary  size  format  vocabulary  size  np  set  printoptions  precision  suppress  True  print  Known  topic  mixture  of  first  training  document  format  topic  mixtures  training  print  nNumber  of  topics  format  num  topics  matplotlib  inline  fig  plot  topic  data  documents  training  10  nrows  ncols  cmap  gray  with  colorbar  False  fig  suptitle  Example  Documents  fig  set  dpi  160  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  data  training  astype  float32  buf  seek  key  ntm  data  ,amazon
boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  buf  s3  train  data  s3  train  format  bucket  prefix  key  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  boto3  Session  region  name  ntm  sess  sagemaker  Session  ntm  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  s3  output  format  bucket  prefix  sagemaker  session  sess  ntm  set  hyperparameters  num  topics  num  topics  feature  dim  vocabulary  size  ntm  fit  train  s3  train  data  ntm  predictor  ntm  deploy  initial  instance  count  instance  type  ml  m4  xlarge  ntm  predictor  content  type  text  csv  ntm  predictor  serializer  csv  serializer  ntm  predictor  deserializer  json  deserializerresults  ntm  predictor  predict  documents  training  10  print  results  predictions  np  array  prediction  topic  weights  for  prediction  in  results  predictions  p,amazon
rint  predictions  print  topic  mixtures  training  known  topic  mixture  print  predictions  computed  topic  mixturedef  predict  batches  data  rows  1000  split  array  np  array  split  data  int  data  shape  float  rows  predictions  for  array  in  split  array  results  ntm  predictor  predict  array  predictions  topic  weights  for  in  results  predictions  return  np  array  predictions  predictions  predict  batches  documents  training  data  pd  DataFrame  np  concatenate  topic  mixtures  training  predictions  axis  columns  actual  format  for  in  range  predictions  format  for  in  range  display  data  corr  pd  plotting  scatter  matrix  pd  DataFrame  np  concatenate  topic  mixtures  training  predictions  axis  figsize  12  12  plt  show  sagemaker  Session  delete  endpoint  ntm  predictor  endpoint  ,amazon
from  future  import  division  import  gcp  bigquery  as  bq  import  matplotlib  pyplot  as  plot  import  numpy  as  np  sql  module  wikipedia  SELECT  title  YEAR  SEC  TO  TIMESTAMP  timestamp  as  year  count  as  revisions  FROM  publicdata  samples  wikipedia  WHERE  wp  namespace  AND  is  bot  is  NULL  GROUP  BY  year  title  HAVING  count  100  bigquery  sample  query  wikipediadf  bq  Query  wikipedia  to  dataframe  years  for  year  in  set  df  year  years  year  df  df  year  year  sort  revisions  ascending  False  yearsyears  2007  first  last  df  groupby  title  agg  year  np  min  np  max  first  last  head  first  last  columnsfirst  last  span  first  last  year  amax  first  last  year  amin  first  last  span  max  first  last  first  last  span  first  last  first  last  span  nc  df  df  title  Nicolaus  Copernicus  nc  plot  kind  scatter  year  revisions  ,google
Lab  11  MNIST  and  Deep  learning  CNN  import  tensorflow  as  tf  import  matplotlib  pyplot  as  plt  from  tensorflow  examples  tutorials  mnist  import  input  data  tf  set  random  seed  777  reproducibility  mnist  input  data  read  data  sets  MNIST  data  one  hot  True  Check  out  https  www  tensorflow  org  get  started  mnist  beginners  for  more  information  about  the  mnist  dataset  hyper  parameters  learning  rate  001  training  epochs  15  batch  size  100class  Model  def  init  self  sess  name  self  sess  sess  self  name  name  self  build  net  def  build  net  self  with  tf  variable  scope  self  name  dropout  keep  prob  rate  on  training  but  should  be  for  testing  self  training  tf  placeholder  tf  bool  input  place  holders  self  tf  placeholder  tf  float32  None  784  img  28x28x1  black  white  Input  Layer  img  tf  reshape  self  28  28  self  tf  placeholder  tf  float32  None  10  Convolutional  Layer  Pooling  Layer  conv1  tf  layers  conv2d  inputs,ibm
  img  filters  32  kernel  size  padding  SAME  activation  tf  nn  relu  pool1  tf  layers  max  pooling2d  inputs  conv1  pool  size  padding  SAME  strides  dropout1  tf  layers  dropout  inputs  pool1  rate  training  self  training  W1  tf  Variable  tf  random  normal  32  stddev  01  L1  tf  nn  conv2d  img  W1  strides  padding  SAME  L1  tf  nn  relu  L1  L1  tf  nn  max  pool  L1  ksize  strides  padding  SAME  L1  tf  nn  dropout  L1  keep  prob  self  keep  prob  Convolutional  Layer  and  Pooling  Layer  conv2  tf  layers  conv2d  inputs  dropout1  filters  64  kernel  size  padding  SAME  activation  tf  nn  relu  pool2  tf  layers  max  pooling2d  inputs  conv2  pool  size  padding  SAME  strides  dropout2  tf  layers  dropout  inputs  pool2  rate  training  self  training  Convolutional  Layer  and  Pooling  Layer  conv3  tf  layers  conv2d  inputs  dropout2  filters  128  kernel  size  padding  same  activation  tf  nn  relu  pool3  tf  layers  max  pooling2d  inputs  conv3  pool  size  padd,ibm
ing  same  strides  dropout3  tf  layers  dropout  inputs  pool3  rate  training  self  training  Dense  Layer  with  Relu  flat  tf  reshape  dropout3  128  dense4  tf  layers  dense  inputs  flat  units  625  activation  tf  nn  relu  dropout4  tf  layers  dropout  inputs  dense4  rate  training  self  training  Logits  no  activation  Layer  L5  Final  FC  625  inputs  10  outputs  self  logits  tf  layers  dense  inputs  dropout4  units  10  define  cost  loss  optimizer  self  cost  tf  reduce  mean  tf  nn  softmax  cross  entropy  with  logits  logits  self  logits  labels  self  self  optimizer  tf  train  AdamOptimizer  learning  rate  learning  rate  minimize  self  cost  correct  prediction  tf  equal  tf  argmax  self  logits  tf  argmax  self  self  accuracy  tf  reduce  mean  tf  cast  correct  prediction  tf  float32  def  predict  self  test  training  False  return  self  sess  run  self  logits  feed  dict  self  test  self  training  training  def  get  accuracy  self  test  test  training ,ibm
 False  return  self  sess  run  self  accuracy  feed  dict  self  test  self  test  self  training  training  def  train  self  data  data  training  True  return  self  sess  run  self  cost  self  optimizer  feed  dict  self  data  self  data  self  training  training  initialize  sess  tf  Session  m1  Model  sess  m1  sess  run  tf  global  variables  initializer  print  Learning  Started  train  my  model  for  epoch  in  range  training  epochs  avg  cost  total  batch  int  mnist  train  num  examples  batch  size  for  in  range  total  batch  batch  xs  batch  ys  mnist  train  next  batch  batch  size  m1  train  batch  xs  batch  ys  avg  cost  total  batch  print  Epoch  04d  epoch  cost  9f  format  avg  cost  print  Learning  Finished  Test  model  and  check  accuracy  print  Accuracy  m1  get  accuracy  mnist  test  images  mnist  test  labels  ,ibm
from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  sagemaker  yourbucket  Use  the  name  of  your  s3  bucket  hererole  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  30  This  is  format  train  set  30  from  sagemaker  import  KMeans  data  location  s3  kmeans  highlevel  example  data  format  bucket  output  location  s3  kmeans  highlevel  example  output  format  bucket  print  training  data  will  be  uploaded  to  format  d,amazon
ata  location  print  training  artifacts  will  be  uploaded  to  format  output  location  kmeans  KMeans  role  role  train  instance  count  train  instance  type  ml  c4  8xlarge  output  path  output  location  10  epochs  100  data  location  data  location  time  kmeans  fit  kmeans  record  set  train  set  time  kmeans  predictor  kmeans  deploy  initial  instance  count  instance  type  ml  m4  xlarge  time  result  kmeans  predictor  predict  valid  set  100  clusters  label  closest  cluster  float32  tensor  values  for  in  result  for  cluster  in  range  10  print  nCluster  format  int  cluster  digits  img  for  img  in  zip  clusters  valid  set  if  int  cluster  height  len  digits  width  plt  rcParams  figure  figsize  width  height  subplots  plt  subplots  height  width  subplots  numpy  ndarray  flatten  subplots  for  subplot  image  in  zip  subplots  digits  show  digit  image  subplot  subplot  for  subplot  in  subplots  len  digits  subplot  axis  off  plt  show  result  kmean,amazon
s  predictor  predict  valid  set  230  231  print  result  show  digit  valid  set  230  This  is  format  valid  set  230  ,amazon
from  sagemaker  import  get  execution  role  Bucket  location  to  save  your  custom  code  in  tar  gz  format  custom  code  upload  location  s3  bucket  name  customcode  tensorflow  iris  Bucket  location  where  results  of  model  training  are  saved  model  artifacts  location  s3  bucket  name  artifacts  IAM  execution  role  that  gives  SageMaker  access  to  resources  in  your  AWS  account  role  get  execution  role  cat  iris  dnn  classifier  py  def  estimator  model  path  hyperparameters  feature  columns  tf  feature  column  numeric  column  INPUT  TENSOR  NAME  shape  return  tf  estimator  DNNClassifier  feature  columns  feature  columns  hidden  units  10  20  10  classes  model  dir  model  path  def  train  input  fn  training  dir  hyperparameters  training  set  tf  contrib  learn  datasets  base  load  csv  with  header  filename  os  path  join  training  dir  iris  training  csv  target  dtype  np  int  features  dtype  np  float32  return  tf  estimator  inputs  numpy  i,amazon
nput  fn  INPUT  TENSOR  NAME  np  array  training  set  data  np  array  training  set  target  num  epochs  None  shuffle  True  def  serving  input  fn  hyperparameters  feature  spec  INPUT  TENSOR  NAME  tf  FixedLenFeature  dtype  tf  float32  shape  return  tf  estimator  export  build  parsing  serving  input  receiver  fn  feature  spec  from  sagemaker  tensorflow  import  TensorFlow  iris  estimator  TensorFlow  entry  point  iris  dnn  classifier  py  role  role  framework  version  output  path  model  artifacts  location  code  location  custom  code  upload  location  train  instance  count  train  instance  type  ml  c4  xlarge  training  steps  1000  evaluation  steps  100  time  import  boto3  use  the  region  specific  sample  data  bucket  region  boto3  Session  region  name  train  data  location  s3  sagemaker  sample  data  tensorflow  iris  format  region  iris  estimator  fit  train  data  location  time  iris  predictor  iris  estimator  deploy  initial  instance  count  instance  ,amazon
type  ml  m4  xlarge  iris  predictor  predict  expected  label  to  be  1print  iris  predictor  endpoint  import  sagemaker  sagemaker  Session  delete  endpoint  iris  predictor  endpoint  ,amazon
from  sagemaker  import  get  execution  role  Bucket  location  to  save  your  custom  code  in  tar  gz  format  custom  code  upload  location  s3  bucket  name  customcode  mxnet  Bucket  location  where  results  of  model  training  are  saved  model  artifacts  location  s3  bucket  name  artifacts  IAM  execution  role  that  gives  SageMaker  access  to  resources  in  your  AWS  account  We  can  use  the  SageMaker  Python  SDK  to  get  the  role  from  our  notebook  environment  role  get  execution  role  cat  mnist  pyfrom  sagemaker  mxnet  import  MXNet  mnist  estimator  MXNet  entry  point  mnist  py  role  role  output  path  model  artifacts  location  code  location  custom  code  upload  location  train  instance  count  train  instance  type  ml  m4  xlarge  framework  version  hyperparameters  learning  rate  time  import  boto3  region  boto3  Session  region  name  train  data  location  s3  sagemaker  sample  data  mxnet  mnist  train  format  region  test  data  location  s3  sa,amazon
gemaker  sample  data  mxnet  mnist  test  format  region  mnist  estimator  fit  train  train  data  location  test  test  data  location  time  predictor  mnist  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  IPython  display  import  HTML  HTML  open  input  html  read  response  predictor  predict  data  print  Raw  prediction  result  print  response  labeled  predictions  list  zip  range  10  response  print  Labeled  predictions  print  labeled  predictions  labeled  predictions  sort  key  lambda  label  and  prob  label  and  prob  print  Most  likely  answer  format  labeled  predictions  print  Endpoint  name  predictor  endpoint  import  sagemaker  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
import  os  import  boto3  import  sagemaker  from  sagemaker  mxnet  import  MXNet  from  mxnet  import  gluon  nd  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  def  input  transformer  data  label  return  nd  transpose  data  astype  np  float32  255  label  astype  np  float32  train  gluon  data  vision  FashionMNIST  data  fmnist  train  train  True  transform  input  transformer  test  gluon  data  vision  FashionMNIST  data  fmnist  test  train  False  transform  input  transformer  inputs  sagemaker  session  upload  data  path  data  fmnist  key  prefix  data  fminst  fmnist  cat  fmnist  cnn  py  batch  size  100  epochs  10  learning  rate  01  momentum  log  interval  100m  MXNet  fmnist  cnn  py  role  role  train  instance  count  train  instance  type  ml  p2  xlarge  hyperparameters  batch  size  batch  size  epochs  epochs  learning  rate  learning  rate  momentum  momentum  log  interval  log  interval  fit  inputs  predi,amazon
ctor  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  sagemaker  predictor  import  csv  serializer  json  deserializer  predictor  content  type  text  csv  predictor  serializer  csv  serializer  predictor  deserializer  json  deserializerimport  numpy  as  np  import  gzip  import  struct  import  matplotlib  pyplot  as  plt  matplotlib  inline  def  read  data  label  url  image  url  with  gzip  open  label  url  as  flbl  magic  num  struct  unpack  II  flbl  read  label  np  fromstring  flbl  read  dtype  np  int8  with  gzip  open  image  url  rb  as  fimg  magic  num  rows  cols  struct  unpack  IIII  fimg  read  16  image  np  fromstring  fimg  read  dtype  np  uint8  reshape  len  label  rows  cols  return  label  image  val  lbl  val  img  read  data  data  fmnist  test  t10k  labels  idx1  ubyte  gz  data  fmnist  test  t10k  images  idx3  ubyte  gz  idx  32  This  number  can  be  changed  to  get  another  image  plt  imshow  val  img  idx  cmap  Greys  plt  axis  off  p,amazon
lt  show  image  nd  array  val  img  idx  reshape  28  28  asnumpy  tolist  predictor  predict  str  image  import  boto3  client  boto3  client  sagemaker  runtime  response  client  invoke  endpoint  EndpointName  predictor  endpoint  Body  str  image  ContentType  text  csv  response  Body  read  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
import  tensorflow  as  tf  import  numpy  as  np  tf  set  random  seed  777  for  reproducibilityimport  sys  import  types  import  pandas  as  pd  from  botocore  client  import  Config  import  ibm  boto3  def  iter  self  return  The  following  code  accesses  file  in  your  IBM  Cloud  Object  Storage  It  includes  your  credentials  You  might  want  to  remove  those  credentials  before  you  share  your  notebook  client  ace6f71a1b9946cbb684ca0d9e6c34c0  ibm  boto3  client  service  name  s3  ibm  api  key  id  xBNCWwO4mooXZRHuqZ71nRmyIlQt8ca3ZWN8pOo56X69Dx  masking  use  your  own  ibm  auth  endpoint  https  iam  ng  bluemix  net  oidc  token  config  Config  signature  version  oauth  endpoint  url  https  s3  api  us  geo  objectstorage  service  networklayer  com  body  client  ace6f71a1b9946cbb684ca0d9e6c34c0  get  object  Bucket  tensorflowlabwithwatsonstudio  donotdelete  pr  neiwaip4a29fcg  Key  data  04  zoo  csv  Body  add  missing  iter  method  so  pandas  accepts  body  as  file  ,ibm
like  object  if  not  hasattr  body  iter  body  iter  types  MethodType  iter  body  df  data  pd  read  csv  body  comment  xy  df  data  values  df  data  head  Predicting  animal  type  based  on  various  features  xy  np  loadtxt  data  04  zoo  csv  delimiter  dtype  np  float32  data  xy  data  xy  print  data  shape  data  shape  nb  classes  tf  placeholder  tf  float32  None  16  tf  placeholder  tf  int32  None  6y  data  10  one  hot  tf  one  hot  nb  classes  one  hot  print  one  hot  one  hot  one  hot  tf  reshape  one  hot  nb  classes  print  reshape  one  hot  tf  Variable  tf  random  normal  16  nb  classes  name  weight  tf  Variable  tf  random  normal  nb  classes  name  bias  tf  nn  softmax  computes  softmax  activations  softmax  exp  logits  reduce  sum  exp  logits  dim  logits  tf  matmul  hypothesis  tf  nn  softmax  logits  Cross  entropy  cost  loss  cost  tf  nn  softmax  cross  entropy  with  logits  logits  logits  labels  one  hot  cost  tf  reduce  mean  cost  optimiz,ibm
er  tf  train  GradientDescentOptimizer  learning  rate  minimize  cost  prediction  tf  argmax  hypothesis  correct  prediction  tf  equal  prediction  tf  argmax  one  hot  accuracy  tf  reduce  mean  tf  cast  correct  prediction  tf  float32  Launch  graph  sess  tf  Session  sess  run  tf  global  variables  initializer  for  step  in  range  2000  sess  run  optimizer  feed  dict  data  data  if  step  100  loss  acc  sess  run  cost  accuracy  feed  dict  data  data  print  Step  tLoss  3f  tAcc  format  step  loss  acc  Let  see  if  we  can  predict  pred  sess  run  prediction  feed  dict  data  data  flatten  matches  pred  shape  for  in  zip  pred  data  flatten  print  Prediction  True  format  int  int  Step  Loss  106  Acc  37  62  Step  100  Loss  800  Acc  79  21  Step  200  Loss  486  Acc  88  12  Step  300  Loss  349  Acc  90  10  Step  400  Loss  272  Acc  94  06  Step  500  Loss  222  Acc  95  05  Step  600  Loss  187  Acc  97  03  Step  700  Loss  161  Acc  97  03  Step  800  Loss  140 ,ibm
 Acc  97  03  Step  900  Loss  124  Acc  97  03  Step  1000  Loss  111  Acc  97  03  Step  1100  Loss  101  Acc  99  01  Step  1200  Loss  092  Acc  100  00  Step  1300  Loss  084  Acc  100  00  True  Prediction  True  True  Prediction  True  True  Prediction  True  True  Prediction  True  True  Prediction  True  True  Prediction  True  True  Prediction  True  True  Prediction  True  True  Prediction  True  True  Prediction  True  ,ibm
conda  install  scipy  matplotlib  inline  import  os  re  import  boto3  import  matplotlib  pyplot  as  plt  import  numpy  as  np  np  set  printoptions  precision  suppress  True  some  helpful  utility  functions  are  defined  in  the  Python  module  generate  example  data  located  in  the  same  directory  as  this  notebook  from  generate  example  data  import  generate  griffiths  data  plot  lda  match  estimated  topics  accessing  the  SageMaker  Python  SDK  import  sagemaker  from  sagemaker  amazon  common  import  numpy  to  record  serializer  from  sagemaker  predictor  import  csv  serializer  json  deserializerfrom  sagemaker  import  get  execution  role  role  get  execution  role  bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  lda  introduction  print  Training  input  output  will  be  stored  in  format  bucket  prefix  print  nIAM  Role  format  role  print  Generating  example  data  num  documents  6000  num  topics  known  alpha  known  beta  documents  topic ,amazon
 mixtures  generate  griffiths  data  num  documents  num  documents  num  topics  num  topics  vocabulary  size  len  documents  separate  the  generated  data  into  training  and  tests  subsets  num  documents  training  int  num  documents  num  documents  test  num  documents  num  documents  training  documents  training  documents  num  documents  training  documents  test  documents  num  documents  training  topic  mixtures  training  topic  mixtures  num  documents  training  topic  mixtures  test  topic  mixtures  num  documents  training  print  documents  training  shape  format  documents  training  shape  print  documents  test  shape  format  documents  test  shape  print  First  training  document  format  documents  print  nVocabulary  size  format  vocabulary  size  print  Known  topic  mixture  of  first  document  format  topic  mixtures  training  print  nNumber  of  topics  format  num  topics  print  Sum  of  elements  format  topic  mixtures  training  sum  matplotlib  inline  fig  p,amazon
lot  lda  documents  training  nrows  ncols  cmap  gray  with  colorbar  True  fig  suptitle  Example  Document  Word  Counts  fig  set  dpi  160  convert  documents  training  to  Protobuf  RecordIO  format  recordio  protobuf  serializer  numpy  to  record  serializer  fbuffer  recordio  protobuf  serializer  documents  training  upload  to  S3  in  bucket  prefix  train  fname  lda  data  s3  object  os  path  join  prefix  train  fname  boto3  Session  resource  s3  Bucket  bucket  Object  s3  object  upload  fileobj  fbuffer  s3  train  data  s3  format  bucket  s3  object  print  Uploaded  data  to  S3  format  s3  train  data  select  the  algorithm  container  based  on  this  notebook  current  location  containers  us  west  266724342769  dkr  ecr  us  west  amazonaws  com  lda  latest  us  east  766337827248  dkr  ecr  us  east  amazonaws  com  lda  latest  us  east  999911452149  dkr  ecr  us  east  amazonaws  com  lda  latest  eu  west  999678624901  dkr  ecr  eu  west  amazonaws  com  lda  lates,amazon
t  region  name  boto3  Session  region  name  container  containers  region  name  print  Using  SageMaker  LDA  container  format  container  region  name  session  sagemaker  Session  specify  general  training  job  information  lda  sagemaker  estimator  Estimator  container  role  output  path  s3  output  format  bucket  prefix  train  instance  count  train  instance  type  ml  c4  2xlarge  sagemaker  session  session  set  algorithm  specific  hyperparameters  lda  set  hyperparameters  num  topics  num  topics  feature  dim  vocabulary  size  mini  batch  size  num  documents  training  alpha0  run  the  training  job  on  input  data  stored  in  S3  lda  fit  train  s3  train  data  print  Training  job  name  format  lda  latest  training  job  job  name  lda  inference  lda  deploy  initial  instance  count  instance  type  ml  m4  xlarge  LDA  inference  may  work  better  at  scale  on  ml  c4  instances  print  Endpoint  name  format  lda  inference  endpoint  lda  inference  content  type  t,amazon
ext  csv  lda  inference  serializer  csv  serializer  lda  inference  deserializer  json  deserializerresults  lda  inference  predict  documents  test  12  print  results  computed  topic  mixtures  np  array  prediction  topic  mixture  for  prediction  in  results  predictions  print  computed  topic  mixtures  print  topic  mixtures  test  known  test  topic  mixture  print  computed  topic  mixtures  computed  topic  mixture  topics  permuted  sagemaker  Session  delete  endpoint  lda  inference  endpoint  ,amazon
From  AML  studio  it  looks  like  library  AzureML  ws  workspace  dat  download  datasets  ws  paho  who  cases  reported  2016  03  02  csv  Otherwise  dat  read  csv  paho  who  cases  reported  2016  03  02  csv  head  dat  50  install  packages  dplyr  repos  http  cloud  project  org  library  dplyr  summary  dat  colnames  dat  library  dplyr  dat  susp  filter  dat  Measure  Names  Suspected  Suspected  cases  head  dat  susp  dat  conf  filter  dat  Measure  Names  Suspected  head  dat  conf  Interactive  graphics  replace  the  lib  argument  with  your  local  user  library  install  packages  DT  htmlwidgets  repos  http  cloud  project  org  World  map  plotting  packages  install  packages  rworldmap  RColorBrewer  repos  http  cloud  project  org  Convert  categorical  to  numerical  remove  susp  cases  num  gsub  dat  susp  Measure  Values  fixed  dat  susp  Measure  Values  as  numeric  susp  cases  num  head  dat  susp  library  rworldmap  head  countrySynonyms  tmp  aggregate  dat  susp ,microsoft
 Measure  Values  by  list  Country  territory  dat  susp  Country  territory  FUN  sum  head  tmp  library  rworldmap  library  RColorBrewer  Some  renaming  of  Countries  needs  to  happen  because  of  non  standard  names  in  dataset  dat  susp  which  dat  susp  Country  territory  Bolivia  Country  territory  Bolivia  Plurinational  State  of  dat  susp  which  dat  susp  Country  territory  Bonaire  Country  territory  Bonaire  Sint  Eustatius  and  Saba  dat  susp  which  dat  susp  Country  territory  Curacao  Country  territory  Cura  ao  dat  susp  which  dat  susp  Country  territory  Saint  Martin  Country  territory  Saint  Martin  French  part  dat  susp  which  dat  susp  Country  territory  Sint  Maarten  Country  territory  Sint  Maarten  Dutch  part  dat  susp  which  dat  susp  Country  territory  United  States  Virgin  Islands  Country  territory  Virgin  Islands  dat  susp  which  dat  susp  Country  territory  Venezuela  Country  territory  Venezuela  Bolivarian  Republic  of  dat  s,microsoft
usp  ISO3  none  head  dat  susp  for  in  nrow  dat  susp  name  dat  susp  Country  territory  countrySynonyms  which  countrySynonyms  name1  name  ISO3  if  length  dat  susp  ISO3  toupper  else  dat  susp  ISO3  none  head  dat  susp  sPDF  joinCountryData2Map  dF  dat  susp  joinCode  ISO3  nameJoinColumn  ISO3  using  your  green  colours  numCats  10  set  number  of  categories  to  use  palette  colorRampPalette  brewer  pal  name  Greens  numCats  mapCountryData  sPDF  mapTitle  Zika  Virus  Suspected  Cases  2015  2016  by  Country  nameColumnToPlot  Measure  Values  catMethod  fixedWidth  numCats  numCats  colourPalette  palette  print  hello  Berlin  install  packages  plotly  repos  http  cloud  project  org  Work  in  progress  Scatter  Plot  library  plotly  set  seed  123  rnorm  1000  rchisq  1000  df  ncp  group  sample  LETTERS  size  1000  replace  size  sample  size  1000  replace  ds  data  frame  group  size  plot  ly  ds  mode  markers  group  group  size  size  layout  title  Scatt,microsoft
er  Plot  embed  notebook  library  plotly  set  seed  100  diamonds  sample  nrow  diamonds  1000  plot  ly  carat  price  text  paste  Clarity  clarity  mode  markers  color  carat  size  carat  embed  notebook  ,microsoft
import  numpy  as  np  import  tensorflow  as  tf  import  matplotlib  pyplot  as  plt  from  tensorflow  examples  tutorials  mnist  import  input  data  matplotlib  inline  mnist  input  data  read  data  sets  MNIST  data  one  hot  True  def  weight  variable  shape  name  return  tf  Variable  tf  truncated  normal  shape  shape  stddev  name  def  bias  variable  shape  name  return  tf  Variable  tf  zeros  shape  shape  name  def  plot  reconstruct  origin  img  reconstruct  img  10  plt  figure  figsize  10  for  in  range  display  original  ax  plt  subplot  plt  imshow  origin  img  reshape  28  28  plt  gray  ax  get  xaxis  set  visible  False  ax  get  yaxis  set  visible  False  display  reconstruction  ax  plt  subplot  plt  imshow  reconstruct  img  reshape  28  28  plt  gray  ax  get  xaxis  set  visible  False  ax  get  yaxis  set  visible  False  plt  show  batch  size  50  tf  placeholder  tf  float32  shape  None  784  weights  784  500  encoder  bias  500  encoder  tf  nn  relu  tf  ad,microsoft
d  tf  matmul  weights  500  200  encoder  append  bias  200  encoder  tf  nn  relu  tf  add  tf  matmul  latent  weights  500  latent  append  latent  latent  bias  latent  mean  tf  add  tf  matmul  latent  latent  log  sigma  tf  add  tf  matmul  latent  latent  eps  tf  random  normal  batch  size  ftype  tf  float32  tf  add  mean  tf  mul  tf  sqrt  tf  exp  log  sigma  eps  tf  reduce  sum  log  sigma  tf  square  mean  tf  exp  log  sigma  ,microsoft
time  import  boto3  from  sagemaker  import  get  execution  role  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  role  get  execution  role  bucket  bucket  name  customize  to  your  bucket  training  image  get  image  uri  boto3  Session  region  name  image  classification  import  os  import  urllib  request  def  download  url  filename  url  split  if  not  os  path  exists  filename  urllib  request  urlretrieve  url  filename  Caltech  256  image  files  download  http  www  vision  caltech  edu  Image  Datasets  Caltech256  256  ObjectCategories  tar  tar  xf  256  ObjectCategories  tar  Tool  for  creating  lst  file  download  https  raw  githubusercontent  com  apache  incubator  mxnet  master  tools  im2rec  py  bash  mkdir  caltech  256  train  60  for  in  256  ObjectCategories  do  basename  mkdir  caltech  256  train  60  for  in  ls  jpg  shuf  head  60  do  mv  caltech  256  train  60  done  done  python  im2rec  py  list  recursive  caltech  256  60  train  caltech,amazon
  256  train  60  python  im2rec  py  list  recursive  caltech  256  60  val  256  ObjectCategories  head  caltech  256  60  train  lst  example  lst  open  example  lst  lst  content  read  print  lst  content  Four  channels  train  validation  train  lst  and  validation  lst  s3train  s3  train  format  bucket  s3validation  s3  validation  format  bucket  s3train  lst  s3  train  lst  format  bucket  s3validation  lst  s3  validation  lst  format  bucket  upload  the  image  files  to  train  and  validation  channels  aws  s3  cp  caltech  256  train  60  s3train  recursive  quiet  aws  s3  cp  256  ObjectCategories  s3validation  recursive  quiet  upload  the  lst  files  to  train  lst  and  validation  lst  channels  aws  s3  cp  caltech  256  60  train  lst  s3train  lst  quiet  aws  s3  cp  caltech  256  60  val  lst  s3validation  lst  quiet  bash  python  im2rec  py  resize  256  quality  90  num  thread  16  caltech  256  60  val  256  ObjectCategories  python  im2rec  py  resize  256  quality  ,amazon
90  num  thread  16  caltech  256  60  train  caltech  256  train  60  The  algorithm  supports  multiple  network  depth  number  of  layers  They  are  18  34  50  101  152  and  200  For  this  training  we  will  use  18  layers  num  layers  18  we  need  to  specify  the  input  image  shape  for  the  training  data  image  shape  224  224  we  also  need  to  specify  the  number  of  training  samples  in  the  training  set  num  training  samples  15240  specify  the  number  of  output  classes  num  classes  257  batch  size  for  training  mini  batch  size  128  number  of  epochs  epochs  learning  rate  learning  rate  01  report  top  accuracy  top  resize  image  before  training  resize  256  period  to  store  model  parameters  in  number  of  epochs  in  this  case  we  will  save  parameters  from  epoch  and  checkpoint  frequency  Since  we  are  using  transfer  learning  we  set  use  pretrained  model  to  so  that  weights  can  be  initialized  with  pre  trained  weights  use  ,amazon
pretrained  model  time  import  time  import  boto3  from  time  import  gmtime  strftime  s3  boto3  client  s3  create  unique  job  name  job  name  prefix  sagemaker  imageclassification  notebook  timestamp  time  strftime  time  gmtime  job  name  job  name  prefix  timestamp  training  params  specify  the  training  docker  image  AlgorithmSpecification  TrainingImage  training  image  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  s3  output  format  bucket  job  name  prefix  ResourceConfig  InstanceCount  InstanceType  ml  p2  xlarge  VolumeSizeInGB  50  TrainingJobName  job  name  HyperParameters  image  shape  image  shape  num  layers  str  num  layers  num  training  samples  str  num  training  samples  num  classes  str  num  classes  mini  batch  size  str  mini  batch  size  epochs  str  epochs  learning  rate  str  learning  rate  top  str  top  resize  str  resize  checkpoint  frequency  str  checkpoint  frequency  use  pretrained  model  str  use  pretrained  mo,amazon
del  StoppingCondition  MaxRuntimeInSeconds  360000  Training  data  should  be  inside  subdirectory  called  train  Validation  data  should  be  inside  subdirectory  called  validation  The  algorithm  currently  only  supports  fullyreplicated  model  where  data  is  copied  onto  each  machine  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  image  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  image  CompressionType  None  ChannelName  train  lst  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  lst  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  image  CompressionType  None  ChannelName  validation  lst  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3,amazon
  validation  lst  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  image  CompressionType  None  print  Training  job  name  format  job  name  print  nInput  Data  Location  format  training  params  InputDataConfig  DataSource  S3DataSource  create  the  Amazon  SageMaker  training  job  sagemaker  boto3  client  service  name  sagemaker  sagemaker  create  training  job  training  params  confirm  that  the  training  job  has  started  status  sagemaker  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  Training  job  current  status  format  status  try  wait  for  the  job  to  finish  and  report  the  ending  status  sagemaker  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  except  print  Training  failed  to  start  if  exceptio,amazon
n  is  raised  that  means  it  has  failed  message  sagemaker  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  print  training  info  time  import  boto3  from  time  import  gmtime  strftime  sage  boto3  Session  client  service  name  sagemaker  timestamp  time  strftime  time  gmtime  model  name  image  classification  model  timestamp  print  model  name  info  sage  describe  training  job  TrainingJobName  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  data  hosting  image  get  image  uri  boto3  Session  region  name  image  classification  primary  container  Image  hosting  image  ModelDataUrl  model  data  create  model  response  sage  create  model  ModelName  model  name  ExecutionRoleArn  role  Prima,amazon
ryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  timestamp  time  strftime  time  gmtime  endpoint  config  name  job  name  prefix  epc  timestamp  endpoint  config  response  sage  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  p2  xlarge  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  configuration  name  format  endpoint  config  name  print  Endpoint  configuration  arn  format  endpoint  config  response  EndpointConfigArn  time  import  time  timestamp  time  strftime  time  gmtime  endpoint  name  job  name  prefix  ep  timestamp  print  Endpoint  name  format  endpoint  name  endpoint  params  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  endpoint  response  sagemaker  create  endpoint  endpoint  params  print  EndpointArn  format  endpoint  response  EndpointArn  get  the  status  of  the  endpoint  response ,amazon
 sagemaker  describe  endpoint  EndpointName  endpoint  name  status  response  EndpointStatus  print  EndpointStatus  format  status  try  sagemaker  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  finally  resp  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Create  endpoint  ended  with  status  status  if  status  InService  message  sagemaker  describe  endpoint  EndpointName  endpoint  name  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Endpoint  creation  did  not  succeed  import  boto3  runtime  boto3  Session  client  service  name  runtime  sagemaker  wget  tmp  test  jpg  http  www  vision  caltech  edu  Image  Datasets  Caltech256  images  008  bathtub  008  0007  jpg  file  name  tmp  test  jpg  test  image  from  IPython  display  import  Image  Image  file  name  import  json  import  numpy  as  np  with  open  file  name  rb  as  p,amazon
ayload  read  payload  bytearray  payload  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  image  Body  payload  result  response  Body  read  result  will  be  in  json  format  and  convert  it  to  ndarray  result  json  loads  result  the  result  will  output  the  probabilities  for  all  classes  find  the  class  with  maximum  probability  and  print  the  class  index  index  np  argmax  result  object  categories  ak47  american  flag  backpack  baseball  bat  baseball  glove  basketball  hoop  bat  bathtub  bear  beer  mug  billiards  binoculars  birdbath  blimp  bonsai  101  boom  box  bowling  ball  bowling  pin  boxing  glove  brain  101  breadmaker  buddha  101  bulldozer  butterfly  cactus  cake  calculator  camel  cannon  canoe  car  tire  cartman  cd  centipede  cereal  box  chandelier  101  chess  board  chimp  chopsticks  cockroach  coffee  mug  coffin  coin  comet  computer  keyboard  computer  monitor  computer  mouse  conch  cormorant  cover,amazon
ed  wagon  cowboy  hat  crab  101  desk  globe  diamond  ring  dice  dog  dolphin  101  doorknob  drinking  straw  duck  dumb  bell  eiffel  tower  electric  guitar  101  elephant  101  elk  ewer  101  eyeglasses  fern  fighter  jet  fire  extinguisher  fire  hydrant  fire  truck  fireworks  flashlight  floppy  disk  football  helmet  french  horn  fried  egg  frisbee  frog  frying  pan  galaxy  gas  pump  giraffe  goat  golden  gate  bridge  goldfish  golf  ball  goose  gorilla  grand  piano  101  grapes  grasshopper  guitar  pick  hamburger  hammock  harmonica  harp  harpsichord  hawksbill  101  head  phones  helicopter  101  hibiscus  homer  simpson  horse  horseshoe  crab  hot  air  balloon  hot  dog  hot  tub  hourglass  house  fly  human  skeleton  hummingbird  ibis  101  ice  cream  cone  iguana  ipod  iris  jesus  christ  joy  stick  kangaroo  101  kayak  ketch  101  killer  whale  knife  ladder  laptop  101  lathe  leopards  101  license  plate  lightbulb  light  house  lightning  llama  101  mailbox,amazon
  mandolin  mars  mattress  megaphone  menorah  101  microscope  microwave  minaret  minotaur  motorbikes  101  mountain  bike  mushroom  mussels  necktie  octopus  ostrich  owl  palm  pilot  palm  tree  paperclip  paper  shredder  pci  card  penguin  people  pez  dispenser  photocopier  picnic  table  playing  card  porcupine  pram  praying  mantis  pyramid  raccoon  radio  telescope  rainbow  refrigerator  revolver  101  rifle  rotary  phone  roulette  wheel  saddle  saturn  school  bus  scorpion  101  screwdriver  segway  self  propelled  lawn  mower  sextant  sheet  music  skateboard  skunk  skyscraper  smokestack  snail  snake  sneaker  snowmobile  soccer  ball  socks  soda  can  spaghetti  speed  boat  spider  spoon  stained  glass  starfish  101  steering  wheel  stirrups  sunflower  101  superman  sushi  swan  swiss  army  knife  sword  syringe  tambourine  teapot  teddy  bear  teepee  telephone  box  tennis  ball  tennis  court  tennis  racket  theodolite  toaster  tomato  tombstone  top  hat  tourin,amazon
g  bike  tower  pisa  traffic  light  treadmill  triceratops  tricycle  trilobite  101  tripod  shirt  tuning  fork  tweezer  umbrella  101  unicorn  vcr  video  projector  washing  machine  watch  101  waterfall  watermelon  welding  mask  wheelbarrow  windmill  wine  bottle  xylophone  yarmulke  yo  yo  zebra  airplanes  101  car  side  101  faces  easy  101  greyhound  tennis  shoes  toad  clutter  print  Result  label  object  categories  index  probability  str  result  index  sage  delete  endpoint  EndpointName  endpoint  name  ,amazon
Configure  SageMaker  import  boto3  import  sagemaker  import  warnings  from  sagemaker  mxnet  import  MXNet  warnings  simplefilter  ignore  sagemaker  session  sagemaker  Session  role  sagemaker  get  execution  role  Upload  the  Training  and  Testing  Data  to  S3  input  data  sagemaker  session  upload  data  path  tmp  data  key  prefix  input  data  bucket  input  data  split  print  S3  Bucket  Name  format  bucket  Create  MXNet  Estimator  mxnet  estimator  MXNet  model  py  role  role  train  instance  count  train  instance  type  ml  p3  2xlarge  output  path  s3  bucket  hyperparameters  epochs  12  optmizer  adam  learning  rate  batch  size  256  Run  the  Estimator  mxnet  estimator  fit  input  data  ,amazon
import  os  import  sagemaker  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  import  utils  utils  cifar10  download  inputs  sagemaker  session  upload  data  path  tmp  cifar10  data  key  prefix  data  DEMO  cifar10  cat  cifar10  cnn  pydef  keras  model  fn  hyperparameters  model  Sequential  model  add  InputLayer  input  shape  HEIGHT  WIDTH  DEPTH  name  PREDICT  INPUTS  model  add  Conv2D  32  padding  same  model  add  Activation  relu  model  add  Conv2D  32  model  add  Activation  relu  model  add  MaxPooling2D  pool  size  model  add  Dropout  25  model  add  Conv2D  64  padding  same  model  add  Activation  relu  model  add  Conv2D  64  model  add  Activation  relu  model  add  MaxPooling2D  pool  size  model  add  Dropout  25  model  add  Flatten  model  add  Dense  512  model  add  Activation  relu  model  add  Dropout  model  add  Dense  NUM  CLASSES  model  add  Activation  softmax  model  tf  keras  Model  inputs  model,amazon
  input  outputs  model  output  opt  RMSprop  lr  hyperparameters  learning  rate  decay  hyperparameters  decay  model  compile  loss  categorical  crossentropy  optimizer  opt  metrics  accuracy  return  modeldef  serving  input  fn  params  Notice  that  the  input  placeholder  has  the  same  input  shape  as  the  Keras  model  input  tensor  tf  placeholder  tf  float32  shape  None  HEIGHT  WIDTH  DEPTH  The  inputs  key  PREDICT  INPUTS  matches  the  Keras  InputLayer  name  inputs  PREDICT  INPUTS  tensor  return  tf  estimator  export  ServingInputReceiver  inputs  inputs  def  train  input  fn  training  dir  params  return  input  tf  estimator  ModeKeys  TRAIN  batch  size  BATCH  SIZE  data  dir  training  dir  def  eval  input  fn  training  dir  params  return  input  tf  estimator  ModeKeys  EVAL  batch  size  BATCH  SIZE  data  dir  training  dir  from  sagemaker  tensorflow  import  TensorFlow  estimator  TensorFlow  entry  point  cifar10  cnn  py  role  role  framework  version  hyperpa,amazon
rameters  learning  rate  1e  decay  1e  training  steps  1000  evaluation  steps  100  train  instance  count  train  instance  type  ml  c4  xlarge  estimator  fit  inputs  predictor  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  Creating  fake  prediction  data  import  numpy  as  np  data  np  random  randn  32  32  predictor  predict  data  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
library  ggplot2  library  randomForest  data  iris  head  iris  qplot  iris  Sepal  Length  geom  histogram  rf  randomForest  Species  data  iris  importance  TRUE  proximity  TRUE  table  iris  Species  rf  predicted  ,amazon
import  pandas  as  pd  import  boto3  import  jsoncomprehend  boto3  client  service  name  comprehend  text  love  my  girlfriend  print  Calling  DetectSentiment  print  json  dumps  comprehend  detect  sentiment  Text  text  LanguageCode  en  sort  keys  True  indent  print  End  of  DetectSentiment  def  sentiment  analysis  file  doc  open  file  output  doc  readlines  whole  doc  join  map  str  output  print  Calling  DetectSentiment  print  json  dumps  comprehend  detect  sentiment  Text  whole  doc  LanguageCode  en  sort  keys  True  indent  print  End  of  DetectSentiment  file  the  little  prince  txt  sentiment  analysis  file  ,amazon
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  ntm  synthetic  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  numpy  as  np  from  generate  example  data  import  generate  griffiths  data  plot  topic  data  import  io  import  os  import  time  import  json  import  sys  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  from  IPython  display  import  display  import  scipy  import  sagemaker  import  sagemaker  amazon  common  as  smac  from  sagemaker  predictor  import  csv  serializer  json  deserializer  generate  the  sample  data  num  documents  5000  num  topics  vocabulary  size  25  known  alpha  known  beta  documents  topic  mixtures  generate  griffiths  data  num  documents  num  documents  num  topics  num  topics  vocabulary  size  vocabulary  size  separate  the  generated  data  into  training  and  tests  subsets  num  documents  training  int  num  documents  num  documents  tes,amazon
t  num  documents  num  documents  training  documents  training  documents  num  documents  training  documents  test  documents  num  documents  training  topic  mixtures  training  topic  mixtures  num  documents  training  topic  mixtures  test  topic  mixtures  num  documents  training  data  training  documents  training  np  zeros  num  documents  training  data  test  documents  test  np  zeros  num  documents  test  print  First  training  document  format  documents  print  nVocabulary  size  format  vocabulary  size  np  set  printoptions  precision  suppress  True  print  Known  topic  mixture  of  first  training  document  format  topic  mixtures  training  print  nNumber  of  topics  format  num  topics  matplotlib  inline  fig  plot  topic  data  documents  training  10  nrows  ncols  cmap  gray  with  colorbar  False  fig  suptitle  Example  Documents  fig  set  dpi  160  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  data  training  astype  float32  buf  seek  key  ntm  data  ,amazon
boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  buf  s3  train  data  s3  train  format  bucket  prefix  key  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  ntm  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  ntm  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  ntm  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  ntm  latest  sess  sagemaker  Session  ntm  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  s3  output  format  bucket  prefix  sagemaker  session  sess  ntm  set  hyperparameters  num  topics  num  topics  feature  dim  vocabulary  size  ntm  fit  train  s3  train  data  ntm  predictor  ntm  deploy  initial  instance  count  instance  type  ml  m4  xlarge  ntm  predictor  content  type  text  csv  ntm  predictor  serializer  csv  serializer  ntm  predictor  ,amazon
deserializer  json  deserializerresults  ntm  predictor  predict  documents  training  10  print  results  predictions  np  array  prediction  topic  weights  for  prediction  in  results  predictions  print  predictions  print  topic  mixtures  training  known  topic  mixture  print  predictions  computed  topic  mixturedef  predict  batches  data  rows  1000  split  array  np  array  split  data  int  data  shape  float  rows  predictions  for  array  in  split  array  results  ntm  predictor  predict  array  predictions  topic  weights  for  in  results  predictions  return  np  array  predictions  predictions  predict  batches  documents  training  data  pd  DataFrame  np  concatenate  topic  mixtures  training  predictions  axis  columns  actual  format  for  in  range  predictions  format  for  in  range  display  data  corr  pd  plotting  scatter  matrix  pd  DataFrame  np  concatenate  topic  mixtures  training  predictions  axis  figsize  12  12  plt  show  sagemaker  Session  delete  endpoint  ntm  ,amazon
predictor  endpoint  ,amazon
2018  Click  on  this  cell  to  select  Insert  to  code  Insert  credentials  from  right  hand  panel  import  ibmos2spark  bmos  ibmos2spark  bluemix  sc  YourCredentials  learnspark  weather  sc  textFile  bmos  url  YourCredentials  container  2017  csv  weather  take  print  Total  records  in  the  data  set  weather  count  print  The  first  row  in  the  data  set  weather  first  header  sc  parallelize  STATION  DATE  METRIC  VALUE  C5  C6  C7  C8  union  header  union  weather  weather  union  weather  take  weatherParse  weather  map  lambda  line  line  split  weatherParse  first  weatherParse  first  weatherParse  first  weatherPrecp  weatherParse  filter  lambda  PRCP  is  the  station  is  the  precipitation  value  weatherPrecpCountByKey  weatherPrecp  map  lambda  int  weatherPrecpCountByKey  first  weatherPrecpAddByKey  weatherPrecpCountByKey  reduceByKey  lambda  v1  v2  v1  v2  v1  v2  weatherPrecpAddByKey  first  weatherAverages  weatherPrecpAddByKey  map  lambda  float  weatherAverag,ibm
es  first  for  pair  in  weatherAverages  top  10  print  Station  had  average  precipitations  of  pair  pair  precTop10  stationsTop10  for  pair  in  weatherAverages  map  lambda  top  10  precTop10  append  pair  stationsTop10  append  pair  print  Station  had  average  precipitations  of  pair  pair  matplotlib  inline  import  numpy  as  np  import  matplotlib  pyplot  as  plt  10  index  np  arange  bar  width  plt  bar  index  precTop10  bar  width  color  plt  xlabel  Stations  plt  ylabel  Precipitations  plt  title  10  stations  with  the  highest  average  precipitation  plt  xticks  index  bar  width  stationsTop10  rotation  90  plt  show  weatherSnow  weatherParse  filter  lambda  SNOW  weatherSnow  count  from  datetime  import  datetime  from  pyspark  sql  import  Row  spark  SparkSession  builder  getOrCreate  Convert  each  line  of  snowWeather  RDD  into  Row  object  snowRows  weatherSnow  map  lambda  Row  station  month  datetime  strptime  month  date  datetime  strptime  day  me,ibm
tric  value  int  Apply  Row  schema  snowSchema  spark  createDataFrame  snowRows  Register  snow2017  table  with  columns  station  month  date  metric  and  value  snowSchema  registerTempTable  snow2017  snow  US10chey021  spark  sql  SELECT  month  COUNT  AS  snowdays  FROM  snow2017  WHERE  station  US10chey021  GROUP  BY  month  ORDER  BY  month  collect  US10chey021  snowdays  12  for  row  in  snow  US10chey021  US10chey021  snowdays  row  month  row  snowdays  print  US10chey021  snowdays  ysnow  USW00094985  spark  sql  SELECT  month  COUNT  AS  snowdays  FROM  snow2017  WHERE  station  USW00094985  GROUP  BY  month  ORDER  BY  month  collect  USW00094985  snowdays  12  for  row  in  snow  USW00094985  USW00094985  snowdays  row  month  row  snowdays  print  USW00094985  snowdays  matplotlib  inline  import  matplotlib  import  numpy  as  np  import  matplotlib  pyplot  as  plt  12  ind  np  arange  width  35  pUS10chey021  plt  bar  ind  US10chey021  snowdays  width  color  label  US10chey021  pU,ibm
SW00094985  plt  bar  ind  width  USW00094985  snowdays  width  color  label  USW00094985  plt  ylabel  SNOW  DAYS  plt  xlabel  MONTH  plt  title  Snow  Days  in  2017  at  Stations  US10chey021  vs  USW00094985  plt  xticks  ind  width  Jan  Feb  Mar  Apr  May  Jun  Jul  Aug  Sep  Oct  Nov  Dec  plt  legend  plt  show  snowStations  spark  sql  SELECT  station  COUNT  AS  snowdays  FROM  snow2017  WHERE  station  LIKE  US  GROUP  BY  station  ORDER  BY  station  LIMIT  100  snowStations  head  snowStations  registerTempTable  snowdays  2017  snowStations  new  spark  sql  SELECT  station  snowdays  FROM  snowdays  2017  ORDER  BY  snowdays  DESC  LIMIT  collect  for  row  in  snowStations  new  print  rowsnowdays  100  spark  sql  SELECT  station  snowdays  FROM  snowdays  2017  snowday  stations  snowdays  100  rdd  map  lambda  snowdays  station  reduceByKey  lambda  for  snowday  in  snowday  stations  collect  print  Snow  days  str  snowday  Stations  str  snowday  Save  as  parquet  file  If  you  are,ibm
  running  this  cell  multiple  times  you  will  need  to  overwrite  the  data  in  the  parquet  file  snowStations  write  mode  overwrite  parquet  bmos  url  CONTAINER  snowStations  parquet  snowStations  write  parquet  bmos  url  YourCredentials  container  snowStations  parquet  parquetFile  spark  read  parquet  bmos  url  YourCredentials  container  snowStations  parquet  parquetFile  registerTempTable  snow  from  parquet  ,ibm
https  www  tensorflow  org  api  guides  python  array  ops  import  tensorflow  as  tf  import  numpy  as  np  import  pprint  tf  set  random  seed  777  for  reproducibility  pp  pprint  PrettyPrinter  indent  10  width  20  sess  tf  InteractiveSession  np  array  pp  pprint  print  ndim  rank  print  shape  shape  print  print  print  np  array  10  11  12  pp  pprint  print  ndim  rank  print  shape  shapet  tf  constant  tf  shape  eval  tf  shape  tf  constant  tf  shape  eval  tf  constant  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  tf  shape  eval  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  matrix1  tf  constant  matrix2  tf  constant  tf  shape  matrix2  eval  tf  matmul  matrix1  matrix2  eval  shape  matrix1  matrix2  eval  matrix1  tf  constant  matrix2  tf  constant  matrix1  matrix2  eval  matrix1  tf  constant  matrix2  tf  constant  matrix1  matrix2  eval  tf  random  normal  eval  tf  random  norm,ibm
al  dtype  tf  float32  mean  eval  tf  random  uniform  10  eval  tf  random  uniform  eval  tf  reduce  mean  axis  eval  tf  reduce  mean  eval  tf  reduce  mean  axis  eval  tf  reduce  mean  axis  eval  tf  reduce  mean  axis  eval  tf  reduce  sum  eval  tf  reduce  sum  axis  eval  tf  reduce  sum  axis  eval  tf  reduce  mean  tf  reduce  sum  axis  eval  tf  argmax  axis  eval  tf  argmax  axis  eval  tf  argmax  axis  eval  tf  argmin  axis  eval  np  array  10  11  shapetf  reshape  shape  eval  tf  reshape  shape  eval  tf  squeeze  eval  tf  expand  dims  eval  tf  one  hot  depth  eval  tf  one  hot  depth  eval  tf  one  hot  depth  tf  reshape  shape  eval  tf  cast  tf  int32  eval  tf  cast  True  False  tf  int32  eval  Pack  along  first  dim  tf  stack  eval  tf  stack  axis  eval  tf  ones  like  eval  tf  zeros  like  eval  for  in  zip  print  for  in  zip  print  np  array  10  11  pp  pprint  shape  pp  pprint  t1  tf  transpose  pp  pprint  sess  run  t1  shape  pp  pprint  sess  ru,ibm
n  t1  tf  transpose  t1  pp  pprint  sess  run  shape  pp  pprint  sess  run  t2  tf  transpose  pp  pprint  sess  run  t2  shape  pp  pprint  sess  run  t2  tf  transpose  t2  pp  pprint  sess  run  shape  pp  pprint  sess  run  ,ibm
import  pandas  as  pd  import  boto3  import  jsoncomprehend  boto3  client  service  name  comprehend  text  love  my  girlfriend  print  Calling  DetectSentiment  print  json  dumps  comprehend  detect  sentiment  Text  text  LanguageCode  en  sort  keys  True  indent  print  End  of  DetectSentiment  def  sentiment  analysis  file  doc  open  file  output  doc  readlines  whole  doc  join  map  str  output  print  Calling  DetectSentiment  print  json  dumps  comprehend  detect  sentiment  Text  whole  doc  LanguageCode  en  sort  keys  True  indent  print  End  of  DetectSentiment  file  the  little  prince  txt  sentiment  analysis  file  ,amazon
from  gcloud  nlp  api  import  Specify  some  gcs  uri  that  you  have  read  access  to  gcs  uri  gs  qa  nlp  appspot  com  messages  KoD6Ht5pST  rdxZTXkZ  res  nlp  gcs  gcs  uri  print  res  bucket  name  qa  nlp  appspot  com  blob  name  messages  KoD6Ht5pST  rdxZTXkZ  nlp  write  gcs  bucket  name  blob  name  res  res1  read  gcs  bucket  name  blob  name  print  res1  texts  Hello  world  President  Obama  is  speaking  at  the  White  House  Ladies  and  gentlemen  Google  Cloud  Natural  Language  API  REST  API  API  Google  Cloud  Storage  for  text  in  texts  split  print  nlp  text  text  ,google
from  sagemaker  import  get  execution  role  from  sagemaker  session  import  Session  sagemaker  session  Session  region  sagemaker  session  boto  session  region  name  sample  data  bucket  sagemaker  sample  data  format  region  S3  bucket  for  saving  files  Feel  free  to  redefine  this  variable  to  the  bucket  of  your  choice  bucket  sagemaker  session  default  bucket  Bucket  location  where  your  custom  code  will  be  saved  in  the  tar  gz  format  custom  code  upload  location  s3  mxnet  mnist  example  code  format  bucket  Bucket  location  where  results  of  model  training  are  saved  model  artifacts  location  s3  mxnet  mnist  example  artifacts  format  bucket  IAM  execution  role  that  gives  SageMaker  access  to  resources  in  your  AWS  account  We  can  use  the  SageMaker  Python  SDK  to  get  the  role  from  our  notebook  environment  role  get  execution  role  cat  mnist  pyfrom  sagemaker  mxnet  import  MXNet  mnist  estimator  MXNet  entry  point  mni,amazon
st  py  role  role  output  path  model  artifacts  location  code  location  custom  code  upload  location  train  instance  count  train  instance  type  ml  m4  xlarge  framework  version  hyperparameters  learning  rate  time  train  data  location  s3  mxnet  mnist  train  format  sample  data  bucket  test  data  location  s3  mxnet  mnist  test  format  sample  data  bucket  mnist  estimator  fit  train  train  data  location  test  test  data  location  transformer  mnist  estimator  transformer  instance  count  instance  type  ml  m4  xlarge  input  file  path  batch  transform  mnist  1000  samples  transformer  transform  s3  format  sample  data  bucket  input  file  path  content  type  text  csv  transformer  wait  print  transformer  output  path  import  ast  def  predicted  label  transform  output  output  ast  literal  eval  transform  output  probabilities  output  return  probabilities  index  max  probabilities  import  json  from  urllib  parse  import  urlparse  import  boto3  parsed,amazon
  url  urlparse  transformer  output  path  bucket  name  parsed  url  netloc  prefix  parsed  url  path  s3  boto3  resource  s3  predictions  for  in  range  10  file  key  data  csv  out  format  prefix  output  obj  s3  Object  bucket  name  file  key  output  output  obj  get  Body  read  decode  utf  predictions  append  predicted  label  output  import  os  tmp  dir  tmp  data  if  not  os  path  exists  tmp  dir  os  makedirs  tmp  dir  from  numpy  import  genfromtxt  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  for  in  range  10  input  file  name  data  csv  format  input  file  key  format  input  file  path  input  file  name  s3  Bucket  sample  data  bucket  download  file  input  file  key  os  path  join  tmp  dir  input  file  name  input  data  genfromtxt  os  path  join ,amazon
 tmp  dir  input  file  name  delimiter  show  digit  input  data  print  predictions  ,amazon
matplotlib  inline  import  sys  from  urllib  request  import  urlretrieve  import  zipfile  from  dateutil  parser  import  parse  import  json  from  random  import  shuffle  import  random  import  datetime  import  os  import  boto3  import  s3fs  import  sagemaker  import  numpy  as  np  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  from  future  import  print  function  from  ipywidgets  import  interact  interactive  fixed  interact  manual  import  ipywidgets  as  widgets  from  ipywidgets  import  IntSlider  FloatSlider  Checkbox  set  random  seeds  for  reproducibility  np  random  seed  42  random  seed  42  sagemaker  session  sagemaker  Session  s3  bucket  sagemaker  Session  default  bucket  replace  with  an  existing  bucket  if  needed  s3  prefix  deepar  electricity  demo  notebook  prefix  used  for  all  data  stored  within  the  bucket  role  sagemaker  get  execution  role  IAM  role  to  use  by  SageMakerregion  sagemaker  session  boto  region  name  s3  data  path,amazon
  s3  data  format  s3  bucket  s3  prefix  s3  output  path  s3  output  format  s3  bucket  s3  prefix  image  name  sagemaker  amazon  amazon  estimator  get  image  uri  region  forecasting  deepar  latest  DATA  HOST  https  archive  ics  uci  edu  DATA  PATH  ml  machine  learning  databases  00321  ARCHIVE  NAME  LD2011  2014  txt  zip  FILE  NAME  ARCHIVE  NAME  def  progress  report  hook  count  block  size  total  size  mb  int  count  block  size  1e6  if  count  500  sys  stdout  write  MB  downloaded  format  mb  sys  stdout  flush  if  not  os  path  isfile  FILE  NAME  print  downloading  dataset  258MB  can  take  few  minutes  depending  on  your  connection  urlretrieve  DATA  HOST  DATA  PATH  ARCHIVE  NAME  ARCHIVE  NAME  reporthook  progress  report  hook  print  nextracting  data  archive  zip  ref  zipfile  ZipFile  ARCHIVE  NAME  zip  ref  extractall  zip  ref  close  else  print  File  found  skipping  download  data  pd  read  csv  FILE  NAME  sep  index  col  parse  dates  True  de,amazon
cimal  num  timeseries  data  shape  data  kw  data  resample  2H  sum  timeseries  for  in  range  num  timeseries  timeseries  append  np  trim  zeros  data  kw  iloc  trim  fig  axs  plt  subplots  figsize  20  20  sharex  True  axx  axs  ravel  for  in  range  10  timeseries  loc  2014  01  01  2014  01  14  plot  ax  axx  axx  set  xlabel  date  axx  set  ylabel  kW  consumption  axx  grid  which  minor  axis  we  use  hour  frequency  for  the  time  series  freq  2H  we  predict  for  days  prediction  length  12  we  also  use  days  as  context  length  this  is  the  number  of  state  updates  accomplished  before  making  predictions  context  length  12start  dataset  pd  Timestamp  2014  01  01  00  00  00  freq  freq  end  training  pd  Timestamp  2014  09  01  00  00  00  freq  freq  training  data  start  str  start  dataset  target  ts  start  dataset  end  training  tolist  We  use  because  pandas  indexing  includes  the  upper  bound  for  ts  in  timeseries  print  len  training  data  ,amazon
num  test  windows  test  data  start  str  start  dataset  target  ts  start  dataset  end  training  prediction  length  tolist  for  in  range  num  test  windows  for  ts  in  timeseries  print  len  test  data  def  write  dicts  to  file  path  data  with  open  path  wb  as  fp  for  in  data  fp  write  json  dumps  encode  utf  fp  write  encode  utf  time  write  dicts  to  file  train  json  training  data  write  dicts  to  file  test  json  test  data  s3  boto3  resource  s3  def  copy  to  s3  local  file  s3  path  override  False  assert  s3  path  startswith  s3  split  s3  path  split  bucket  split  path  join  split  buk  s3  Bucket  bucket  if  len  list  buk  objects  filter  Prefix  path  if  not  override  print  File  s3  already  exists  nSet  override  to  upload  anyway  format  s3  bucket  s3  path  return  else  print  Overwriting  existing  file  with  open  local  file  rb  as  data  print  Uploading  file  to  format  s3  path  buk  put  object  Key  path  Body  data  time  c,amazon
opy  to  s3  train  json  s3  data  path  train  train  json  copy  to  s3  test  json  s3  data  path  test  test  json  s3filesystem  s3fs  S3FileSystem  with  s3filesystem  open  s3  data  path  train  train  json  rb  as  fp  print  fp  readline  decode  utf  100  estimator  sagemaker  estimator  Estimator  sagemaker  session  sagemaker  session  image  name  image  name  role  role  train  instance  count  train  instance  type  ml  c4  2xlarge  base  job  name  deepar  electricity  demo  output  path  s3  output  path  hyperparameters  time  freq  freq  epochs  400  early  stopping  patience  40  mini  batch  size  64  learning  rate  5E  context  length  str  context  length  prediction  length  str  prediction  length  estimator  set  hyperparameters  hyperparameters  time  data  channels  train  train  format  s3  data  path  test  test  format  s3  data  path  estimator  fit  inputs  data  channels  wait  True  class  DeepARPredictor  sagemaker  predictor  RealTimePredictor  def  init  self  args  k,amazon
wargs  super  init  args  content  type  sagemaker  content  types  CONTENT  TYPE  JSON  kwargs  def  predict  self  ts  cat  None  dynamic  feat  None  num  samples  100  return  samples  False  quantiles  Requests  the  prediction  of  for  the  time  series  listed  in  ts  each  with  the  optional  corresponding  category  listed  in  cat  ts  pandas  Series  object  the  time  series  to  predict  cat  integer  the  group  associated  to  the  time  series  default  None  num  samples  integer  number  of  samples  to  compute  at  prediction  time  default  100  return  samples  boolean  indicating  whether  to  include  samples  in  the  response  default  False  quantiles  list  of  strings  specifying  the  quantiles  to  compute  default  Return  value  list  of  pandas  DataFrame  objects  each  containing  the  predictions  prediction  time  ts  index  quantiles  str  for  in  quantiles  req  self  encode  request  ts  cat  dynamic  feat  num  samples  return  samples  quantiles  res  super  Deep,amazon
ARPredictor  self  predict  req  return  self  decode  response  res  ts  index  freq  prediction  time  return  samples  def  encode  request  self  ts  cat  dynamic  feat  num  samples  return  samples  quantiles  instance  series  to  dict  ts  cat  if  cat  is  not  None  else  None  dynamic  feat  if  dynamic  feat  else  None  configuration  num  samples  num  samples  output  types  quantiles  samples  if  return  samples  else  quantiles  quantiles  quantiles  http  request  data  instances  instance  configuration  configuration  return  json  dumps  http  request  data  encode  utf  def  decode  response  self  response  freq  prediction  time  return  samples  we  only  sent  one  time  series  so  we  only  receive  one  in  return  however  if  possible  one  will  pass  multiple  time  series  as  predictions  will  then  be  faster  predictions  json  loads  response  decode  utf  predictions  prediction  length  len  next  iter  predictions  quantiles  values  prediction  index  pd  DatetimeIn,amazon
dex  start  prediction  time  freq  freq  periods  prediction  length  if  return  samples  dict  of  samples  sample  str  for  in  enumerate  predictions  samples  else  dict  of  samples  return  pd  DataFrame  data  predictions  quantiles  dict  of  samples  index  prediction  index  def  set  frequency  self  freq  self  freq  freq  def  encode  target  ts  return  if  np  isfinite  else  NaN  for  in  ts  def  series  to  dict  ts  cat  None  dynamic  feat  None  Given  pandas  Series  object  returns  dictionary  encoding  the  time  series  ts  pands  Series  object  with  the  target  time  series  cat  an  integer  indicating  the  time  series  category  Return  value  dictionary  obj  start  str  ts  index  target  encode  target  ts  if  cat  is  not  None  obj  cat  cat  if  dynamic  feat  is  not  None  obj  dynamic  feat  dynamic  feat  return  objpredictor  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  predictor  cls  DeepARPredictor  predictor  predict  ts  tim,amazon
eseries  120  quantiles  10  90  head  def  plot  predictor  target  ts  cat  None  dynamic  feat  None  forecast  date  end  training  show  samples  False  plot  history  12  confidence  80  print  calling  served  model  to  generate  predictions  starting  from  format  str  forecast  date  assert  confidence  50  and  confidence  100  low  quantile  confidence  005  up  quantile  confidence  005  we  first  construct  the  argument  to  call  our  model  args  ts  target  ts  forecast  date  return  samples  show  samples  quantiles  low  quantile  up  quantile  num  samples  100  if  dynamic  feat  is  not  None  args  dynamic  feat  dynamic  feat  fig  plt  figure  figsize  20  ax  plt  subplot  else  fig  plt  figure  figsize  20  ax  plt  subplot  if  cat  is  not  None  args  cat  cat  ax  text  cat  format  cat  transform  ax  transAxes  call  the  end  point  to  get  the  prediction  prediction  predictor  predict  args  plot  the  samples  if  show  samples  for  key  in  prediction  keys  if  s,amazon
ample  in  key  prediction  key  plot  color  lightskyblue  alpha  label  nolegend  plot  the  target  target  section  target  ts  forecast  date  plot  history  forecast  date  prediction  length  target  section  plot  color  black  label  target  plot  the  confidence  interval  and  the  median  predicted  ax  fill  between  prediction  str  low  quantile  index  prediction  str  low  quantile  values  prediction  str  up  quantile  values  color  alpha  label  confidence  interval  format  confidence  prediction  plot  color  label  P50  ax  legend  loc  fix  the  scale  as  the  samples  may  change  it  ax  set  ylim  target  section  min  target  section  max  if  dynamic  feat  is  not  None  for  in  enumerate  dynamic  feat  start  ax  plt  subplot  len  dynamic  feat  len  dynamic  feat  sharex  ax  feat  ts  pd  Series  index  pd  DatetimeIndex  start  target  ts  index  freq  target  ts  index  freq  periods  len  data  feat  ts  forecast  date  plot  history  forecast  date  prediction  length,amazon
  plot  ax  ax  color  style  description  width  initial  interact  manual  customer  id  IntSlider  min  max  369  value  91  style  style  forecast  day  IntSlider  min  max  100  value  51  style  style  confidence  IntSlider  min  60  max  95  value  80  step  style  style  history  weeks  plot  IntSlider  min  max  20  value  style  style  show  samples  Checkbox  value  False  continuous  update  False  def  plot  interact  customer  id  forecast  day  confidence  history  weeks  plot  show  samples  plot  predictor  target  ts  timeseries  customer  id  forecast  date  end  training  datetime  timedelta  days  forecast  day  show  samples  show  samples  plot  history  history  weeks  plot  12  confidence  confidence  def  create  special  day  feature  ts  fraction  05  First  select  random  day  indices  plus  the  forecast  day  num  days  ts  index  ts  index  days  rand  indices  list  np  random  randint  num  days  int  num  days  num  days  feature  value  np  zeros  like  ts  for  in  rand  ,amazon
indices  feature  value  12  12  feature  pd  Series  index  ts  index  data  feature  value  return  feature  def  drop  at  random  ts  drop  probability  assert  drop  probability  random  mask  np  random  random  len  ts  drop  probability  return  ts  mask  random  mask  special  day  features  create  special  day  feature  ts  for  ts  in  timeseries  timeseries  uplift  ts  feat  for  ts  feat  in  zip  timeseries  special  day  features  time  series  processed  drop  at  random  ts  for  ts  in  timeseries  uplift  fig  axs  plt  subplots  figsize  20  20  sharex  True  axx  axs  ravel  for  in  range  10  ax  axx  ts  time  series  processed  400  ts  plot  ax  ax  ax  set  ylim  ts  max  ts  max  ax2  ax  twinx  special  day  features  400  plot  ax  ax2  color  ax2  set  ylim  time  training  data  new  features  start  str  start  dataset  target  encode  target  ts  start  dataset  end  training  dynamic  feat  special  day  features  start  dataset  end  training  tolist  for  ts  in  enumera,amazon
te  time  series  processed  print  len  training  data  new  features  as  in  our  previous  example  we  do  rolling  evaluation  over  the  next  days  num  test  windows  test  data  new  features  start  str  start  dataset  target  encode  target  ts  start  dataset  end  training  prediction  length  dynamic  feat  special  day  features  start  dataset  end  training  prediction  length  tolist  for  in  range  num  test  windows  for  ts  in  enumerate  timeseries  uplift  def  check  dataset  consistency  train  dataset  test  dataset  None  train  dataset  has  dynamic  feat  dynamic  feat  in  if  has  dynamic  feat  num  dynamic  feat  len  dynamic  feat  has  cat  cat  in  if  has  cat  num  cat  len  cat  def  check  ds  ds  for  in  enumerate  ds  if  has  dynamic  feat  assert  dynamic  feat  in  assert  num  dynamic  feat  len  dynamic  feat  for  in  dynamic  feat  assert  len  target  len  if  has  cat  assert  cat  in  assert  len  cat  num  cat  check  ds  train  dataset  if  test  data,amazon
set  is  not  None  check  ds  test  dataset  check  dataset  consistency  training  data  new  features  test  data  new  features  time  write  dicts  to  file  train  new  features  json  training  data  new  features  write  dicts  to  file  test  new  features  json  test  data  new  features  time  s3  data  path  new  features  s3  new  features  data  format  s3  bucket  s3  prefix  s3  output  path  new  features  s3  new  features  output  format  s3  bucket  s3  prefix  print  Uploading  to  S3  this  may  take  few  minutes  depending  on  your  connection  copy  to  s3  train  new  features  json  s3  data  path  new  features  train  train  new  features  json  override  True  copy  to  s3  test  new  features  json  s3  data  path  new  features  test  test  new  features  json  override  True  time  estimator  new  features  sagemaker  estimator  Estimator  sagemaker  session  sagemaker  session  image  name  image  name  role  role  train  instance  count  train  instance  type  ml  c4  2xlar,amazon
ge  base  job  name  deepar  electricity  demo  new  features  output  path  s3  output  path  new  features  hyperparameters  time  freq  freq  context  length  str  context  length  prediction  length  str  prediction  length  epochs  400  learning  rate  5E  mini  batch  size  64  early  stopping  patience  40  num  dynamic  feat  auto  this  will  use  the  dynamic  feat  field  if  it  present  in  the  data  estimator  new  features  set  hyperparameters  hyperparameters  estimator  new  features  fit  inputs  train  train  format  s3  data  path  new  features  test  test  format  s3  data  path  new  features  wait  True  time  predictor  new  features  estimator  new  features  deploy  initial  instance  count  instance  type  ml  m4  xlarge  predictor  cls  DeepARPredictor  customer  id  120  predictor  new  features  predict  ts  time  series  processed  customer  id  prediction  length  dynamic  feat  special  day  features  customer  id  tolist  quantiles  head  interact  manual  customer  id  In,amazon
tSlider  min  max  369  value  13  style  style  forecast  day  IntSlider  min  max  100  value  21  style  style  confidence  IntSlider  min  60  max  95  value  80  step  style  style  missing  ratio  FloatSlider  min  max  95  value  step  05  style  style  show  samples  Checkbox  value  False  continuous  update  False  def  plot  interact  customer  id  forecast  day  confidence  missing  ratio  show  samples  forecast  date  end  training  datetime  timedelta  days  forecast  day  target  time  series  processed  customer  id  start  dataset  forecast  date  prediction  length  target  drop  at  random  target  missing  ratio  dynamic  feat  special  day  features  customer  id  start  dataset  forecast  date  prediction  length  tolist  plot  predictor  new  features  target  ts  target  dynamic  feat  dynamic  feat  forecast  date  forecast  date  show  samples  show  samples  plot  history  12  confidence  confidence  predictor  delete  endpoint  predictor  new  features  delete  endpoint  ,amazon
Put  file  path  as  string  here  CIFAR  DIR  cifar  10  batches  py  def  unpickle  file  import  pickle  with  open  file  rb  as  fo  cifar  dict  pickle  load  fo  encoding  bytes  return  cifar  dictdirs  batches  meta  data  batch  data  batch  data  batch  data  batch  data  batch  test  batch  all  data  for  direc  in  zip  all  data  dirs  all  data  unpickle  CIFAR  DIR  direc  batch  meta  all  data  data  batch1  all  data  data  batch2  all  data  data  batch3  all  data  data  batch4  all  data  data  batch5  all  data  test  batch  all  data  batch  metadata  batch1  keys  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  npX  data  batch1  data  reshape  10000  32  32  transpose  astype  uint8  max  255  max  plt  imshow  plt  imshow  plt  imshow  def  one  hot  encode  vec  vals  10  For  use  to  one  hot  encode  the  10  possible  labels  len  vec  out  np  zeros  vals  out  range  vec  return  outclass  CifarHelper  def  init  self  self  self  all  train  batc,amazon
hes  data  batch1  data  batch2  data  batch3  data  batch4  data  batch5  self  test  batch  test  batch  self  training  images  None  self  training  labels  None  self  test  images  None  self  test  labels  None  def  set  up  images  self  print  Setting  Up  Training  Images  and  Labels  self  training  images  np  vstack  data  for  in  self  all  train  batches  train  len  len  self  training  images  self  training  images  self  training  images  reshape  train  len  32  32  transpose  255  self  training  labels  one  hot  encode  np  hstack  labels  for  in  self  all  train  batches  10  print  Setting  Up  Test  Images  and  Labels  self  test  images  np  vstack  data  for  in  self  test  batch  test  len  len  self  test  images  self  test  images  self  test  images  reshape  test  len  32  32  transpose  255  self  test  labels  one  hot  encode  np  hstack  labels  for  in  self  test  batch  10  def  next  batch  self  batch  size  self  training  images  self  self  batch  size  res,amazon
hape  100  32  32  self  training  labels  self  self  batch  size  self  self  batch  size  len  self  training  images  return  Before  Your  tf  Session  run  these  two  lines  ch  CifarHelper  ch  set  up  images  During  your  session  to  grab  the  next  batch  use  this  line  Just  like  we  did  for  mnist  train  next  batch  batch  ch  next  batch  100  import  tensorflow  as  tfx  tf  placeholder  tf  float32  shape  None  32  32  true  tf  placeholder  tf  float32  shape  None  10  hold  prob  tf  placeholder  tf  float32  def  init  weights  shape  init  random  dist  tf  truncated  normal  shape  stddev  return  tf  Variable  init  random  dist  def  init  bias  shape  init  bias  vals  tf  constant  shape  shape  return  tf  Variable  init  bias  vals  def  conv2d  return  tf  nn  conv2d  strides  padding  SAME  def  max  pool  2by2  return  tf  nn  max  pool  ksize  strides  padding  SAME  def  convolutional  layer  input  shape  init  weights  shape  init  bias  shape  return  tf  nn  relu,amazon
  conv2d  input  def  normal  full  layer  input  layer  size  input  size  int  input  layer  get  shape  init  weights  input  size  size  init  bias  size  return  tf  matmul  input  layer  bconvo  convolutional  layer  shape  32  convo  pooling  max  pool  2by2  convo  convo  convolutional  layer  convo  pooling  shape  32  64  convo  pooling  max  pool  2by2  convo  64convo  flat  tf  reshape  convo  pooling  64  full  layer  one  tf  nn  relu  normal  full  layer  convo  flat  1024  full  one  dropout  tf  nn  dropout  full  layer  one  keep  prob  hold  prob  pred  normal  full  layer  full  one  dropout  10  cross  entropy  tf  reduce  mean  tf  nn  softmax  cross  entropy  with  logits  labels  true  logits  pred  optimizer  tf  train  AdamOptimizer  learning  rate  001  train  optimizer  minimize  cross  entropy  init  tf  global  variables  initializer  with  tf  Session  as  sess  sess  run  tf  global  variables  initializer  for  in  range  5000  batch  ch  next  batch  100  sess  run  train  fe,amazon
ed  dict  batch  true  batch  hold  prob  PRINT  OUT  MESSAGE  EVERY  100  STEPS  if  100  print  Currently  on  step  format  print  Accuracy  is  Test  the  Train  Model  matches  tf  equal  tf  argmax  pred  tf  argmax  true  acc  tf  reduce  mean  tf  cast  matches  tf  float32  print  sess  run  acc  feed  dict  ch  test  images  true  ch  test  labels  hold  prob  print  ,amazon
from  IPython  core  debugger  import  set  trace  import  azure  mgmt  billing  import  adal  from  msrestazure  azure  active  directory  import  AADTokenCredentials  from  dotenv  import  load  dotenv  find  dotenv  import  os  load  dotenv  find  dotenv  Parameters  need  for  API  subscription  os  getenv  AZURE  SUBSCRIPTION  ID  tenant  os  getenv  AZURE  TENANT  ID  client  id  os  getenv  AZURE  CLIENT  ID  client  secret  os  getenv  AZURE  CLIENT  SECRET  def  authenticate  client  key  tenant  client  id  client  secret  Authenticate  using  service  principal  key  authority  host  uri  https  login  microsoftonline  com  authority  uri  authority  host  uri  tenant  resource  uri  https  management  core  windows  net  context  adal  AuthenticationContext  authority  uri  api  version  None  mgmt  token  context  acquire  token  with  client  credentials  resource  uri  client  id  client  secret  credentials  AADTokenCredentials  mgmt  token  client  id  return  credentialscredentials  authenti,microsoft
cate  client  key  tenant  client  id  client  secret  access  token  credentials  token  get  access  token  client  azure  mgmt  billing  BillingManagementClient  credentials  subscription  client  dict  invoice  client  invoices  get  latest  invoices  client  invoices  list  invoice  list  list  invoices  invoice  list  as  dict  invoice  is  xml  model  invoice  download  url  urlperiods  client  billing  periods  list  for  period  in  periods  print  period  dict  client  operations  list  for  op  in  client  operations  list  print  op  dict  ,microsoft
Lab  10  MNIST  and  High  level  TF  API  from  tensorflow  contrib  layers  import  fully  connected  batch  norm  dropout  from  tensorflow  contrib  framework  import  arg  scope  import  tensorflow  as  tf  import  random  import  matplotlib  pyplot  as  plt  from  tensorflow  examples  tutorials  mnist  import  input  data  tf  set  random  seed  777  reproducibility  parameters  learning  rate  01  we  can  use  large  learning  rate  using  Batch  Normalization  training  epochs  15  batch  size  100  keep  prob  input  place  holders  tf  placeholder  tf  float32  None  784  tf  placeholder  tf  float32  None  10  train  mode  tf  placeholder  tf  bool  name  train  mode  layer  output  size  hidden  output  size  512  final  output  size  10xavier  init  tf  contrib  layers  xavier  initializer  bn  params  is  training  train  mode  decay  updates  collections  None  We  can  build  short  code  using  arg  scope  to  avoid  duplicate  code  same  function  with  different  arguments  with  arg  sc,ibm
ope  fully  connected  activation  fn  tf  nn  relu  weights  initializer  xavier  init  biases  initializer  None  normalizer  fn  batch  norm  normalizer  params  bn  params  hidden  layer1  fully  connected  hidden  output  size  scope  h1  h1  drop  dropout  hidden  layer1  keep  prob  is  training  train  mode  hidden  layer2  fully  connected  h1  drop  hidden  output  size  scope  h2  h2  drop  dropout  hidden  layer2  keep  prob  is  training  train  mode  hidden  layer3  fully  connected  h2  drop  hidden  output  size  scope  h3  h3  drop  dropout  hidden  layer3  keep  prob  is  training  train  mode  hidden  layer4  fully  connected  h3  drop  hidden  output  size  scope  h4  h4  drop  dropout  hidden  layer4  keep  prob  is  training  train  mode  hypothesis  fully  connected  h4  drop  final  output  size  activation  fn  None  scope  hypothesis  define  cost  loss  optimizer  cost  tf  reduce  mean  tf  nn  softmax  cross  entropy  with  logits  logits  hypothesis  labels  optimizer  tf  train ,ibm
 AdamOptimizer  learning  rate  learning  rate  minimize  cost  initialize  sess  tf  Session  sess  run  tf  global  variables  initializer  mnist  input  data  read  data  sets  MNIST  data  one  hot  True  Check  out  https  www  tensorflow  org  get  started  mnist  beginners  for  more  information  about  the  mnist  dataset  train  my  model  for  epoch  in  range  training  epochs  avg  cost  total  batch  int  mnist  train  num  examples  batch  size  for  in  range  total  batch  batch  xs  batch  ys  mnist  train  next  batch  batch  size  feed  dict  train  batch  xs  batch  ys  train  mode  True  feed  dict  cost  batch  xs  batch  ys  train  mode  False  opt  sess  run  optimizer  feed  dict  feed  dict  train  sess  run  cost  feed  dict  feed  dict  cost  avg  cost  total  batch  print  Epoch  cost  format  epoch  avg  cost  print  Epoch  04d  epoch  cost  9f  format  avg  cost  print  Learning  Finished  Test  model  and  check  accuracy  correct  prediction  tf  equal  tf  argmax  hypothesis,ibm
  tf  argmax  accuracy  tf  reduce  mean  tf  cast  correct  prediction  tf  float32  print  Accuracy  sess  run  accuracy  feed  dict  mnist  test  images  mnist  test  labels  train  mode  False  Get  one  and  predict  random  randint  mnist  test  num  examples  print  Label  sess  run  tf  argmax  mnist  test  labels  print  Prediction  sess  run  tf  argmax  hypothesis  feed  dict  mnist  test  images  train  mode  False  plt  imshow  mnist  test  images  reshape  28  28  cmap  Greys  interpolation  nearest  plt  show  Epoch  cost  519417209  Epoch  cost  432551052  Epoch  cost  404978843  Epoch  cost  392039919  Epoch  cost  382165317  Epoch  cost  377987834  Epoch  cost  372577601  Epoch  cost  367208552  Epoch  cost  365525589  Epoch  10  cost  361964276  Epoch  11  cost  359540287  Epoch  12  cost  356423751  Epoch  13  cost  354478216  Epoch  14  cost  353212552  Epoch  15  cost  35230893  Learning  Finished  Accuracy  9826  ,ibm
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  kmeans  byom  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  numpy  as  np  import  sklearn  cluster  import  pickle  import  gzip  import  urllib  request  import  json  import  mxnet  as  mx  import  boto3  import  time  import  io  import  osurllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  gzip  open  mnist  pkl  gz  rb  train  set  valid  set  test  set  pickle  load  encoding  latin1  close  kmeans  sklearn  cluster  KMeans  clusters  10  fit  train  set  centroids  mx  ndarray  array  kmeans  cluster  centers  mx  ndarray  save  model  algo  centroids  tar  czvf  model  tar  gz  model  algo  1boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  model  tar  gz  upload  file  model  tar  gz  kmeans  model  DEMO  kmeans  byom  time  strftime  time  gmtime  sm  boto3  client  sagemaker  con,amazon
tainers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  kmeans  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  kmeans  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  kmeans  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  kmeans  latest  create  model  response  sm  create  model  ModelName  kmeans  model  ExecutionRoleArn  role  PrimaryContainer  Image  containers  boto3  Session  region  name  ModelDataUrl  s3  model  tar  gz  format  bucket  prefix  print  create  model  response  ModelArn  kmeans  endpoint  config  DEMO  kmeans  byom  endpoint  config  time  strftime  time  gmtime  print  kmeans  endpoint  config  create  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  kmeans  endpoint  config  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  kmeans  model  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  tim,amazon
e  kmeans  endpoint  DEMO  kmeans  byom  endpoint  time  strftime  time  gmtime  print  kmeans  endpoint  create  endpoint  response  sm  create  endpoint  EndpointName  kmeans  endpoint  EndpointConfigName  kmeans  endpoint  config  print  create  endpoint  response  EndpointArn  resp  sm  describe  endpoint  EndpointName  kmeans  endpoint  status  resp  EndpointStatus  print  Status  status  sm  get  waiter  endpoint  in  service  wait  EndpointName  kmeans  endpoint  resp  sm  describe  endpoint  EndpointName  kmeans  endpoint  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Status  status  if  status  InService  raise  Exception  Endpoint  creation  did  not  succeed  def  np2csv  arr  csv  io  BytesIO  np  savetxt  csv  arr  delimiter  fmt  return  csv  getvalue  decode  rstrip  runtime  boto3  Session  client  runtime  sagemaker  payload  np2csv  train  set  100  response  runtime  invoke  endpoint  EndpointName  kmeans  endpoint  ContentType  text  csv  Body  payload  result  json  ,amazon
loads  response  Body  read  decode  scored  labels  np  array  closest  cluster  for  in  result  predictions  scored  labels  kmeans  labels  100  Remove  endpoint  to  avoid  stray  charges  sm  delete  endpoint  EndpointName  kmeans  endpoint  ,amazon
import  os  import  boto3  import  sagemaker  from  sagemaker  mxnet  import  MXNet  from  mxnet  import  gluon  nd  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  bash  sample  notebooks  sagemaker  python  sdk  mxnet  gluon  mnist  setup  sh  role  get  execution  role  gluon  data  vision  FashionMNIST  data  train  train  True  gluon  data  vision  FashionMNIST  data  test  train  False  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  mnist  cat  fmnist  py  batch  size  100  epochs  10  learning  rate  01  momentum  log  interval  100m  MXNet  fmnist  py  role  role  train  instance  count  train  instance  type  local  hyperparameters  batch  size  batch  size  epochs  epochs  learning  rate  learning  rate  momentum  momentum  log  interval  log  interval  fit  inputs  predictor  deploy  initial  instance  count  instance  type  ml  m4  xlarge  import  numpy  as  np  import  gzip  import  struct  import  matplotlib  pyplot  as  plt  matpl,amazon
otlib  inline  def  read  data  label  url  image  url  with  gzip  open  label  url  as  flbl  magic  num  struct  unpack  II  flbl  read  label  np  fromstring  flbl  read  dtype  np  int8  with  gzip  open  image  url  rb  as  fimg  magic  num  rows  cols  struct  unpack  IIII  fimg  read  16  image  np  fromstring  fimg  read  dtype  np  uint8  reshape  len  label  rows  cols  return  label  image  val  lbl  val  img  read  data  data  test  t10k  labels  idx1  ubyte  gz  data  test  t10k  images  idx3  ubyte  gz  idx  32  This  number  can  be  changed  to  get  another  image  plt  imshow  val  img  idx  cmap  Greys  plt  axis  off  plt  show  image  nd  array  val  img  idx  reshape  28  28  asnumpy  tolist  response  predictor  predict  image  print  int  response  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
urllib  is  built  in  Python  library  to  download  files  from  URLs  Objective  retrieve  the  latest  version  of  the  ONNX  MNIST  model  files  from  the  ONNX  Model  Zoo  and  save  it  in  the  same  folder  as  this  tutorial  import  urllib  request  onnx  model  url  https  www  cntk  ai  OnnxModels  mnist  opset  mnist  tar  gz  urllib  request  urlretrieve  onnx  model  url  filename  mnist  tar  gz  the  magic  command  tells  our  jupyter  notebook  kernel  to  run  the  following  line  of  code  from  the  command  line  instead  of  the  notebook  kernel  We  use  tar  and  xvcf  to  unzip  the  files  we  just  retrieved  from  the  ONNX  model  zoo  tar  xvzf  mnist  tar  gz  Check  core  SDK  version  number  import  azureml  core  print  SDK  version  azureml  core  VERSION  from  azureml  core  import  Workspace  ws  Workspace  from  config  print  ws  name  ws  resource  group  ws  location  sep  model  dir  mnist  replace  this  with  the  location  of  your  model  files  leave  a,microsoft
s  is  if  it  in  the  same  folder  as  this  notebookfrom  azureml  core  model  import  Model  model  Model  register  workspace  ws  model  path  model  dir  model  onnx  model  name  mnist  tags  onnx  demo  description  MNIST  image  classification  CNN  from  ONNX  Model  Zoo  models  ws  models  for  name  in  models  items  print  Name  name  tVersion  version  tDescription  description  tags  for  images  and  plots  in  this  notebook  import  matplotlib  pyplot  as  plt  from  IPython  display  import  Image  display  images  inline  matplotlib  inlineImage  url  http  bp  blogspot  com  UpN7DfJA0j4  TJtUBWPk0SI  AAAAAAAAABY  oWPMtmqJn3k  s1600  mnist  originals  png  width  200  height  200  writefile  score  py  import  json  import  numpy  as  np  import  onnxruntime  import  sys  import  os  from  azureml  core  model  import  Model  import  time  def  init  global  session  input  name  output  name  model  Model  get  model  path  model  name  mnist  session  onnxruntime  InferenceSession  ,microsoft
model  None  input  name  session  get  inputs  name  output  name  session  get  outputs  name  def  run  input  data  Purpose  evaluate  test  input  in  Azure  Cloud  using  onnxruntime  We  will  call  the  run  function  later  from  our  Jupyter  Notebook  so  our  azure  service  can  evaluate  our  model  input  in  the  cloud  try  load  in  our  data  convert  to  readable  format  data  np  array  json  loads  input  data  data  astype  float32  start  time  time  session  run  output  name  input  name  data  end  time  time  result  choose  class  result  dict  result  result  time  in  sec  end  start  except  Exception  as  result  dict  error  str  return  json  dumps  result  dict  def  choose  class  result  prob  We  use  argmax  to  determine  the  right  label  to  choose  from  our  output  return  int  np  argmax  result  prob  axis  from  azureml  core  conda  dependencies  import  CondaDependencies  myenv  CondaDependencies  myenv  add  pip  package  numpy  myenv  add  pip  package  a,microsoft
zureml  core  myenv  add  pip  package  onnxruntime  with  open  myenv  yml  as  write  myenv  serialize  to  string  from  azureml  core  image  import  ContainerImage  help  ContainerImage  image  configuration  from  azureml  core  image  import  ContainerImage  image  config  ContainerImage  image  configuration  execution  script  score  py  runtime  python  conda  file  myenv  yml  description  MNIST  ONNX  Runtime  container  tags  demo  onnx  image  ContainerImage  create  name  onnxtest  this  is  the  model  object  models  model  image  config  image  config  workspace  ws  image  wait  for  creation  show  output  True  print  image  image  build  log  uri  from  azureml  core  webservice  import  AciWebservice  aciconfig  AciWebservice  deploy  configuration  cpu  cores  memory  gb  tags  demo  onnx  description  ONNX  for  mnist  model  from  azureml  core  webservice  import  Webservice  aci  service  name  onnx  demo  mnist20  print  Service  aci  service  name  aci  service  Webservice  deplo,microsoft
y  from  image  deployment  config  aciconfig  image  image  name  aci  service  name  workspace  ws  aci  service  wait  for  deployment  True  print  aci  service  state  if  aci  service  state  Healthy  run  this  command  for  debugging  print  aci  service  get  logs  If  your  deployment  fails  make  sure  to  delete  your  aci  service  or  rename  your  service  before  trying  again  aci  service  delete  to  manipulate  our  arrays  import  numpy  as  np  read  in  test  data  protobuf  files  included  with  the  model  import  onnx  from  onnx  import  numpy  helper  to  use  parsers  to  read  in  our  model  data  import  json  import  os  test  inputs  test  outputs  read  in  testing  images  from  pb  files  test  data  size  for  in  np  arange  test  data  size  input  test  data  os  path  join  model  dir  test  data  set  format  input  pb  output  test  data  os  path  join  model  dir  test  data  set  format  output  pb  convert  protobuf  tensors  to  np  arrays  using  the  Tensor,microsoft
Proto  reader  from  ONNX  tensor  onnx  TensorProto  with  open  input  test  data  rb  as  tensor  ParseFromString  read  input  data  numpy  helper  to  array  tensor  test  inputs  append  input  data  with  open  output  test  data  rb  as  tensor  ParseFromString  read  output  data  numpy  helper  to  array  tensor  test  outputs  append  output  data  if  len  test  inputs  test  data  size  print  Test  data  loaded  successfully  plt  figure  figsize  16  for  test  image  in  np  arange  plt  subplot  15  test  image  plt  axhline  plt  axvline  plt  imshow  test  inputs  test  image  reshape  28  28  cmap  plt  cm  Greys  plt  show  plt  figure  figsize  16  frameon  False  plt  subplot  plt  text  30  True  Label  fontsize  13  color  black  plt  text  20  Result  fontsize  13  color  black  plt  text  10  Inference  Time  fontsize  13  color  black  plt  text  14  Model  Input  fontsize  12  color  black  plt  text  18  28  28  fontsize  12  color  black  plt  imshow  np  ones  28  28  cmap  plt,microsoft
  cm  Greys  for  in  np  arange  test  data  size  input  data  json  dumps  data  test  inputs  tolist  predict  using  the  deployed  model  json  loads  aci  service  run  input  data  if  error  in  print  error  break  result  result  time  ms  np  round  time  in  sec  1000  ground  truth  int  np  argmax  test  outputs  compare  actual  value  vs  the  predicted  values  plt  subplot  plt  axhline  plt  axvline  use  different  color  for  misclassified  sample  font  color  red  if  ground  truth  result  else  black  clr  map  plt  cm  gray  if  ground  truth  result  else  plt  cm  Greys  ground  truth  labels  are  in  blue  plt  text  10  30  ground  truth  fontsize  18  color  blue  predictions  are  in  black  if  correct  red  if  incorrect  plt  text  10  20  result  fontsize  18  color  font  color  plt  text  10  str  time  ms  ms  fontsize  14  color  font  color  plt  imshow  test  inputs  reshape  28  28  cmap  clr  map  plt  show  Preprocessing  functions  take  your  image  and  format,microsoft
  it  so  it  can  be  passed  as  input  into  our  ONNX  model  import  cv2  def  rgb2gray  rgb  Convert  the  input  image  into  grayscale  return  np  dot  rgb  299  587  114  def  resize  img  img  Resize  image  to  MNIST  model  input  dimensions  img  cv2  resize  img  dsize  28  28  interpolation  cv2  INTER  AREA  img  resize  28  28  return  img  def  preprocess  img  Resize  input  images  and  convert  them  to  grayscale  if  img  shape  28  28  img  resize  28  28  return  img  grayscale  rgb2gray  img  processed  img  resize  img  grayscale  return  processed  img  Replace  this  string  with  your  own  path  test  image  Make  sure  your  image  is  square  and  the  dimensions  are  equal  100  100  pixels  or  28  28  pixels  Any  PNG  or  JPG  image  file  should  work  your  test  image  Users  vinitra  swamy  Pictures  handwritten  digit  png  import  matplotlib  image  as  mpimg  if  your  test  image  path  to  file  img  mpimg  imread  your  test  image  plt  subplot  plt  imshow  i,microsoft
mg  cmap  plt  cm  Greys  print  Old  Dimensions  img  shape  img  preprocess  img  print  New  Dimensions  img  shape  else  img  Noneif  img  is  None  print  Add  the  path  for  your  image  data  else  input  data  json  dumps  data  img  tolist  try  json  loads  aci  service  run  input  data  result  result  time  ms  np  round  time  in  sec  1000  except  Exception  as  print  str  plt  figure  figsize  16  plt  subplot  15  plt  axhline  plt  axvline  plt  text  100  20  Model  prediction  fontsize  14  plt  text  100  10  Inference  time  fontsize  14  plt  text  20  str  result  fontsize  14  plt  text  10  str  time  ms  ms  fontsize  14  plt  text  100  14  Input  image  fontsize  14  plt  imshow  img  reshape  28  28  cmap  plt  cm  gray  Image  url  https  www  cntk  ai  jup  cntk103d  conv2d  final  gif  width  200  remember  to  delete  your  service  after  you  are  done  using  it  aci  service  delete  ,microsoft
cat  container  Dockerfile  sh  The  name  of  our  algorithm  algorithm  name  decision  trees  sample  cd  container  chmod  decision  trees  train  chmod  decision  trees  serve  account  aws  sts  get  caller  identity  query  Account  output  text  Get  the  region  defined  in  the  current  configuration  default  to  us  west  if  none  defined  region  aws  configure  get  region  region  region  us  west  fullname  account  dkr  ecr  region  amazonaws  com  algorithm  name  latest  If  the  repository  doesn  exist  in  ECR  create  it  aws  ecr  describe  repositories  repository  names  algorithm  name  dev  null  if  ne  then  aws  ecr  create  repository  repository  name  algorithm  name  dev  null  fi  Get  the  login  command  from  ECR  and  execute  it  directly  aws  ecr  get  login  region  region  no  include  email  Build  the  docker  image  locally  with  the  image  name  and  then  push  it  to  ECR  with  the  full  name  docker  build  algorithm  name  docker  tag  algorithm  name,amazon
  fullname  docker  push  fullname  S3  prefix  prefix  DEMO  scikit  byo  iris  Define  IAM  role  import  boto3  import  re  import  os  import  numpy  as  np  import  pandas  as  pd  from  sagemaker  import  get  execution  role  role  get  execution  role  import  sagemaker  as  sage  from  time  import  gmtime  strftime  sess  sage  Session  WORK  DIRECTORY  data  data  location  sess  upload  data  WORK  DIRECTORY  key  prefix  prefix  account  sess  boto  session  client  sts  get  caller  identity  Account  region  sess  boto  session  region  name  image  dkr  ecr  amazonaws  com  decision  trees  sample  format  account  region  tree  sage  estimator  Estimator  image  role  ml  c4  2xlarge  output  path  s3  output  format  sess  default  bucket  sagemaker  session  sess  tree  fit  data  location  from  sagemaker  predictor  import  csv  serializer  predictor  tree  deploy  ml  m4  xlarge  serializer  csv  serializer  shape  pd  read  csv  data  iris  csv  header  None  import  itertools  50  for ,amazon
 in  range  40  for  in  range  10  indices  for  in  itertools  product  test  data  shape  iloc  indices  test  test  data  iloc  test  test  data  iloc  print  predictor  predict  test  values  decode  utf  sess  delete  endpoint  predictor  endpoint  ,amazon
bucket  sagemaker  walebadr  prefix  sagemaker  sagemaker  pca  mnist  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  This  is  number  28  28  784  format  train  set  import  io  import  numpy  as  np  import  sagemaker  amazon  common  as  smac  vectors  np  array  tolist  for  in  train  set  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  bu,amazon
f  vectors  buf  seek  import  boto3  import  os  key  recordio  pb  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  buf  s3  train  data  s3  train  format  bucket  prefix  key  output  location  s3  output  format  bucket  prefix  print  uploaded  training  data  location  format  s3  train  data  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  pca  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  pca  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  pca  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  pca  latest  ap  northeast  351501993468  dkr  ecr  ap  northeast  amazonaws  com  pca  latest  import  boto3  import  sagemaker  sess  sagemaker  Session  pca  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  output  location  sagemaker  session  sess  pca  set  hyp,amazon
erparameters  feature  dim  784  num  components  50  subtract  mean  True  algorithm  mode  randomized  mini  batch  size  200  pca  fit  train  s3  train  data  Create  SageMaker  Client  client  boto3  client  sagemaker  Defining  the  VPC  Configurations  SecurityGroups  security  group  id  subnets  subnet  id  jobName  PCA  trainingJob  hyperparameters  feature  dim  784  num  components  50  subtract  mean  True  algorithm  mode  randomized  mini  batch  size  200  Create  training  Job  response  client  create  training  job  TrainingJobName  jobName  HyperParameters  hyperparameters  AlgorithmSpecification  TrainingImage  containers  boto3  Session  region  name  TrainingInputMode  File  RoleArn  arn  aws  iam  625616379791  role  service  role  AmazonSageMaker  ExecutionRole  20171130T114980  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  data  S3DataDistributionType  FullyReplicated  OutputDataConfig  S3OutputPath  output  location  ResourceC,amazon
onfig  InstanceType  ml  c4  xlarge  InstanceCount  VolumeSizeInGB  10  VpcConfig  SecurityGroupIds  SecurityGroups  Subnets  subnets  StoppingCondition  MaxRuntimeInSeconds  2400  status  client  describe  training  job  TrainingJobName  jobName  TrainingJobStatus  print  status  client  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  jobName  print  Training  job  ended  with  status  status  if  status  Failed  message  sm  describe  training  job  TrainingJobName  distributed  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  pca  predictor  pca  deploy  initial  instance  count  instance  type  ml  t2  medium  time  import  boto3  from  time  import  gmtime  strftime  model  name  jobName  mod  strftime  gmtime  vpcConfigurations  SecurityGroupIds  SecurityGroups  Subnets  subnets  info  client  describe  training  job  TrainingJobName  jobName  model  artifacts  info  ModelArtifacts  S3Mode,amazon
lArtifacts  primary  container  Image  containers  boto3  Session  region  name  ModelDataUrl  model  artifacts  create  model  response  client  create  model  ModelName  model  name  VpcConfig  vpcConfigurations  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  endpoint  config  name  PCA  Demo  strftime  gmtime  create  endpoint  config  response  client  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialVariantWeight  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  import  time  endpoint  name  PCA  Demo  strftime  gmtime  create  endpoint  response  client  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  print  create  endpoint  response  EndpointArn  endpoint  response  client  des,amazon
cribe  endpoint  EndpointName  endpoint  name  status  endpoint  response  EndpointStatus  print  Status  status  while  status  Creating  time  sleep  60  resp  client  describe  endpoint  EndpointName  endpoint  name  status  endpoint  response  EndpointStatus  print  Status  status  print  Arn  resp  EndpointArn  print  Status  status  from  sagemaker  predictor  import  csv  serializer  json  deserializer  pca  predictor  content  type  text  csv  pca  predictor  serializer  csv  serializer  pca  predictor  deserializer  json  deserializerresult  pca  predictor  predict  train  set  time  import  struct  import  io  import  boto3  prefix  sagemaker  DEMO  xgboost  mnist  s3  input  train  sagemaker  s3  input  s3  data  s3  train  format  bucket  prefix  content  type  libsvm  s3  input  validation  sagemaker  s3  input  s3  data  s3  validation  format  bucket  prefix  content  type  libsvm  output  location  s3  output  format  bucket  prefix  def  write  to  s3  fobj  bucket  key  return  boto3  Sessio,amazon
n  resource  s3  Bucket  bucket  Object  key  upload  fileobj  fobj  def  get  pca  projected  data  eigendigits  eigendigits  for  array  in  np  array  split  train  set  800  result  pca  predictor  predict  array  eigendigits  projection  for  in  result  projections  train  eigendigits  np  array  eigendigits  for  array  in  np  array  split  valid  set  800  result  pca  predictor  predict  array  eigendigits  projection  for  in  result  projections  valid  eigendigits  np  array  eigendigits  return  train  eigendigits  valid  eigendigits  time  from  sklearn  linear  model  import  LogisticRegression  train  projected  valid  projected  get  pca  projected  data  logisticRegr  LogisticRegression  solver  lbfgs  logisticRegr  fit  train  projected  train  set  valid  acc  logisticRegr  score  valid  projected  valid  set  100  print  The  validation  Accuracy  is  format  valid  acc  from  IPython  display  import  HTML  HTML  open  input  html  read  pca  result  pca  predictor  predict  np  array  ,amazon
data  processed  result  np  array  projection  for  in  pca  result  projections  reg  result  logisticRegr  predict  processed  result  reshape  print  The  number  is  format  reg  result  aws  s3  cp  s3  sagemaker  walebadr  sagemaker  DEMO  pca  mnist  output  pca  2018  06  15  15  06  42  733  output  model  tar  gz  s3  prod  model  artifacts  prod  models  acl  bucket  owner  full  control  metadata  One  Two  aws  s3  cp  deploy  template  zip  s3  prod  model  artifacts  prod  models  acl  bucket  owner  full  control  metadata  One  Two  import  sagemaker  sagemaker  Session  delete  endpoint  pca  predictor  endpoint  ,amazon
time  import  boto3  import  re  from  sagemaker  import  get  execution  role  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  role  get  execution  role  bucket  bucket  name  customize  to  your  bucket  training  image  get  image  uri  boto3  Session  region  name  image  classification  import  os  import  urllib  request  import  boto3  def  download  url  filename  url  split  if  not  os  path  exists  filename  urllib  request  urlretrieve  url  filename  def  upload  to  s3  channel  file  s3  boto3  resource  s3  data  open  file  rb  key  channel  file  s3  Bucket  bucket  put  object  Key  key  Body  data  caltech  256  download  http  data  mxnet  io  data  caltech  256  caltech  256  60  train  rec  upload  to  s3  train  caltech  256  60  train  rec  download  http  data  mxnet  io  data  caltech  256  caltech  256  60  val  rec  upload  to  s3  validation  caltech  256  60  val  rec  The  algorithm  supports  multiple  network  depth  number  of  layers  They  are  18  3,amazon
4  50  101  152  and  200  For  this  training  we  will  use  18  layers  num  layers  18  we  need  to  specify  the  input  image  shape  for  the  training  data  image  shape  224  224  we  also  need  to  specify  the  number  of  training  samples  in  the  training  set  for  caltech  it  is  15420  num  training  samples  15420  specify  the  number  of  output  classes  num  classes  257  batch  size  for  training  mini  batch  size  64  number  of  epochs  epochs  learning  rate  learning  rate  01  time  import  time  import  boto3  from  time  import  gmtime  strftime  s3  boto3  client  s3  create  unique  job  name  job  name  prefix  DEMO  imageclassification  timestamp  time  strftime  time  gmtime  job  name  job  name  prefix  timestamp  training  params  specify  the  training  docker  image  AlgorithmSpecification  TrainingImage  training  image  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  s3  output  format  bucket  job  name  prefix  ResourceConfig  Instance,amazon
Count  InstanceType  ml  p2  xlarge  VolumeSizeInGB  50  TrainingJobName  job  name  HyperParameters  image  shape  image  shape  num  layers  str  num  layers  num  training  samples  str  num  training  samples  num  classes  str  num  classes  mini  batch  size  str  mini  batch  size  epochs  str  epochs  learning  rate  str  learning  rate  StoppingCondition  MaxRuntimeInSeconds  360000  Training  data  should  be  inside  subdirectory  called  train  Validation  data  should  be  inside  subdirectory  called  validation  The  algorithm  currently  only  supports  fullyreplicated  model  where  data  is  copied  onto  each  machine  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  recordio  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  S3DataDistributionType  FullyReplicated  Conten,amazon
tType  application  recordio  CompressionType  None  print  Training  job  name  format  job  name  print  nInput  Data  Location  format  training  params  InputDataConfig  DataSource  S3DataSource  create  the  Amazon  SageMaker  training  job  sagemaker  boto3  client  service  name  sagemaker  sagemaker  create  training  job  training  params  confirm  that  the  training  job  has  started  status  sagemaker  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  Training  job  current  status  format  status  try  wait  for  the  job  to  finish  and  report  the  ending  status  sagemaker  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  except  print  Training  failed  to  start  if  exception  is  raised  that  means  it  has  failed  message  sagemaker  describe  trai,amazon
ning  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  time  import  boto3  from  time  import  gmtime  strftime  sage  boto3  Session  client  service  name  sagemaker  model  name  DEMO  full  image  classification  model  print  model  name  info  sage  describe  training  job  TrainingJobName  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  data  hosting  image  get  image  uri  boto3  Session  region  name  image  classification  primary  container  Image  hosting  image  ModelDataUrl  model  data  create  model  response  sage  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  timestamp  time  strftime  ti,amazon
me  gmtime  endpoint  config  name  job  name  prefix  epc  timestamp  endpoint  config  response  sage  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  configuration  name  format  endpoint  config  name  print  Endpoint  configuration  arn  format  endpoint  config  response  EndpointConfigArn  time  import  time  timestamp  time  strftime  time  gmtime  endpoint  name  job  name  prefix  ep  timestamp  print  Endpoint  name  format  endpoint  name  endpoint  params  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  endpoint  response  sagemaker  create  endpoint  endpoint  params  print  EndpointArn  format  endpoint  response  EndpointArn  get  the  status  of  the  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  response  EndpointStatus  print  EndpointStatus  format  status  wait  u,amazon
ntil  the  status  has  changed  sagemaker  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  print  the  status  of  the  endpoint  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  endpoint  response  EndpointStatus  print  Endpoint  creation  ended  with  EndpointStatus  format  status  if  status  InService  raise  Exception  Endpoint  creation  failed  import  boto3  runtime  boto3  Session  client  service  name  runtime  sagemaker  wget  tmp  test  jpg  http  www  vision  caltech  edu  Image  Datasets  Caltech256  images  008  bathtub  008  0007  jpg  file  name  tmp  test  jpg  test  image  from  IPython  display  import  Image  Image  file  name  import  json  import  numpy  as  np  with  open  file  name  rb  as  payload  read  payload  bytearray  payload  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  image  Body  payload  result  response  Body  read  result  will  be  in  json  format  and  ,amazon
convert  it  to  ndarray  result  json  loads  result  the  result  will  output  the  probabilities  for  all  classes  find  the  class  with  maximum  probability  and  print  the  class  index  index  np  argmax  result  object  categories  ak47  american  flag  backpack  baseball  bat  baseball  glove  basketball  hoop  bat  bathtub  bear  beer  mug  billiards  binoculars  birdbath  blimp  bonsai  101  boom  box  bowling  ball  bowling  pin  boxing  glove  brain  101  breadmaker  buddha  101  bulldozer  butterfly  cactus  cake  calculator  camel  cannon  canoe  car  tire  cartman  cd  centipede  cereal  box  chandelier  101  chess  board  chimp  chopsticks  cockroach  coffee  mug  coffin  coin  comet  computer  keyboard  computer  monitor  computer  mouse  conch  cormorant  covered  wagon  cowboy  hat  crab  101  desk  globe  diamond  ring  dice  dog  dolphin  101  doorknob  drinking  straw  duck  dumb  bell  eiffel  tower  electric  guitar  101  elephant  101  elk  ewer  101  eyeglasses  fern  fighter  ,amazon
jet  fire  extinguisher  fire  hydrant  fire  truck  fireworks  flashlight  floppy  disk  football  helmet  french  horn  fried  egg  frisbee  frog  frying  pan  galaxy  gas  pump  giraffe  goat  golden  gate  bridge  goldfish  golf  ball  goose  gorilla  grand  piano  101  grapes  grasshopper  guitar  pick  hamburger  hammock  harmonica  harp  harpsichord  hawksbill  101  head  phones  helicopter  101  hibiscus  homer  simpson  horse  horseshoe  crab  hot  air  balloon  hot  dog  hot  tub  hourglass  house  fly  human  skeleton  hummingbird  ibis  101  ice  cream  cone  iguana  ipod  iris  jesus  christ  joy  stick  kangaroo  101  kayak  ketch  101  killer  whale  knife  ladder  laptop  101  lathe  leopards  101  license  plate  lightbulb  light  house  lightning  llama  101  mailbox  mandolin  mars  mattress  megaphone  menorah  101  microscope  microwave  minaret  minotaur  motorbikes  101  mountain  bike  mushroom  mussels  necktie  octopus  ostrich  owl  palm  pilot  palm  tree  paperclip  paper  shredde,amazon
r  pci  card  penguin  people  pez  dispenser  photocopier  picnic  table  playing  card  porcupine  pram  praying  mantis  pyramid  raccoon  radio  telescope  rainbow  refrigerator  revolver  101  rifle  rotary  phone  roulette  wheel  saddle  saturn  school  bus  scorpion  101  screwdriver  segway  self  propelled  lawn  mower  sextant  sheet  music  skateboard  skunk  skyscraper  smokestack  snail  snake  sneaker  snowmobile  soccer  ball  socks  soda  can  spaghetti  speed  boat  spider  spoon  stained  glass  starfish  101  steering  wheel  stirrups  sunflower  101  superman  sushi  swan  swiss  army  knife  sword  syringe  tambourine  teapot  teddy  bear  teepee  telephone  box  tennis  ball  tennis  court  tennis  racket  theodolite  toaster  tomato  tombstone  top  hat  touring  bike  tower  pisa  traffic  light  treadmill  triceratops  tricycle  trilobite  101  tripod  shirt  tuning  fork  tweezer  umbrella  101  unicorn  vcr  video  projector  washing  machine  watch  101  waterfall  watermelon  wel,amazon
ding  mask  wheelbarrow  windmill  wine  bottle  xylophone  yarmulke  yo  yo  zebra  airplanes  101  car  side  101  faces  easy  101  greyhound  tennis  shoes  toad  clutter  print  Result  label  object  categories  index  probability  str  result  index  sage  delete  endpoint  EndpointName  endpoint  name  ,amazon
import  boto3  bucket  eduthie  sagemaker  prefix  gluon  recommender  client  boto3  client  athena  query  string  select  customer  id  product  id  star  rating  product  title  from  train  inner  join  select  customer  id  from  train  group  by  customer  id  having  count  customer  id  on  customer  id  customer  id  inner  join  select  product  id  from  train  group  by  product  id  having  count  product  id  10  on  product  id  product  id  where  customer  id  is  not  null  create  query  response  client  start  query  execution  QueryString  query  string  QueryExecutionContext  Database  product  ratings  ResultConfiguration  OutputLocation  s3  processed  format  bucket  prefix  create  query  responseimport  time  waiting  True  query  execution  response  None  while  waiting  query  execution  response  client  get  query  execution  QueryExecutionId  create  query  response  QueryExecutionId  if  query  execution  response  QueryExecution  Status  State  SUCCEEDED  waiting  False  e,amazon
lse  time  sleep  query  execution  responsefile  location  query  execution  response  QueryExecution  ResultConfiguration  OutputLocation  key  file  location  len  s3  format  bucket  boto3  client  s3  copy  Bucket  bucket  Key  key  bucket  prefix  train  ready  amazon  reviews  us  Digital  Video  Download  csv  ,amazon
import  data  science  and  visualization  libraries  matplotlib  inline  from  sklearn  model  selection  import  train  test  split  import  pandas  as  pd  import  numpy  as  np  import  seaborn  as  sns  download  the  raw  data  and  unzip  wget  https  archive  ics  uci  edu  ml  machine  learning  databases  covtype  covtype  data  gz  gunzip  covtype  data  gz  read  the  csv  and  extract  features  and  labels  covtype  pd  read  csv  covtype  data  delimiter  dtype  float32  values  covtype  features  covtype  labels  covtype  54  covtype  54  transform  labels  to  index  covtype  labels  shuffle  and  split  into  train  and  test  sets  np  random  seed  train  features  test  features  train  labels  test  labels  train  test  split  covtype  features  covtype  labels  test  size  further  split  the  test  set  into  validation  and  test  sets  val  features  test  features  val  labels  test  labels  train  test  split  test  features  test  labels  test  size  assign  label  names  and  cou,amazon
nt  label  frequencies  label  map  Spruce  Fir  Lodgepole  Pine  Ponderosa  Pine  Cottonwood  Willow  Aspen  Douglas  fir  Krummholz  label  counts  pd  DataFrame  data  train  labels  map  label  map  value  counts  sort  False  sort  index  ascending  False  label  counts  plot  barh  color  tomato  title  Label  Counts  import  sagemaker  from  sagemaker  amazon  amazon  estimator  import  RecordSet  import  boto3  instantiate  the  LinearLearner  estimator  object  multiclass  estimator  sagemaker  LinearLearner  role  sagemaker  get  execution  role  train  instance  count  train  instance  type  ml  m4  xlarge  predictor  type  multiclass  classifier  num  classes  wrap  data  in  RecordSet  objects  train  records  multiclass  estimator  record  set  train  features  train  labels  channel  train  val  records  multiclass  estimator  record  set  val  features  val  labels  channel  validation  test  records  multiclass  estimator  record  set  test  features  test  labels  channel  test  start  train,amazon
ing  job  multiclass  estimator  fit  train  records  val  records  test  records  deploy  model  hosting  endpoint  multiclass  predictor  multiclass  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  def  evaluate  metrics  predictor  test  features  test  labels  Evaluate  model  on  test  set  using  the  given  prediction  endpoint  Display  classification  metrics  split  the  test  dataset  into  100  batches  and  evaluate  using  prediction  endpoint  prediction  batches  predictor  predict  batch  for  batch  in  np  array  split  test  features  100  parse  protobuf  responses  to  extract  predicted  labels  extract  label  lambda  label  predicted  label  float32  tensor  values  test  preds  np  concatenate  np  array  extract  label  for  in  batch  for  batch  in  prediction  batches  test  preds  test  preds  reshape  calculate  accuracy  accuracy  test  preds  test  labels  sum  test  labels  shape  calculate  recall  for  each  class  recall  per  class  classes  ,amazon
for  target  label  in  np  unique  test  labels  recall  numerator  np  logical  and  test  preds  target  label  test  labels  target  label  sum  recall  denominator  test  labels  target  label  sum  recall  per  class  append  recall  numerator  recall  denominator  classes  append  label  map  target  label  recall  pd  DataFrame  recall  recall  per  class  class  label  classes  recall  sort  values  class  label  ascending  False  inplace  True  calculate  confusion  matrix  label  mapper  np  vectorize  lambda  label  map  confusion  matrix  pd  crosstab  label  mapper  test  labels  label  mapper  test  preds  rownames  Actuals  colnames  Predictions  normalize  index  display  results  sns  heatmap  confusion  matrix  annot  True  fmt  2f  cmap  YlGnBu  set  title  Confusion  Matrix  ax  recall  plot  kind  barh  class  label  recall  color  steelblue  title  Recall  legend  False  ax  set  ylabel  print  Accuracy  3f  format  accuracy  evaluate  metrics  of  the  model  trained  with  default  hy,amazon
perparameters  evaluate  metrics  multiclass  predictor  test  features  test  labels  instantiate  the  LinearLearner  estimator  object  balanced  multiclass  estimator  sagemaker  LinearLearner  role  sagemaker  get  execution  role  train  instance  count  train  instance  type  ml  m4  xlarge  predictor  type  multiclass  classifier  num  classes  balance  multiclass  weights  True  start  training  job  balanced  multiclass  estimator  fit  train  records  val  records  test  records  deploy  model  hosting  endpoint  balanced  multiclass  predictor  balanced  multiclass  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  evaluate  metrics  of  the  model  trained  with  balanced  class  weights  evaluate  metrics  balanced  multiclass  predictor  test  features  test  labels  delete  endpoints  multiclass  predictor  delete  endpoint  balanced  multiclass  predictor  delete  endpoint  ,amazon
import  adal  from  msrestazure  azure  active  directory  import  AADTokenCredentials  from  dotenv  import  load  dotenv  find  dotenv  import  os  load  dotenv  find  dotenv  os  getenv  AZURE  SUBSCRIPTION  import  requests  Parameters  need  for  API  subscription  os  getenv  AZURE  SUBSCRIPTION  ID  tenant  os  getenv  AZURE  TENANT  ID  client  id  os  getenv  AZURE  CLIENT  ID  client  secret  os  getenv  AZURE  CLIENT  SECRET  offer  MS  AZR  0003P  currency  USD  locale  en  US  region  US  def  authenticate  client  key  tenant  client  id  client  secret  Authenticate  using  service  principal  key  authority  host  uri  https  login  microsoftonline  com  authority  uri  authority  host  uri  tenant  resource  uri  https  management  core  windows  net  context  adal  AuthenticationContext  authority  uri  api  version  None  mgmt  token  context  acquire  token  with  client  credentials  resource  uri  client  id  client  secret  credentials  AADTokenCredentials  mgmt  token  client  id  retu,microsoft
rn  credentials  credentials  authenticate  client  key  tenant  client  id  client  secret  access  token  credentials  token  get  access  token  azure  mgmt  uri  https  management  azure  com  443  subscriptions  subscriptionId  format  subscriptionId  subscription  azure  mgmt  uri  Azure  Resource  RateCard  API  https  docs  microsoft  com  en  us  azure  billing  billing  usage  rate  card  overview  uri  str  azure  mgmt  uri  providers  Microsoft  Commerce  RateCard  api  version  2016  08  31  preview  filter  OfferDurableId  eq  offerId  and  Currency  eq  currencyId  and  Locale  eq  localeId  and  RegionInfo  eq  regionId  rateCardUrl  uri  str  format  azure  mgmt  uri  azure  mgmt  uri  offerId  offer  currencyId  currency  localeId  locale  regionId  region  Don  allow  redirects  and  call  the  RateCard  API  response  requests  get  rateCardUrl  allow  redirects  False  headers  Authorization  Bearer  access  token  Look  at  response  headers  to  get  the  redirect  URL  redirectUrl  res,microsoft
ponse  headers  Location  Get  the  ratecard  content  by  making  another  call  to  go  the  redirect  URL  rateCard  requests  get  redirectUrl  import  pandas  as  pd  import  jsonr  json  loads  rateCard  content  decode  utf  df  rates  pd  DataFrame  from  dict  Meters  df  ratesuri  str  azure  mgmt  uri  providers  Microsoft  Commerce  UsageAggregates  api  version  2015  06  01  preview  aggregationGranularity  Daily  reportedstartTime  2018  06  12  00  3a00  3a00Z  reportedEndTime  2018  06  14  00  3a00  3a00Z  usage  url  uri  str  format  azure  mgmt  uri  azure  mgmt  uri  usage  urlresponse  requests  get  usage  url  allow  redirects  False  headers  Authorization  Bearer  access  token  usage  response  json  usage  API  doesn  return  more  than  1000  aggregates  see  https  stackoverflow  com  questions  50948666  get  azurermconsumptionusagedetail  limited  response  to  1000  items  len  usage  value  df  usage  pd  DataFrame  from  dict  usage  value  df  usage  create  dataframe  for,microsoft
m  the  properties  key  on  each  usage  record  df  usage  detail  pd  DataFrame  df  usage  properties  values  tolist  df  usage  detail  https  msdn  microsoft  com  en  us  library  azure  mt219001  aspx  Note  the  query  is  by  reported  time  but  the  return  is  by  usage  time  we  ask  callers  to  query  by  Reported  Time  to  ensure  that  they  get  all  the  usage  events  reported  within  specific  time  period  within  the  billing  system  Even  though  the  query  is  made  with  the  Reported  Time  the  usage  response  is  aggregated  by  the  resource  usage  time  which  is  the  useful  pivot  for  callers  We  queried  from  12  to  14  This  usage  shows  up  on  12  to  13  but  if  you  quesry  12  to  13  this  record  will  not  be  picked  up  as  it  was  reported  13  to  14  df  usage  detail  loc  df  usage  detail  meterId  9995d93a  7d35  4d3f  9c69  7a7fea447ef4  invoice  list  url  https  management  azure  com  443  subscriptions  subscriptionId  providers  Micros,microsoft
oft  Billing  invoices  api  version  2017  04  24  preview  format  subscriptionId  subscription  invoice  list  urlresponse  requests  get  invoice  list  url  allow  redirects  False  headers  Authorization  Bearer  access  token  There  are  invoices  against  this  subscription  so  far  response  json  invoice  name  response  json  value  name  invoice  nameinvoice  url  https  management  azure  com  443  subscriptions  subscriptionId  providers  Microsoft  Billing  invoices  invoiceName  api  version  2017  04  24  preview  format  subscriptionId  subscription  invoiceName  invoice  name  download  invoice  pdf  response  requests  get  invoice  url  allow  redirects  False  headers  Authorization  Bearer  access  token  invoice  pdf  url  response  json  properties  downloadUrl  url  response  requests  get  invoice  pdf  url  open  invoice  pdf  wb  write  response  content  close  open  invoice  pdf  pickle  everything  import  pickle  pickle  dump  df  usage  open  df  usage  wb  pickle  dump  df,microsoft
  rates  open  df  rates  wb  pickle  dump  df  usage  detail  open  df  usage  detail  wb  TODO  aggregate  usage  on  rate  left  join  usage  on  rates  iter  process  row  into  rated  record  ,microsoft
import  import  ipynb  from  mnist  import  load  mnist  Dataset  load  normalize  flatten  True  784  False  1x28x28  one  hot  label  one  hot  encoding  True  False  train  train  test  test  load  mnist  flatten  True  normalize  False  print  train  shape  60000  784  print  train  shape  60000  print  test  shape  10000  784  print  test  shape  10000  import  numpy  as  np  from  PIL  import  Image  def  img  show  img  pil  img  Image  fromarray  np  uint8  img  pil  img  show  train  train  test  test  load  mnist  flatten  True  normalize  False  img  train  label  train  print  label  print  img  shape  784  img  img  reshape  28  28  print  img  shape  28  28  img  show  img  import  Chapter  03  as  af  import  pickle  def  get  data  train  train  test  test  load  mnist  flatten  True  normalize  False  one  hot  label  False  return  test  test  def  init  network  with  open  sample  weight  pkl  rb  as  network  pickle  load  return  network  def  predict  network  W1  W2  W3  network  W1  n,amazon
etwork  W2  network  W3  b1  b2  b3  network  b1  network  b2  network  b3  a1  np  dot  W1  b1  z1  af  relu  a1  a2  np  dot  z1  W2  b2  z2  af  relu  a2  a3  np  dot  z2  W3  b3  af  softmax  a3  return  yx  get  data  network  init  network  accuracy  cnt  for  in  range  len  predict  network  np  argmax  if  accuracy  cnt  print  Accuracy  str  float  accuracy  cnt  len  get  data  network  init  network  batch  size  100  accuracy  cnt  for  in  range  len  batch  size  batch  batch  size  batch  predict  network  batch  np  argmax  batch  axis  accuracy  cnt  np  sum  batch  size  print  Accuracy  str  float  accuracy  cnt  len  ,amazon
import  sagemaker  sagemaker  session  sagemaker  Session  bucket  sagemaker  session  default  bucket  prefix  sagemaker  DEMO  pytorch  mnist  role  sagemaker  get  execution  role  from  torchvision  import  datasets  transforms  datasets  MNIST  data  download  True  transform  transforms  Compose  transforms  ToTensor  transforms  Normalize  1307  3081  inputs  sagemaker  session  upload  data  path  data  bucket  bucket  key  prefix  prefix  print  input  spec  in  this  case  just  an  S3  path  format  inputs  pygmentize  mnist  pyfrom  sagemaker  pytorch  import  PyTorch  estimator  PyTorch  entry  point  mnist  py  role  role  framework  version  train  instance  count  train  instance  type  ml  c4  xlarge  hyperparameters  epochs  backend  gloo  estimator  fit  training  inputs  predictor  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  IPython  display  import  HTML  HTML  open  input  html  read  import  numpy  as  np  image  np  array  data  dtype  np  float32,amazon
  response  predictor  predict  image  prediction  response  argmax  axis  print  prediction  estimator  delete  endpoint  ,amazon
import  os  from  pyspark  import  SparkContext  SparkConf  from  pyspark  sql  import  SparkSession  import  sagemaker  from  sagemaker  import  get  execution  role  import  sagemaker  pyspark  role  get  execution  role  Configure  Spark  to  use  the  SageMaker  Spark  dependency  jars  jars  sagemaker  pyspark  classpath  jars  classpath  join  sagemaker  pyspark  classpath  jars  See  the  SageMaker  Spark  Github  repo  under  sagemaker  pyspark  sdk  to  learn  how  to  connect  to  remote  EMR  cluster  running  Spark  from  Notebook  Instance  spark  SparkSession  builder  config  spark  driver  extraClassPath  classpath  master  local  getOrCreate  import  boto3  region  boto3  Session  region  name  trainingData  spark  read  format  libsvm  option  numFeatures  784  load  s3a  sagemaker  sample  data  spark  mnist  train  format  region  testData  spark  read  format  libsvm  option  numFeatures  784  load  s3a  sagemaker  sample  data  spark  mnist  test  format  region  trainingData  show  impo,amazon
rt  random  from  sagemaker  pyspark  import  IAMRole  S3DataPath  from  sagemaker  pyspark  algorithms  import  KMeansSageMakerEstimator  kmeans  estimator  KMeansSageMakerEstimator  sagemakerRole  IAMRole  role  trainingInstanceType  ml  m4  xlarge  trainingInstanceCount  endpointInstanceType  ml  m4  xlarge  endpointInitialInstanceCount  kmeans  estimator  setK  10  kmeans  estimator  setFeatureDim  784  train  model  kmeans  estimator  fit  trainingData  transformedData  model  transform  testData  transformedData  show  from  pyspark  sql  types  import  DoubleType  import  matplotlib  pyplot  as  plt  import  numpy  as  np  helper  function  to  display  digit  def  show  digit  img  caption  xlabel  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axes  get  xaxis  set  ticks  subplot  axes  get  yaxis  set  ticks  plt  title  caption  plt  xlabel  xlabel  subplot  imshow  imgr  cmap  gray  images  np  array  transformedData  select  features  cache  take  ,amazon
250  clusters  transformedData  select  closest  cluster  cache  take  250  for  cluster  in  range  10  print  nCluster  format  int  cluster  digits  img  for  img  in  zip  clusters  images  if  int  closest  cluster  cluster  height  len  digits  width  plt  rcParams  figure  figsize  width  height  subplots  plt  subplots  height  width  subplots  np  ndarray  flatten  subplots  for  subplot  image  in  zip  subplots  digits  show  digit  image  subplot  subplot  for  subplot  in  subplots  len  digits  subplot  axis  off  plt  show  Delete  the  endpoint  from  sagemaker  pyspark  import  SageMakerResourceCleanup  resource  cleanup  SageMakerResourceCleanup  model  sagemakerClient  resource  cleanup  deleteResources  model  getCreatedResources  ,amazon
import  tensorflow  as  tf  import  numpy  as  np  tf  set  random  seed  777  for  reproducibility  xy  np  loadtxt  data  01  test  score  csv  delimiter  dtype  np  float32  source  by  insert  to  code  import  sys  import  types  import  pandas  as  pd  from  botocore  client  import  Config  import  ibm  boto3  def  iter  self  return  The  following  code  accesses  file  in  your  IBM  Cloud  Object  Storage  It  includes  your  credentials  You  might  want  to  remove  those  credentials  before  you  share  your  notebook  client  ace6f71a1b9946cbb684ca0d9e6c34c0  ibm  boto3  client  service  name  s3  ibm  api  key  id  XXBNCWwO4mooXZRHuqZ71nRmyIlQt8ca3ZWN8pOo56X69DX  ibm  auth  endpoint  https  iam  ng  bluemix  net  oidc  token  config  Config  signature  version  oauth  endpoint  url  https  s3  api  us  geo  objectstorage  service  networklayer  com  body  client  ace6f71a1b9946cbb684ca0d9e6c34c0  get  object  Bucket  tensorflowlabwithwatsonstudio  donotdelete  pr  neiwaip4a29fcg  Key  data  0,ibm
1  test  score  csv  Body  add  missing  iter  method  so  pandas  accepts  body  as  file  like  object  if  not  hasattr  body  iter  body  iter  types  MethodType  iter  body  df  pd  read  csv  body  header  None  pandas  to  numpy  xy  df  values  data  xy  data  xy  Make  sure  the  shape  and  data  are  OK  print  data  shape  data  len  data  print  data  shape  data  placeholders  for  tensor  that  will  be  always  fed  tf  placeholder  tf  float32  shape  None  tf  placeholder  tf  float32  shape  None  tf  Variable  tf  random  normal  name  weight  tf  Variable  tf  random  normal  name  bias  Hypothesis  hypothesis  tf  matmul  Simplified  cost  loss  function  cost  tf  reduce  mean  tf  square  hypothesis  Minimize  optimizer  tf  train  GradientDescentOptimizer  learning  rate  1e  train  optimizer  minimize  cost  Launch  the  graph  in  session  sess  tf  Session  Initializes  global  variables  in  the  graph  sess  run  tf  global  variables  initializer  for  step  in  range  2001  cos,ibm
t  val  hy  val  sess  run  cost  hypothesis  train  feed  dict  data  data  if  step  100  print  step  Cost  cost  val  nPrediction  hy  val  Ask  my  score  print  Your  score  will  be  sess  run  hypothesis  feed  dict  100  70  101  print  Other  scores  will  be  sess  run  hypothesis  feed  dict  60  70  110  90  100  80  ,ibm
ppf  Tbar  100  Kbar  100  Lbar  400  sfmplot  autarky  Lbar  Tbar  Kbar  eqn  autarky  pw  Lao  wo  eqn  pw  Lao  wo  wo  pwsfmtrade  sfmplot2  import  numpy  as  np  from  scipy  optimize  import  fsolve  np  seterr  divide  ignore  invalid  ignore  import  matplotlib  pyplot  as  plt  from  ipywidgets  import  interact  fixed  import  seaborn  matplotlib  inlineplt  style  use  seaborn  colorblind  plt  rcParams  figure  figsize  plt  rcParams  axes  spines  right  True  plt  rcParams  axes  spines  top  False  plt  rcParams  font  size  18  plt  rcParams  figure  figsize  10  plt  rcParams  axes  grid  TrueTbar  100  Fixed  specific  land  in  ag  Kbar  100  Fixed  specific  capital  in  manuf  Lbar  400  Total  number  of  mobile  workers  LbarMax  400  Lbar  will  be  on  slider  max  value  00  initial  rel  price  of  ag  goods  Pa  Pm  alpha  beta  labor  share  in  ag  manufLa  np  linspace  LbarMax  LbarMax  Lm  Lbar  Ladef  La  Tbar  Tbar  return  Tbar  alpha  La  alpha  def  Lm  Kbar  Kbar  retur,microsoft
n  Kbar  beta  Lm  beta  def  MPLa  La  return  alpha  Tbar  alpha  La  alpha  def  MPLm  Lm  return  beta  Kbar  beta  Lm  beta  def  MPT  La  Tbar  Tbar  return  alpha  Tbar  alpha  La  alpha  def  MPK  Lm  Kbar  Kbar  return  beta  Kbar  beta  Lm  betadef  ppf  Tbar  Tbar  Kbar  Kbar  Lbar  Lbar  Qa  La  Tbar  La  Lbar  Qm  Lm  Kbar  plt  title  Production  Possibility  Frontier  plt  xlabel  plt  ylabel  plt  plot  Qa  Qm  plt  gca  set  aspect  equal  ppf  Tbar  100  Kbar  100  Lbar  400  LDa  MPLa  La  La  Lbar  for  Cobb  Douglas  MPL  can  be  written  this  way  LDm  MPLm  Lbar  La  def  eqn  Lbar  Lbar  returns  numerically  found  equilibrium  labor  allocation  and  wage  def  func  La  return  MPLa  La  MPLm  Lbar  La  Laeq  fsolve  func  return  Laeq  MPLa  Laeq  def  Utility  function  return  def  XD  Lbar  Lbar  Cobb  Douglas  demand  for  goods  given  world  prices  national  income  computed  LAe  we  eqn  gdp  at  world  prices  measured  in  manuf  goods  gdp  LAe  Tbar  Tbar  Lbar  LAe ,microsoft
 Kbar  Kbar  return  gdp  gdp  def  indif  ubar  return  ubar  def  autarky  Lbar  Lbar  Tbar  Tbar  Kbar  Kbar  Find  autarky  product  prices  By  Walras  law  enough  to  find  price  that  sets  excess  demand  in  just  one  market  def  excessdemandA  LAe  eqn  QA  LAe  Tbar  Tbar  CA  CM  XD  Lbar  Lbar  return  QA  CA  peq  fsolve  excessdemandA  return  peq  autarky  def  sfmtrade  Ca  np  linspace  200  200  LAe  we  eqn  LAe  Tbar  Tbar  Lbar  LAe  gdp  plt  scatter  LAe  Tbar  Lbar  LAe  Tbar  label  Trade  produce  plt  scatter  XD  label  Trade  consume  marker  plt  scatter  XD  autarky  marker  label  Autarky  plt  plot  gdp  gdp  ppf  100  ub  XD  plt  ylim  gdp  plt  xlim  gdp  plt  plot  Ca  indif  Ca  ub  plt  grid  False  plt  legend  plt  gca  spines  bottom  set  position  zero  plt  gca  spines  left  set  position  zero  sfmtrade  La  np  arange  LbarMax  this  is  always  over  the  LbarMax  range  def  sfmplot  Lbar  LbarMax  show  True  Lm  Lbar  La  Qa  Tbar  alpha  La  alpha  La ,microsoft
 Lbar  Qm  Kbar  beta  Lbar  La  beta  pMPLa  alpha  Qa  La  La  Lbar  for  Cobb  Douglass  MPL  can  be  written  this  way  MPLm  beta  Qm  Lbar  La  LA  weq  eqn  ymax  plt  ylim  ymax  plt  xlim  LbarMax  plt  title  Specific  Factors  Model  plt  plot  La  pMPLa  linewidth  label  AG  labor  demand  plt  plot  La  MPLm  linewidth  label  MF  labor  demand  plt  scatter  LA  weq  100  color  black  plt  axhline  weq  linestyle  dashed  plt  axvline  Lbar  linewidth  plt  plot  LA  LA  weq  linestyle  dashed  plt  xlabel  Labor  plt  ylabel  Real  wage  frac  plt  grid  if  show  plt  legend  loc  04  print  La  Lm  0f  0f  Pm  Pa  2f  2f  format  LA  Lbar  LA  weq  weq  print  Pa  Pm  1f  1f  Pa  Pm  1f  1f  format  MPT  LA  MPT  LA  MPK  Lbar  LA  MPK  Lbar  LA  plt  show  sfmplot  def  sfmplot2  sfmplot  show  False  sfmplot  show  False  plt  grid  False  if  plt  title  SF  Model  else  La0  w0  eqn  plt  scatter  La0  w0  100  color  black  where  wage  would  rise  to  without  labor  movement  if  ,microsoft
plt  title  frac  uparrow  rightarrow  frac  uparrow  frac  downarrow  elif  plt  title  frac  downarrow  rightarrow  frac  downarrow  frac  uparrow  plt  show  sfmplot2  interact  sfmplot2  interact  sfmplot2  Lbar  fixed  400  show  fixed  True  ,microsoft
urllib  is  built  in  Python  library  to  download  files  from  URLs  Objective  retrieve  the  latest  version  of  the  ONNX  Emotion  FER  model  files  from  the  ONNX  Model  Zoo  and  save  it  in  the  same  folder  as  this  tutorial  import  urllib  request  onnx  model  url  https  www  cntk  ai  OnnxModels  emotion  ferplus  opset  emotion  ferplus  tar  gz  urllib  request  urlretrieve  onnx  model  url  filename  emotion  ferplus  tar  gz  the  magic  command  tells  our  jupyter  notebook  kernel  to  run  the  following  line  of  code  from  the  command  line  instead  of  the  notebook  kernel  We  use  tar  and  xvcf  to  unzip  the  files  we  just  retrieved  from  the  ONNX  model  zoo  tar  xvzf  emotion  ferplus  tar  gz  Check  core  SDK  version  number  import  azureml  core  print  SDK  version  azureml  core  VERSION  from  azureml  core  import  Workspace  ws  Workspace  from  config  print  ws  name  ws  location  ws  resource  group  sep  model  dir  emotion  ferplus  replac,microsoft
e  this  with  the  location  of  your  model  files  leave  as  is  if  it  in  the  same  folder  as  this  notebookfrom  azureml  core  model  import  Model  model  Model  register  model  path  model  dir  model  onnx  model  name  onnx  emotion  tags  onnx  demo  description  FER  emotion  recognition  CNN  from  ONNX  Model  Zoo  workspace  ws  models  ws  models  for  name  in  models  items  print  Name  name  tVersion  version  tDescription  description  tags  for  images  and  plots  in  this  notebook  import  matplotlib  pyplot  as  plt  from  IPython  display  import  Image  display  images  inline  matplotlib  inline  writefile  score  py  import  json  import  numpy  as  np  import  onnxruntime  import  sys  import  os  from  azureml  core  model  import  Model  import  time  def  init  global  session  input  name  output  name  model  Model  get  model  path  model  name  onnx  emotion  session  onnxruntime  InferenceSession  model  None  input  name  session  get  inputs  name  output  name ,microsoft
 session  get  outputs  name  def  run  input  data  Purpose  evaluate  test  input  in  Azure  Cloud  using  onnxruntime  We  will  call  the  run  function  later  from  our  Jupyter  Notebook  so  our  azure  service  can  evaluate  our  model  input  in  the  cloud  try  load  in  our  data  convert  to  readable  format  data  np  array  json  loads  input  data  data  astype  float32  start  time  time  session  run  output  name  input  name  data  end  time  time  result  emotion  map  postprocess  result  dict  result  result  time  in  sec  end  start  except  Exception  as  result  dict  error  str  return  json  dumps  result  dict  def  emotion  map  classes  Take  the  most  probable  labels  output  of  postprocess  and  returns  the  top  emotional  labels  that  fit  the  picture  emotion  table  neutral  happiness  surprise  sadness  anger  disgust  fear  contempt  emotion  keys  list  emotion  table  keys  emotions  for  in  range  emotions  append  emotion  keys  classes  return  emotions ,microsoft
 def  softmax  Compute  softmax  values  probabilities  from  to  for  each  possible  label  reshape  np  exp  np  max  return  sum  axis  def  postprocess  scores  This  function  takes  the  scores  generated  by  the  network  and  returns  the  class  IDs  in  decreasing  order  of  probability  prob  softmax  scores  prob  np  squeeze  prob  classes  np  argsort  prob  return  classesfrom  azureml  core  conda  dependencies  import  CondaDependencies  myenv  CondaDependencies  myenv  add  pip  package  numpy  myenv  add  pip  package  azureml  core  myenv  add  pip  package  onnxruntime  with  open  myenv  yml  as  write  myenv  serialize  to  string  from  azureml  core  image  import  ContainerImage  image  config  ContainerImage  image  configuration  execution  script  score  py  runtime  python  conda  file  myenv  yml  description  Emotion  ONNX  Runtime  container  tags  demo  onnx  image  ContainerImage  create  name  onnxtest  this  is  the  model  object  models  model  image  config  image  c,microsoft
onfig  workspace  ws  image  wait  for  creation  show  output  True  print  image  image  build  log  uri  from  azureml  core  webservice  import  AciWebservice  aciconfig  AciWebservice  deploy  configuration  cpu  cores  memory  gb  tags  demo  onnx  description  ONNX  for  emotion  recognition  model  from  azureml  core  webservice  import  Webservice  aci  service  name  onnx  demo  emotion  print  Service  aci  service  name  aci  service  Webservice  deploy  from  image  deployment  config  aciconfig  image  image  name  aci  service  name  workspace  ws  aci  service  wait  for  deployment  True  print  aci  service  state  if  aci  service  state  Healthy  run  this  command  for  debugging  print  aci  service  get  logs  If  your  deployment  fails  make  sure  to  delete  your  aci  service  before  trying  again  aci  service  delete  def  emotion  map  classes  Take  the  most  probable  labels  output  of  postprocess  and  returns  the  top  emotional  labels  that  fit  the  picture  emotio,microsoft
n  table  neutral  happiness  surprise  sadness  anger  disgust  fear  contempt  emotion  keys  list  emotion  table  keys  emotions  for  in  range  emotions  append  emotion  keys  classes  return  emotions  def  softmax  Compute  softmax  values  probabilities  from  to  for  each  possible  label  reshape  np  exp  np  max  return  sum  axis  def  postprocess  scores  This  function  takes  the  scores  generated  by  the  network  and  returns  the  class  IDs  in  decreasing  order  of  probability  prob  softmax  scores  prob  np  squeeze  prob  classes  np  argsort  prob  return  classes  to  manipulate  our  arrays  import  numpy  as  np  read  in  test  data  protobuf  files  included  with  the  model  import  onnx  from  onnx  import  numpy  helper  to  use  parsers  to  read  in  our  model  data  import  json  import  os  test  inputs  test  outputs  read  in  testing  images  from  pb  files  test  data  size  for  in  np  arange  test  data  size  input  test  data  os  path  join  model  dir ,microsoft
 test  data  set  format  input  pb  output  test  data  os  path  join  model  dir  test  data  set  format  output  pb  convert  protobuf  tensors  to  np  arrays  using  the  TensorProto  reader  from  ONNX  tensor  onnx  TensorProto  with  open  input  test  data  rb  as  tensor  ParseFromString  read  input  data  numpy  helper  to  array  tensor  test  inputs  append  input  data  with  open  output  test  data  rb  as  tensor  ParseFromString  read  output  data  numpy  helper  to  array  tensor  output  processed  emotion  map  postprocess  output  data  test  outputs  append  output  processed  plt  figure  figsize  20  20  for  test  image  in  np  arange  test  inputs  test  image  reshape  64  64  plt  subplot  test  image  plt  axhline  plt  axvline  plt  text  10  10  test  outputs  test  image  fontsize  18  plt  imshow  test  inputs  test  image  reshape  64  64  cmap  plt  cm  gray  plt  show  plt  figure  figsize  16  frameon  False  plt  subplot  plt  text  30  True  Label  fontsize  13  co,microsoft
lor  black  plt  text  20  Result  fontsize  13  color  black  plt  text  10  Inference  Time  fontsize  13  color  black  plt  text  14  Model  Input  fontsize  12  color  black  plt  text  18  64  64  fontsize  12  color  black  plt  imshow  np  ones  28  28  cmap  plt  cm  Greys  for  in  np  arange  test  data  size  input  data  json  dumps  data  test  inputs  tolist  predict  using  the  deployed  model  json  loads  aci  service  run  input  data  if  error  in  print  error  break  result  result  time  ms  np  round  time  in  sec  1000  ground  truth  test  outputs  compare  actual  value  vs  the  predicted  values  plt  subplot  plt  axhline  plt  axvline  use  different  color  for  misclassified  sample  font  color  red  if  ground  truth  result  else  black  clr  map  plt  cm  Greys  if  ground  truth  result  else  plt  cm  gray  ground  truth  labels  are  in  blue  plt  text  10  70  ground  truth  fontsize  18  color  blue  predictions  are  in  black  if  correct  red  if  incorrect  pl,microsoft
t  text  10  45  result  fontsize  18  color  font  color  plt  text  22  str  time  ms  ms  fontsize  14  color  font  color  plt  imshow  test  inputs  reshape  64  64  cmap  clr  map  plt  show  Preprocessing  functions  take  your  image  and  format  it  so  it  can  be  passed  as  input  into  our  ONNX  model  import  cv2  def  rgb2gray  rgb  Convert  the  input  image  into  grayscale  return  np  dot  rgb  299  587  114  def  resize  img  img  Resize  image  to  MNIST  model  input  dimensions  img  cv2  resize  img  dsize  64  64  interpolation  cv2  INTER  AREA  img  resize  64  64  return  img  def  preprocess  img  Resize  input  images  and  convert  them  to  grayscale  if  img  shape  64  64  img  resize  64  64  return  img  grayscale  rgb2gray  img  processed  img  resize  img  grayscale  return  processed  img  Replace  the  following  string  with  your  own  path  test  image  Make  sure  your  image  is  square  and  the  dimensions  are  equal  100  100  pixels  or  28  28  pixels  Any,microsoft
  PNG  or  JPG  image  file  should  work  Make  sure  to  include  the  entire  path  with  instead  of  your  test  image  Users  vinitra  swamy  Pictures  emotion  test  images  img  png  import  matplotlib  image  as  mpimg  if  your  test  image  path  to  file  img  mpimg  imread  your  test  image  plt  subplot  plt  imshow  img  cmap  plt  cm  Greys  print  Old  Dimensions  img  shape  img  preprocess  img  print  New  Dimensions  img  shape  else  img  Noneif  img  is  None  print  Add  the  path  for  your  image  data  else  input  data  json  dumps  data  img  tolist  try  json  loads  aci  service  run  input  data  result  result  time  ms  np  round  time  in  sec  1000  except  Exception  as  print  str  plt  figure  figsize  16  plt  subplot  plt  axhline  plt  axvline  plt  text  10  40  Model  prediction  fontsize  14  plt  text  10  25  Inference  time  fontsize  14  plt  text  100  40  str  result  fontsize  14  plt  text  100  25  str  time  ms  ms  fontsize  14  plt  text  10  10  Model,microsoft
  Input  image  fontsize  14  plt  imshow  img  reshape  64  64  cmap  plt  cm  gray  remember  to  delete  your  service  after  you  are  done  using  it  aci  service  delete  ,microsoft
import  scala  sys  process  val  socialSecurityDataFile  oasdi  tx  clean  csv  val  zipcodeDataFile  zip  codes  states  csv  wget  socialSecurityDataFile  https  raw  githubusercontent  com  djccarew  sparketldemo  master  data  oasdi  tx  clean  csv  wget  zipcodeDataFile  https  raw  githubusercontent  com  djccarew  sparketldemo  master  data  zip  codes  states  csv  import  org  apache  spark  sql  SparkSession  import  org  apache  spark  sql  types  val  spark  SparkSession  builder  getOrCreate  Specify  schema  for  resulting  DataFrame  val  socialSecurityDataSchema  StructType  Array  StructField  Zip  StringType  false  StructField  NumTotal  IntegerType  false  StructField  NumRetired  IntegerType  false  StructField  NumDisabled  IntegerType  false  StructField  NumWidowerOrParent  IntegerType  false  StructField  NumSpouses  IntegerType  false  StructField  NumChildren  IntegerType  false  StructField  BenTotal  IntegerType  false  StructField  BenRetired  IntegerType  false  StructField  Be,ibm
nWidowerOrParent  IntegerType  false  StructField  NumSeniors  IntegerType  false  Read  CSV  file  into  DataFrame  using  schema  val  dfSocialSecurityDataRaw  spark  read  format  org  apache  spark  sql  execution  datasources  csv  CSVFileFormat  option  header  true  schema  socialSecurityDataSchema  load  oasdi  tx  clean  csv  Validate  DataFrame  was  created  correctly  dfSocialSecurityDataRaw  printSchema  val  zipDataSchema  StructType  Array  StructField  Zip  StringType  false  StructField  Latitude  DoubleType  false  StructField  Longitude  DoubleType  false  StructField  City  StringType  false  StructField  State  StringType  false  StructField  County  StringType  false  val  dfZipDataRaw  spark  read  format  org  apache  spark  sql  execution  datasources  csv  CSVFileFormat  option  header  true  schema  zipDataSchema  load  zip  codes  states  csv  dfZipDataRaw  printSchema  val  dfCounties  dfZipDataRaw  select  Zip  County  dfCounties  printSchema  var  dfSocialSecurityDataWithCounty ,ibm
 dfSocialSecurityDataRaw  join  dfCounties  Zip  dfSocialSecurityDataWithCounty  printSchema  dfSocialSecurityDataWithCounty  dfSocialSecurityDataWithCounty  drop  Zip  dfSocialSecurityDataWithCounty  printSchema  dfSocialSecurityDataWithCounty  createOrReplaceTempView  aggregated  by  county  val  dfSocialSecurityDataByCounty  spark  sql  select  County  sum  NumTotal  as  NumTotal  sum  NumRetired  as  NumRetired  sum  NumDisabled  as  NumDisabled  sum  NumWidowerOrParent  as  NumWidowerOrParent  sum  NumSpouses  as  NumSpouses  sum  NumChildren  as  NumChildren  sum  BenTotal  as  BenTotal  sum  BenRetired  as  BenRetired  sum  BenWidowerOrParent  as  BenWidowerOrParent  sum  NumSeniors  as  NumSeniors  from  aggregated  by  county  group  by  County  order  by  County  dfSocialSecurityDataByCounty  take  val  jdbcURL  your  jdbc  url  val  destTable  your  dest  table  val  jdbcProperties  new  java  util  Properties  jdbcProperties  setProperty  driver  com  ibm  db2  jcc  DB2Driver  jdbcProperties  setP,ibm
roperty  user  your  db  user  jdbcProperties  setProperty  password  your  db  password  dfSocialSecurityDataByCounty  write  mode  overwrite  jdbc  jdbcURL  destTable  jdbcProperties  ,ibm
bin  bash  setup  shimport  sagemaker  sagemaker  session  sagemaker  Session  bucket  sagemaker  session  default  bucket  prefix  sagemaker  DEMO  pytorch  imagenet12  role  sagemaker  get  execution  role  import  os  import  subprocess  instance  type  local  if  subprocess  call  nvidia  smi  Set  type  to  GPU  if  one  is  present  instance  type  local  gpu  print  Instance  type  instance  type  from  utils  cifar  import  get  train  data  loader  get  test  data  loader  imshow  classes  trainloader  get  train  data  loader  testloader  get  test  data  loader  import  numpy  as  np  import  torchvision  torch  get  some  random  training  images  dataiter  iter  trainloader  images  labels  dataiter  next  show  images  imshow  torchvision  utils  make  grid  images  print  labels  print  join  9s  classes  labels  for  in  range  inputs  sagemaker  session  upload  data  path  data  bucket  bucket  key  prefix  data  pygmentize  source  imagenet12  pyfrom  sagemaker  pytorch  import  PyTorch  im,amazon
agenet12  estimator  PyTorch  entry  point  source  imagenet12  py  role  role  train  instance  count  train  instance  type  instance  type  imagenet12  estimator  fit  inputs  from  sagemaker  pytorch  import  PyTorchModel  imagenet12  predictor  imagenet12  estimator  deploy  initial  instance  count  instance  type  instance  type  get  some  test  images  dataiter  iter  testloader  images  labels  dataiter  next  print  images  imshow  torchvision  utils  make  grid  images  print  GroundTruth  join  4s  classes  labels  for  in  range  outputs  cifar10  predictor  predict  images  numpy  predicted  torch  max  torch  from  numpy  np  array  outputs  print  Predicted  join  4s  classes  predicted  for  in  range  imagenet12  estimator  delete  endpoint  ,amazon
Numpy  import  numpy  as  np  Numpy  np  array  type  print  Numpy  np  array  np  array  print  print  print  print  print  dimension  array  np  array  np  array  print  print  print  print  shape  or  print  print  dtype  print  print  print  print  10  print  np  array  np  array  10  20  print  for  row  in  print  row  np  array  flatten  print  print  print  Matplotlib  import  matplotlib  pyplot  as  pltx  np  arange  np  sin  sin  plt  plot  plot  plt  show  np  arange  y1  np  sin  y2  np  cos  plt  plot  y1  label  sin  label  plt  plot  y2  linestyle  label  cos  cos  style  plt  xlabel  plt  ylabel  plt  title  sin  cos  plt  legend  legend  plt  show  import  from  matplotlib  image  import  imread  img  imread  images  ausg  png  image  plt  imshow  img  plt  show  ,amazon
export  LDCC  ldcc  20140209  mkdir  workspace  notebooks  corpora  ldcc  cd  workspace  notebooks  corpora  ldcc  wget  http  www  rondhuit  com  download  LDCC  tar  gz  tar  xvfz  LDCC  tar  gz  rm  LDCC  tar  gzimport  sys  from  os  import  listdir  path  from  pyknp  import  Jumanpp  from  gensim  models  import  Doc2Vec  from  gensim  models  doc2vec  import  LabeledSentencedef  corpus  files  dirs  path  join  corpora  ldcc  text  for  in  listdir  corpora  ldcc  text  if  not  endswith  txt  docs  path  join  for  in  dirs  for  in  listdir  if  not  startswith  LICENSE  return  docsdef  read  document  path  with  open  path  encoding  utf  as  return  read  def  split  into  words  text  result  Jumanpp  analysis  text  return  mrph  midasi  for  mrph  in  result  mrph  list  def  doc  to  sentence  doc  name  words  split  into  words  doc  return  LabeledSentence  words  words  tags  name  def  corpus  to  sentences  corpus  docs  read  document  for  in  corpus  for  idx  doc  name  in  enumerat,google
e  zip  docs  corpus  sys  stdout  write  format  idx  len  corpus  yield  doc  to  sentence  doc  name  corpus  corpus  files  sentences  corpus  to  sentences  corpus  model  Doc2Vec  sentences  dm  size  300  window  15  alpha  025  min  alpha  025  min  count  sample  1e  print  for  epoch  in  range  20  print  Epoch  format  epoch  model  train  sentences  total  examples  model  corpus  count  epochs  model  iter  model  alpha  025  0001  19  model  min  alpha  model  alpha  print  mkdir  models  model  save  models  doc2vec  model  model  Doc2Vec  load  models  doc2vec  model  model  docvecs  most  similar  corpora  ldcc  text  livedoor  homme  livedoor  homme  5625149  txt  topn  model  docvecs  similarity  corpora  ldcc  text  livedoor  homme  livedoor  homme  4700669  txt  corpora  ldcc  text  movie  enter  movie  enter  5947726  txt  ,google
import  os  import  sagemaker  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  import  utils  utils  cifar10  download  inputs  sagemaker  session  upload  data  path  tmp  cifar10  data  key  prefix  data  DEMO  cifar10  from  sagemaker  tensorflow  import  TensorFlow  source  dir  os  path  join  os  getcwd  source  dir  estimator  TensorFlow  entry  point  resnet  cifar  10  py  source  dir  source  dir  role  role  hyperparameters  min  eval  frequency  10  training  steps  1000  evaluation  steps  100  train  instance  count  train  instance  type  ml  c4  xlarge  base  job  name  tensorboard  example  estimator  fit  inputs  run  tensorboard  locally  True  predictor  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  import  numpy  as  np  random  image  data  np  random  rand  32  32  predictor  predict  random  image  data  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
import  mnist  import  mxnet  as  mx  from  mxnet  import  gluon  autograd  import  timebatch  size  100  epochs  learning  rate  01  momentum  log  interval  100train  data  mnist  get  train  data  data  train  batch  size  val  data  mnist  get  val  data  data  test  batch  size  ctx  mx  gpu  define  the  network  net  mnist  define  network  Collect  all  parameters  from  net  and  its  children  then  initialize  them  net  initialize  mx  init  Xavier  magnitude  24  ctx  ctx  Trainer  is  for  updating  parameters  with  gradient  trainer  gluon  Trainer  net  collect  params  sgd  learning  rate  learning  rate  momentum  momentum  metric  mx  metric  Accuracy  loss  gluon  loss  SoftmaxCrossEntropyLoss  Train  the  model  for  epoch  in  range  epochs  reset  data  iterator  and  metric  at  begining  of  epoch  metric  reset  btic  time  time  for  data  label  in  enumerate  train  data  Copy  data  to  ctx  if  necessary  data  data  as  in  context  ctx  label  label  as  in  context  ctx  Sta,amazon
rt  recording  computation  graph  with  record  section  Recorded  graphs  can  then  be  differentiated  with  backward  with  autograd  record  output  net  data  loss  output  label  backward  take  gradient  step  with  batch  size  equal  to  data  shape  trainer  step  data  shape  update  metric  at  last  metric  update  label  output  if  log  interval  and  name  acc  metric  get  print  Epoch  Batch  Training  samples  epoch  name  acc  batch  size  time  time  btic  btic  time  time  name  acc  metric  get  print  Epoch  Training  epoch  name  acc  name  val  acc  mnist  test  ctx  net  val  data  print  Epoch  Validation  epoch  name  val  acc  ,amazon
import  google  datalab  ml  as  ml  import  json  import  math  import  matplotlib  pyplot  as  plot  import  mltoolbox  regression  dnn  as  regression  import  numpy  as  np  import  pandas  as  pd  import  os  import  seaborn  as  sns  import  sklearn  metrics  as  metrics  string  to  hold  the  path  workspace  path  content  datalab  workspace  census  mkdir  workspace  path  gsutil  cp  gs  cloud  datalab  samples  census  ss14psd  csv  workspace  path  data  census  csv  gsutil  cp  gs  cloud  datalab  samples  census  ss14psd  csv  workspace  path  data  census  csv  ls  workspace  path  data  check  the  contents  of  folderdf  data  pd  read  csv  os  path  join  workspace  path  data  census  csv  dtype  str  print  rows  len  df  data  print  df  data  head  def  transform  data  df  interesting  columns  WAGP  SERIALNO  AGEP  COW  ESP  ESR  FOD1P  HINS4  INDP  JWMNP  JWTR  MAR  POWPUMA  PUMA  RAC1P  SCHL  SCIENGRLP  SEX  WKW  df  df  interesting  columns  replace  whitespace  with  NaN  and  Na,google
N  with  empty  string  df  df  replace  np  nan  regex  True  fillna  drop  wages  column  that  are  empty  df  df  df  WAGP  divide  wages  by  1000  to  make  smaller  for  easier  calculation  df  WAGP  df  WAGP  astype  np  int64  1000  take  out  outliers  extreme  wages  on  either  end  of  scale  wages  10k  and  150k  df  df  df  WAGP  10  df  WAGP  150  return  the  function  value  the  dataframe  now  cleaned  and  made  usable  return  dfdf  data  transform  data  df  data  print  rows  len  df  data  df  data  head  setup  functions  to  call  for  creating  schema  and  to  split  dataset  into  training  and  eval  sets  create  schema  for  reuse  def  create  schema  df  fields  for  name  dtype  in  zip  df  columns  df  dtypes  if  dtype  in  np  str  np  object  Categorical  columns  should  have  type  STRING  fields  append  name  name  type  STRING  elif  dtype  in  np  int32  np  int64  np  float32  np  float64  Numerical  columns  have  type  FLOAT  fields  append  name  name  type,google
  FLOAT  else  raise  ValueError  Unsupported  column  type  in  column  str  dtype  name  return  fields  create  the  training  and  evaluation  sets  def  create  datasets  df  random  values  np  random  rand  len  df  df  train  df  random  values  df  eval  df  random  values  return  df  train  df  eval  call  the  functions  to  create  sets  and  schema  df  train  df  eval  create  datasets  df  data  schema  create  schema  df  data  set  the  path  and  files  training  data  path  os  path  join  workspace  path  data  train  csv  eval  data  path  os  path  join  workspace  path  data  eval  csv  schema  path  os  path  join  workspace  path  data  schema  json  convert  the  files  to  csv  df  train  to  csv  training  data  path  header  False  index  False  df  eval  to  csv  eval  data  path  header  False  index  False  write  the  schema  json  file  with  open  schema  path  as  write  json  dumps  schema  indent  ls  workspace  path  data  create  Datasets  which  are  reference  to  on,google
e  or  more  files  identified  by  path  path  pattern  along  with  schema  train  data  ml  CsvDataSet  file  pattern  training  data  path  schema  file  schema  path  eval  data  ml  CsvDataSet  file  pattern  eval  data  path  schema  file  schema  path  analysis  path  for  pull  pass  over  the  training  data  for  gathering  info  for  building  model  analysis  path  os  path  join  workspace  path  analysis  regression  analyze  dataset  train  data  output  dir  analysis  path  ls  analysis  path  features  WAGP  transform  target  SERIALNO  transform  key  AGEP  transform  embedding  embedding  dim  Age  COW  transform  one  hot  Class  of  worker  ESP  transform  embedding  embedding  dim  Employment  status  of  parents  ESR  transform  one  hot  Employment  status  FOD1P  transform  embedding  embedding  dim  Field  of  degree  HINS4  transform  one  hot  Medicaid  INDP  transform  embedding  embedding  dim  Industry  JWMNP  transform  embedding  embedding  dim  Travel  time  to  work  JWTR  ,google
transform  one  hot  Transportation  MAR  transform  one  hot  Marital  status  POWPUMA  transform  one  hot  Place  of  work  PUMA  transform  one  hot  Area  code  RAC1P  transform  one  hot  Race  SCHL  transform  one  hot  School  SCIENGRLP  transform  one  hot  Science  SEX  transform  one  hot  WKW  transform  one  hot  Weeks  worked  training  WAGP  target  value  what  the  model  learns  to  predict  key  unique  identifier  of  instances  features  from  above  target  inputs  from  input  data  apply  transforms  training  path  os  path  join  workspace  path  training  regression  train  train  dataset  train  data  eval  dataset  eval  data  output  dir  training  path  analysis  dir  analysis  path  features  features  max  steps  2000  layer  sizes  tensorboard  pid  ml  TensorBoard  start  training  path  ml  TensorBoard  stop  tensorboard  pid  ls  training  path  model  evaluation  understand  how  well  model  performed  evaluation  path  os  path  join  workspace  path  evaluation  regres,google
sion  batch  predict  training  dir  training  path  prediction  input  file  eval  data  path  output  dir  evaluation  path  output  format  json  mode  evaluation  ls  evaluation  path  df  eval  pd  read  json  os  path  join  evaluation  path  predictions  00000  of  00001  json  lines  True  df  eval  head  mse  metrics  mean  squared  error  df  eval  target  df  eval  predicted  rmse  math  sqrt  mse  print  Root  Mean  Squared  Error  3f  rmsedf  eval  error  df  eval  predicted  df  eval  target  plot  hist  df  eval  error  bins  20  file  workspace  path  data  prediction  csv  SERIALNO  AGEP  COW  ESP  ESR  FOD1P  HINS4  INDP  JWMNP  JWTR  MAR  POWPUMA  PUMA  RAC1P  SCHL  SCIENGRLP  SEX  WKW  490  64  8090  015  01  00590  00500  18  1225  32  5301  9680  015  01  00100  00100  21  1226  30  8680  020  01  00100  00100  16  1df  instances  pd  read  csv  os  path  join  workspace  path  data  prediction  csv  df  instancesdf  predictions  regression  predict  training  dir  training  path  data  ,google
df  instances  df  predictionsdf  instances  set  index  keys  SERIALNO  inplace  True  df  predictions  set  index  keys  SERIALNO  inplace  True  df  data  df  predictions  join  other  df  instances  df  data  ,google
sh  echo  kernel  python  credentials  url  http  EMR  master  node  private  IP  8998  session  configs  executorMemory  2g  executorCores  numExecutors  sparkmagic  config  json  less  sparkmagic  config  json  CHANGE  ME  input  file  path  s3  bucket  ml  data  data  csv  s3  model  bucket  bucket  spark  model  location  s3  s3  model  bucket  models  car  price  prediction  model  pipeline  name  pipeline1  model  name  model  from  pyspark  ml  import  Pipeline  from  pyspark  ml  regression  import  GBTRegressor  data  spark  read  csv  path  input  file  path  header  True  quote  sep  inferSchema  True  data  printSchema  from  pyspark  sql  functions  import  col  from  pyspark  sql  import  DataFrame  from  pyspark  ml  feature  import  StringIndexer  VectorAssembler  def  get  indexer  input  data  str  cols  value  for  in  data  data  columns  dtypes  if  string  str  cols  value  StringIndexer  inputCol  outputCol  indexed  fit  data  return  str  cols  valuedata  test  data  train  data  rand,amazon
omSplit  weights  seed  10  get  indexer  input  get  indexer  input  data  print  get  indexer  input  def  model  training  data  train  indexer  input  cols  list  set  data  train  columns  set  indexer  input  keys  Price  str  ind  cols  indexed  column  for  column  in  indexer  input  keys  indexers  indexer  input  values  pipeline  tr  Pipeline  stages  indexers  data  tr  pipeline  tr  fit  data  train  transform  data  train  assembler  VectorAssembler  inputCols  cols  outputCol  features  gbt  GBTRegressor  featuresCol  features  labelCol  Price  stepSize  008  maxDepth  subsamplingRate  75  seed  10  maxIter  20  minInstancesPerNode  checkpointInterval  100  maxBins  64  pipeline  training  Pipeline  stages  assembler  gbt  model  pipeline  training  fit  data  tr  return  model  def  model  testing  model  data  test  indexer  input  indexers  indexer  input  values  pipeline  te  Pipeline  stages  indexers  data  te  pipeline  te  fit  data  test  transform  data  test  predictions  model  tr,amazon
ansform  data  te  predictions  select  prediction  show  10  False  model  model  training  data  train  get  indexer  input  model  write  overwrite  save  spark  model  location  Load  the  saved  model  and  Test  the  model  from  pyspark  ml  import  PipelineModel  from  pyspark  ml  import  Pipeline  import  json  from  pyspark  sql  functions  import  col  from  pyspark  ml  feature  import  StringIndexer  VectorAssembler  def  get  indexer  input  data  str  cols  value  for  in  data  data  columns  dtypes  if  string  str  cols  value  StringIndexer  inputCol  outputCol  indexed  fit  data  return  str  cols  value  def  model  testing  model  data  test  indexer  input  indexers  indexer  input  values  pipeline  te  Pipeline  stages  indexers  data  te  pipeline  te  fit  data  test  transform  data  test  data  te  show  False  predictions  model  transform  data  test  predictions  select  prediction  show  10  False  sameModel  PipelineModel  load  path  s3  neilawsml  models  CarPrices  Price,amazon
  9041  9062544231  Mileage  26191  Make  Chevrolet  Model  AVEO  Trim  SVM  Sedan  4D  Type  Sedan  Cylinder  Liter  Doors  Cruise  Sound  Leather  json  dumps  jsonRDD  sc  parallelize  df  spark  read  json  jsonRDD  get  indexer  input  get  indexer  input  df  model  testing  sameModel  df  get  indexer  input  row  df  data  test  limit  row  df  show  Serialize  to  MLeap  Bundle  import  mleap  pyspark  from  mleap  pyspark  spark  support  import  SimpleSparkSerializer  model  serializeToBundle  jar  file  tmp  model  name  zip  model  transform  row  df  Save  the  Bundle  to  S3  import  boto3  s3  boto3  resource  s3  data  open  tmp  model  name  zip  rb  s3  Bucket  s3  model  bucket  put  object  Key  models  pipeline  name  model  name  zip  Body  data  python  CHANGE  ME  s3  model  bucket  bucket  spark  model  location  s3  s3  model  bucket  models  car  price  prediction  model  pipeline  name  pipeline1  model  name  model  home  home  ec2  user  models  Tar  Zip  the  model  and  save  ,amazon
back  to  S3  import  boto3  os  s3  boto3  resource  s3  s3  Bucket  s3  model  bucket  download  file  models  pipeline  name  model  name  zip  home  model  name  zip  cmd  cd  home  tar  czvf  model  name  tgz  model  name  zip  print  cmd  print  os  system  cmd  data  open  home  model  name  tgz  rb  s3  Bucket  s3  model  bucket  put  object  Key  models  pipeline  name  model  name  tgz  Body  data  ,amazon
import  mxnet  as  mx  from  movielens  data  import  get  data  iter  max  id  from  matrix  fact  import  train  import  recotools  If  MXNet  is  not  compiled  with  GPU  support  on  OSX  set  to  mx  cpu  Can  be  changed  to  mx  gpu  mx  gpu  mx  gpu  if  there  are  GPUs  ctx  mx  gpu  pos  train  data  pos  test  data  get  data  iter  batch  size  100  max  user  max  item  max  id  ml  100k  data  max  user  max  item  train  data  recotools  NegativeSamplingDataIter  pos  train  data  sample  ratio  positive  label  negative  label  test  data  recotools  NegativeSamplingDataIter  pos  test  data  sample  ratio  positive  label  negative  label  train  test  data  train  data  test  data  def  plain  net  input  user  mx  symbol  Variable  user  item  mx  symbol  Variable  item  label  mx  symbol  Variable  score  user  feature  lookup  user  mx  symbol  Embedding  data  user  input  dim  max  user  output  dim  item  feature  lookup  item  mx  symbol  Embedding  data  item  input  dim  max  item,amazon
  output  dim  loss  layer  pred  recotools  CosineLoss  user  item  label  label  return  pred  net1  plain  net  64  mx  viz  plot  network  net1  results1  train  net1  train  test  data  num  epoch  20  learning  rate  02  ctx  ctx  ,amazon
from  sagemaker  import  get  execution  role  import  os  import  sagemaker  sagemaker  session  sagemaker  Session  role  get  execution  role  IAM  execution  role  that  gives  SageMaker  access  to  resources  in  your  AWS  account  data  folder  s3  wmp  machinelearning  poc  december2017  data  cat  recommender2017  pyfrom  sagemaker  tensorflow  import  TensorFlow  recEstimator  TensorFlow  entry  point  recommender2017  py  role  role  training  steps  50  evaluation  steps  hyperparameters  learning  rate  001  train  instance  count  train  instance  type  ml  c4  xlarge  recEstimator  fit  data  folder  time  recommenderDeploy  recEstimator  deploy  initial  instance  count  instance  type  ml  c4  xlarge  time  recommenderDeploy  predict  recommenderDeploy  accept  ,amazon
from  future  import  print  function  Use  function  definition  from  future  version  say  from  interpreter  from  PIL  import  Image  import  numpy  as  np  import  pickle  as  cp  import  os  import  shutil  import  sysimgSize  32def  saveImage  fname  data  pad  key  parms  data  in  CIFAR  10  dataset  is  in  CHW  format  pixData  data  reshape  imgSize  imgSize  if  pad  pixData  np  pad  pixData  pad  pad  pad  pad  mode  constant  constant  values  128  img  Image  new  RGB  imgSize  pad  imgSize  pad  pixels  img  load  for  in  range  img  size  for  in  range  img  size  pixels  pixData  pixData  pixData  img  save  fname  def  saveTrainImages  datapath  foldername  if  not  os  path  exists  foldername  os  makedirs  foldername  data  for  ifile  in  range  print  reading  file  data  batch  ifile  with  open  os  path  join  datapath  data  batch  str  ifile  rb  as  if  sys  version  info  data  cp  load  else  data  cp  load  encoding  latin1  for  in  range  10000  fdir  os  path  join  os,microsoft
  path  abspath  foldername  data  labels  fname  os  path  join  fdir  05d  png  data  labels  ifile  10000  saveImage  fname  data  data  def  saveTestImages  datapath  foldername  if  not  os  path  exists  foldername  os  makedirs  foldername  with  open  os  path  join  datapath  test  batch  rb  as  if  sys  version  info  data  cp  load  else  data  cp  load  encoding  latin1  for  in  range  10000  fdir  os  path  join  os  path  abspath  foldername  data  labels  fname  os  path  join  fdir  05d  png  data  labels  saveImage  fname  data  data  create  label  folders  datapath  datasets  cifar  10  batches  py  trainpath  os  path  join  os  path  abspath  datapath  train  testpath  os  path  join  os  path  abspath  datapath  test  created  labeled  folders  for  label  in  range  10  directory  os  path  join  trainpath  label  if  not  os  path  exists  directory  os  makedirs  directory  for  label  in  range  10  directory  os  path  join  testpath  label  if  not  os  path  exists  directory  o,microsoft
s  makedirs  directory  create  png  images  from  original  datasets  print  Converting  train  data  to  png  images  saveTrainImages  datapath  os  path  join  datapath  trainpath  print  Done  print  Converting  test  data  to  png  images  saveTestImages  datapath  os  path  join  datapath  testpath  print  Done  import  keras  from  keras  datasets  import  cifar10  from  keras  preprocessing  image  import  ImageDataGenerator  from  keras  models  import  Sequential  from  keras  layers  import  Dense  Dropout  Activation  Flatten  from  keras  layers  import  Conv2D  MaxPooling2D  from  keras  import  backend  as  import  os  import  matplotlib  pyplot  as  pltbatch  size  32  num  classes  10  epochs  100  data  augmentation  True  num  predictions  20  save  dir  os  path  abspath  models  model  name  sample  cifar10  model  h5  read  dataset  from  directory  this  is  the  augmentation  configuration  we  will  use  for  training  train  datagen  ImageDataGenerator  rescale  255  shear  range  zo,microsoft
om  range  horizontal  flip  True  this  is  the  augmentation  configuration  we  will  use  for  testing  only  rescaling  test  datagen  ImageDataGenerator  rescale  255  train  generator  train  datagen  flow  from  directory  trainpath  target  size  imgSize  imgSize  batch  size  batch  size  class  mode  categorical  validation  generator  test  datagen  flow  from  directory  testpath  target  size  imgSize  imgSize  batch  size  batch  size  class  mode  categorical  nb  train  samples  50000  nb  validation  samples  10000  if  image  data  format  channels  first  input  shape  imgSize  imgSize  else  input  shape  imgSize  imgSize  model  Sequential  model  add  Conv2D  32  padding  same  input  shape  input  shape  model  add  Activation  relu  model  add  Conv2D  32  model  add  Activation  relu  model  add  MaxPooling2D  pool  size  model  add  Dropout  25  model  add  Conv2D  64  padding  same  model  add  Activation  relu  model  add  Conv2D  64  model  add  Activation  relu  model  add  MaxP,microsoft
ooling2D  pool  size  model  add  Dropout  25  model  add  Flatten  model  add  Dense  512  model  add  Activation  relu  model  add  Dropout  model  add  Dense  num  classes  model  add  Activation  softmax  initiate  RMSprop  optimizer  opt  keras  optimizers  rmsprop  lr  0001  decay  1e  Let  train  the  model  using  RMSprop  model  compile  loss  categorical  crossentropy  optimizer  opt  metrics  accuracy  history  model  fit  generator  train  generator  steps  per  epoch  nb  train  samples  batch  size  epochs  epochs  validation  data  validation  generator  validation  steps  nb  validation  samples  batch  size  Save  model  and  weights  if  not  os  path  isdir  save  dir  os  makedirs  save  dir  model  path  os  path  join  save  dir  model  name  model  save  model  path  print  Saved  trained  model  at  model  path  Load  model  model  path  os  path  join  save  dir  model  name  model  keras  models  load  model  model  path  Get  training  and  test  loss  histories  loss  history  hist,microsoft
ory  loss  acc  history  history  acc  val  loss  history  history  val  loss  val  acc  history  history  val  acc  Create  count  of  the  number  of  epochs  epoch  count  range  len  loss  Visualize  loss  history  fig  plt  figure  figsize  12  plt  figure  fig  add  subplot  121  plt  plot  epoch  count  acc  plt  plot  epoch  count  loss  plt  legend  acc  loss  plt  xlabel  epoch  plt  ylabel  acc  vs  loss  fig  add  subplot  122  plt  plot  epoch  count  acc  plt  plot  epoch  count  val  acc  plt  legend  acc  val  acc  plt  xlabel  epoch  plt  ylabel  acc  vs  val  acc  plt  show  import  requests  from  PIL  import  Image  import  cv2  from  keras  preprocessing  import  image  from  io  import  BytesIO  test  url  https  boygeniusreport  files  wordpress  com  2017  01  cat  jpg  test  url  https  www  pensketruckrental  com  img  hero  hero  trucks  22  jpg  requests  get  test  url  img  Image  open  BytesIO  content  rsimg  cv2  resize  np  array  img  32  32  interpolation  cv2  INTER  AREA ,microsoft
 plt  imshow  rsimg  img  Image  fromarray  rsimg  image  img  to  array  img  np  expand  dims  axis  255  classes  model  predict  print  classes  np  argmax  classes  np  argmax  classes  labels  airplane  automobile  bird  cat  deer  dog  frog  horse  ship  truck  print  labels  ,microsoft
import  pandas  as  pd  import  numpy  as  np  from  sklearn  import  datasets  iris  datasets  load  iris  iris  data  iris  target  dataset  np  insert  iris  data  iris  target  axis  df  pd  DataFrame  data  dataset  columns  iris  id  iris  feature  names  df  head  from  sklearn  model  selection  import  train  test  split  train  test  train  test  train  test  split  test  size  33  random  state  42  stratify  with  open  iris  train  csv  as  csv  for  in  zip  train  train  line  join  list  map  str  csv  write  line  csv  flush  csv  close  with  open  iris  test  csv  as  csv  for  in  zip  test  test  line  join  list  map  str  csv  write  line  csv  flush  csv  close  import  sagemaker  import  boto3  from  sagemaker  import  get  execution  role  from  sklearn  model  selection  import  train  test  split  role  get  execution  role  prefix  mlops  iris  Retrieve  the  default  bucket  sagemaker  session  sagemaker  Session  bucket  sagemaker  session  default  bucket  Upload  the  dataset ,amazon
 to  an  S3  bucket  input  train  sagemaker  session  upload  data  path  iris  train  csv  key  prefix  data  prefix  input  test  sagemaker  session  upload  data  path  iris  test  csv  key  prefix  data  prefix  train  data  sagemaker  session  s3  input  s3  data  input  train  content  type  csv  test  data  sagemaker  session  s3  input  s3  data  input  test  content  type  csv  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  xgboost  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  xgboost  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  xgboost  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  xgboost  latest  Create  the  estimator  xgb  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  role  train  instance  count  train  instance  type  ml  m4  xlarge  output  path  s3  output  format  bucket  prefix  sagemaker  session  sagemaker  session  Set  the  hyperparameters  xgb  set  hyperparamete,amazon
rs  eta  max  depth  10  gamma  reg  lambda  10  num  class  len  np  unique  alpha  10  min  child  weight  silent  objective  multi  softmax  num  round  30  time  takes  around  3min  11s  xgb  fit  train  train  data  validation  test  data  time  xgb  predictor  xgb  deploy  initial  instance  count  instance  type  ml  m4  xlarge  endpoint  name  xgb  predictor  endpoint  model  name  boto3  client  sagemaker  describe  endpoint  config  EndpointConfigName  endpoint  name  ProductionVariants  ModelName  from  sagemaker  predictor  import  csv  serializer  from  sklearn  metrics  import  f1  score  xgb  predictor  content  type  text  csv  xgb  predictor  serializer  csv  serializer  xgb  predictor  deserializer  Nonepredictions  test  float  xgb  predictor  predict  decode  utf  for  in  test  score  f1  score  test  predictions  test  labels  average  micro  print  F1  Score  micro  1f  score  100  sm  boto3  client  sagemaker  runtime  from  sagemaker  predictor  import  csv  serializer  resp  sm  inv,amazon
oke  endpoint  EndpointName  endpoint  name  ContentType  text  csv  Body  csv  serializer  test  prediction  float  resp  Body  read  decode  utf  print  Predicted  class  1f  for  prediction  csv  serializer  test  from  sagemaker  tuner  import  IntegerParameter  CategoricalParameter  ContinuousParameter  HyperparameterTuner  hyperparameter  ranges  eta  ContinuousParameter  min  child  weight  ContinuousParameter  10  alpha  ContinuousParameter  gamma  ContinuousParameter  10  max  depth  IntegerParameter  10  objective  metric  name  validation  merror  tuner  HyperparameterTuner  xgb  objective  metric  name  hyperparameter  ranges  max  jobs  20  max  parallel  jobs  objective  type  Minimize  tuner  fit  train  train  data  validation  test  data  tuner  wait  job  name  tuner  latest  tuning  job  name  attached  tuner  HyperparameterTuner  attach  job  name  xgb  predictor2  attached  tuner  deploy  initial  instance  count  instance  type  ml  m4  xlarge  endpoint  name  xgb  predictor2  endpoint  ,amazon
model  name  boto3  client  sagemaker  describe  endpoint  config  EndpointConfigName  endpoint  name  ProductionVariants  ModelName  from  sagemaker  predictor  import  csv  serializer  from  sklearn  metrics  import  f1  score  xgb  predictor2  content  type  text  csv  xgb  predictor2  serializer  csv  serializer  xgb  predictor2  deserializer  Nonepredictions  test  float  xgb  predictor2  predict  decode  utf  for  in  test  score  f1  score  test  predictions  test  labels  average  micro  print  F1  Score  micro  1f  score  100  batch  dataset  filename  batch  dataset  csv  with  open  batch  dataset  filename  as  csv  for  in  line  join  list  map  str  csv  write  line  csv  flush  csv  close  input  batch  sagemaker  session  upload  data  path  batch  dataset  filename  key  prefix  data  prefix  import  sagemaker  Initialize  the  transformer  object  transformer  sagemaker  transformer  Transformer  base  transform  job  name  mlops  iris  model  name  model  name  instance  count  instance  t,amazon
ype  ml  c4  xlarge  output  path  s3  batch  output  format  bucket  prefix  To  start  transform  job  transformer  transform  input  batch  content  type  text  csv  split  type  Line  Then  wait  until  transform  job  is  completed  transformer  wait  import  boto3  predictions  filename  iris  predictions  csv  s3  boto3  client  s3  s3  download  file  bucket  batch  output  out  format  prefix  batch  dataset  filename  predictions  filename  df2  pd  read  csv  predictions  filename  sep  encoding  utf  header  None  names  predicted  iris  id  df3  df  copy  df3  predicted  iris  id  df2  predicted  iris  id  df3  head  from  sklearn  metrics  import  f1  score  score  f1  score  df3  iris  id  df3  predicted  iris  id  labels  average  micro  print  F1  Score  micro  1f  score  100  matplotlib  inline  import  seaborn  as  sns  import  matplotlib  pyplot  as  plt  from  sklearn  metrics  import  confusion  matrix  cnf  matrix  confusion  matrix  df3  iris  id  df3  predicted  iris  id  ax  plt  sub,amazon
plots  figsize  15  sns  heatmap  cnf  matrix  annot  True  fmt  mask  np  zeros  like  cnf  matrix  dtype  np  bool  cmap  sns  diverging  palette  220  10  as  cmap  True  square  True  ax  ax  ,amazon
import  pandas  as  pd  import  glob  import  cv2  import  osdef  statistics  folder  data  path  os  path  join  folder  JPEG  for  file  in  glob  glob  path  img  cv2  imread  file  width  height  depth  img  shape  data  append  name  os  path  basename  file  width  width  height  height  depth  depth  df  pd  DataFrame  from  dict  data  return  dfdf  val  statistics  home  etanchik  Documents  research  pytorch  examples  imagenet  data  val  df  val  name  width  height  depth  describe  df  train  statistics  home  etanchik  Documents  research  pytorch  examples  imagenet  data  train  df  train  name  width  height  depth  describe  ,amazon
mkdir  data  wget  data  aclImdb  v1  tar  gz  http  ai  stanford  edu  amaas  data  sentiment  aclImdb  v1  tar  gz  tar  zxf  data  aclImdb  v1  tar  gz  dataimport  os  import  glob  def  read  imdb  data  data  dir  data  aclImdb  data  labels  for  data  type  in  train  test  data  data  type  labels  data  type  for  sentiment  in  pos  neg  data  data  type  sentiment  labels  data  type  sentiment  path  os  path  join  data  dir  data  type  sentiment  txt  files  glob  glob  path  for  in  files  with  open  as  review  data  data  type  sentiment  append  review  read  Here  we  represent  positive  review  by  and  negative  review  by  labels  data  type  sentiment  append  if  sentiment  pos  else  assert  len  data  data  type  sentiment  len  labels  data  type  sentiment  data  size  does  not  match  labels  size  format  data  type  sentiment  return  data  labelsdata  labels  read  imdb  data  print  IMDB  reviews  train  pos  neg  test  pos  neg  format  len  data  train  pos  len  data ,amazon
 train  neg  len  data  test  pos  len  data  test  neg  from  sklearn  utils  import  shuffle  def  prepare  imdb  data  data  labels  Prepare  training  and  test  sets  from  IMDb  movie  reviews  Combine  positive  and  negative  reviews  and  labels  data  train  data  train  pos  data  train  neg  data  test  data  test  pos  data  test  neg  labels  train  labels  train  pos  labels  train  neg  labels  test  labels  test  pos  labels  test  neg  Shuffle  reviews  and  corresponding  labels  within  training  and  test  sets  data  train  labels  train  shuffle  data  train  labels  train  data  test  labels  test  shuffle  data  test  labels  test  Return  unified  training  data  test  data  training  labels  test  labets  return  data  train  data  test  labels  train  labels  testtrain  test  train  test  prepare  imdb  data  data  labels  print  IMDb  reviews  combined  train  test  format  len  train  len  test  train  100  import  nltk  nltk  download  stopwords  from  nltk  corpus  import  stop,amazon
words  from  nltk  stem  porter  import  stemmer  PorterStemmer  import  re  from  bs4  import  BeautifulSoup  def  review  to  words  review  text  BeautifulSoup  review  html  parser  get  text  Remove  HTML  tags  text  re  sub  zA  Z0  text  lower  Convert  to  lower  case  words  text  split  Split  string  into  words  words  for  in  words  if  not  in  stopwords  words  english  Remove  stopwords  words  PorterStemmer  stem  for  in  words  stem  return  wordsimport  pickle  cache  dir  os  path  join  cache  sentiment  analysis  where  to  store  cache  files  os  makedirs  cache  dir  exist  ok  True  ensure  cache  directory  exists  def  preprocess  data  data  train  data  test  labels  train  labels  test  cache  dir  cache  dir  cache  file  preprocessed  data  pkl  Convert  each  review  to  words  read  from  cache  if  available  If  cache  file  is  not  None  try  to  read  from  it  first  cache  data  None  if  cache  file  is  not  None  try  with  open  os  path  join  cache  dir  cach,amazon
e  file  rb  as  cache  data  pickle  load  print  Read  preprocessed  data  from  cache  file  cache  file  except  pass  unable  to  read  from  cache  but  that  okay  If  cache  is  missing  then  do  the  heavy  lifting  if  cache  data  is  None  Preprocess  training  and  test  data  to  obtain  words  for  each  review  words  train  list  map  review  to  words  data  train  words  test  list  map  review  to  words  data  test  words  train  review  to  words  review  for  review  in  data  train  words  test  review  to  words  review  for  review  in  data  test  Write  to  cache  file  for  future  runs  if  cache  file  is  not  None  cache  data  dict  words  train  words  train  words  test  words  test  labels  train  labels  train  labels  test  labels  test  with  open  os  path  join  cache  dir  cache  file  wb  as  pickle  dump  cache  data  print  Wrote  preprocessed  data  to  cache  file  cache  file  else  Unpack  data  loaded  from  cache  file  words  train  words  test  labels  tr,amazon
ain  labels  test  cache  data  words  train  cache  data  words  test  cache  data  labels  train  cache  data  labels  test  return  words  train  words  test  labels  train  labels  test  Preprocess  data  train  test  train  test  preprocess  data  train  test  train  test  import  numpy  as  np  from  sklearn  feature  extraction  text  import  CountVectorizer  from  sklearn  externals  import  joblib  joblib  is  an  enhanced  version  of  pickle  that  is  more  efficient  for  storing  NumPy  arrays  def  extract  BoW  features  words  train  words  test  vocabulary  size  5000  cache  dir  cache  dir  cache  file  bow  features  pkl  Extract  Bag  of  Words  for  given  set  of  documents  already  preprocessed  into  words  If  cache  file  is  not  None  try  to  read  from  it  first  cache  data  None  if  cache  file  is  not  None  try  with  open  os  path  join  cache  dir  cache  file  rb  as  cache  data  joblib  load  print  Read  features  from  cache  file  cache  file  except  pass  una,amazon
ble  to  read  from  cache  but  that  okay  If  cache  is  missing  then  do  the  heavy  lifting  if  cache  data  is  None  Fit  vectorizer  to  training  documents  and  use  it  to  transform  them  NOTE  Training  documents  have  already  been  preprocessed  and  tokenized  into  words  pass  in  dummy  functions  to  skip  those  steps  preprocessor  lambda  vectorizer  CountVectorizer  max  features  vocabulary  size  preprocessor  lambda  tokenizer  lambda  already  preprocessed  features  train  vectorizer  fit  transform  words  train  toarray  Apply  the  same  vectorizer  to  transform  the  test  documents  ignore  unknown  words  features  test  vectorizer  transform  words  test  toarray  NOTE  Remember  to  convert  the  features  using  toarray  for  compact  representation  Write  to  cache  file  for  future  runs  store  vocabulary  as  well  if  cache  file  is  not  None  vocabulary  vectorizer  vocabulary  cache  data  dict  features  train  features  train  features  test  features  ,amazon
test  vocabulary  vocabulary  with  open  os  path  join  cache  dir  cache  file  wb  as  joblib  dump  cache  data  print  Wrote  features  to  cache  file  cache  file  else  Unpack  data  loaded  from  cache  file  features  train  features  test  vocabulary  cache  data  features  train  cache  data  features  test  cache  data  vocabulary  Return  both  the  extracted  features  as  well  as  the  vocabulary  return  features  train  features  test  vocabulary  Extract  Bag  of  Words  features  for  both  training  and  test  datasets  train  test  vocabulary  extract  BoW  features  train  test  import  pandas  as  pd  val  pd  DataFrame  train  10000  train  pd  DataFrame  train  10000  val  pd  DataFrame  train  10000  train  pd  DataFrame  train  10000  First  we  make  sure  that  the  local  directory  in  which  we  like  to  store  the  training  and  validation  csv  files  exists  data  dir  data  xgboost  if  not  os  path  exists  data  dir  os  makedirs  data  dir  First  save  the  test  ,amazon
data  to  test  csv  in  the  data  dir  directory  Note  that  we  do  not  save  the  associated  ground  truth  labels  instead  we  will  use  them  later  to  compare  with  our  model  output  Solution  The  test  data  shouldn  contain  the  ground  truth  labels  as  they  are  what  the  model  is  trying  to  predict  We  will  end  up  using  them  afterward  to  compare  the  predictions  to  pd  concat  test  test  axis  to  csv  os  path  join  data  dir  test  csv  header  False  index  False  pd  DataFrame  test  to  csv  os  path  join  data  dir  test  csv  header  False  index  False  pd  concat  val  val  axis  to  csv  os  path  join  data  dir  validation  csv  header  False  index  False  pd  concat  train  train  axis  to  csv  os  path  join  data  dir  train  csv  header  False  index  False  To  save  bit  of  memory  we  can  set  text  train  val  train  and  val  to  None  test  train  val  train  val  Noneimport  sagemaker  session  sagemaker  Session  Store  the  current  SageM,amazon
aker  session  S3  prefix  which  folder  will  we  use  prefix  sentiment  xgboost  test  location  session  upload  data  os  path  join  data  dir  test  csv  key  prefix  prefix  val  location  session  upload  data  os  path  join  data  dir  validation  csv  key  prefix  prefix  train  location  session  upload  data  os  path  join  data  dir  train  csv  key  prefix  prefix  from  sagemaker  import  get  execution  role  Our  current  execution  role  is  require  when  creating  the  model  as  the  training  and  inference  code  will  need  to  access  the  model  artifacts  role  get  execution  role  We  need  to  retrieve  the  location  of  the  container  which  is  provided  by  Amazon  for  using  XGBoost  As  matter  of  convenience  the  training  and  inference  code  both  use  the  same  container  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  session  boto  region  name  xgboost  TODO  Create  SageMaker  estimator  using  the  containe,amazon
r  location  determined  in  the  previous  cell  It  is  recommended  that  you  use  single  training  instance  of  type  ml  m4  xlarge  It  is  also  recommended  that  you  use  s3  output  format  session  default  bucket  prefix  as  the  output  path  xgb  None  TODO  Set  the  XGBoost  hyperparameters  in  the  xgb  object  Don  forget  that  in  this  case  we  have  binary  label  so  we  should  be  using  the  binary  logistic  objective  xgb  sagemaker  estimator  Estimator  container  The  location  of  the  container  we  wish  to  use  role  What  is  our  current  IAM  Role  train  instance  count  How  many  compute  instances  train  instance  type  ml  m4  xlarge  What  kind  of  compute  instances  output  path  s3  output  format  session  default  bucket  prefix  sagemaker  session  session  xgb  set  hyperparameters  max  depth  eta  gamma  min  child  weight  subsample  silent  objective  binary  logistic  early  stopping  rounds  10  num  round  500  First  make  sure  to  import  ,amazon
the  relevant  objects  used  to  construct  the  tuner  from  sagemaker  tuner  import  IntegerParameter  ContinuousParameter  HyperparameterTuner  TODO  Create  the  hyperparameter  tuner  object  xgb  hyperparameter  tuner  HyperparameterTuner  estimator  xgb  The  estimator  object  to  use  as  the  basis  for  the  training  jobs  objective  metric  name  validation  rmse  The  metric  used  to  compare  trained  models  objective  type  Minimize  Whether  we  wish  to  minimize  or  maximize  the  metric  max  jobs  20  The  total  number  of  models  to  train  max  parallel  jobs  The  number  of  models  to  train  in  parallel  hyperparameter  ranges  max  depth  IntegerParameter  12  eta  ContinuousParameter  05  min  child  weight  IntegerParameter  subsample  ContinuousParameter  gamma  ContinuousParameter  10  s3  input  train  sagemaker  s3  input  s3  data  train  location  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  val  location  content  type  csv  xgb  hyper,amazon
parameter  tuner  fit  train  s3  input  train  validation  s3  input  validation  xgb  hyperparameter  tuner  wait  TODO  Create  new  estimator  object  attached  to  the  best  training  job  found  during  hyperparameter  tuning  xgb  attached  sagemaker  estimator  Estimator  attach  xgb  hyperparameter  tuner  best  training  job  TODO  Create  transformer  object  from  the  attached  estimator  Using  an  instance  count  of  and  an  instance  type  of  ml  m4  xlarge  should  be  more  than  enough  xgb  transformer  xgb  attached  transformer  instance  count  instance  type  ml  m4  xlarge  TODO  Start  the  transform  job  Make  sure  to  specify  the  content  type  and  the  split  type  of  the  test  data  xgb  transformer  transform  test  location  content  type  text  csv  split  type  Line  xgb  transformer  wait  aws  s3  cp  recursive  xgb  transformer  output  path  data  dirpredictions  pd  read  csv  os  path  join  data  dir  test  csv  out  header  None  predictions  round  num  fo,amazon
r  num  in  predictions  squeeze  values  from  sklearn  metrics  import  accuracy  score  accuracy  score  test  predictions  First  we  will  remove  all  of  the  files  contained  in  the  data  dir  directory  rm  data  dir  And  then  we  delete  the  directory  itself  rmdir  data  dir  Similarly  we  will  remove  the  files  in  the  cache  dir  directory  and  the  directory  itself  rm  cache  dir  rmdir  cache  dir  ,amazon
import  tensorflow  as  tf  import  matplotlib  pyplot  as  plt  from  tensorflow  examples  tutorials  mnist  import  input  data  matplotlib  inline  mnist  input  data  read  data  sets  MNIST  data  one  hot  True  mnist  train  images  shapeclass  Model  Network  Model  Class  Note  that  this  class  has  only  the  constructor  The  actual  model  is  defined  inside  the  constructor  Attributes  tf  float32  This  is  tensorflow  placeholder  for  MNIST  images  Expected  shape  is  None  784  tf  float32  This  is  tensorflow  placeholder  for  MNIST  labels  one  hot  encoded  Expected  shape  is  None  10  mode  tf  bool  This  is  used  for  the  batch  normalization  It  True  at  training  time  and  False  at  test  time  loss  tf  float32  The  loss  function  is  softmax  cross  entropy  train  op  This  is  simply  the  training  op  that  minimizes  the  loss  accuracy  tf  float32  The  accuracy  operation  Examples  model  Model  Batch  Norm  32  10  def  init  self  name  input  dim  ou,ibm
tput  dim  hidden  dims  32  32  use  batchnorm  True  activation  fn  tf  nn  relu  optimizer  tf  train  AdamOptimizer  lr  01  Constructor  Parameters  name  str  The  name  of  this  network  The  entire  network  will  be  created  under  tf  variable  scope  name  input  dim  int  The  input  dimension  In  this  example  784  output  dim  int  The  number  of  output  labels  There  are  10  labels  hidden  dims  list  default  32  32  len  hidden  dims  number  of  layers  each  element  is  the  number  of  hidden  units  use  batchnorm  bool  default  True  If  true  it  will  create  the  batchnormalization  layer  activation  fn  TF  functions  default  tf  nn  relu  Activation  Function  optimizer  TF  optimizer  default  tf  train  AdamOptimizer  Optimizer  Function  lr  float  default  01  Learning  rate  with  tf  variable  scope  name  Placeholders  are  defined  self  tf  placeholder  tf  float32  None  input  dim  name  self  tf  placeholder  tf  float32  None  output  dim  name  self  mode,ibm
  tf  placeholder  tf  bool  name  train  mode  Loop  over  hidden  layers  net  self  for  dim  in  enumerate  hidden  dims  with  tf  variable  scope  layer  format  net  tf  layers  dense  net  dim  if  use  batchnorm  net  tf  layers  batch  normalization  net  training  self  mode  net  activation  fn  net  Attach  fully  connected  layers  net  tf  contrib  layers  flatten  net  net  tf  layers  dense  net  output  dim  self  loss  tf  nn  softmax  cross  entropy  with  logits  logits  net  labels  self  self  loss  tf  reduce  mean  self  loss  name  loss  When  using  the  batchnormalization  layers  it  is  necessary  to  manually  add  the  update  operations  because  the  moving  averages  are  not  included  in  the  graph  update  ops  tf  get  collection  tf  GraphKeys  UPDATE  OPS  scope  name  with  tf  control  dependencies  update  ops  self  train  op  optimizer  lr  minimize  self  loss  Accuracy  etc  softmax  tf  nn  softmax  net  name  softmax  self  accuracy  tf  equal  tf  argmax  so,ibm
ftmax  tf  argmax  self  self  accuracy  tf  reduce  mean  tf  cast  self  accuracy  tf  float32  class  Solver  Solver  class  This  class  will  contain  the  model  class  and  session  Attributes  model  Model  class  sess  TF  session  Methods  train  Run  the  train  op  and  Returns  the  loss  evalulate  batch  size  None  Returns  Loss  and  Accuracy  If  batch  size  is  given  it  computed  using  batch  size  because  most  GPU  memories  can  not  handle  the  entire  training  data  at  once  Example  sess  tf  InteractiveSession  model  Model  BatchNorm  32  10  solver  Solver  sess  model  Train  solver  train  Evaluate  solver  evaluate  def  init  self  sess  model  self  model  model  self  sess  sess  def  train  self  feed  self  model  self  model  self  model  mode  True  train  op  self  model  train  op  loss  self  model  loss  return  self  sess  run  train  op  loss  feed  dict  feed  def  evaluate  self  batch  size  None  if  batch  size  shape  total  loss  total  acc  for  in  ,ibm
range  batch  size  batch  batch  size  batch  batch  size  feed  self  model  batch  self  model  batch  self  model  mode  False  loss  self  model  loss  accuracy  self  model  accuracy  step  loss  step  acc  self  sess  run  loss  accuracy  feed  dict  feed  total  loss  step  loss  batch  shape  total  acc  step  acc  batch  shape  total  loss  total  acc  return  total  loss  total  acc  else  feed  self  model  self  model  self  model  mode  False  loss  self  model  loss  accuracy  self  model  accuracy  return  self  sess  run  loss  accuracy  feed  dict  feed  input  dim  784  output  dim  10  55000  tf  reset  default  graph  sess  tf  InteractiveSession  We  create  two  models  one  with  the  batch  norm  and  other  without  bn  Model  batchnorm  input  dim  output  dim  use  batchnorm  True  nn  Model  no  norm  input  dim  output  dim  use  batchnorm  False  We  create  two  solvers  to  train  both  models  at  the  same  time  for  comparison  Usually  we  only  need  one  solver  class  ,ibm
bn  solver  Solver  sess  bn  nn  solver  Solver  sess  nn  epoch  10  batch  size  32  Save  Losses  and  Accuracies  every  epoch  We  are  going  to  plot  them  later  train  losses  train  accs  valid  losses  valid  accs  init  tf  global  variables  initializer  sess  run  init  for  epoch  in  range  epoch  for  in  range  batch  size  batch  batch  mnist  train  next  batch  batch  size  bn  loss  bn  solver  train  batch  batch  nn  loss  nn  solver  train  batch  batch  loss  acc  bn  solver  evaluate  mnist  train  images  mnist  train  labels  batch  size  loss  acc  nn  solver  evaluate  mnist  train  images  mnist  train  labels  batch  size  Save  train  losses  acc  train  losses  append  loss  loss  train  accs  append  acc  acc  print  Epoch  epoch  TRAIN  Batchnorm  Loss  Acc  loss  5f  acc  vs  No  Batchnorm  Loss  Acc  loss  5f  acc  loss  acc  bn  solver  evaluate  mnist  validation  images  mnist  validation  labels  loss  acc  nn  solver  evaluate  mnist  validation  images  mnist  va,ibm
lidation  labels  Save  valid  losses  acc  valid  losses  append  loss  loss  valid  accs  append  acc  acc  print  Epoch  epoch  VALID  Batchnorm  Loss  Acc  loss  5f  acc  vs  No  Batchnorm  Loss  Acc  loss  5f  acc  print  bn  solver  evaluate  mnist  test  images  mnist  test  labels  nn  solver  evaluate  mnist  test  images  mnist  test  labels  def  plot  compare  loss  list  list  ylim  None  title  None  None  bn  for  in  loss  list  nn  for  in  loss  list  plt  figure  figsize  15  10  plt  plot  bn  label  With  BN  plt  plot  nn  label  Without  BN  if  ylim  plt  ylim  ylim  if  title  plt  title  title  plt  legend  plt  grid  on  plt  show  plot  compare  train  losses  title  Training  Loss  at  Epoch  plot  compare  train  accs  title  Training  Acc  at  Epoch  plot  compare  valid  losses  title  Validation  Loss  at  Epoch  plot  compare  valid  accs  title  Validation  Acc  at  Epoch  ,ibm
time  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  bucket  name  customize  to  your  bucket  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  image  classification  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  image  classification  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  image  classification  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  image  classification  latest  training  image  containers  boto3  Session  region  name  print  training  image  import  os  import  urllib  request  import  boto3  def  download  url  filename  url  split  if  not  os  path  exists  filename  urllib  request  urlretrieve  url  filename  def  upload  to  s3  channel  file  s3  boto3  resource  s3  data  open  file  rb  key  channel  file  s3  Bucket  bucket  put  object  Key  key  Body  data  caltech  256  download  http  data  mxnet  io  data  caltech  256  calte,amazon
ch  256  60  train  rec  download  http  data  mxnet  io  data  caltech  256  caltech  256  60  val  rec  upload  to  s3  validation  caltech  256  60  val  rec  upload  to  s3  train  caltech  256  60  train  rec  The  algorithm  supports  multiple  network  depth  number  of  layers  They  are  18  34  50  101  152  and  200  For  this  training  we  will  use  18  layers  num  layers  18  we  need  to  specify  the  input  image  shape  for  the  training  data  image  shape  224  224  we  also  need  to  specify  the  number  of  training  samples  in  the  training  set  for  caltech  it  is  15420  num  training  samples  15420  specify  the  number  of  output  classes  num  classes  257  batch  size  for  training  mini  batch  size  128  number  of  epochs  epochs  learning  rate  learning  rate  01  top  Since  we  are  using  transfer  learning  we  set  use  pretrained  model  to  so  that  weights  can  be  initialized  with  pre  trained  weights  use  pretrained  model  time  import  time  impo,amazon
rt  boto3  from  time  import  gmtime  strftime  s3  boto3  client  s3  create  unique  job  name  job  name  prefix  sagemaker  imageclassification  notebook  timestamp  time  strftime  time  gmtime  job  name  job  name  prefix  timestamp  training  params  specify  the  training  docker  image  AlgorithmSpecification  TrainingImage  training  image  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  s3  output  format  bucket  job  name  prefix  ResourceConfig  InstanceCount  InstanceType  ml  p2  8xlarge  VolumeSizeInGB  50  TrainingJobName  job  name  HyperParameters  image  shape  image  shape  num  layers  str  num  layers  num  training  samples  str  num  training  samples  num  classes  str  num  classes  mini  batch  size  str  mini  batch  size  epochs  str  epochs  learning  rate  str  learning  rate  use  pretrained  model  str  use  pretrained  model  StoppingCondition  MaxRuntimeInSeconds  360000  Training  data  should  be  inside  subdirectory  called  train  Validation ,amazon
 data  should  be  inside  subdirectory  called  validation  The  algorithm  currently  only  supports  fullyreplicated  model  where  data  is  copied  onto  each  machine  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  recordio  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  recordio  CompressionType  None  print  Training  job  name  format  job  name  print  nInput  Data  Location  format  training  params  InputDataConfig  DataSource  S3DataSource  create  the  Amazon  SageMaker  training  job  sagemaker  boto3  client  service  name  sagemaker  sagemaker  create  training  job  training  params  confirm  that  the  training  job  has  started  status  sagemaker  describe  training  job  TrainingJobName  job  name  ,amazon
TrainingJobStatus  print  Training  job  current  status  format  status  try  wait  for  the  job  to  finish  and  report  the  ending  status  sagemaker  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  except  print  Training  failed  to  start  if  exception  is  raised  that  means  it  has  failed  message  sagemaker  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  time  import  boto3  from  time  import  gmtime  strftime  sage  boto3  Session  client  service  name  sagemaker  model  name  test  image  classification  model  print  mo,amazon
del  name  info  sage  describe  training  job  TrainingJobName  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  data  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  image  classification  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  image  classification  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  image  classification  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  image  classification  latest  hosting  image  containers  boto3  Session  region  name  primary  container  Image  hosting  image  ModelDataUrl  model  data  create  model  response  sage  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  timestamp  time  strftime  time  gmtime  endpoint  config  name  job  name  prefix  epc  timestamp  endpoint  config  response  sage  create  endpoint  config ,amazon
 EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  configuration  name  format  endpoint  config  name  print  Endpoint  configuration  arn  format  endpoint  config  response  EndpointConfigArn  time  import  time  timestamp  time  strftime  time  gmtime  endpoint  name  job  name  prefix  ep  timestamp  print  Endpoint  name  format  endpoint  name  endpoint  params  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  endpoint  response  sagemaker  create  endpoint  endpoint  params  print  EndpointArn  format  endpoint  response  EndpointArn  get  the  status  of  the  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  response  EndpointStatus  print  EndpointStatus  format  status  wait  until  the  status  has  changed  sagemaker  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  print  the  st,amazon
atus  of  the  endpoint  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  endpoint  response  EndpointStatus  print  Endpoint  creation  ended  with  EndpointStatus  format  status  if  status  InService  raise  Exception  Endpoint  creation  failed  import  boto3  runtime  boto3  Session  client  service  name  runtime  sagemaker  wget  tmp  test  jpg  http  www  vision  caltech  edu  Image  Datasets  Caltech256  images  008  bathtub  008  0007  jpg  file  name  tmp  test  jpg  test  image  from  IPython  display  import  Image  Image  file  name  import  json  import  numpy  as  np  with  open  file  name  rb  as  payload  read  payload  bytearray  payload  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  image  Body  payload  result  response  Body  read  result  will  be  in  json  format  and  convert  it  to  ndarray  result  json  loads  result  the  result  will  output  the  probabilities  for  all  classes  find  the,amazon
  class  with  maximum  probability  and  print  the  class  index  index  np  argmax  result  object  categories  ak47  american  flag  backpack  baseball  bat  baseball  glove  basketball  hoop  bat  bathtub  bear  beer  mug  billiards  binoculars  birdbath  blimp  bonsai  101  boom  box  bowling  ball  bowling  pin  boxing  glove  brain  101  breadmaker  buddha  101  bulldozer  butterfly  cactus  cake  calculator  camel  cannon  canoe  car  tire  cartman  cd  centipede  cereal  box  chandelier  101  chess  board  chimp  chopsticks  cockroach  coffee  mug  coffin  coin  comet  computer  keyboard  computer  monitor  computer  mouse  conch  cormorant  covered  wagon  cowboy  hat  crab  101  desk  globe  diamond  ring  dice  dog  dolphin  101  doorknob  drinking  straw  duck  dumb  bell  eiffel  tower  electric  guitar  101  elephant  101  elk  ewer  101  eyeglasses  fern  fighter  jet  fire  extinguisher  fire  hydrant  fire  truck  fireworks  flashlight  floppy  disk  football  helmet  french  horn  fried  e,amazon
gg  frisbee  frog  frying  pan  galaxy  gas  pump  giraffe  goat  golden  gate  bridge  goldfish  golf  ball  goose  gorilla  grand  piano  101  grapes  grasshopper  guitar  pick  hamburger  hammock  harmonica  harp  harpsichord  hawksbill  101  head  phones  helicopter  101  hibiscus  homer  simpson  horse  horseshoe  crab  hot  air  balloon  hot  dog  hot  tub  hourglass  house  fly  human  skeleton  hummingbird  ibis  101  ice  cream  cone  iguana  ipod  iris  jesus  christ  joy  stick  kangaroo  101  kayak  ketch  101  killer  whale  knife  ladder  laptop  101  lathe  leopards  101  license  plate  lightbulb  light  house  lightning  llama  101  mailbox  mandolin  mars  mattress  megaphone  menorah  101  microscope  microwave  minaret  minotaur  motorbikes  101  mountain  bike  mushroom  mussels  necktie  octopus  ostrich  owl  palm  pilot  palm  tree  paperclip  paper  shredder  pci  card  penguin  people  pez  dispenser  photocopier  picnic  table  playing  card  porcupine  pram  praying  mantis  pyrami,amazon
d  raccoon  radio  telescope  rainbow  refrigerator  revolver  101  rifle  rotary  phone  roulette  wheel  saddle  saturn  school  bus  scorpion  101  screwdriver  segway  self  propelled  lawn  mower  sextant  sheet  music  skateboard  skunk  skyscraper  smokestack  snail  snake  sneaker  snowmobile  soccer  ball  socks  soda  can  spaghetti  speed  boat  spider  spoon  stained  glass  starfish  101  steering  wheel  stirrups  sunflower  101  superman  sushi  swan  swiss  army  knife  sword  syringe  tambourine  teapot  teddy  bear  teepee  telephone  box  tennis  ball  tennis  court  tennis  racket  theodolite  toaster  tomato  tombstone  top  hat  touring  bike  tower  pisa  traffic  light  treadmill  triceratops  tricycle  trilobite  101  tripod  shirt  tuning  fork  tweezer  umbrella  101  unicorn  vcr  video  projector  washing  machine  watch  101  waterfall  watermelon  welding  mask  wheelbarrow  windmill  wine  bottle  xylophone  yarmulke  yo  yo  zebra  airplanes  101  car  side  101  faces  easy  ,amazon
101  greyhound  tennis  shoes  toad  clutter  print  Result  label  object  categories  index  probability  str  result  index  sage  delete  endpoint  EndpointName  endpoint  name  ,amazon
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  linear  mnist  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  matplotlib  inline  import  matplotlib  pyplot  as  plt  plt  rcParams  figure  figsize  10  def  show  digit  img  caption  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axis  off  subplot  imshow  imgr  cmap  gray  plt  title  caption  show  digit  train  set  30  This  is  format  train  set  30  import  io  import  numpy  as  np  import  sagemaker  amazon  common  as  smac  vectors  np  array  tolist  for  in  train  set  astype  float32  labels  np  where  np  array  tolist  for ,amazon
 in  train  set  astype  float32  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  vectors  labels  buf  seek  import  boto3  import  os  key  recordio  pb  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  buf  s3  train  data  s3  train  format  bucket  prefix  key  print  uploaded  training  data  location  format  s3  train  data  output  location  s3  output  format  bucket  prefix  print  training  artifacts  will  be  uploaded  to  format  output  location  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  linear  learner  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  linear  learner  latest  import  boto3  import  sagemaker  sess  sagemaker  Session  linear  sagemaker  estimator  Estimator  containers  boto3  Session  region ,amazon
 name  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  output  location  sagemaker  session  sess  linear  set  hyperparameters  feature  dim  784  predictor  type  binary  classifier  mini  batch  size  200  linear  fit  train  s3  train  data  linear  predictor  linear  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  sagemaker  predictor  import  csv  serializer  json  deserializer  linear  predictor  content  type  text  csv  linear  predictor  serializer  csv  serializer  linear  predictor  deserializer  json  deserializerresult  linear  predictor  predict  train  set  30  31  print  result  import  numpy  as  np  predictions  for  array  in  np  array  split  test  set  100  result  linear  predictor  predict  array  predictions  predicted  label  for  in  result  predictions  predictions  np  array  predictions  import  pandas  as  pd  pd  crosstab  np  where  test  set  predictions  rownames  actuals  colnames  predictions  import  sagemaker  s,amazon
agemaker  Session  delete  endpoint  linear  predictor  endpoint  ,amazon
from  sagemaker  import  get  execution  role  role  get  execution  role  pygmentize  super  resolution  py  aws  s3  cp  s3  onnx  mxnet  examples  super  resolution  onnx  import  tarfile  from  sagemaker  session  import  Session  with  tarfile  open  onnx  model  tar  gz  mode  gz  as  archive  archive  add  super  resolution  onnx  model  data  Session  upload  data  path  onnx  model  tar  gz  key  prefix  model  from  sagemaker  mxnet  import  MXNetModel  mxnet  model  MXNetModel  model  data  model  data  entry  point  super  resolution  py  role  role  framework  version  time  predictor  mxnet  model  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  IPython  display  import  Image  as  Img  from  mxnet  test  utils  import  download  img  name  super  res  input  jpg  img  url  https  s3  amazonaws  com  onnx  mxnet  examples  format  img  name  download  img  url  img  name  Img  filename  img  name  import  numpy  as  np  from  PIL  import  Image  input  image  dim  224  im,amazon
g  Image  open  img  name  resize  input  image  dim  input  image  dim  img  ycbcr  img  convert  YCbCr  img  img  cb  img  cr  img  ycbcr  split  input  image  np  array  img  np  newaxis  np  newaxis  out  predictor  predict  input  image  img  out  Image  fromarray  np  uint8  np  asarray  out  mode  result  img  Image  merge  YCbCr  img  out  img  cb  resize  img  out  size  Image  BICUBIC  img  cr  resize  img  out  size  Image  BICUBIC  convert  RGB  output  img  dim  672  assert  result  img  size  output  img  dim  output  img  dim  result  img  file  output  jpg  result  img  save  result  img  file  Img  filename  result  img  file  naive  output  Image  open  img  name  resize  output  img  dim  output  img  dim  naive  output  file  naive  output  jpg  naive  output  save  naive  output  file  Img  naive  output  file  predictor  delete  endpoint  ,amazon
matplotlib  inline  import  numpy  as  np  import  pandas  as  pd  import  matplotlib  pyplot  as  pltcolumns  count  season  holiday  workingday  weather  temp  atemp  humidity  windspeed  year  month  day  dayofweek  df  pd  read  csv  train  csv  parse  dates  datetime  df  test  pd  read  csv  test  csv  parse  dates  datetime  df  head  df  dtypesdf  test  head  We  need  to  convert  datetime  to  numeric  for  training  Let  extract  key  features  into  separate  numeric  columns  def  add  features  df  df  year  df  datetime  dt  year  df  month  df  datetime  dt  month  df  day  df  datetime  dt  day  df  dayofweek  df  datetime  dt  dayofweek  df  hour  df  datetime  dt  houradd  features  df  add  features  df  test  df  head  df  test  head  plt  boxplot  df  count  labels  count  plt  title  Box  Plot  Count  plt  ylabel  Target  plt  grid  True  Let  see  how  the  data  distribution  changes  with  log1p  Evenly  distributed  plt  boxplot  df  count  map  np  log1p  labels  log1p  count  plt ,amazon
 title  Box  Plot  log1p  Count  plt  ylabel  Target  plt  grid  True  df  count  df  count  map  np  log1p  df  head  df  test  head  df  corr  count  group  month  df  groupby  month  average  by  month  group  month  count  mean  plt  plot  average  by  month  index  average  by  month  plt  xlabel  Month  plt  ylabel  Count  plt  xticks  np  arange  12  plt  grid  True  plt  title  Rental  Count  by  Month  group  year  month  df  groupby  year  month  average  year  month  group  year  month  count  mean  for  year  in  average  year  month  index  levels  print  year  print  average  year  month  year  plt  plot  average  year  month  year  index  average  year  month  year  label  year  plt  legend  plt  xlabel  month  plt  ylabel  Count  plt  grid  True  plt  title  Rental  Count  by  Year  Month  plt  scatter  df  temp  df  count  label  Temperature  plt  grid  True  plt  xlabel  Temperature  plt  ylabel  Count  plt  scatter  df  humidity  df  count  label  Humidity  plt  grid  True  plt  xlabel  Hum,amazon
idity  plt  ylabel  Count  group  hour  df  groupby  hour  average  by  hour  group  hour  count  mean  plt  plot  average  by  hour  index  average  by  hour  plt  xlabel  hour  plt  ylabel  Count  plt  xticks  np  arange  24  plt  grid  True  plt  title  Rental  Count  Average  by  hour  group  year  hour  df  groupby  year  hour  average  year  hour  group  year  hour  count  mean  for  year  in  average  year  hour  index  levels  print  year  print  average  year  month  year  plt  plot  average  year  hour  year  index  average  year  hour  year  label  year  plt  legend  plt  xlabel  hour  plt  ylabel  Count  plt  xticks  np  arange  24  plt  grid  True  plt  title  Rental  Count  Average  by  Year  Hour  group  workingday  hour  df  groupby  workingday  hour  average  workingday  hour  group  workingday  hour  count  mean  for  workingday  in  average  workingday  hour  index  levels  print  year  print  average  year  month  year  plt  plot  average  workingday  hour  workingday  index  average  work,amazon
ingday  hour  workingday  label  workingday  plt  legend  plt  xlabel  hour  plt  ylabel  Count  plt  xticks  np  arange  24  plt  grid  True  plt  title  Rental  Count  Average  by  Working  Day  Hour  df  dtypes  Save  all  data  df  to  csv  bike  all  csv  index  False  columns  columns  Training  70  of  the  data  Validation  30  of  the  data  Randomize  the  datset  np  random  seed  list  df  index  np  random  shuffle  df  df  iloc  rows  df  shape  train  int  rows  test  int  rows  rows  train  testcolumns  Write  Training  Set  df  train  to  csv  bike  train  csv  index  False  header  False  columns  columns  Write  Validation  Set  df  train  to  csv  bike  validation  csv  index  False  header  False  columns  columns  Test  Data  has  only  input  features  df  test  to  csv  bike  test  csv  index  False  join  columns  Write  Column  List  with  open  bike  train  column  list  txt  as  write  join  columns  df  test  columnsdf  test  season  unique  df  test  holiday  unique  df  test  wo,amazon
rkingday  unique  df  test  weather  unique  df  test  temp  unique  df  test  atemp  unique  df  test  humidity  unique  df  test  windspeed  unique  df  test  year  unique  df  test  month  unique  df  test  day  unique  df  test  shape  ,amazon
matplotlib  inline  import  tensorflow  as  tf  import  os  import  matplotlib  pyplot  as  plt  import  matplotlib  patheffects  as  PathEffects  import  matplotlib  import  numpy  as  np  from  sklearn  decomposition  import  PCA  from  sklearn  manifold  import  TSNE  from  tensorflow  examples  tutorials  mnist  import  input  data  mnist  input  data  read  data  sets  MNIST  data  one  hot  True  def  weight  variable  shape  name  initial  tf  truncated  normal  shape  stddev  return  tf  Variable  initial  name  def  bias  variable  shape  name  initial  tf  constant  shape  shape  return  tf  Variable  initial  name  def  conv2d  return  tf  nn  conv2d  strides  padding  SAME  def  deconv2d  output  shape  return  tf  nn  conv2d  transpose  output  shape  strides  padding  SAME  tf  reset  default  graph  tf  placeholder  tf  float32  shape  None  784  origin  tf  reshape  28  28  conv1  weight  variable  16  conv1  conv1  bias  variable  16  conv1  conv1  tf  nn  relu  tf  add  conv2d  origin  conv1,microsoft
  conv1  conv2  weight  variable  16  32  conv2  conv2  bias  variable  32  conv2  conv2  tf  nn  relu  tf  add  conv2d  conv1  conv2  conv2  code  layer  conv2  print  code  layer  shape  conv2  get  shape  conv1  weight  variable  16  32  conv1  conv1  bias  variable  conv1  output  shape  conv1  tf  stack  tf  shape  14  14  16  conv1  tf  nn  relu  deconv2d  conv2  conv1  output  shape  conv1  conv2  weight  variable  16  conv2  conv2  bias  variable  16  conv2  output  shape  conv2  tf  stack  tf  shape  28  28  conv2  tf  nn  relu  deconv2d  conv1  conv2  output  shape  conv2  reconstruct  conv2  print  reconstruct  layer  shape  reconstruct  get  shape  cost  tf  reduce  mean  tf  pow  reconstruct  origin  optimizer  tf  train  AdamOptimizer  01  minimize  cost  sess  tf  InteractiveSession  batch  size  60  init  op  tf  global  variables  initializer  sess  run  init  op  for  epoch  in  range  5000  batch  mnist  train  next  batch  batch  size  if  epoch  1500  if  epoch  100  print  step  loss  ep,microsoft
och  cost  eval  feed  dict  batch  else  if  epoch  1000  print  step  loss  epoch  cost  eval  feed  dict  batch  optimizer  run  feed  dict  batch  def  plot  reconstruct  origin  img  reconstruct  img  10  plt  figure  figsize  20  for  in  range  ax  plt  subplot  plt  imshow  origin  img  reshape  28  28  plt  gray  ax  get  xaxis  set  visible  False  ax  get  yaxis  set  visible  False  ax  plt  subplot  plt  imshow  reconstruct  img  reshape  28  28  plt  gray  ax  get  xaxis  set  visible  False  ax  get  yaxis  set  visible  False  plt  show  test  size  10  test  origin  img  mnist  test  images  test  size  test  reconstruct  img  np  reshape  reconstruct  eval  feed  dict  test  origin  img  28  28  plot  reconstruct  test  origin  img  test  reconstruct  img  plot  code  layer  result  import  math  def  plot  conv  layer  layer  image  num  filters  output  sess  run  layer  feed  dict  image  num  grids  int  math  ceil  math  sqrt  num  filters  fig  axes  plt  subplots  num  grids  num  gri,microsoft
ds  for  ax  in  enumerate  axes  flat  if  num  grids  num  grids  img  output  ax  imshow  img  interpolation  nearest  cmap  gray  ax  set  xticks  ax  set  yticks  plt  show  image1  mnist  test  images  plot  conv  layer  code  layer  image1  16  Max  unpooling  def  conv2d  return  tf  nn  conv2d  strides  padding  SAME  def  deconv2d  output  shape  return  tf  nn  conv2d  transpose  output  shape  strides  padding  SAME  def  max  unpool  2x2  output  shape  out  tf  concat  tf  zeros  like  out  tf  concat  out  tf  zeros  like  out  out  size  output  shape  return  tf  reshape  out  out  size  def  max  pool  2x2  argmax  tf  nn  max  pool  with  argmax  ksize  strides  padding  SAME  pool  tf  nn  max  pool  ksize  strides  padding  SAME  return  pool  argmax  tf  reset  default  graph  tf  placeholder  tf  float32  shape  None  784  origin  tf  reshape  28  28  conv1  weight  variable  16  conv1  conv1  bias  variable  16  conv1  conv1  tf  nn  relu  tf  add  conv2d  origin  conv1  conv1  pool1  ,microsoft
argmax  pool1  max  pool  2x2  conv1  conv2  weight  variable  16  32  conv2  conv2  bias  variable  32  conv2  conv2  tf  nn  relu  tf  add  conv2d  pool1  conv2  conv2  pool2  argmax  pool2  max  pool  2x2  conv2  code  layer  pool2  print  code  layer  shape  code  layer  get  shape  conv1  weight  variable  16  32  conv1  conv1  bias  variable  conv1  convolutional  layer  shape  output  shape  conv1  tf  stack  tf  shape  16  conv1  tf  nn  sigmoid  deconv2d  code  layer  conv1  output  shape  conv1  max  unpool  layer  shape  output  shape  pool1  tf  stack  tf  shape  14  14  16  pool1  max  unpool  2x2  conv1  output  shape  pool1  conv2  weight  variable  16  conv2  conv2  bias  variable  16  conv2  output  shape  conv2  tf  stack  tf  shape  14  14  conv2  tf  nn  sigmoid  deconv2d  pool1  conv2  output  shape  conv2  output  shape  pool2  tf  stack  tf  shape  28  28  pool2  max  unpool  2x2  conv2  output  shape  pool2  reconstruct  pool2  print  reconstruct  layer  shape  reconstruct  get  shape ,microsoft
 cost  tf  reduce  mean  tf  pow  reconstruct  origin  optimizer  tf  train  AdamOptimizer  01  minimize  cost  sess  tf  InteractiveSession  batch  size  60  init  op  tf  global  variables  initializer  sess  run  init  op  for  epoch  in  range  5000  batch  mnist  train  next  batch  batch  size  if  epoch  1500  if  epoch  100  print  step  loss  epoch  cost  eval  feed  dict  batch  else  if  epoch  1000  print  step  loss  epoch  cost  eval  feed  dict  batch  optimizer  run  feed  dict  batch  print  final  loss  cost  eval  feed  dict  mnist  test  images  test  size  10  test  origin  img  mnist  test  images  test  size  test  reconstruct  img  np  reshape  reconstruct  eval  feed  dict  test  origin  img  28  28  plot  reconstruct  test  origin  img  test  reconstruct  img  code  layer  image1  mnist  test  images  plot  conv  layer  code  layer  image1  16  deconvolution  plot  conv  layer  conv1  image1  16  unpooling  plot  conv  layer  pool1  image1  16  Max  unpooling  nearest  neighbor  def ,microsoft
 conv2d  return  tf  nn  conv2d  strides  padding  SAME  def  deconv2d  output  shape  return  tf  nn  conv2d  transpose  output  shape  strides  padding  SAME  def  max  pool  2x2  argmax  tf  nn  max  pool  with  argmax  ksize  strides  padding  SAME  pool  tf  nn  max  pool  ksize  strides  padding  SAME  return  pool  argmax  def  max  unpool  2x2  shape  inference  tf  image  resize  nearest  neighbor  tf  stack  shape  shape  return  inference  tf  reset  default  graph  tf  placeholder  tf  float32  shape  None  784  origin  tf  reshape  28  28  conv1  weight  variable  16  conv1  conv1  bias  variable  16  conv1  conv1  tf  nn  relu  tf  add  conv2d  origin  conv1  conv1  pool1  argmax  pool1  max  pool  2x2  conv1  conv2  weight  variable  16  32  conv2  conv2  bias  variable  32  conv2  conv2  tf  nn  relu  tf  add  conv2d  pool1  conv2  conv2  pool2  argmax  pool2  max  pool  2x2  conv2  code  layer  pool2  print  code  layer  shape  code  layer  get  shape  conv1  weight  variable  16  32  conv1  ,microsoft
conv1  bias  variable  conv1  convolutional  layer  shape  output  shape  conv1  tf  stack  tf  shape  16  conv1  tf  nn  sigmoid  deconv2d  code  layer  conv1  output  shape  conv1  max  unpool  layer  shape  pool1  max  unpool  2x2  conv1  16  conv2  weight  variable  16  conv2  conv2  bias  variable  16  conv2  convolutional  layer  shape  output  shape  conv2  tf  stack  tf  shape  14  14  conv2  tf  nn  sigmoid  deconv2d  pool1  conv2  output  shape  conv2  max  unpool  layer  shape  pool2  max  unpool  2x2  conv2  14  14  reconstruct  pool2  print  reconstruct  layer  shape  reconstruct  get  shape  cost  tf  reduce  mean  tf  pow  reconstruct  origin  optimizer  tf  train  AdamOptimizer  01  minimize  cost  sess  tf  InteractiveSession  batch  size  60  init  op  tf  global  variables  initializer  sess  run  init  op  for  epoch  in  range  5000  batch  mnist  train  next  batch  batch  size  if  epoch  1500  if  epoch  100  print  step  loss  epoch  cost  eval  feed  dict  batch  else  if  epoch  100,microsoft
0  print  step  loss  epoch  cost  eval  feed  dict  batch  optimizer  run  feed  dict  batch  print  final  loss  cost  eval  feed  dict  mnist  test  images  test  size  10  test  origin  img  mnist  test  images  test  size  test  reconstruct  img  np  reshape  reconstruct  eval  feed  dict  test  origin  img  28  28  plot  reconstruct  test  origin  img  test  reconstruct  img  code  layer  image1  mnist  test  images  plot  conv  layer  code  layer  image1  16  unconvolutional  layer  image1  mnist  test  images  plot  conv  layer  conv1  image1  16  unpooling  layer  image1  mnist  test  images  plot  conv  layer  pool1  image1  16  ,microsoft
cat  container  Dockerfile  sh  your  username  YOUR  USERNAME  HERE  The  name  of  our  algorithm  algorithm  name  scikit  decision  tree  your  username  cd  container  chmod  decision  trees  train  chmod  decision  trees  serve  account  aws  sts  get  caller  identity  query  Account  output  text  Get  the  region  defined  in  the  current  configuration  default  to  us  west  if  none  defined  region  aws  configure  get  region  region  region  us  west  fullname  account  dkr  ecr  region  amazonaws  com  algorithm  name  latest  If  the  repository  doesn  exist  in  ECR  create  it  aws  ecr  describe  repositories  repository  names  algorithm  name  dev  null  if  ne  then  aws  ecr  create  repository  repository  name  algorithm  name  dev  null  fi  Get  the  login  command  from  ECR  and  execute  it  directly  aws  ecr  get  login  region  region  no  include  email  Build  the  docker  image  locally  with  the  image  name  and  then  push  it  to  ECR  with  the  full  name  docker ,amazon
 build  algorithm  name  docker  tag  algorithm  name  fullname  docker  push  fullname  S3  prefix  your  username  YOUR  USERNAME  HERE  prefix  scikit  byo  iris  format  your  username  Define  IAM  role  import  boto3  import  re  import  os  import  numpy  as  np  import  pandas  as  pd  from  sagemaker  import  get  execution  role  role  get  execution  role  import  sagemaker  as  sage  from  time  import  gmtime  strftime  sess  sage  Session  WORK  DIRECTORY  data  data  location  sess  upload  data  WORK  DIRECTORY  key  prefix  prefix  algorithm  name  scikit  decision  tree  format  your  username  account  sess  boto  session  client  sts  get  caller  identity  Account  region  sess  boto  session  region  name  image  dkr  ecr  amazonaws  com  latest  format  account  region  algorithm  name  tree  sage  estimator  Estimator  image  role  ml  c4  2xlarge  output  path  s3  output  format  sess  default  bucket  sagemaker  session  sess  tree  fit  data  location  from  sagemaker  predictor  i,amazon
mport  csv  serializer  predictor  tree  deploy  ml  m4  xlarge  serializer  csv  serializer  shape  pd  read  csv  data  iris  csv  header  None  import  itertools  50  for  in  range  40  for  in  range  10  indices  for  in  itertools  product  test  data  shape  iloc  indices  test  test  data  iloc  test  test  data  iloc  print  predictor  predict  test  values  decode  utf  sess  delete  endpoint  predictor  endpoint  ,amazon
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  linear  time  series  forecast  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  import  io  import  os  import  time  import  json  import  sagemaker  amazon  common  as  smac  import  sagemaker  from  sagemaker  predictor  import  csv  serializer  json  deserializer  wget  http  robjhyndman  com  data  gasoline  csvgas  pd  read  csv  gasoline  csv  header  None  names  thousands  barrels  display  gas  head  plt  plot  gas  plt  show  gas  thousands  barrels  lag1  gas  thousands  barrels  shift  gas  thousands  barrels  lag2  gas  thousands  barrels  shift  gas  thousands  barrels  lag3  gas  thousands  barrels  shift  gas  thousands  barrels  lag4  gas  thousands  barrels  shift  gas  trend  np  arange  len  gas  gas  log  trend  np  log1p  np  arange  len  gas  gas  sq  trend  np  arange ,amazon
 len  gas  weeks  pd  get  dummies  np  array  list  range  52  15  len  gas  prefix  week  gas  pd  concat  gas  weeks  axis  gas  gas  iloc  split  train  int  len  gas  split  test  int  len  gas  train  gas  thousands  barrels  split  train  train  gas  drop  thousands  barrels  axis  iloc  split  train  as  matrix  validation  gas  thousands  barrels  split  train  split  test  validation  gas  drop  thousands  barrels  axis  iloc  split  train  split  test  as  matrix  test  gas  thousands  barrels  split  test  test  gas  drop  thousands  barrels  axis  iloc  split  test  as  matrix  buf  io  BytesIO  smac  write  numpy  to  dense  tensor  buf  np  array  train  astype  float32  np  array  train  astype  float32  buf  seek  key  linear  train  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  buf  s3  train  data  s3  train  format  bucket  prefix  key  print  uploaded  training  data  location  format  s3  train  data  buf  io  BytesIO  smac  write,amazon
  numpy  to  dense  tensor  buf  np  array  validation  astype  float32  np  array  validation  astype  float32  buf  seek  key  linear  validation  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  key  upload  fileobj  buf  s3  validation  data  s3  validation  format  bucket  prefix  key  print  uploaded  validation  data  location  format  s3  validation  data  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  linear  learner  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  linear  learner  latest  sess  sagemaker  Session  linear  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  role  train  instance  count  train  instance  type  ml  c4  xlarge  output  path  s3  output  format  bucket  prefix  sagemaker  session  sess  linear  set  ,amazon
hyperparameters  feature  dim  59  mini  batch  size  100  predictor  type  regressor  epochs  10  num  models  32  loss  absolute  loss  linear  fit  train  s3  train  data  validation  s3  validation  data  linear  predictor  linear  deploy  initial  instance  count  instance  type  ml  m4  xlarge  gas  thousands  barrels  lag52  gas  thousands  barrels  shift  52  gas  thousands  barrels  lag104  gas  thousands  barrels  shift  104  gas  thousands  barrels  naive  forecast  gas  thousands  barrels  lag52  gas  thousands  barrels  lag104  naive  gas  split  test  thousands  barrels  naive  forecast  as  matrix  print  Naive  MdAPE  np  median  np  abs  test  naive  test  plt  plot  np  array  test  label  actual  plt  plot  naive  label  naive  plt  legend  plt  show  linear  predictor  content  type  text  csv  linear  predictor  serializer  csv  serializer  linear  predictor  deserializer  json  deserializerresult  linear  predictor  predict  test  one  step  np  array  score  for  in  result  predictions,amazon
  print  One  step  ahead  MdAPE  np  median  np  abs  test  one  step  test  plt  plot  np  array  test  label  actual  plt  plot  one  step  label  forecast  plt  legend  plt  show  multi  step  lags  test  for  row  in  test  row  lags  result  linear  predictor  predict  row  prediction  result  predictions  score  multi  step  append  prediction  lags  lags  lags  prediction  multi  step  np  array  multi  step  print  Multi  step  ahead  MdAPE  np  median  np  abs  test  multi  step  test  plt  plot  np  array  test  label  actual  plt  plot  one  step  label  forecast  plt  legend  plt  show  sagemaker  Session  delete  endpoint  linear  predictor  endpoint  ,amazon
matplotlib  inline  reload  ext  autoreload  autoreload  2from  fastai  structured  import  from  fastai  column  data  import  np  set  printoptions  threshold  50  edgeitems  20  PATH  data  stock  def  concat  csvs  dirname  path  PATH  dirname  filenames  glob  PATH  csv  wrote  header  False  with  open  path  csv  as  outputfile  for  filename  in  filenames  name  filename  split  with  open  filename  as  line  readline  if  not  wrote  header  wrote  header  True  outputfile  write  file  line  for  line  in  outputfile  write  name  line  outputfile  write  table  names  train  test  str  PATH  fname  csv  tables  pd  read  csv  PATH  fname  csv  low  memory  False  for  fname  in  table  names  from  IPython  display  import  HTMLfor  in  tables  display  head  for  in  tables  display  DataFrameSummary  summary  train  tables  len  train  def  join  df  left  right  left  on  right  on  None  suffix  if  right  on  is  None  right  on  left  on  return  left  merge  right  how  left  left  on  lef,amazon
t  on  right  on  right  on  suffixes  suffix  train  drop  second  inplace  True  train  drop  minute  inplace  True  train  drop  action  inplace  True  train  drop  timePeriodStart  inplace  True  df  train  df  head  df  columnsjoined  df  join  df  joined  df  timePeriodStart  joined  to  feather  PATH  joined  df  df  set  index  timePeriodStart  df  head  df  reset  index  inplace  True  joined  test  reset  index  inplace  True  df  head  df  to  feather  PATH  df  df  pd  read  feather  PATH  df  convert  to  date  objects  df  timePeriodStart  pd  to  datetime  df  timePeriodStart  df  columnsjoined  joined  joined  priceClose  joined  reset  index  inplace  True  joined  to  feather  PATH  joined  joined  pd  read  feather  PATH  joined  joined  head  head  40  cat  vars  hour  dayOfWeek  month  year  contin  vars  priceOpen  tradesCount  volumeTraded  priceHigh  priceLow  len  joined  ndep  priceClose  joined  joined  cat  vars  contin  vars  dep  timePeriodStart  copy  joined  test  joined  cat  ,amazon
vars  contin  vars  dep  timePeriodStart  copy  for  in  cat  vars  joined  joined  astype  category  cat  as  ordered  apply  cats  joined  test  joined  for  in  contin  vars  joined  joined  fillna  astype  float32  joined  test  joined  test  fillna  astype  float32  samp  size  joined  samp  joined  set  index  timePeriodStart  joined  samp  head  df  nas  mapper  proc  df  joined  samp  priceClose  do  scale  True  yl  np  log  joined  test  joined  test  set  index  timePeriodStart  df  test  nas  mapper  proc  df  joined  test  priceClose  do  scale  True  skip  flds  Id  mapper  mapper  na  dict  nas  df  head  train  ratio  75  train  ratio  train  size  int  samp  size  train  ratio  train  size  val  idx  list  range  train  size  len  df  TODO  SHOULD  NOT  HAVE  TO  DO  THIS  CONVERT  INDEX  TO  DATETIME  df  index  pd  to  datetime  df  index  2018  04  03T23  53  00  726Z  val  idx  np  flatnonzero  df  index  datetime  datetime  2018  df  index  datetime  datetime  2018  val  idx  df  head  h,amazon
ead  40  df  index  def  inv  return  np  exp  def  exp  rmspe  pred  targ  targ  inv  targ  pct  var  targ  inv  pred  targ  return  math  sqrt  pct  var  mean  max  log  np  max  yl  range  max  log  dfmd  ColumnarModelData  from  data  frame  PATH  val  idx  df  yl  astype  np  float32  cat  flds  cat  vars  bs  128  test  df  df  test  todo  add  test  setcat  sz  len  joined  samp  cat  categories  for  in  cat  vars  cat  szemb  szs  min  50  for  in  cat  sz  emb  szsm  md  get  learner  emb  szs  len  df  columns  len  cat  vars  04  1000  500  001  01  range  range  lr  1e  3m  lr  find  sched  plot  100  ,amazon
Lab  10  MNIST  and  NN  import  tensorflow  as  tf  import  random  import  matplotlib  pyplot  as  plt  from  tensorflow  examples  tutorials  mnist  import  input  data  tf  set  random  seed  777  reproducibility  parameters  learning  rate  001  training  epochs  15  batch  size  100  input  place  holders  tf  placeholder  tf  float32  None  784  tf  placeholder  tf  float32  None  10  weights  bias  for  nn  layers  W1  tf  Variable  tf  random  normal  784  256  b1  tf  Variable  tf  random  normal  256  L1  tf  nn  relu  tf  matmul  W1  b1  W2  tf  Variable  tf  random  normal  256  256  b2  tf  Variable  tf  random  normal  256  L2  tf  nn  relu  tf  matmul  L1  W2  b2  W3  tf  Variable  tf  random  normal  256  10  b3  tf  Variable  tf  random  normal  10  hypothesis  tf  matmul  L2  W3  b3  define  cost  loss  optimizer  cost  tf  reduce  mean  tf  nn  softmax  cross  entropy  with  logits  logits  hypothesis  labels  optimizer  tf  train  AdamOptimizer  learning  rate  learning  rate  minimize  c,ibm
ost  initialize  sess  tf  Session  sess  run  tf  global  variables  initializer  mnist  input  data  read  data  sets  MNIST  data  one  hot  True  Check  out  https  www  tensorflow  org  get  started  mnist  beginners  for  more  information  about  the  mnist  dataset  train  my  model  for  epoch  in  range  training  epochs  avg  cost  total  batch  int  mnist  train  num  examples  batch  size  for  in  range  total  batch  batch  xs  batch  ys  mnist  train  next  batch  batch  size  feed  dict  batch  xs  batch  ys  sess  run  cost  optimizer  feed  dict  feed  dict  avg  cost  total  batch  print  Epoch  04d  epoch  cost  9f  format  avg  cost  print  Learning  Finished  Test  model  and  check  accuracy  correct  prediction  tf  equal  tf  argmax  hypothesis  tf  argmax  accuracy  tf  reduce  mean  tf  cast  correct  prediction  tf  float32  print  Accuracy  sess  run  accuracy  feed  dict  mnist  test  images  mnist  test  labels  Get  one  and  predict  random  randint  mnist  test  num  example,ibm
s  print  Label  sess  run  tf  argmax  mnist  test  labels  print  Prediction  sess  run  tf  argmax  hypothesis  feed  dict  mnist  test  images  plt  imshow  mnist  test  images  reshape  28  28  cmap  Greys  interpolation  nearest  plt  show  Epoch  0001  cost  141  207671860  Epoch  0002  cost  38  788445864  Epoch  0003  cost  23  977515479  Epoch  0004  cost  16  315132428  Epoch  0005  cost  11  702554882  Epoch  0006  cost  573139748  Epoch  0007  cost  370995680  Epoch  0008  cost  537178684  Epoch  0009  cost  216900532  Epoch  0010  cost  329708954  Epoch  0011  cost  715552875  Epoch  0012  cost  189857912  Epoch  0013  cost  820965160  Epoch  0014  cost  624131458  Epoch  0015  cost  454633765  Learning  Finished  Accuracy  9455  ,ibm
APIKEY  Replace  with  your  API  key  pip  install  upgrade  google  api  python  client  running  Translate  API  from  googleapiclient  discovery  import  build  service  build  translate  v2  developerKey  APIKEY  use  the  service  inputs  is  it  really  this  easy  amazing  technology  wow  outputs  service  translations  list  source  en  target  fr  inputs  execute  print  outputs  for  input  output  in  zip  inputs  outputs  translations  print  format  input  output  translatedText  Running  Vision  API  import  base64  IMAGE  gs  cloud  training  demos  vision  sign2  jpg  vservice  build  vision  v1  developerKey  APIKEY  request  vservice  images  annotate  body  requests  image  source  gcs  image  uri  IMAGE  features  type  TEXT  DETECTION  maxResults  responses  request  execute  num  retries  print  responses  foreigntext  responses  responses  textAnnotations  description  foreignlang  responses  responses  textAnnotations  locale  print  foreignlang  foreigntext  inputs  foreigntext  out,google
puts  service  translations  list  source  foreignlang  target  en  inputs  execute  print  outputs  for  input  output  in  zip  inputs  outputs  translations  print  format  input  output  translatedText  lservice  build  language  v1beta1  developerKey  APIKEY  quotes  To  succeed  you  must  have  tremendous  perseverance  tremendous  will  It  not  that  so  smart  it  just  that  stay  with  problems  longer  Love  is  quivering  happiness  Love  is  of  all  passions  the  strongest  for  it  attacks  simultaneously  the  head  the  heart  and  the  senses  What  difference  does  it  make  to  the  dead  the  orphans  and  the  homeless  whether  the  mad  destruction  is  wrought  under  the  name  of  totalitarianism  or  in  the  holy  name  of  liberty  or  democracy  When  someone  you  love  dies  and  you  re  not  expecting  it  you  don  lose  her  all  at  once  you  lose  her  in  pieces  over  long  time  the  way  the  mail  stops  coming  and  her  scent  fades  from  the  pillows  and  ,google
even  from  the  clothes  in  her  closet  and  drawers  for  quote  in  quotes  response  lservice  documents  analyzeSentiment  body  document  type  PLAIN  TEXT  content  quote  execute  polarity  response  documentSentiment  polarity  magnitude  response  documentSentiment  magnitude  print  POLARITY  MAGNITUDE  for  polarity  magnitude  quote  sservice  build  speech  v1beta1  developerKey  APIKEY  response  sservice  speech  syncrecognize  body  config  encoding  LINEAR16  sampleRate  16000  audio  uri  gs  cloud  training  demos  vision  audio  raw  execute  print  response  print  response  results  alternatives  transcript  print  Confidence  response  results  alternatives  confidence  ,google
import  sagemaker  from  sagemaker  import  get  execution  role  import  boto3  import  json  sess  sagemaker  Session  role  get  execution  role  print  role  This  is  the  role  that  SageMaker  would  use  to  leverage  AWS  resources  S3  CloudWatch  on  your  behalf  bucket  sess  default  bucket  Replace  with  your  own  bucket  name  if  needed  print  bucket  prefix  SagemakerBlazingText  Replace  with  the  prefix  under  which  you  want  to  store  the  data  if  neededs3  train  data  s3  format  sagemaker  test  ninja  BlazingTextInput  s3  output  location  s3  BlazingText  Model  Output  word2vec  pitchfork  2018  09  19  format  sagemaker  test  ninja  region  name  boto3  Session  region  namecontainer  sagemaker  amazon  amazon  estimator  get  image  uri  region  name  blazingtext  latest  print  Using  SageMaker  BlazingText  container  format  container  region  name  bt  model  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  c4  2x,amazon
large  train  volume  size  train  max  run  360000  input  mode  File  output  path  s3  output  location  sagemaker  session  sess  bt  model  set  hyperparameters  mode  batch  skipgram  epochs  min  count  sampling  threshold  0001  learning  rate  05  window  size  vector  dim  100  negative  samples  batch  size  11  window  size  Preferred  Used  only  if  mode  is  batch  skipgram  evaluation  True  Perform  similarity  evaluation  on  WS  353  dataset  at  the  end  of  training  subwords  False  Subword  embedding  learning  is  not  supported  by  batch  skipgramtrain  data  sagemaker  session  s3  input  s3  train  data  distribution  FullyReplicated  content  type  text  plain  s3  data  type  S3Prefix  data  channels  train  train  data  bt  model  fit  inputs  data  channels  logs  True  bt  endpoint  bt  model  deploy  initial  instance  count  instance  type  ml  m4  xlarge  words  addictive  recorded  payload  instances  words  response  bt  endpoint  predict  json  dumps  payload  vecs  jso,amazon
n  loads  response  print  vecs  s3  boto3  resource  s3  key  bt  model  model  data  bt  model  model  data  find  s3  Bucket  sagemaker  test  ninja  download  file  key  model  tar  gz  tar  xvzf  model  tar  gz  cat  eval  jsonimport  numpy  as  np  from  sklearn  preprocessing  import  normalize  Read  the  400  most  frequent  word  vectors  The  vectors  in  the  file  are  in  descending  order  of  frequency  num  points  400  first  line  True  index  to  word  with  open  vectors  txt  as  for  line  num  line  in  enumerate  if  first  line  dim  int  line  strip  split  word  vecs  np  zeros  num  points  dim  dtype  float  first  line  False  continue  line  line  strip  word  line  split  vec  word  vecs  line  num  for  index  vec  val  in  enumerate  line  split  vec  index  float  vec  val  index  to  word  append  word  if  line  num  num  points  break  word  vecs  normalize  word  vecs  copy  False  return  norm  False  from  sklearn  manifold  import  TSNE  tsne  TSNE  perplexity  40  c,amazon
omponents  init  pca  iter  10000  two  embeddings  tsne  fit  transform  word  vecs  num  points  labels  index  to  word  num  points  from  matplotlib  import  pylab  matplotlib  inline  def  plot  embeddings  labels  pylab  figure  figsize  20  20  for  label  in  enumerate  labels  embeddings  pylab  scatter  pylab  annotate  label  xy  xytext  textcoords  offset  points  ha  right  va  bottom  pylab  show  plot  two  embeddings  labels  sess  delete  endpoint  bt  endpoint  endpoint  ,amazon
Basic  set  up  import  boto3  import  re  import  os  import  numpy  as  np  import  pandas  as  pd  from  sagemaker  import  get  execution  role  role  get  execution  role  import  sagemaker  as  sage  from  time  import  gmtime  strftime  sess  sage  Session  account  sess  boto  session  client  sts  get  caller  identity  Account  region  sess  boto  session  region  name  Note  make  sure  to  include  the  Docker  image  tag  eg  latest  since  there  seem  to  be  some  issues  with  deploying  model  if  you  don  include  the  tag  image  dkr  ecr  amazonaws  com  h2o  automl  latest  format  account  region  automl  sage  estimator  Estimator  image  role  ml  c4  2xlarge  output  path  s3  output  format  sess  default  bucket  sagemaker  session  sess  The  current  Docker  image  needs  both  training  and  testing  data  so  they  re  specified  in  two  different  channels  It  assumed  that  the  user  has  already  uploaded  the  required  data  into  couple  of  different  directories  an,amazon
d  this  dictionary  just  specifies  where  the  training  and  testing  data  are  respectively  data  location  training  s3  sagemaker  test  bucket  2018  h2o  automl  test  train  training  testing  s3  sagemaker  test  bucket  2018  h2o  automl  test  train  testing  Run  AutoML  This  can  take  little  while  and  you  will  likely  want  to  make  sure  that  the  cell  runs  entirely  before  you  try  deploying  predictor  especially  because  you  will  have  partial  output  even  during  run  automl  fit  data  location  Deplying  an  actual  predictor  so  that  we  can  make  predictions  on  test  data  here  from  sagemaker  predictor  import  csv  serializer  predictor  automl  deploy  ml  m4  xlarge  serializer  csv  serializer  This  is  just  simple  way  to  try  out  prediction  on  particular  set  of  data  The  output  currently  in  the  form  of  prediction  and  the  class  probabilities  for  each  of  the  classes  sample  data  np  asarray  age  workclass  fnlwgt  education  ,amazon
education  num  marital  status  occupation  relationship  race  sex  capital  gain  capital  loss  68  Self  emp  not  inc  273088  Some  college  10  Married  civ  spouse  Craft  repair  Husband  White  Male  print  predictor  predict  sample  data  decode  utf  Cleaning  up  the  endpoint  to  avoid  getting  charged  for  it  unnecessarily  sess  delete  endpoint  predictor  endpoint  ,amazon
import  tensorflow  as  tf  import  os  import  time  os  environ  TF  CPP  MIN  LOG  LEVEL  print  tensorflow  Version  is  str  tf  version  import  numpy  as  np  os  environ  KERAS  BACKEND  tensorflow  from  keras  import  backend  as  print  os  environ  KERAS  BACKEND  Fashion  MNIST  Dataset  CNN  model  development  https  github  com  zalandoresearch  fashion  mnist  from  keras  datasets  import  fashion  mnist  from  keras  models  import  Sequential  from  keras  layers  import  Dense  Dropout  Flatten  from  keras  layers  import  Conv2D  MaxPooling2D  from  keras  import  utils  losses  optimizers  import  matplotlib  pyplot  as  plt  no  of  classes  num  classes  10  batch  size  and  training  iterations  epochs  batch  size  128  epochs  24  input  image  dimensions  img  rows  img  cols  28  28  data  for  train  and  testing  train  train  test  test  fashion  mnist  load  data  print  train  shape  train  set  print  test  shape  test  set  Define  the  text  labels  fashion  mnist  labe,microsoft
ls  Top  index  Trouser  index  Jumper  index  Dress  index  Coat  index  Sandal  index  Shirt  index  Trainer  index  Bag  index  Ankle  boot  index  img  index  90  label  index  train  img  index  plt  imshow  train  img  index  print  Label  Index  str  label  index  Fashion  Labels  fashion  mnist  labels  label  index  type  convert  and  scale  the  test  and  training  data  train  train  astype  float32  test  test  astype  float32  train  255  test  255  one  hot  encoding  train  utils  to  categorical  train  num  classes  test  utils  to  categorical  test  num  classes  formatting  issues  for  depth  of  image  greyscale  with  different  kernels  tensorflow  cntk  etc  if  image  data  format  channels  first  train  train  reshape  train  shape  img  rows  img  cols  test  test  reshape  test  shape  img  rows  img  cols  input  shape  img  rows  img  cols  else  train  train  reshape  train  shape  img  rows  img  cols  test  test  reshape  test  shape  img  rows  img  cols  input  shape  im,microsoft
g  rows  img  cols  Define  the  CNN  model  model  Sequential  model  add  Conv2D  64  kernel  size  activation  relu  input  shape  input  shape  model  add  MaxPooling2D  pool  size  model  add  Conv2D  64  kernel  size  activation  relu  model  add  MaxPooling2D  pool  size  model  add  Flatten  model  add  Dense  128  activation  relu  model  add  Dropout  model  add  Dense  num  classes  activation  softmax  model  summary  compile  how  to  measure  loss  model  compile  loss  losses  categorical  crossentropy  optimizer  optimizers  Adam  metrics  accuracy  train  the  model  and  return  loss  and  accuracy  for  each  epoch  history  dictionary  start  time  time  hist  model  fit  train  train  batch  size  batch  size  epochs  epochs  verbose  validation  data  test  test  end  time  time  evaluate  the  model  on  the  test  data  score  model  evaluate  test  test  verbose  print  Test  Loss  score  print  Test  Accuracy  score  print  Time  to  run  end  start  epoch  list  list  range  len  hi,microsoft
st  history  acc  plt  plot  epoch  list  hist  history  acc  epoch  list  hist  history  val  acc  plt  legend  Training  Accuracy  Validation  Accuracy  plt  show  predictions  model  predict  test  Plot  random  sample  of  10  test  images  their  predicted  labels  and  ground  truth  figure  plt  figure  figsize  20  for  index  in  enumerate  np  random  choice  test  shape  size  15  replace  False  ax  figure  add  subplot  xticks  yticks  Display  each  image  ax  imshow  np  squeeze  test  index  predict  index  np  argmax  predictions  index  true  index  np  argmax  test  index  Set  the  title  for  each  image  ax  set  title  format  fashion  mnist  labels  predict  index  fashion  mnist  labels  true  index  color  green  if  predict  index  true  index  else  red  ,microsoft
if  require  mFilter  install  packages  mFilter  library  mFilter  ,microsoft
import  numpy  as  np  from  keras  models  import  Sequential  from  keras  layers  import  Dense  from  keras  regularizers  import  L1L2  from  keras  utils  np  utils  import  to  categorical  from  keras  import  backend  as  import  matplotlib  pyplot  as  pltlabel  Iris  setosa  Iris  versicolor  Iris  virginica  iris  data  np  genfromtxt  assets  iris  iris  csv  delimiter  print  type  iris  data  iris  data  shape  print  iris  data  train  iris  data  print  train  shape  train  yo  train  iris  data  print  yo  train  shape  yo  train  label  int  yo  train  convert  to  OHE  format  train  to  categorical  yo  train  num  classes  len  label  print  train  clear  session  input  dim  out  dim  len  label  model  Sequential  model  add  Dense  out  dim  activation  softmax  kernel  regularizer  L1L2  l1  l2  input  dim  input  dim  model  compile  optimizer  sgd  loss  categorical  crossentropy  metrics  accuracy  history  model  fit  train  train  epochs  100  epochs  500  Get  training  and  te,microsoft
st  loss  histories  loss  history  history  loss  acc  history  history  acc  Create  count  of  the  number  of  epochs  epoch  count  range  len  loss  Visualize  loss  history  plt  plot  epoch  count  acc  plt  plot  epoch  count  loss  plt  legend  acc  loss  plt  xlabel  epoch  plt  ylabel  acc  vs  loss  plt  show  idx  50  test  train  idx  val  yo  train  idx  print  test  val  np  expand  dims  test  axis  model  predict  predict  np  argmax  print  predict  label  predict  import  sys  import  numpy  import  pandas  from  pandas  tools  plotting  import  scatter  matrix  import  matplotlib  pyplot  as  plt  import  numpy  as  np  import  warningsurl  assets  iris  iris  plot  csv  dataset  pandas  read  csv  url  print  dataset  head  10  dataset  Species  unique  setosa  dataset  dataset  Species  Iris  setosa  versicolor  dataset  dataset  Species  Iris  versicolor  virginica  dataset  dataset  Species  Iris  virginica  print  dataset  describe  plt  figure  fig  ax  plt  subplots  figsize  17  ,microsoft
dataset  plot  SepalLengthCm  SepalWidthCm  kind  scatter  ax  ax  sharex  False  sharey  False  label  sepal  color  dataset  plot  PetalLengthCm  PetalWidthCm  kind  scatter  ax  ax  sharex  False  sharey  False  label  petal  color  ax  set  title  Sepal  comparasion  ylabel  sepal  width  ax  set  title  Petal  Comparasion  ylabel  petal  width  ax  legend  ax  legend  plt  figure  fig  ax  plt  subplots  figsize  21  10  setosa  plot  SepalLengthCm  SepalWidthCm  kind  scatter  ax  ax  label  setosa  color  versicolor  plot  SepalLengthCm  SepalWidthCm  kind  scatter  ax  ax  label  versicolor  color  virginica  plot  SepalLengthCm  SepalWidthCm  kind  scatter  ax  ax  label  virginica  color  setosa  plot  PetalLengthCm  PetalWidthCm  kind  scatter  ax  ax  label  setosa  color  versicolor  plot  PetalLengthCm  PetalWidthCm  kind  scatter  ax  ax  label  versicolor  color  virginica  plot  PetalLengthCm  PetalWidthCm  kind  scatter  ax  ax  label  virginica  color  ax  set  title  Sepal  comparasion  yl,microsoft
abel  sepal  width  ax  set  title  Petal  Comparasion  ylabel  petal  width  ax  legend  ax  legend  ,microsoft
matplotlib  inline  import  os  import  numpy  as  np  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  from  sklearn  datasets  import  load  boston  import  sklearn  model  selectionimport  sagemaker  from  sagemaker  import  get  execution  role  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  from  sagemaker  predictor  import  csv  serializer  This  is  an  object  that  represents  the  SageMaker  session  that  we  are  currently  operating  in  This  object  contains  some  useful  information  that  we  will  need  to  access  later  such  as  our  region  session  sagemaker  Session  This  is  an  object  that  represents  the  IAM  role  that  we  are  currently  assigned  When  we  construct  and  launch  the  training  job  later  we  will  need  to  tell  it  what  IAM  role  it  should  have  Since  our  use  case  is  relatively  simple  we  will  simply  assign  the  training  job  the  role  we  currently  have  role  get  execution  role  boston  load  bosto,amazon
n  First  we  package  up  the  input  data  and  the  target  variable  the  median  value  as  pandas  dataframes  This  will  make  saving  the  data  to  file  little  easier  later  on  bos  pd  pd  DataFrame  boston  data  columns  boston  feature  names  bos  pd  pd  DataFrame  boston  target  We  split  the  dataset  into  training  and  testing  sets  train  test  train  test  sklearn  model  selection  train  test  split  bos  pd  bos  pd  test  size  33  Then  we  split  the  training  set  further  into  training  and  validation  sets  train  val  train  val  sklearn  model  selection  train  test  split  train  train  test  size  33  This  is  our  local  data  directory  We  need  to  make  sure  that  it  exists  data  dir  data  boston  if  not  os  path  exists  data  dir  os  makedirs  data  dir  We  use  pandas  to  save  our  train  and  validation  data  to  csv  files  Note  that  we  make  sure  not  to  include  header  information  or  an  index  as  this  is  required  by  the  buil,amazon
t  in  algorithms  provided  by  Amazon  Also  it  is  assumed  that  the  first  entry  in  each  row  is  the  target  variable  pd  concat  val  val  axis  to  csv  os  path  join  data  dir  validation  csv  header  False  index  False  pd  concat  train  train  axis  to  csv  os  path  join  data  dir  train  csv  header  False  index  False  prefix  boston  xgboost  deploy  hl  val  location  session  upload  data  os  path  join  data  dir  validation  csv  key  prefix  prefix  train  location  session  upload  data  os  path  join  data  dir  train  csv  key  prefix  prefix  As  stated  above  we  use  this  utility  method  to  construct  the  image  name  for  the  training  container  container  get  image  uri  session  boto  region  name  xgboost  Now  that  we  know  which  container  to  use  we  can  construct  the  estimator  object  xgb  sagemaker  estimator  Estimator  container  The  name  of  the  training  container  role  The  IAM  role  to  use  our  current  role  in  this  case  trai,amazon
n  instance  count  The  number  of  instances  to  use  for  training  train  instance  type  ml  m4  xlarge  The  type  of  instance  ot  use  for  training  output  path  s3  output  format  session  default  bucket  prefix  Where  to  save  the  output  the  model  artifacts  sagemaker  session  session  The  current  SageMaker  sessionxgb  set  hyperparameters  max  depth  eta  gamma  min  child  weight  subsample  objective  reg  linear  early  stopping  rounds  10  num  round  200  This  is  wrapper  around  the  location  of  our  train  and  validation  data  to  make  sure  that  SageMaker  knows  our  data  is  in  csv  format  s3  input  train  sagemaker  s3  input  s3  data  train  location  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  val  location  content  type  csv  xgb  fit  train  s3  input  train  validation  s3  input  validation  xgb  predictor  xgb  deploy  initial  instance  count  instance  type  ml  m4  xlarge  We  need  to  tell  the  endpoint  what  fo,amazon
rmat  that  data  we  are  sending  is  in  xgb  predictor  content  type  text  csv  xgb  predictor  serializer  csv  serializer  pred  xgb  predictor  predict  test  values  decode  utf  predictions  is  currently  comma  delimited  string  and  so  we  would  like  to  break  it  up  as  numpy  array  pred  np  fromstring  pred  sep  plt  scatter  test  pred  plt  xlabel  Median  Price  plt  ylabel  Predicted  Price  plt  title  Median  Price  vs  Predicted  Price  xgb  predictor  delete  endpoint  First  we  will  remove  all  of  the  files  contained  in  the  data  dir  directory  rm  data  dir  And  then  we  delete  the  directory  itself  rmdir  data  dir  ,amazon
import  sagemaker  bucket  sagemaker  Session  default  bucket  prefix  radix  mnist  fashion  tutorial  role  sagemaker  get  execution  role  import  boto3  from  time  import  gmtime  strftime  from  sagemaker  tensorflow  import  TensorFlow  from  sagemaker  tuner  import  IntegerParameter  CategoricalParameter  ContinuousParameter  HyperparameterTuner  cat  cnn  fashion  mnist  py  estimator  TensorFlow  entry  point  cnn  fashion  mnist  py  role  role  input  mode  Pipe  training  steps  20  000  evaluation  steps  100  train  instance  count  train  instance  type  ml  c5  2xlarge  base  job  name  radix  mnist  fashion  Define  which  objective  has  to  be  optimised  objective  metric  name  loss  objective  type  Minimize  metric  definitions  Name  loss  Regex  loss  Define  hyperparameter  ranges  hyperparameter  ranges  learning  rate  ContinuousParameter  0001  001  dropout  rate  ContinuousParameter  nw  depth  IntegerParameter  optimizer  type  CategoricalParameter  sgd  adam  Instantiate  H,amazon
yperparameterTuner  instance  tuner  HyperparameterTuner  estimator  objective  metric  name  hyperparameter  ranges  metric  definitions  max  jobs  16  max  parallel  jobs  objective  type  objective  type  Fit  the  HyperparameterTuner  to  start  the  hyperparameter  optimisation  process  train  data  s3  sagemaker  eu  central  959924085179  radix  mnist  fashion  tutorial  data  mnist  train  tfrecords  eval  data  s3  sagemaker  eu  central  959924085179  radix  mnist  fashion  tutorial  data  mnist  validation  tfrecords  tuner  fit  train  train  data  eval  eval  data  logs  False  Sanity  check  if  the  optimisation  process  has  started  boto3  client  sagemaker  describe  hyper  parameter  tuning  job  HyperParameterTuningJobName  tuner  latest  tuning  job  job  name  HyperParameterTuningJobStatus  ,amazon
matplotlib  inline  import  tensorflow  as  tf  import  os  import  matplotlib  pyplot  as  plt  import  matplotlib  patheffects  as  PathEffects  import  matplotlib  import  numpy  as  np  from  sklearn  decomposition  import  PCA  from  sklearn  manifold  import  TSNE  from  tensorflow  examples  tutorials  mnist  import  input  data  mnist  input  data  read  data  sets  MNIST  data  one  hot  True  current  dir  os  getcwd  sess  tf  InteractiveSession  def  weight  variable  shape  name  initial  tf  truncated  normal  shape  stddev  return  tf  Variable  initial  name  def  bias  variable  shape  name  initial  tf  constant  shape  shape  return  tf  Variable  initial  name  def  conv2d  return  tf  nn  conv2d  strides  padding  SAME  def  max  pool  2x2  return  tf  nn  max  pool  ksize  strides  padding  SAME  def  add  layer  conv  tf  nn  relu  conv2d  return  max  pool  2x2  conv  tf  placeholder  tf  float32  shape  None  784  tf  placeholder  tf  float32  shape  None  10  image  tf  reshape  28  ,microsoft
28  layer1  add  layer  image  weight  variable  32  conv1  bias  variable  32  conv1  layer2  tf  nn  relu  conv2d  layer1  weight  variable  32  48  conv2  bias  variable  48  conv2  layer3  add  layer  layer2  weight  variable  48  64  conv3  bias  variable  64  conv3  fc1  weight  variable  64  1024  fc1  fc1  bias  variable  1024  fc1  pool2  flat  tf  reshape  layer3  64  fc1  tf  nn  relu  tf  matmul  pool2  flat  fc1  fc1  keep  prob  tf  placeholder  tf  float32  fc1  drop  tf  nn  dropout  fc1  keep  prob  fc2  weight  variable  1024  10  fc2  fc2  bias  variable  10  fc2  conv  tf  matmul  fc1  drop  fc2  fc2  cross  entropy  tf  reduce  mean  tf  nn  softmax  cross  entropy  with  logits  logits  conv  labels  train  step  tf  train  AdamOptimizer  1e  minimize  cross  entropy  correct  prediction  tf  equal  tf  argmax  conv  tf  argmax  accuracy  tf  reduce  mean  tf  cast  correct  prediction  tf  float32  saver  restore  sess  os  path  join  current  dir  model  mnist  cnn  layer  model  ckpt,microsoft
  saver  tf  train  Saver  sess  run  tf  global  variables  initializer  for  in  range  20000  batch  mnist  train  next  batch  50  if  100  train  accuracy  accuracy  eval  feed  dict  batch  batch  keep  prob  print  step  training  accuracy  train  accuracy  train  step  run  feed  dict  batch  batch  keep  prob  print  test  accuracy  accuracy  eval  feed  dict  mnist  test  images  mnist  test  labels  keep  prob  def  pca  components  pca  PCA  components  components  pca  fit  return  pca  transform  def  tsne  components  model  TSNE  components  perplexity  40  return  model  fit  transform  def  plot  scatter  labels  title  txt  False  plt  title  title  ax  plt  subplot  ax  scatter  labels  txts  if  txt  for  in  range  10  xtext  ytext  np  median  labels  axis  txt  ax  text  xtext  ytext  str  fontsize  24  txt  set  path  effects  PathEffects  Stroke  linewidth  foreground  PathEffects  Normal  txts  append  txt  plt  show  test  size  5000  test  data  mnist  test  images  test  size  te,microsoft
st  label  mnist  test  labels  test  size  test  label  index  np  argmax  test  label  axis  layer1  reshape  tf  reshape  layer1  14  14  32  test  layer1  pca  pca  layer1  reshape  eval  feed  dict  test  data  plot  scatter  test  layer1  pca  test  label  index  conv  layer1  with  pca  layer2  reshape  tf  reshape  layer2  14  14  48  test  layer2  pca  pca  layer2  reshape  eval  feed  dict  test  data  plot  scatter  test  layer2  pca  test  label  index  layer2  with  pca  layer3  reshape  tf  reshape  layer3  64  test  layer3  pca  pca  layer3  reshape  eval  feed  dict  test  data  plot  scatter  test  layer3  pca  test  label  index  conv  layer3  with  pca  fc1  pca  pca  fc1  eval  feed  dict  test  data  plot  scatter  fc1  pca  test  label  index  fc  layer1  with  pca  txt  True  layer1  reshape  tf  reshape  layer1  14  14  32  layer1  pca  pca  layer1  reshape  eval  feed  dict  test  data  50  layer1  tsne  tsne  layer1  pca  plot  scatter  layer1  tsne  test  label  index  conv  layer1 ,microsoft
 with  tsne  layer2  reshape  tf  reshape  layer2  14  14  48  layer2  pca  pca  layer2  reshape  eval  feed  dict  test  data  50  layer2  tsne  tsne  layer2  pca  plot  scatter  layer2  tsne  test  label  index  conv  layer2  with  tsne  txt  True  layer3  reshape  tf  reshape  layer3  64  layer3  pca  pca  layer3  reshape  eval  feed  dict  test  data  50  layer3  tsne  tsne  layer3  pca  plot  scatter  layer3  tsne  test  label  index  conv  layer3  with  tsne  txt  True  fc1  pca  pca  fc1  eval  feed  dict  test  data  50  fc1  tsne  tsne  fc1  pca  plot  scatter  fc1  tsne  test  label  index  fc  layer1  with  tsne  txt  True  fc2  tsne  tsne  conv  eval  feed  dict  test  data  keep  prob  plot  scatter  fc2  tsne  test  label  index  fc  layer2  with  tsne  txt  True  ,microsoft
mkdir  data  wget  data  aclImdb  v1  tar  gz  http  ai  stanford  edu  amaas  data  sentiment  aclImdb  v1  tar  gz  tar  zxf  data  aclImdb  v1  tar  gz  dataimport  os  import  glob  def  read  imdb  data  data  dir  data  aclImdb  data  labels  for  data  type  in  train  test  data  data  type  labels  data  type  for  sentiment  in  pos  neg  data  data  type  sentiment  labels  data  type  sentiment  path  os  path  join  data  dir  data  type  sentiment  txt  files  glob  glob  path  for  in  files  with  open  as  review  data  data  type  sentiment  append  review  read  Here  we  represent  positive  review  by  and  negative  review  by  labels  data  type  sentiment  append  if  sentiment  pos  else  assert  len  data  data  type  sentiment  len  labels  data  type  sentiment  data  size  does  not  match  labels  size  format  data  type  sentiment  return  data  labelsdata  labels  read  imdb  data  print  IMDB  reviews  train  pos  neg  test  pos  neg  format  len  data  train  pos  len  data ,amazon
 train  neg  len  data  test  pos  len  data  test  neg  from  sklearn  utils  import  shuffle  def  prepare  imdb  data  data  labels  Prepare  training  and  test  sets  from  IMDb  movie  reviews  Combine  positive  and  negative  reviews  and  labels  data  train  data  train  pos  data  train  neg  data  test  data  test  pos  data  test  neg  labels  train  labels  train  pos  labels  train  neg  labels  test  labels  test  pos  labels  test  neg  Shuffle  reviews  and  corresponding  labels  within  training  and  test  sets  data  train  labels  train  shuffle  data  train  labels  train  data  test  labels  test  shuffle  data  test  labels  test  Return  unified  training  data  test  data  training  labels  test  labets  return  data  train  data  test  labels  train  labels  testtrain  test  train  test  prepare  imdb  data  data  labels  print  IMDb  reviews  combined  train  test  format  len  train  len  test  print  train  100  print  train  100  import  nltk  from  nltk  corpus  import  stopw,amazon
ords  from  nltk  stem  porter  import  import  re  from  bs4  import  BeautifulSoup  def  review  to  words  review  nltk  download  stopwords  quiet  True  stemmer  PorterStemmer  text  BeautifulSoup  review  html  parser  get  text  Remove  HTML  tags  text  re  sub  zA  Z0  text  lower  Convert  to  lower  case  words  text  split  Split  string  into  words  words  for  in  words  if  not  in  stopwords  words  english  Remove  stopwords  words  PorterStemmer  stem  for  in  words  stem  return  words  TODO  Apply  review  to  words  to  review  train  100  or  any  other  review  import  pickle  cache  dir  os  path  join  cache  sentiment  analysis  where  to  store  cache  files  os  makedirs  cache  dir  exist  ok  True  ensure  cache  directory  exists  def  preprocess  data  data  train  data  test  labels  train  labels  test  cache  dir  cache  dir  cache  file  preprocessed  data  pkl  Convert  each  review  to  words  read  from  cache  if  available  If  cache  file  is  not  None  try  to  re,amazon
ad  from  it  first  cache  data  None  if  cache  file  is  not  None  try  with  open  os  path  join  cache  dir  cache  file  rb  as  cache  data  pickle  load  print  Read  preprocessed  data  from  cache  file  cache  file  except  pass  unable  to  read  from  cache  but  that  okay  If  cache  is  missing  then  do  the  heavy  lifting  if  cache  data  is  None  Preprocess  training  and  test  data  to  obtain  words  for  each  review  words  train  list  map  review  to  words  data  train  words  test  list  map  review  to  words  data  test  words  train  review  to  words  review  for  review  in  data  train  words  test  review  to  words  review  for  review  in  data  test  Write  to  cache  file  for  future  runs  if  cache  file  is  not  None  cache  data  dict  words  train  words  train  words  test  words  test  labels  train  labels  train  labels  test  labels  test  with  open  os  path  join  cache  dir  cache  file  wb  as  pickle  dump  cache  data  print  Wrote  preprocessed ,amazon
 data  to  cache  file  cache  file  else  Unpack  data  loaded  from  cache  file  words  train  words  test  labels  train  labels  test  cache  data  words  train  cache  data  words  test  cache  data  labels  train  cache  data  labels  test  return  words  train  words  test  labels  train  labels  test  Preprocess  data  train  test  train  test  preprocess  data  train  test  train  test  import  numpy  as  np  def  build  dict  data  vocab  size  5000  Construct  and  return  dictionary  mapping  each  of  the  most  frequently  appearing  words  to  unique  integer  TODO  Determine  how  often  each  word  appears  in  data  Note  that  data  is  list  of  sentences  and  that  sentence  is  list  of  words  word  count  dict  storing  the  words  that  appear  in  the  reviews  along  with  how  often  they  occur  TODO  Sort  the  words  found  in  data  so  that  sorted  words  is  the  most  frequently  appearing  word  and  sorted  words  is  the  least  frequently  appearing  words  sorted  wo,amazon
rds  None  word  dict  This  is  what  we  are  building  dictionary  that  translates  words  into  integers  for  idx  word  in  enumerate  sorted  words  vocab  size  The  is  so  that  we  save  room  for  the  no  word  word  dict  word  idx  infrequent  labels  return  word  dictword  dict  build  dict  train  TODO  Use  this  space  to  determine  the  five  most  frequently  appearing  words  in  the  training  set  data  dir  data  pytorch  The  folder  we  will  use  for  storing  data  if  not  os  path  exists  data  dir  Make  sure  that  the  folder  exists  os  makedirs  data  dir  with  open  os  path  join  data  dir  word  dict  pkl  wb  as  pickle  dump  word  dict  def  convert  and  pad  word  dict  sentence  pad  500  NOWORD  We  will  use  to  represent  the  no  word  category  INFREQ  and  we  use  to  represent  the  infrequent  words  words  not  appearing  in  word  dict  working  sentence  NOWORD  pad  for  word  index  word  in  enumerate  sentence  pad  if  word  in  word  dict ,amazon
 working  sentence  word  index  word  dict  word  else  working  sentence  word  index  INFREQ  return  working  sentence  min  len  sentence  pad  def  convert  and  pad  data  word  dict  data  pad  500  result  lengths  for  sentence  in  data  converted  leng  convert  and  pad  word  dict  sentence  pad  result  append  converted  lengths  append  leng  return  np  array  result  np  array  lengths  train  train  len  convert  and  pad  data  word  dict  train  test  test  len  convert  and  pad  data  word  dict  test  Use  this  cell  to  examine  one  of  the  processed  reviews  to  make  sure  everything  is  working  as  intended  import  pandas  as  pd  pd  concat  pd  DataFrame  train  pd  DataFrame  train  len  pd  DataFrame  train  axis  to  csv  os  path  join  data  dir  train  csv  header  False  index  False  import  sagemaker  sagemaker  session  sagemaker  Session  bucket  sagemaker  session  default  bucket  prefix  sagemaker  sentiment  rnn  role  sagemaker  get  execution  role  input,amazon
  data  sagemaker  session  upload  data  path  data  dir  bucket  bucket  key  prefix  prefix  pygmentize  train  model  pyimport  torch  import  torch  utils  data  Read  in  only  the  first  250  rows  train  sample  pd  read  csv  os  path  join  data  dir  train  csv  header  None  names  None  nrows  250  Turn  the  input  pandas  dataframe  into  tensors  train  sample  torch  from  numpy  train  sample  values  float  squeeze  train  sample  torch  from  numpy  train  sample  drop  axis  values  long  Build  the  dataset  train  sample  ds  torch  utils  data  TensorDataset  train  sample  train  sample  Build  the  dataloader  train  sample  dl  torch  utils  data  DataLoader  train  sample  ds  batch  size  50  def  train  model  train  loader  epochs  optimizer  loss  fn  device  for  epoch  in  range  epochs  model  train  total  loss  for  batch  in  train  loader  batch  batch  batch  batch  batch  to  device  batch  batch  to  device  TODO  Complete  this  train  method  to  train  the  model ,amazon
 provided  total  loss  loss  data  item  print  Epoch  BCELoss  format  epoch  total  loss  len  train  loader  import  torch  optim  as  optim  from  train  model  import  LSTMClassifier  device  torch  device  cuda  if  torch  cuda  is  available  else  cpu  model  LSTMClassifier  32  100  5000  to  device  optimizer  optim  Adam  model  parameters  loss  fn  torch  nn  BCELoss  train  model  train  sample  dl  optimizer  loss  fn  device  from  sagemaker  pytorch  import  PyTorch  estimator  PyTorch  entry  point  train  py  source  dir  train  role  role  framework  version  train  instance  count  train  instance  type  ml  p2  xlarge  hyperparameters  epoch  10  hidden  dim  200  estimator  fit  training  input  data  TODO  Deploy  the  trained  model  test  pd  concat  pd  DataFrame  test  len  pd  DataFrame  test  axis  We  split  the  data  into  chunks  and  send  each  chunk  seperately  accumulating  the  results  def  predict  data  rows  512  split  array  np  array  split  data  int  data  sha,amazon
pe  float  rows  predictions  np  array  for  array  in  split  array  predictions  np  append  predictions  predictor  predict  array  return  predictionspredictions  predict  test  values  predictions  round  num  for  num  in  predictions  from  sklearn  metrics  import  accuracy  score  accuracy  score  test  predictions  test  review  The  simplest  pleasures  in  life  are  the  best  and  this  film  is  one  of  them  Combining  rather  basic  storyline  of  love  and  adventure  this  movie  transcends  the  usual  weekend  fair  with  wit  and  unmitigated  charm  TODO  Convert  test  review  into  form  usable  by  the  model  and  save  the  results  in  test  data  test  data  Nonepredictor  predict  test  data  estimator  delete  endpoint  pygmentize  serve  predict  pyfrom  sagemaker  predictor  import  RealTimePredictor  from  sagemaker  pytorch  import  PyTorchModel  class  StringPredictor  RealTimePredictor  def  init  self  endpoint  name  sagemaker  session  super  StringPredictor  self  i,amazon
nit  endpoint  name  sagemaker  session  content  type  text  plain  model  PyTorchModel  model  data  estimator  model  data  role  role  framework  version  entry  point  predict  py  source  dir  serve  predictor  cls  StringPredictor  predictor  model  deploy  initial  instance  count  instance  type  ml  m4  xlarge  import  glob  def  test  reviews  data  dir  data  aclImdb  stop  250  results  ground  We  make  sure  to  test  both  positive  and  negative  reviews  for  sentiment  in  pos  neg  path  os  path  join  data  dir  test  sentiment  txt  files  glob  glob  path  files  read  print  Starting  sentiment  files  Iterate  through  the  files  and  send  them  to  the  predictor  for  in  files  with  open  as  review  First  we  store  the  ground  truth  was  the  review  positive  or  negative  if  sentiment  pos  ground  append  else  ground  append  Read  in  the  review  and  convert  to  utf  for  transmission  via  HTTP  review  input  review  read  encode  utf  Send  the  review  to  the,amazon
  predictor  and  store  the  results  results  append  float  predictor  predict  review  input  Sending  reviews  to  our  endpoint  one  at  time  takes  while  so  we  only  send  small  number  of  reviews  files  read  if  files  read  stop  break  return  ground  resultsground  results  test  reviews  from  sklearn  metrics  import  accuracy  score  accuracy  score  ground  results  predictor  predict  test  review  predictor  endpointpredictor  delete  endpoint  ,amazon
Lab  11  MNIST  and  Deep  learning  CNN  https  www  tensorflow  org  tutorials  layers  import  tensorflow  as  tf  import  numpy  as  np  from  tensorflow  examples  tutorials  mnist  import  input  data  tf  set  random  seed  777  reproducibility  mnist  input  data  read  data  sets  MNIST  data  one  hot  True  Check  out  https  www  tensorflow  org  get  started  mnist  beginners  for  more  information  about  the  mnist  dataset  hyper  parameters  learning  rate  001  training  epochs  20  batch  size  100class  Model  def  init  self  sess  name  self  sess  sess  self  name  name  self  build  net  def  build  net  self  with  tf  variable  scope  self  name  dropout  keep  prob  rate  on  training  but  should  be  for  testing  self  training  tf  placeholder  tf  bool  input  place  holders  self  tf  placeholder  tf  float32  None  784  img  28x28x1  black  white  Input  Layer  img  tf  reshape  self  28  28  self  tf  placeholder  tf  float32  None  10  Convolutional  Layer  and  Pooling  L,ibm
ayer  conv1  tf  layers  conv2d  inputs  img  filters  32  kernel  size  padding  SAME  activation  tf  nn  relu  pool1  tf  layers  max  pooling2d  inputs  conv1  pool  size  padding  SAME  strides  dropout1  tf  layers  dropout  inputs  pool1  rate  training  self  training  Convolutional  Layer  and  Pooling  Layer  conv2  tf  layers  conv2d  inputs  dropout1  filters  64  kernel  size  padding  SAME  activation  tf  nn  relu  pool2  tf  layers  max  pooling2d  inputs  conv2  pool  size  padding  SAME  strides  dropout2  tf  layers  dropout  inputs  pool2  rate  training  self  training  Convolutional  Layer  and  Pooling  Layer  conv3  tf  layers  conv2d  inputs  dropout2  filters  128  kernel  size  padding  SAME  activation  tf  nn  relu  pool3  tf  layers  max  pooling2d  inputs  conv3  pool  size  padding  SAME  strides  dropout3  tf  layers  dropout  inputs  pool3  rate  training  self  training  Dense  Layer  with  Relu  flat  tf  reshape  dropout3  128  dense4  tf  layers  dense  inputs  flat  unit,ibm
s  625  activation  tf  nn  relu  dropout4  tf  layers  dropout  inputs  dense4  rate  training  self  training  Logits  no  activation  Layer  L5  Final  FC  625  inputs  10  outputs  self  logits  tf  layers  dense  inputs  dropout4  units  10  define  cost  loss  optimizer  self  cost  tf  reduce  mean  tf  nn  softmax  cross  entropy  with  logits  logits  self  logits  labels  self  self  optimizer  tf  train  AdamOptimizer  learning  rate  learning  rate  minimize  self  cost  correct  prediction  tf  equal  tf  argmax  self  logits  tf  argmax  self  self  accuracy  tf  reduce  mean  tf  cast  correct  prediction  tf  float32  def  predict  self  test  training  False  return  self  sess  run  self  logits  feed  dict  self  test  self  training  training  def  get  accuracy  self  test  test  training  False  return  self  sess  run  self  accuracy  feed  dict  self  test  self  test  self  training  training  def  train  self  data  data  training  True  return  self  sess  run  self  cost  self  opt,ibm
imizer  feed  dict  self  data  self  data  self  training  training  initialize  sess  tf  Session  models  num  models  for  in  range  num  models  models  append  Model  sess  model  str  sess  run  tf  global  variables  initializer  print  Learning  Started  train  my  model  for  epoch  in  range  training  epochs  avg  cost  list  np  zeros  len  models  total  batch  int  mnist  train  num  examples  batch  size  for  in  range  total  batch  batch  xs  batch  ys  mnist  train  next  batch  batch  size  train  each  model  for  idx  in  enumerate  models  train  batch  xs  batch  ys  avg  cost  list  idx  total  batch  print  Epoch  04d  epoch  cost  avg  cost  list  print  Learning  Finished  Test  model  and  check  accuracy  test  size  len  mnist  test  labels  predictions  np  zeros  test  size  10  reshape  test  size  10  for  idx  in  enumerate  models  print  idx  Accuracy  get  accuracy  mnist  test  images  mnist  test  labels  predict  mnist  test  images  predictions  pensemble  correct ,ibm
 prediction  tf  equal  tf  argmax  predictions  tf  argmax  mnist  test  labels  ensemble  accuracy  tf  reduce  mean  tf  cast  ensemble  correct  prediction  tf  float32  print  Ensemble  accuracy  sess  run  ensemble  accuracy  Accuracy  9933  Accuracy  9946  Accuracy  9934  Accuracy  9935  Accuracy  9935  Accuracy  9949  Accuracy  9941  Ensemble  accuracy  9952  ,ibm
Use  the  Azure  Machine  Learning  data  collector  to  log  various  metrics  from  azureml  logging  import  get  azureml  logger  logger  get  azureml  logger  Use  Azure  Machine  Learning  history  magic  to  control  history  collection  History  is  off  by  default  options  are  on  off  or  show  azureml  history  on  az  login  Once  you  are  logged  in  now  let  execute  the  following  commands  to  register  our  environment  providers  az  provider  register  Microsoft  MachineLearningCompute  az  provider  register  Microsoft  ContainerRegistry  az  provider  register  Microsoft  ContainerService  az  provider  show  Microsoft  MachineLearningCompute  az  provider  show  Microsoft  ContainerRegistry  az  provider  show  Microsoft  ContainerService  command  format  az  group  create  name  group  name  location  azure  region  az  group  create  name  mlistrg  location  westus  command  format  az  ml  account  modelmanagement  create  resource  targeted  region  model  management  name  na,microsoft
me  of  created  resource  group  az  ml  account  modelmanagement  create  eastus2  mlistmodelmgmt  mlistrg  command  format  az  ml  account  modelmanagement  set  your  model  management  account  name  name  of  created  resource  group  az  ml  account  modelmanagement  set  mlistmodelmgmt  mlistrg  command  format  az  ml  env  setup  name  your  environment  name  location  azure  region  name  of  created  resource  group  az  ml  env  setup  name  mlistenv  location  eastus2  mlistrg  debug  This  may  take  10  20mins  command  format  az  ml  env  show  name  of  created  resource  group  your  environment  name  az  ml  env  show  mlistrg  mlistenv  command  format  az  ml  env  set  your  environment  name  name  of  created  resource  group  az  ml  env  set  mlistenv  mlistrg  debug  command  format  az  ml  service  create  realtime  model  file  model  file  folder  path  scoring  file  your  web  service  name  json  schema  file  runtime  choice  conda  dependencies  file  az  ml  service  ,microsoft
create  realtime  model  pkl  score  iris  py  classifierservice  service  schema  json  python  debug  Test  your  endpoint  usign  below  code  snippet  import  requests  import  json  data  input  df  sepal  width  sepal  length  petal  length  petal  width  25  body  str  encode  data  url  http  YOUR  SERVICE  IP  api  v1  service  classifierservice  score  api  key  YOUR  API  KEY  headers  Content  Type  application  json  Authorization  Bearer  api  key  resp  requests  post  url  body  headers  headers  resp  text  ,microsoft
mkdir  data  wget  data  aclImdb  v1  tar  gz  http  ai  stanford  edu  amaas  data  sentiment  aclImdb  v1  tar  gz  tar  zxf  data  aclImdb  v1  tar  gz  dataimport  os  import  glob  def  read  imdb  data  data  dir  data  aclImdb  data  labels  for  data  type  in  train  test  data  data  type  labels  data  type  for  sentiment  in  pos  neg  data  data  type  sentiment  labels  data  type  sentiment  path  os  path  join  data  dir  data  type  sentiment  txt  files  glob  glob  path  for  in  files  with  open  as  review  data  data  type  sentiment  append  review  read  Here  we  represent  positive  review  by  and  negative  review  by  labels  data  type  sentiment  append  if  sentiment  pos  else  assert  len  data  data  type  sentiment  len  labels  data  type  sentiment  data  size  does  not  match  labels  size  format  data  type  sentiment  return  data  labelsdata  labels  read  imdb  data  print  IMDB  reviews  train  pos  neg  test  pos  neg  format  len  data  train  pos  len  data ,amazon
 train  neg  len  data  test  pos  len  data  test  neg  from  sklearn  utils  import  shuffle  def  prepare  imdb  data  data  labels  Prepare  training  and  test  sets  from  IMDb  movie  reviews  Combine  positive  and  negative  reviews  and  labels  data  train  data  train  pos  data  train  neg  data  test  data  test  pos  data  test  neg  labels  train  labels  train  pos  labels  train  neg  labels  test  labels  test  pos  labels  test  neg  Shuffle  reviews  and  corresponding  labels  within  training  and  test  sets  data  train  labels  train  shuffle  data  train  labels  train  data  test  labels  test  shuffle  data  test  labels  test  Return  unified  training  data  test  data  training  labels  test  labets  return  data  train  data  test  labels  train  labels  testtrain  test  train  test  prepare  imdb  data  data  labels  print  IMDb  reviews  combined  train  test  format  len  train  len  test  train  100  import  nltk  nltk  download  stopwords  from  nltk  corpus  import  stop,amazon
words  from  nltk  stem  porter  import  stemmer  PorterStemmer  import  re  from  bs4  import  BeautifulSoup  def  review  to  words  review  text  BeautifulSoup  review  html  parser  get  text  Remove  HTML  tags  text  re  sub  zA  Z0  text  lower  Convert  to  lower  case  words  text  split  Split  string  into  words  words  for  in  words  if  not  in  stopwords  words  english  Remove  stopwords  words  PorterStemmer  stem  for  in  words  stem  return  wordsimport  pickle  cache  dir  os  path  join  cache  sentiment  analysis  where  to  store  cache  files  os  makedirs  cache  dir  exist  ok  True  ensure  cache  directory  exists  def  preprocess  data  data  train  data  test  labels  train  labels  test  cache  dir  cache  dir  cache  file  preprocessed  data  pkl  Convert  each  review  to  words  read  from  cache  if  available  If  cache  file  is  not  None  try  to  read  from  it  first  cache  data  None  if  cache  file  is  not  None  try  with  open  os  path  join  cache  dir  cach,amazon
e  file  rb  as  cache  data  pickle  load  print  Read  preprocessed  data  from  cache  file  cache  file  except  pass  unable  to  read  from  cache  but  that  okay  If  cache  is  missing  then  do  the  heavy  lifting  if  cache  data  is  None  Preprocess  training  and  test  data  to  obtain  words  for  each  review  words  train  list  map  review  to  words  data  train  words  test  list  map  review  to  words  data  test  words  train  review  to  words  review  for  review  in  data  train  words  test  review  to  words  review  for  review  in  data  test  Write  to  cache  file  for  future  runs  if  cache  file  is  not  None  cache  data  dict  words  train  words  train  words  test  words  test  labels  train  labels  train  labels  test  labels  test  with  open  os  path  join  cache  dir  cache  file  wb  as  pickle  dump  cache  data  print  Wrote  preprocessed  data  to  cache  file  cache  file  else  Unpack  data  loaded  from  cache  file  words  train  words  test  labels  tr,amazon
ain  labels  test  cache  data  words  train  cache  data  words  test  cache  data  labels  train  cache  data  labels  test  return  words  train  words  test  labels  train  labels  test  Preprocess  data  train  test  train  test  preprocess  data  train  test  train  test  import  numpy  as  np  from  sklearn  feature  extraction  text  import  CountVectorizer  from  sklearn  externals  import  joblib  joblib  is  an  enhanced  version  of  pickle  that  is  more  efficient  for  storing  NumPy  arrays  def  extract  BoW  features  words  train  words  test  vocabulary  size  5000  cache  dir  cache  dir  cache  file  bow  features  pkl  Extract  Bag  of  Words  for  given  set  of  documents  already  preprocessed  into  words  If  cache  file  is  not  None  try  to  read  from  it  first  cache  data  None  if  cache  file  is  not  None  try  with  open  os  path  join  cache  dir  cache  file  rb  as  cache  data  joblib  load  print  Read  features  from  cache  file  cache  file  except  pass  una,amazon
ble  to  read  from  cache  but  that  okay  If  cache  is  missing  then  do  the  heavy  lifting  if  cache  data  is  None  Fit  vectorizer  to  training  documents  and  use  it  to  transform  them  NOTE  Training  documents  have  already  been  preprocessed  and  tokenized  into  words  pass  in  dummy  functions  to  skip  those  steps  preprocessor  lambda  vectorizer  CountVectorizer  max  features  vocabulary  size  preprocessor  lambda  tokenizer  lambda  already  preprocessed  features  train  vectorizer  fit  transform  words  train  toarray  Apply  the  same  vectorizer  to  transform  the  test  documents  ignore  unknown  words  features  test  vectorizer  transform  words  test  toarray  NOTE  Remember  to  convert  the  features  using  toarray  for  compact  representation  Write  to  cache  file  for  future  runs  store  vocabulary  as  well  if  cache  file  is  not  None  vocabulary  vectorizer  vocabulary  cache  data  dict  features  train  features  train  features  test  features  ,amazon
test  vocabulary  vocabulary  with  open  os  path  join  cache  dir  cache  file  wb  as  joblib  dump  cache  data  print  Wrote  features  to  cache  file  cache  file  else  Unpack  data  loaded  from  cache  file  features  train  features  test  vocabulary  cache  data  features  train  cache  data  features  test  cache  data  vocabulary  Return  both  the  extracted  features  as  well  as  the  vocabulary  return  features  train  features  test  vocabulary  Extract  Bag  of  Words  features  for  both  training  and  test  datasets  train  test  vocabulary  extract  BoW  features  train  test  import  pandas  as  pd  TODO  Split  the  train  and  train  arrays  into  the  DataFrames  val  train  and  val  train  Make  sure  that  val  and  val  contain  10  000  entires  while  train  and  train  contain  the  remaining  15  000  entries  val  pd  DataFrame  train  10000  train  pd  DataFrame  train  10000  val  pd  DataFrame  train  10000  train  pd  DataFrame  train  10000  First  we  make  sure  ,amazon
that  the  local  directory  in  which  we  like  to  store  the  training  and  validation  csv  files  exists  data  dir  data  xgboost  if  not  os  path  exists  data  dir  os  makedirs  data  dir  First  save  the  test  data  to  test  csv  in  the  data  dir  directory  Note  that  we  do  not  save  the  associated  ground  truth  labels  instead  we  will  use  them  later  to  compare  with  our  model  output  pd  DataFrame  test  to  csv  os  path  join  data  dir  test  csv  header  False  index  False  TODO  Save  the  training  and  validation  data  to  train  csv  and  validation  csv  in  the  data  dir  directory  Make  sure  that  the  files  you  create  are  in  the  correct  format  pd  concat  val  val  axis  to  csv  os  path  join  data  dir  validation  csv  header  False  index  False  pd  concat  train  train  axis  to  csv  os  path  join  data  dir  train  csv  header  False  index  False  To  save  bit  of  memory  we  can  set  text  train  val  train  and  val  to  None  test,amazon
  train  val  train  val  Noneimport  sagemaker  session  sagemaker  Session  Store  the  current  SageMaker  session  S3  prefix  which  folder  will  we  use  prefix  sentiment  xgboost  TODO  Upload  the  test  csv  train  csv  and  validation  csv  files  which  are  contained  in  data  dir  to  S3  using  sess  upload  data  test  location  session  upload  data  os  path  join  data  dir  test  csv  key  prefix  prefix  val  location  session  upload  data  os  path  join  data  dir  validation  csv  key  prefix  prefix  train  location  session  upload  data  os  path  join  data  dir  train  csv  key  prefix  prefix  from  sagemaker  import  get  execution  role  Our  current  execution  role  is  require  when  creating  the  model  as  the  training  and  inference  code  will  need  to  access  the  model  artifacts  role  get  execution  role  We  need  to  retrieve  the  location  of  the  container  which  is  provided  by  Amazon  for  using  XGBoost  As  matter  of  convenience  the  training,amazon
  and  inference  code  both  use  the  same  container  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  session  boto  region  name  xgboost  TODO  Create  SageMaker  estimator  using  the  container  location  determined  in  the  previous  cell  It  is  recommended  that  you  use  single  training  instance  of  type  ml  m4  xlarge  It  is  also  recommended  that  you  use  s3  output  format  session  default  bucket  prefix  as  the  output  path  xgb  None  TODO  Set  the  XGBoost  hyperparameters  in  the  xgb  object  Don  forget  that  in  this  case  we  have  binary  label  so  we  should  be  using  the  binary  logistic  objective  xgb  sagemaker  estimator  Estimator  container  The  location  of  the  container  we  wish  to  use  role  What  is  our  current  IAM  Role  train  instance  count  How  many  compute  instances  train  instance  type  ml  m4  xlarge  What  kind  of  compute  instances  output  path  s3  output  format  session  de,amazon
fault  bucket  prefix  sagemaker  session  session  xgb  set  hyperparameters  max  depth  eta  gamma  min  child  weight  subsample  silent  objective  binary  logistic  early  stopping  rounds  10  num  round  500  s3  input  train  sagemaker  s3  input  s3  data  train  location  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  val  location  content  type  csv  xgb  fit  train  s3  input  train  validation  s3  input  validation  TODO  Create  transformer  object  from  the  trained  model  Using  an  instance  count  of  and  an  instance  type  of  ml  m4  xlarge  should  be  more  than  enough  xgb  transformer  xgb  transformer  instance  count  instance  type  ml  m4  xlarge  TODO  Start  the  transform  job  Make  sure  to  specify  the  content  type  and  the  split  type  of  the  test  data  xgb  transformer  transform  test  location  content  type  text  csv  split  type  Line  xgb  transformer  wait  aws  s3  cp  recursive  xgb  transformer  output  path  data  dirpr,amazon
edictions  pd  read  csv  os  path  join  data  dir  test  csv  out  header  None  predictions  round  num  for  num  in  predictions  squeeze  values  from  sklearn  metrics  import  accuracy  score  accuracy  score  test  predictions  First  we  will  remove  all  of  the  files  contained  in  the  data  dir  directory  rm  data  dir  And  then  we  delete  the  directory  itself  rmdir  data  dir  Similarly  we  will  remove  the  files  in  the  cache  dir  directory  and  the  directory  itself  rm  cache  dir  rmdir  cache  dir  ,amazon
Lab  11  MNIST  and  Convolutional  Neural  Network  import  tensorflow  as  tf  import  random  import  matplotlib  pyplot  as  plt  from  tensorflow  examples  tutorials  mnist  import  input  data  tf  set  random  seed  777  reproducibility  mnist  input  data  read  data  sets  MNIST  data  one  hot  True  Check  out  https  www  tensorflow  org  get  started  mnist  beginners  for  more  information  about  the  mnist  dataset  dir  MNIST  data  input  place  holders  tf  placeholder  tf  float32  None  784  image  img  tf  reshape  28  28  img  28x28x1  black  white  tf  placeholder  tf  float32  None  10  lable  L1  ImgIn  shape  28  28  W1  tf  Variable  tf  random  normal  32  stddev  01  32  filter  Conv  28  28  32  Pool  14  14  32  L1  tf  nn  conv2d  img  W1  strides  padding  SAME  print  L1  L1  tf  nn  relu  L1  print  L1  L1  tf  nn  max  pool  L1  ksize  strides  padding  SAME  print  L1  Tensor  Conv2D  shape  28  28  32  dtype  float32  Tensor  Relu  shape  28  28  32  dtype  float32  Te,ibm
nsor  MaxPool  shape  14  14  32  dtype  float32  L2  ImgIn  shape  14  14  32  W2  tf  Variable  tf  random  normal  32  64  stddev  01  64  filter  Conv  14  14  64  Pool  64  L2  tf  nn  conv2d  L1  W2  strides  padding  SAME  print  L2  L2  tf  nn  relu  L2  print  L2  L2  tf  nn  max  pool  L2  ksize  strides  padding  SAME  print  L2  L2  flat  tf  reshape  L2  64  full  connected  Tensor  Conv2D  shape  14  14  64  dtype  float32  Tensor  Relu  shape  14  14  64  dtype  float32  Tensor  MaxPool  shape  64  dtype  float32  Tensor  Reshape  shape  3136  dtype  float32  Final  FC  7x7x64  inputs  10  outputs  hyper  parameters  learning  rate  001  training  epochs  15  batch  size  100  W3  tf  get  variable  W3  shape  64  10  initializer  tf  contrib  layers  xavier  initializer  tf  Variable  tf  random  normal  10  logits  tf  matmul  L2  flat  W3  define  cost  loss  optimizer  cost  tf  reduce  mean  tf  nn  softmax  cross  entropy  with  logits  logits  logits  labels  optimizer  tf  train  AdamOp,ibm
timizer  learning  rate  learning  rate  minimize  cost  initialize  sess  tf  Session  sess  run  tf  global  variables  initializer  train  my  model  print  Learning  started  It  takes  sometime  for  epoch  in  range  training  epochs  avg  cost  total  batch  int  mnist  train  num  examples  batch  size  for  in  range  total  batch  batch  xs  batch  ys  mnist  train  next  batch  batch  size  feed  dict  batch  xs  batch  ys  sess  run  cost  optimizer  feed  dict  feed  dict  avg  cost  total  batch  print  Epoch  04d  epoch  cost  9f  format  avg  cost  print  Learning  Finished  Test  model  and  check  accuracy  correct  prediction  tf  equal  tf  argmax  logits  tf  argmax  accuracy  tf  reduce  mean  tf  cast  correct  prediction  tf  float32  print  Accuracy  sess  run  accuracy  feed  dict  mnist  test  images  mnist  test  labels  Get  one  and  predict  random  randint  mnist  test  num  examples  print  Label  sess  run  tf  argmax  mnist  test  labels  print  Prediction  sess  run  tf  ar,ibm
gmax  logits  feed  dict  mnist  test  images  plt  imshow  mnist  test  images  reshape  28  28  cmap  Greys  interpolation  nearest  plt  show  Epoch  0001  cost  340291267  Epoch  0002  cost  090731326  Epoch  0003  cost  064477619  Epoch  0004  cost  050683064  Epoch  0005  cost  041864835  Epoch  0006  cost  035760704  Epoch  0007  cost  030572132  Epoch  0008  cost  026207981  Epoch  0009  cost  022622454  Epoch  0010  cost  019055919  Epoch  0011  cost  017758641  Epoch  0012  cost  014156652  Epoch  0013  cost  012397016  Epoch  0014  cost  010693789  Epoch  0015  cost  009469977  Learning  Finished  Accuracy  9885  ,ibm
import  matplotlib  pyplot  as  plt  matplotlib  inlineimport  matplotlib  pyplot  as  plt  matplotlib  inline  mylist  range  11  plt  plot  mylist  plt  show  import  matplotlib  pyplot  as  plt  matplotlib  inline  fig  ax  plt  subplots  plt  plot  range  110  range  110  ax  set  title  line  chart  ax  set  xlabel  The  axis  ax  set  ylabel  The  axis  import  numpy  as  np  import  matplotlib  pyplot  as  plt  matplotlib  inline  fig  ax  plt  subplots  evenly  sample  time  at  200ms  intervals  np  arange  plt  plot  plt  show  import  matplotlib  pyplot  as  plt  matplotlib  inline  fig  ax  plt  subplots  import  numpy  as  np  evenly  sample  time  at  200ms  intervals  np  arange  magenta  dashes  for  green  squares  for  cyan  triangles  for  plt  plot  gs  plt  show  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  np  linspace  10  fig  axes  plt  subplots  nrows  ncols  for  ax  in  axes  ax  plot  ax  set  xlabel  ax  set  ylabel  ax  set  title  Title  fig  ,ibm
tight  layout  import  matplotlib  pyplot  as  plt  matplotlib  inline  fig  ax  plt  subplots  import  numpy  as  np  np  linspace  10  ax  plot  label  2x  ax  plot  label  ax  legend  loc  upper  left  corner  ax  set  xlabel  ax  set  ylabel  ax  set  title  Title  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  fig  axes  plt  subplots  figsize  12  np  linspace  10  axes  plot  axes  set  title  Default  Axes  Range  axes  plot  axes  axis  tight  axes  set  title  Tight  Axes  axes  plot  np  power  np  power  axes  set  ylim  60  axes  set  xlim  axes  set  title  Custom  Axes  Range  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  fig  ax  plt  subplots  np  linspace  10  ax  plot  np  exp  ax  set  title  Scientific  Notation  ax  set  yticks  50  100  150  from  matplotlib  import  ticker  formatter  ticker  ScalarFormatter  useMathText  True  scientific  notation  formatter  set  scientific  True  formatter  set  powerlimits  ax  yaxi,ibm
s  set  major  formatter  formatter  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  define  function  to  plot  needs  numpy  def  return  np  exp  np  cos  np  pi  t1  np  arange  t2  np  arange  parameter  is  optional  specifying  no  parameter  also  creates  single  plot  plt  figure  plt  subplot  211  plt  plot  t1  t1  yo  t2  t2  plt  subplot  212  plt  plot  t2  np  cos  np  pi  t2  plt  show  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  plt  figure  the  first  figure  plt  subplot  211  the  first  subplot  in  the  first  figure  plt  plot  plt  subplot  212  the  second  subplot  in  the  first  figure  plt  plot  plt  figure  second  figure  plt  plot  creates  subplot  111  by  default  plt  figure  figure  current  subplot  212  still  current  plt  subplot  211  make  subplot  211  in  figure1  current  plt  title  Easy  as  subplot  211  titleimport  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  fi,ibm
g  ax  plt  subplots  np  linspace  10  ax  plot  label  alpha  ax  plot  label  alpha  ax  legend  loc  upper  left  corner  ax  set  xlabel  alpha  fontsize  18  ax  set  ylabel  fontsize  18  ax  set  title  Title  import  matplotlib  as  matplotlib  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  fig  ax  plt  subplots  np  linspace  10  matplotlib  rcParams  update  font  size  12  font  family  serif  ax  plot  label  alpha  ax  plot  label  alpha  ax  legend  loc  upper  left  corner  ax  set  xlabel  alpha  ax  set  ylabel  ax  set  title  Title  import  matplotlib  as  matplotlib  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  fig  ax  plt  subplots  np  linspace  10  matplotlib  rcParams  update  font  size  18  font  family  STIXGeneral  mathtext  fontset  stix  ax  plot  label  alpha  ax  plot  label  alpha  ax  legend  loc  upper  left  corner  ax  set  xlabel  alpha  ax  set  ylabel  ax  set  title  My  Cool  Title  import  matplot,ibm
lib  as  matplotlib  matplotlib  rcParams  update  font  size  12  font  family  sans  text  usetex  False  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  50  np  random  rand  generate  50  random  numbers  np  random  rand  plt  scatter  plt  title  Scatter  Plot  plt  show  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  np  arange  y0  np  sin  y0  copy  plt  step  label  sin  plt  legend  plt  title  Step  Plot  plt  show  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  fig  plt  figure  range  11  range  11  plt  bar  color  plt  title  Bar  Graph  plt  show  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  val  10  np  random  rand  the  bar  lengths  pos  np  arange  the  bar  centers  on  the  axis  plt  figure  plot  horizontal  bar  chart  that  is  centrally  aligned  plt  barh  pos  val  align  center  assign  names  to  the  axis  the  variables  plt  yticks  pos  Tom  ,ibm
Dick  Harry  Slim  Jim  plt  xlabel  Performance  plt  title  How  fast  do  you  want  to  go  today  add  grid  to  the  background  plt  grid  True  plt  show  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  menMeans  40  36  31  36  28  womenMeans  35  22  24  30  25  menStd  womenStd  ind  np  arange  the  locations  for  the  groups  width  35  the  width  of  the  bars  can  also  be  len  sequence  p1  plt  bar  ind  menMeans  width  color  p2  plt  bar  ind  womenMeans  width  color  bottom  menMeans  specify  what  going  to  the  bottom  plt  ylabel  Scores  plt  title  Scores  by  group  and  gender  plt  xticks  ind  width  G1  G2  G3  G4  G5  plt  yticks  np  arange  91  10  plt  legend  p1  p2  Men  Women  plt  show  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  menMeans  20  35  30  35  27  womenMeans  25  32  34  20  25  menStd  womenStd  ind  np  arange  the  locations  for  the  groups  width  35  the  width  of  the  bars  c,ibm
an  also  be  len  sequence  p1  plt  bar  ind  menMeans  width  color  bottom  womenMeans  specify  what  going  to  the  bottom  p2  plt  bar  ind  womenMeans  width  color  plt  ylabel  Scores  plt  title  Scores  by  group  and  gender  plt  xticks  ind  width  G1  G2  G3  G4  G5  plt  yticks  np  arange  91  10  plt  legend  p1  p2  Men  Women  plt  show  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  20  21  20  20  define  how  much  the  error  error  12  13  plot  data  plt  bar  color  yellowgreen  align  center  plt  errorbar  yerr  error  linestyle  None  marker  None  color  green  configure  axis  plt  xlim  plt  xticks  configure  axis  plt  ylim  19  21  plt  yticks  20  21  20  20  title  and  labels  plt  xlabel  axis  plt  ylabel  axis  plt  title  Error  Margin  Bar  show  plot  plt  show  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  20  21  20  20  errors  error  12  13  plot  data  plt  plot  linestyle  dashed  marker  c,ibm
olor  green  plt  errorbar  yerr  error  linestyle  None  marker  None  color  green  configure  axis  plt  xlim  plt  xticks  configure  axis  plt  ylim  19  21  plt  yticks  20  21  20  20  title  and  labels  plt  xlabel  axis  plt  ylabel  axis  plt  title  Error  Margin  Line  show  plot  plt  show  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  plt  fill  between  np  power  np  power  color  red  plt  ylim  15  plt  xlim  plt  grid  True  plt  show  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  np  arange  01  y1  np  sin  np  pi  width  35  fig  ax1  ax2  plt  subplots  sharex  True  ax1  fill  between  y1  color  lightskyblue  ax1  set  ylabel  y1  ax2  fill  between  y1  color  lightcoral  ax2  set  ylabel  y1  plt  show  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  np  random  randn  100000  fig  axes  plt  subplots  figsize  12  axes  hist  bins  25  color  axes  set  title  Default  histogram  ax,ibm
es  set  xlim  min  max  axes  hist  cumulative  True  bins  40  color  axes  set  title  Cumulative  detailed  histogram  axes  set  xlim  min  max  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  data  20  15  40  10  10  labels  plt  figure  figsize  make  it  square  colors  yellowgreen  gold  lightskyblue  lightcoral  green  red  plt  pie  data  labels  labels  colors  colors  startangle  90  plt  show  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  plt  figure  figsize  The  slices  are  ordered  and  plotted  counter  clockwise  The  second  slice  is  exploded  and  shadowed  labels  Frogs  Hogs  Dogs  Logs  fracs  15  30  45  10  explode  05  plt  pie  fracs  explode  explode  labels  labels  autopct  1f  shadow  True  startangle  90  plt  title  Raining  Hogs  and  Dogs  bbox  facecolor  pad  plt  show  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  fig  plt  figure  ax  fig  add  axes  polar  True  np ,ibm
 linspace  np  pi  100  ax  plot  color  red  lw  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  21  theta  np  linspace  np  pi  endpoint  False  radii  10  np  random  rand  width  np  pi  np  random  rand  ax  plt  subplot  111  polar  True  polar  True  for  polar  plot  bars  ax  bar  theta  radii  width  width  bottom  Use  custom  colors  and  opacity  for  bar  in  zip  radii  bars  bar  set  facecolor  plt  cm  jet  10  bar  set  alpha  plt  show  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  50  np  random  rand  theta  np  pi  np  random  rand  area  200  np  random  rand  colors  theta  ax  plt  subplot  111  polar  True  plt  scatter  theta  colors  area  cmap  plt  cm  hsv  set  alpha  75  plt  show  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  from  mpl  toolkits  mplot3d  axes3d  import  Axes3D  fig  plt  figure  figsize  14  alpha  phi  ext  np  pi  def  flux  qubit  potential  phi  phi  retu,ibm
rn  alpha  np  cos  phi  np  cos  phi  alpha  np  cos  phi  ext  phi  phi  np  linspace  np  pi  100  phi  np  linspace  np  pi  100  np  meshgrid  phi  phi  flux  qubit  potential  ax  is  3D  aware  axis  instance  because  of  the  projection  3d  keyword  argument  to  add  subplot  ax  fig  add  subplot  projection  3d  ax  plot  surface  rstride  cstride  linewidth  surface  plot  with  color  grading  and  color  bar  ax  fig  add  subplot  projection  3d  ax  plot  surface  rstride  cstride  cmap  plt  cm  coolwarm  linewidth  antialiased  False  cb  fig  colorbar  shrink  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  from  mpl  toolkits  mplot3d  axes3d  import  Axes3D  alpha  phi  ext  np  pi  def  flux  qubit  potential  phi  phi  return  alpha  np  cos  phi  np  cos  phi  alpha  np  cos  phi  ext  phi  phi  np  linspace  np  pi  100  phi  np  linspace  np  pi  100  np  meshgrid  phi  phi  flux  qubit  potential  fig  plt  figure  figsize  ax  fig  add  subplot  pr,ibm
ojection  3d  ax  plot  wireframe  rstride  cstride  color  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  from  mpl  toolkits  mplot3d  axes3d  import  Axes3D  fig  plt  figure  ax  Axes3D  fig  np  arange  25  np  arange  25  np  meshgrid  np  sqrt  np  sin  ax  plot  surface  rstride  cstride  cmap  plt  cm  hot  ax  contourf  zdir  offset  cmap  plt  cm  hot  ax  set  zlim  plt  show  ,ibm
Install  wget  if  you  don  already  have  it  pip  install  wget  import  wget  link  to  data  https  apsportal  ibm  com  exchange  api  v1  entries  8044492073eb964f46597b4be06ff5ea  data  accessKey  9561295fa407698694b1e254d0099600  filename  wget  download  link  to  data  print  filename  spark  SparkSession  builder  getOrCreate  df  data  spark  read  format  org  apache  spark  sql  execution  datasources  csv  CSVFileFormat  option  header  true  option  inferSchema  true  load  filename  df  data  printSchema  df  data  show  print  Number  of  records  str  df  data  count  splitted  data  df  data  randomSplit  18  02  24  train  data  splitted  data  test  data  splitted  data  predict  data  splitted  data  print  Number  of  training  records  str  train  data  count  print  Number  of  testing  records  str  test  data  count  print  Number  of  prediction  records  str  predict  data  count  from  pyspark  ml  feature  import  OneHotEncoder  StringIndexer  IndexToString  VectorAssembler  f,ibm
rom  pyspark  ml  classification  import  RandomForestClassifier  from  pyspark  ml  evaluation  import  MulticlassClassificationEvaluator  from  pyspark  ml  import  Pipeline  ModelstringIndexer  label  StringIndexer  inputCol  PRODUCT  LINE  outputCol  label  fit  df  data  stringIndexer  prof  StringIndexer  inputCol  PROFESSION  outputCol  PROFESSION  IX  stringIndexer  gend  StringIndexer  inputCol  GENDER  outputCol  GENDER  IX  stringIndexer  mar  StringIndexer  inputCol  MARITAL  STATUS  outputCol  MARITAL  STATUS  IX  vectorAssembler  features  VectorAssembler  inputCols  GENDER  IX  AGE  MARITAL  STATUS  IX  PROFESSION  IX  outputCol  features  rf  RandomForestClassifier  labelCol  label  featuresCol  features  labelConverter  IndexToString  inputCol  prediction  outputCol  predictedLabel  labels  stringIndexer  label  labels  pipeline  rf  Pipeline  stages  stringIndexer  label  stringIndexer  prof  stringIndexer  gend  stringIndexer  mar  vectorAssembler  features  rf  labelConverter  train  data ,ibm
 printSchema  model  rf  pipeline  rf  fit  train  data  predictions  model  rf  transform  test  data  evaluatorRF  MulticlassClassificationEvaluator  labelCol  label  predictionCol  prediction  metricName  accuracy  accuracy  evaluatorRF  evaluate  predictions  print  Accuracy  accuracy  print  Test  Error  accuracy  rm  rf  PIP  BUILD  watson  machine  learning  client  pip  install  watson  machine  learning  client  upgradefrom  watson  machine  learning  client  import  WatsonMachineLearningAPIClientwml  credentials  client  WatsonMachineLearningAPIClient  wml  credentials  published  model  details  client  repository  store  model  model  model  rf  meta  props  name  Product  line  model  training  data  train  data  pipeline  pipeline  rf  model  uid  client  repository  get  model  uid  published  model  details  print  model  uid  client  repository  ModelMetaNames  show  loaded  model  client  repository  load  model  uid  print  type  loaded  model  predictions  loaded  model  transform  predict,ibm
  data  predictions  show  predictions  select  predictedLabel  groupBy  predictedLabel  count  show  truncate  False  pip  install  plotly  pip  install  cufflinks  2import  sys  import  pandas  import  plotly  plotly  as  py  from  plotly  offline  import  download  plotlyjs  init  notebook  mode  plot  iplot  import  cufflinks  as  cf  import  plotly  graph  objs  as  go  init  notebook  mode  connected  True  sys  path  append  join  os  environ  HOME  predictions  pdf  predictions  select  prediction  predictedLabel  GENDER  AGE  PROFESSION  MARITAL  STATUS  toPandas  cumulative  stats  predictions  pdf  groupby  predictedLabel  count  product  data  go  Pie  labels  cumulative  stats  index  values  cumulative  stats  GENDER  product  layout  go  Layout  title  Predicted  product  line  client  interest  distribution  fig  go  Figure  data  product  data  layout  product  layout  iplot  fig  age  data  go  Bar  predictions  pdf  groupby  predictedLabel  mean  AGE  cumulative  stats  index  age  layout  ,ibm
go  Layout  title  Mean  AGE  per  predicted  product  line  xaxis  dict  title  Product  Line  showline  False  yaxis  dict  title  Mean  AGE  fig  go  Figure  data  age  data  layout  age  layout  iplot  fig  deployment  details  client  deployments  create  model  uid  name  Product  line  model  deployment  scoring  url  client  deployments  get  scoring  url  deployment  details  print  scoring  url  payload  scoring  fields  GENDER  AGE  MARITAL  STATUS  PROFESSION  values  23  Single  Student  55  Single  Executive  client  deployments  score  scoring  url  payload  scoring  ,ibm
import  boto3  Create  example  file  to  be  uploaded  echo  test  file  test  txtdef  aws  s3  upload  file  name  bucket  name  s3  boto3  resource  s3  Create  bucket  if  it  doesn  already  exist  bucket  names  name  for  in  s3  buckets  all  if  bucket  name  not  in  bucket  names  s3  create  bucket  Bucket  bucket  name  CreateBucketConfiguration  LocationConstraint  EU  print  Bucket  created  format  bucket  name  s3  meta  client  upload  file  file  name  bucket  name  file  name  print  uploaded  to  format  file  name  bucket  name  return  def  aws  s3  get  file  name  bucket  name  s3  boto3  resource  s3  try  s3  Bucket  bucket  name  download  file  file  name  downloaded  file  name  print  downloaded  from  as  format  file  name  bucket  name  downloaded  file  name  except  print  Unable  to  download  from  format  file  name  bucket  name  return  def  aws  s3  delete  file  name  bucket  name  del  bucket  False  s3  boto3  resource  s3  try  s3  meta  client  delete  object  Bu,microsoft
cket  bucket  name  Key  file  name  print  deleted  from  format  file  name  bucket  name  except  print  Unable  to  delete  from  format  file  name  bucket  name  if  del  bucket  try  s3  meta  client  delete  bucket  Bucket  bucket  name  print  Bucket  deleted  format  bucket  name  except  print  Unable  to  delete  bucket  format  bucket  name  returnfile  name  test  txt  bucket  name  test  s3  bucket  000  aws  s3  upload  file  name  bucket  name  aws  s3  get  file  name  bucket  name  aws  s3  delete  file  name  bucket  name  del  bucket  True  ,microsoft
matplotlib  inline  import  os  import  time  from  time  import  gmtime  strftime  import  numpy  as  np  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  from  sklearn  datasets  import  load  boston  import  sklearn  model  selectionimport  sagemaker  from  sagemaker  import  get  execution  role  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  This  is  an  object  that  represents  the  SageMaker  session  that  we  are  currently  operating  in  This  object  contains  some  useful  information  that  we  will  need  to  access  later  such  as  our  region  session  sagemaker  Session  This  is  an  object  that  represents  the  IAM  role  that  we  are  currently  assigned  When  we  construct  and  launch  the  training  job  later  we  will  need  to  tell  it  what  IAM  role  it  should  have  Since  our  use  case  is  relatively  simple  we  will  simply  assign  the  training  job  the  role  we  currently  have  role  get  execution  role  boston  load  boston,amazon
  First  we  package  up  the  input  data  and  the  target  variable  the  median  value  as  pandas  dataframes  This  will  make  saving  the  data  to  file  little  easier  later  on  bos  pd  pd  DataFrame  boston  data  columns  boston  feature  names  bos  pd  pd  DataFrame  boston  target  We  split  the  dataset  into  training  and  testing  sets  train  test  train  test  sklearn  model  selection  train  test  split  bos  pd  bos  pd  test  size  33  Then  we  split  the  training  set  further  into  training  and  validation  sets  train  val  train  val  sklearn  model  selection  train  test  split  train  train  test  size  33  This  is  our  local  data  directory  We  need  to  make  sure  that  it  exists  data  dir  data  boston  if  not  os  path  exists  data  dir  os  makedirs  data  dir  We  use  pandas  to  save  our  train  and  validation  data  to  csv  files  Note  that  we  make  sure  not  to  include  header  information  or  an  index  as  this  is  required  by  the  built,amazon
  in  algorithms  provided  by  Amazon  Also  it  is  assumed  that  the  first  entry  in  each  row  is  the  target  variable  pd  concat  val  val  axis  to  csv  os  path  join  data  dir  validation  csv  header  False  index  False  pd  concat  train  train  axis  to  csv  os  path  join  data  dir  train  csv  header  False  index  False  prefix  boston  xgboost  deploy  ll  val  location  session  upload  data  os  path  join  data  dir  validation  csv  key  prefix  prefix  train  location  session  upload  data  os  path  join  data  dir  train  csv  key  prefix  prefix  We  will  need  to  know  the  name  of  the  container  that  we  want  to  use  for  training  SageMaker  provides  nice  utility  method  to  construct  this  for  us  container  get  image  uri  session  boto  region  name  xgboost  We  now  specify  the  parameters  we  wish  to  use  for  our  training  job  training  params  We  need  to  specify  the  permissions  that  this  training  job  will  have  For  our  purposes  w,amazon
e  can  use  the  same  permissions  that  our  current  SageMaker  session  has  training  params  RoleArn  role  Here  we  describe  the  algorithm  we  wish  to  use  The  most  important  part  is  the  container  which  contains  the  training  code  training  params  AlgorithmSpecification  TrainingImage  container  TrainingInputMode  File  We  also  need  to  say  where  we  would  like  the  resulting  model  artifacst  stored  training  params  OutputDataConfig  S3OutputPath  s3  session  default  bucket  prefix  output  We  also  need  to  set  some  parameters  for  the  training  job  itself  Namely  we  need  to  describe  what  sort  of  compute  instance  we  wish  to  use  along  with  stopping  condition  to  handle  the  case  that  there  is  some  sort  of  error  and  the  training  script  doesn  terminate  training  params  ResourceConfig  InstanceCount  InstanceType  ml  m4  xlarge  VolumeSizeInGB  training  params  StoppingCondition  MaxRuntimeInSeconds  86400  Next  we  set  the  alg,amazon
orithm  specific  hyperparameters  You  may  wish  to  change  these  to  see  what  effect  there  is  on  the  resulting  model  training  params  HyperParameters  max  depth  eta  gamma  min  child  weight  subsample  objective  reg  linear  early  stopping  rounds  10  num  round  200  Now  we  need  to  tell  SageMaker  where  the  data  should  be  retrieved  from  training  params  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  train  location  S3DataDistributionType  FullyReplicated  ContentType  csv  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  val  location  S3DataDistributionType  FullyReplicated  ContentType  csv  CompressionType  None  First  we  need  to  choose  training  job  name  This  is  useful  for  if  we  want  to  recall  information  about  our  training  job  at  later  date  Note  that  SageMaker  requires  training  job  name  and  that  the  name  needs  to  be  unique  which  we,amazon
  accomplish  by  appending  the  current  timestamp  training  job  name  boston  xgboost  strftime  gmtime  training  params  TrainingJobName  training  job  name  And  now  we  ask  SageMaker  to  create  and  execute  the  training  job  training  job  session  sagemaker  client  create  training  job  training  params  session  logs  for  job  training  job  name  wait  True  We  begin  by  asking  SageMaker  to  describe  for  us  the  results  of  the  training  job  The  data  structure  returned  contains  lot  more  information  than  we  currently  need  try  checking  it  out  yourself  in  more  detail  training  job  info  session  sagemaker  client  describe  training  job  TrainingJobName  training  job  name  model  artifacts  training  job  info  ModelArtifacts  S3ModelArtifacts  Just  like  when  we  created  training  job  the  model  name  must  be  unique  model  name  training  job  name  model  We  also  need  to  tell  SageMaker  which  container  should  be  used  for  inference  and,amazon
  where  it  should  retrieve  the  model  artifacts  from  In  our  case  the  xgboost  container  that  we  used  for  training  can  also  be  used  for  inference  primary  container  Image  container  ModelDataUrl  model  artifacts  And  lastly  we  construct  the  SageMaker  model  model  info  session  sagemaker  client  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  As  before  we  need  to  give  our  endpoint  configuration  name  which  should  be  unique  endpoint  config  name  boston  xgboost  endpoint  config  strftime  gmtime  And  then  we  ask  SageMaker  to  construct  the  endpoint  configuration  endpoint  config  info  session  sagemaker  client  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialVariantWeight  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  Again  we  need  unique  name  for  our  endpoint  endpoint  name  boston  xgboo,amazon
st  endpoint  strftime  gmtime  And  then  we  can  deploy  our  endpoint  endpoint  info  session  sagemaker  client  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  endpoint  dec  session  wait  for  endpoint  endpoint  name  First  we  need  to  serialize  the  input  data  In  this  case  we  want  to  send  the  test  data  as  csv  and  so  we  manually  do  this  Of  course  there  are  many  other  ways  to  do  this  payload  str  entry  for  entry  in  row  for  row  in  test  values  payload  join  join  row  for  row  in  payload  This  time  we  use  the  sagemaker  runtime  client  rather  than  the  sagemaker  client  so  that  we  can  invoke  the  endpoint  that  we  created  response  session  sagemaker  runtime  client  invoke  endpoint  EndpointName  endpoint  name  ContentType  text  csv  Body  payload  We  need  to  make  sure  that  we  deserialize  the  result  of  our  endpoint  call  result  response  Body  read  decode  utf  pred  np  from,amazon
string  result  sep  plt  scatter  test  pred  plt  xlabel  Median  Price  plt  ylabel  Predicted  Price  plt  title  Median  Price  vs  Predicted  Price  session  sagemaker  client  delete  endpoint  EndpointName  endpoint  name  First  we  will  remove  all  of  the  files  contained  in  the  data  dir  directory  rm  data  dir  And  then  we  delete  the  directory  itself  rmdir  data  dir  ,amazon
The  Fibonacci  sequence  was  originally  used  as  basic  model  for  rabbit  population  growth  The  Fibonacci  sequence  goes  like  this  13  The  nth  term  is  the  sum  of  the  previous  two  terms  In  the  above  example  the  next  term  would  be  21  13  Create  Fibonacci  sequence  generator  up  to  the  50th  term  The  program  will  then  prompt  the  user  for  up  to  50  Enter  Fibonacci  number  is  arr  None  for  in  range  50  arr  arr  def  fibonacci  num  for  in  range  num  arr  arr  arr  main  fibonacci  50  user  int  input  Enter  number  between  and  50  if  user  print  Too  low  elif  user  50  print  Too  high  else  print  Fibonacci  number  is  format  arr  user  Write  program  that  will  store  names  into  an  array  As  new  name  is  entered  it  will  be  added  to  the  end  of  the  array  The  user  can  keep  adding  names  until  they  enter  the  sentinel  value  ok  Once  this  has  been  done  the  program  will  display  any  duplicate  names  Enter  na,microsoft
me  Bill  Enter  name  Mary  Enter  name  Anisha  Enter  name  Mary  Enter  name  Bill  Enter  name  Mary  Enter  name  exit  Duplicates  Bill  Mary  duplicates  array  while  True  user  input  Enter  name  if  user  exit  print  Duplicates  format  set  duplicates  break  elif  user  in  array  duplicates  append  user  else  array  append  user  For  the  above  program  display  also  how  many  times  each  name  has  been  duplicated  in  descending  order  most  duplicated  name  first  If  there  is  tie  arrange  in  name  alphabetical  order  duplicates  array  while  True  user  input  Enter  name  if  user  exit  dictionary  for  item  in  set  duplicates  dictionary  item  duplicates  count  item  sorted  dictionary  print  Duplicates  format  dictionary  break  elif  user  in  array  duplicates  append  user  else  array  append  user  Create  two  dimensional  10  by  10  array  of  integers  Fill  the  array  with  random  numbers  in  the  range  to  15  Display  the  array  on  the  screen  ,microsoft
showing  the  numbers  import  random  arr  None  for  in  range  10  for  in  range  10  for  in  range  10  for  in  range  10  arr  random  randint  15  for  row  in  range  10  for  column  in  range  10  print  arr  row  column  end  print  print  Create  simple  treasure  hunt  game  Create  two  dimensional  10  by  10  array  of  integers  In  random  position  in  the  array  store  the  number  Repeat  Get  the  user  to  enter  coordinates  where  he  she  think  the  treasure  is  If  there  is  at  this  position  display  Bingo  Until  they  find  the  treasure  import  random  arr  None  for  in  range  10  for  in  range  10  arr  random  randint  random  randint  while  True  user  input  Enter  coordinates  row  column  user  split  row  int  row  column  int  column  if  arr  row  column  print  Bingo  break  For  the  above  treasure  hunt  game  add  feature  to  say  hot  cold  or  warm  depending  on  how  close  the  guess  was  to  the  location  import  random  arr  None  for  in  ra,microsoft
nge  10  for  in  range  10  random  randint  random  randint  arr  while  True  user  input  Enter  coordinates  row  column  user  split  row  int  row  column  int  column  if  arr  row  column  print  Bingo  break  else  dist  from  abs  row  abs  column  if  dist  from  print  Hot  elif  dist  from  print  Warm  else  print  Cold  Write  program  that  will  generate  random  playing  card  Hearts  Queen  Spades  when  the  return  key  is  pressed  Rather  than  generate  random  number  from  to  52  Create  two  random  numbers  one  for  the  suit  and  one  for  the  card  However  we  do  not  want  the  same  card  drawn  twice  Update  this  program  by  using  an  array  to  prevent  the  same  card  being  dealt  twice  from  the  pack  of  cards  Convert  this  code  into  procedure  DealCard  that  will  display  the  card  dealt  or  No  more  cards  Call  your  procedure  53  times  use  loop  from  random  import  randint  suits  Hearts  Spades  Diamond  Clubs  cards  Ace  10  10  11  12  ,microsoft
13  arr  def  dealcard  arr  if  len  arr  52  print  No  more  card  else  output  cards  randint  13  suits  randint  while  output  in  arr  output  cards  randint  13  suits  randint  arr  append  output  print  output  for  in  range  53  dealcard  arr  Play  game  that  draws  two  random  cards  see  above  The  player  then  decides  to  draw  or  stick  If  the  score  goes  over  21  the  player  loses  Keep  drawing  until  the  player  sticks  After  the  player  sticks  draw  two  computer  cards  If  the  player  beats  the  score  they  win  Extension  Aces  can  be  or  11  The  number  used  is  whichever  gets  the  highest  score  The  computer  generates  digit  code  The  user  types  in  digit  code  His  her  guess  The  computer  tells  them  how  many  digits  they  guessed  correctly  import  random  def  match  str1  str2  count  if  len  str1  len  str2  return  False  else  arr  for  in  range  len  str1  if  str1  str2  count  if  count  return  count  True  return  count  False ,microsoft
 computer  random  randint  1000  9999  computer  str  computer  while  True  user  int  input  Enter  digit  code  count  condition  match  computer  str  user  print  Number  of  correct  digits  format  count  if  condition  print  Bingo  break  For  the  above  digit  code  game  the  computer  tells  them  how  many  digits  are  guessed  correctly  in  the  correct  place  and  how  many  digits  have  been  guessed  correctly  but  in  the  wrong  place  The  user  gets  12  guesses  to  either  win  guess  the  right  code  or  lose  run  out  of  guesses  import  random  def  match  str1  str2  count  count1  if  len  str1  len  str2  return  False  else  arr  for  in  range  len  str1  if  str1  str2  count  else  arr  append  str2  for  in  set  arr  if  in  str1  count1  if  count  return  count  count1  True  return  count  count1  False  computer  random  randint  1000  9999  computer  str  computer  num  win  False  while  num  12  user  int  input  Enter  digit  code  correct  pos  num  exist ,microsoft
 condition  match  computer  str  user  print  Correct  number  correct  place  nCorrect  number  wrong  place  format  correct  pos  num  exist  if  condition  print  You  win  win  True  break  num  if  not  win  print  You  lose  Player  types  in  word  Player  has  to  guess  the  word  in  lives  The  display  would  look  like  this  Word  to  guess  You  have  lives  left  Letter  Word  to  guess  EE  You  have  lives  left  Letter  Word  to  guess  EE  You  have  lives  left  Letter  def  new  format  word  to  guess  actual  word  letter  condition  word  to  guess  list  word  to  guess  for  in  range  len  actual  word  if  actual  word  letter  and  word  to  guess  letter  condition  word  to  guess  letter  word  to  guess  join  word  to  guess  if  word  to  guess  count  condition  return  word  to  guess  condition  no  of  lives  player  input  word  to  guess  len  player  print  Word  to  guess  format  word  to  guess  while  no  of  lives  player  input  You  have  lives  left  Letter,microsoft
  format  no  of  lives  word  to  guess  condition  new  format  word  to  guess  player  player  if  condition  print  Word  to  guess  format  word  to  guess  print  You  win  break  elif  condition  print  Word  to  guess  format  word  to  guess  else  no  of  lives  print  Word  to  guess  format  word  to  guess  else  print  You  lose  ,microsoft
import  os  import  random  import  json  import  base64  import  subprocess  from  pickle  import  dump  from  shutil  import  rmtree  copy2  import  cv2  import  boto3  import  mxnet  as  mx  import  numpy  as  np  from  pandas  import  read  csv  import  sagemakersagemaker  session  sagemaker  Session  dataset  size  12000  img  size  128  shape  size  max  rows  14  max  cols  14  target  shape  circle  shapes  circle  triangle  square  img  width  img  height  img  size  time  os  makedirs  data  raw  random  seed  42  with  open  data  dataset  csv  as  dataset  dataset  write  img  path  target  for  in  range  dataset  size  target  number  of  shapes  filename  str  zfill  shape  file  content  img  dim  shp  dim  format  img  size  shape  size  for  in  range  random  randint  max  rows  for  in  range  random  randint  max  cols  shape  idx  random  randint  len  shapes  shape  shapes  shape  idx  if  shape  target  shape  target  number  of  shapes  file  content  file  content  shape  file  conte,amazon
nt  file  content  file  content  file  content  file  content  file  content  with  open  data  raw  format  filename  as  shape  file  shape  file  write  file  content  dataset  write  raw  png  format  filename  str  target  number  of  shapes  time  subprocess  call  java  cp  bin  shaper  all  jar  com  cosminsanda  shaper  compiler  Shaper2Image  source  dir  data  raw  shell  True  df  read  csv  data  dataset  csv  train  df  sample  frac  8333  random  state  42  validation  df  loc  df  index  isin  train  index  sample  frac  random  state  42  test  df  loc  np  logical  not  np  logical  xor  df  index  isin  train  index  df  index  isin  validation  index  def  transform  row  img  cv2  imread  data  format  row  img  path  img  mx  nd  array  img  img  img  astype  np  float32  img  mx  nd  transpose  img  img  img  255  label  np  float32  row  target  return  img  label  time  train  nd  transform  row  for  row  in  train  iterrows  validation  nd  transform  row  for  row  in  validation ,amazon
 iterrows  def  save  to  disk  data  type  os  makedirs  data  pickles  format  type  with  open  data  pickles  data  format  type  wb  as  out  dump  data  out  time  save  to  disk  train  nd  train  save  to  disk  validation  nd  validation  time  inputs  sagemaker  session  upload  data  path  data  pickles  bucket  redacted  key  prefix  sagemaker  demo  rmtree  test  True  os  makedirs  test  for  row  in  test  iterrows  os  makedirs  test  format  row  target  exist  ok  True  copy2  data  format  row  img  path  test  format  row  target  rmtree  data  True  estimator  sagemaker  mxnet  MXNet  object  counting  sagemaker  script  py  role  sagemaker  get  execution  role  train  instance  count  train  instance  type  ml  p2  xlarge  hyperparameters  epochs  py  version  py3  estimator  fit  inputs  predictor  estimator  deploy  ml  m4  xlarge  sagemaker  runtime  client  boto3  client  sagemaker  runtime  for  directory  in  os  listdir  test  batch  for  file  in  os  listdir  test  format  dire,amazon
ctory  with  open  test  format  directory  file  rb  as  image  file  batch  append  base64  b64encode  image  file  read  decode  utf  binary  json  json  dumps  batch  encode  utf  response  sagemaker  runtime  client  invoke  endpoint  EndpointName  predictor  endpoint  Body  binary  json  ContentType  application  json  Accept  application  json  Body  read  individual  predictions  json  loads  response  encoding  utf  total  detected  for  prediction  in  individual  predictions  total  if  int  prediction  int  directory  detected  print  Images  with  circles  Total  Detected  Accuracy  2f  format  directory  str  total  str  detected  detected  total  ,amazon
matplotlib  notebook  import  copy  import  numpy  as  np  import  matplotlib  pyplot  as  plt  import  matplotlib  animation  as  animation  from  matplotlib  patches  import  Polygon  from  matplotlib  collections  import  PatchCollection  Number  of  points  How  far  they  step  each  iteration  step  05  pts  np  random  rand  pts  np  array  choices  np  zeros  dtype  np  int  for  in  range  cs  np  random  choice  replace  False  choices  for  in  sorted  if  else  for  in  cs  choices  Find  the  third  point  of  equilateral  triangle  nearest  to  third  point  def  eq  tri  pts  cur  np  cos  np  pi  np  sin  np  pi  np  array  res  pts  np  linalg  multi  dot  pts  np  array  idx  np  linalg  norm  res  cur  axis  argmin  print  pts  cur  idx  res  idx  return  res  idx  Make  everyone  move  towards  their  nearest  equilateral  triangle  def  update  pts  new  pts  copy  copy  pts  for  in  range  tri  new  pts  choices  cur  tri  ideal  eq  tri  tri  cur  step  np  random  rand  new  pts  cur ,google
 ideal  cur  step  pts  new  pts  Animation  plt  close  fig  ax  plt  subplots  fig  set  size  inches  ax  set  aspect  equal  datalim  colors  200  np  random  rand  def  animate  update  pts  patches  ideals  ax  clear  for  in  range  tri  pts  choices  cur  tri  ideal  np  array  eq  tri  tri  cur  ideals  append  ideal  ideals  append  Polygon  np  concatenate  ideal  tri  True  polygon  Polygon  tri  True  patches  append  polygon  PatchCollection  patches  alpha  p2  PatchCollection  ideals  alpha  05  set  array  np  array  colors  p2  set  array  np  array  colors  ax  add  collection  ax  add  collection  p2  ax  scatter  np  array  ideals  colors  ax  autoscale  view  True  True  True  ani  animation  FuncAnimation  fig  animate  np  arange  200  interval  25  blit  True  print  choiceschoices  np  zeros  dtype  np  int  for  in  range  cs  np  random  choice  replace  False  sorted  if  else  for  in  cs  print  cs  choices  for  in  print  choicesa  np  random  rand  np  random  rand  np  array,google
  np  array  pts  cur  np  cos  np  pi  np  sin  np  pi  np  array  Find  the  third  point  of  equilateral  triangle  nearest  to  third  point  def  eq  tri  pts  cur  diff  pts  pts  res  pts  np  linalg  multi  dot  pts  np  array  print  pts  res  cur  np  repeat  cur  axis  print  pts  shape  res  shape  cur  shape  np  repeat  cur  axis  shape  idx  np  linalg  norm  res  np  repeat  cur  axis  axis  argmin  print  pts  cur  idx  return  res  idx  diff  pts  pts  print  for  in  diff  reshape  np  array  res  pts  np  linalg  multi  dot  pts  np  array  idx  np  linalg  norm  res  axis  argmin  print  foo  res  res  np  linalg  norm  res  axis  idx  res  eq  tri  print  res  xy  np  concatenate  res  colors  black  black  red  green  blue  print  xy  plt  close  fig  ax  plt  subplots  ax  scatter  xy  colors  ax  set  aspect  equal  datalim  plt  show  np  random  rand  np  array  shapeassert  Truedef  printeach  args  for  arg  in  args  print  arg  printeach  xy  diff  reshape  np  repeat  np  arra,google
y  axis  ,google
import  os  import  boto3  import  time  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  Now  let  define  the  S3  bucket  we  ll  used  for  the  remainder  of  this  example  bucket  your  s3  bucket  name  here  enter  your  s3  bucket  where  you  will  copy  data  and  model  artificats  prefix  sagemaker  DEMO  xgboost  place  to  upload  training  files  within  the  bucketimport  numpy  as  np  For  matrix  operations  and  numerical  processing  import  pandas  as  pd  For  munging  tabular  data  import  sklearn  as  sk  For  access  to  variety  of  machine  learning  models  import  matplotlib  pyplot  as  plt  For  charts  and  visualizations  from  IPython  display  import  Image  For  displaying  images  in  the  notebook  from  IPython  display  import  display  For  displaying  outputs  in  the  notebook  from  sklearn  datasets  import  dump  svmlight  file  For  outputting  data  to  libsvm  format  for  xgboost  from  time  import  gmtime  strftime  ,amazon
For  labeling  SageMaker  models  endpoints  etc  import  sys  For  writing  outputs  to  notebook  import  math  For  ceiling  function  import  json  For  parsing  hosting  output  import  io  For  working  with  stream  data  import  sagemaker  amazon  common  as  smac  For  protobuf  data  format  read  the  data  data  pd  read  csv  https  archive  ics  uci  edu  ml  machine  learning  databases  adult  adult  data  header  None  read  test  data  data  test  pd  read  csv  https  archive  ics  uci  edu  ml  machine  learning  databases  adult  adult  test  header  None  skiprows  set  column  names  data  columns  age  workclass  fnlwgt  education  education  num  marital  status  occupation  relationship  race  sex  capital  gain  capital  loss  hours  per  week  native  country  IncomeGroup  data  test  columns  age  workclass  fnlwgt  education  education  num  marital  status  occupation  relationship  race  sex  capital  gain  capital  loss  hours  per  week  native  country  IncomeGroup  set  dis,amazon
play  options  pd  set  option  display  max  columns  100  Make  sure  we  can  see  all  of  the  columns  pd  set  option  display  max  rows  Keep  the  output  on  one  page  disply  data  display  data  display  data  test  display  positive  and  negative  counts  display  data  iloc  14  value  counts  display  data  test  iloc  14  value  counts  Combine  the  two  datasets  to  convert  the  categorical  values  to  binary  indicators  data  combined  pd  concat  data  data  test  convert  the  categorical  variables  to  binary  indicators  data  combined  bin  pd  get  dummies  data  combined  prefix  workclass  education  marital  status  occupation  relationship  race  sex  native  country  IncomeGroup  drop  first  True  combine  the  income  50k  indicators  Income  50k  data  combined  bin  iloc  101  data  combined  bin  iloc  102  make  the  income  indicator  as  first  column  data  combined  bin  pd  concat  Income  50k  data  combined  bin  iloc  100  axis  Post  conversion  to  binary ,amazon
 split  the  data  sets  separately  data  bin  data  combined  bin  iloc  data  shape  data  test  bin  data  combined  bin  iloc  data  shape  display  the  data  sets  post  conversion  to  binary  indicators  display  data  bin  display  data  test  bin  count  number  of  positives  and  negatives  display  data  bin  iloc  value  counts  display  data  test  bin  iloc  value  counts  Split  the  data  randomly  as  80  for  training  and  remaining  20  and  save  them  locally  train  list  np  random  rand  len  data  bin  data  train  data  bin  train  list  data  val  data  bin  train  list  data  train  to  csv  formatted  train  csv  sep  header  False  index  False  save  training  data  data  val  to  csv  formatted  val  csv  sep  header  False  index  False  save  validation  data  data  test  bin  to  csv  formatted  test  csv  sep  header  False  index  False  save  test  data  train  file  formatted  train  csv  val  file  formatted  val  csv  boto3  Session  resource  s3  Bucket  bucket  O,amazon
bject  os  path  join  prefix  train  train  file  upload  file  train  file  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  val  val  file  upload  file  val  file  xgboost  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  xgboost  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  xgboost  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  xgboost  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  xgboost  latest  import  boto3  from  time  import  gmtime  strftime  job  name  DEMO  xgboost  single  censusincome  strftime  gmtime  print  Training  job  job  name  create  training  params  AlgorithmSpecification  TrainingImage  xgboost  containers  boto3  Session  region  name  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  s3  single  xgboost  format  bucket  prefix  ResourceConfig  InstanceCount  InstanceType  ml  m4  4xlarge  VolumeSizeInGB  20  TrainingJobName  job  name  Hyp,amazon
erParameters  max  depth  eta  gamma  min  child  weight  silent  objective  binary  logistic  eval  metric  auc  num  round  20  StoppingCondition  MaxRuntimeInSeconds  60  60  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  S3DataDistributionType  FullyReplicated  ContentType  csv  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  val  format  bucket  prefix  S3DataDistributionType  FullyReplicated  ContentType  csv  CompressionType  None  time  region  boto3  Session  region  name  sm  boto3  client  sagemaker  sm  create  training  job  create  training  params  status  sm  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  status  sm  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  if  status  Failed  message  sm  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  ,amazon
with  the  following  error  format  message  raise  Exception  Training  job  failed  model  name  job  name  mdl  xgboost  hosting  container  Image  xgboost  containers  boto3  Session  region  name  ModelDataUrl  sm  describe  training  job  TrainingJobName  job  name  ModelArtifacts  S3ModelArtifacts  Environment  this  is  create  model  response  sm  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  xgboost  hosting  container  print  create  model  response  ModelArn  print  sm  describe  training  job  TrainingJobName  job  name  ModelArtifacts  S3ModelArtifacts  from  time  import  gmtime  strftime  endpoint  config  name  DEMO  XGBoostEndpointConfig  strftime  gmtime  print  endpoint  config  name  create  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  InitialVariantWeight  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  Conf,amazon
ig  Arn  create  endpoint  config  response  EndpointConfigArn  time  import  time  endpoint  name  DEMO  XGBoostEndpoint  strftime  gmtime  print  endpoint  name  create  endpoint  response  sm  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  print  create  endpoint  response  EndpointArn  resp  sm  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  while  status  Creating  time  sleep  60  resp  sm  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  print  Arn  resp  EndpointArn  print  Status  status  runtime  boto3  client  runtime  sagemaker  Simple  function  to  create  csv  from  our  numpy  array  def  np2csv  arr  csv  io  BytesIO  np  savetxt  csv  arr  delimiter  fmt  return  csv  getvalue  decode  rstrip  Function  to  generate  prediction  through  sample  data  def  do  predict  data  endpoint  name  content  type  payload  np2csv  data  response  r,amazon
untime  invoke  endpoint  EndpointName  endpoint  name  ContentType  content  type  Body  payload  result  response  Body  read  result  result  decode  utf  result  result  split  preds  float  num  for  num  in  result  return  preds  Function  to  iterate  through  larger  data  set  and  generate  batch  predictions  def  batch  predict  data  batch  size  endpoint  name  content  type  items  len  data  arrs  for  offset  in  range  items  batch  size  if  offset  batch  size  items  datav  data  iloc  offset  offset  batch  size  as  matrix  results  do  predict  datav  endpoint  name  content  type  arrs  extend  results  else  datav  data  iloc  offset  items  as  matrix  arrs  extend  do  predict  datav  endpoint  name  content  type  sys  stdout  write  return  arrs  read  the  saved  data  for  scoring  data  train  pd  read  csv  formatted  train  csv  sep  header  None  data  test  pd  read  csv  formatted  test  csv  sep  header  None  data  val  pd  read  csv  formatted  val  csv  sep  header  ,amazon
None  preds  train  xgb  batch  predict  data  train  iloc  1000  endpoint  name  text  csv  preds  val  xgb  batch  predict  data  val  iloc  1000  endpoint  name  text  csv  preds  test  xgb  batch  predict  data  test  iloc  1000  endpoint  name  text  csv  from  sklearn  metrics  import  roc  auc  score  train  labels  data  train  iloc  val  labels  data  val  iloc  test  labels  data  test  iloc  print  Training  AUC  roc  auc  score  train  labels  preds  train  xgb  9161  print  Validation  AUC  roc  auc  score  val  labels  preds  val  xgb  9065  print  Test  AUC  roc  auc  score  test  labels  preds  test  xgb  9112  prefix  sagemaker  DEMO  linear  subfolder  inside  the  data  bucket  to  be  used  for  Linear  Learner  data  train  pd  read  csv  formatted  train  csv  sep  header  None  data  test  pd  read  csv  formatted  test  csv  sep  header  None  data  val  pd  read  csv  formatted  val  csv  sep  header  None  train  data  train  iloc  as  matrix  train  data  train  iloc  as  matrix  va,amazon
l  data  val  iloc  as  matrix  val  data  val  iloc  as  matrix  test  data  test  iloc  as  matrix  test  data  test  iloc  as  matrix  train  file  linear  train  data  io  BytesIO  smac  write  numpy  to  dense  tensor  train  astype  float32  train  astype  float32  seek  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  train  file  upload  fileobj  validation  file  linear  validation  data  io  BytesIO  smac  write  numpy  to  dense  tensor  val  astype  float32  val  astype  float32  seek  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  train  file  upload  fileobj  linear  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  linear  learner  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  linear  learner  latest  linea,amazon
r  job  DEMO  linear  time  strftime  time  gmtime  print  Job  name  is  linear  job  linear  training  params  RoleArn  role  TrainingJobName  linear  job  AlgorithmSpecification  TrainingImage  linear  containers  boto3  Session  region  name  TrainingInputMode  File  ResourceConfig  InstanceCount  InstanceType  ml  c4  2xlarge  VolumeSizeInGB  10  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  S3DataDistributionType  ShardedByS3Key  CompressionType  None  RecordWrapperType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  prefix  S3DataDistributionType  FullyReplicated  CompressionType  None  RecordWrapperType  None  OutputDataConfig  S3OutputPath  s3  format  bucket  prefix  HyperParameters  feature  dim  100  mini  batch  size  100  predictor  type  binary  classifier  epochs  10  num  models  32  loss  logistic  StoppingCondition  MaxRuntimeInSeconds  60  ,amazon
60  print  linear  job  time  region  boto3  Session  region  name  sm  boto3  client  sagemaker  sm  create  training  job  linear  training  params  status  sm  describe  training  job  TrainingJobName  linear  job  TrainingJobStatus  print  status  sm  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  linear  job  if  status  Failed  message  sm  describe  training  job  TrainingJobName  linear  job  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  linear  hosting  container  Image  linear  containers  boto3  Session  region  name  ModelDataUrl  sm  describe  training  job  TrainingJobName  linear  job  ModelArtifacts  S3ModelArtifacts  174872318107  dkr  ecr  us  west  amazonaws  com  create  model  response  sm  create  model  ModelName  linear  job  ExecutionRoleArn  role  PrimaryContainer  linear  hosting  container  print  create  model  response  ModelArn  linear  endpoint  config  DEMO  linear  end,amazon
point  config  time  strftime  time  gmtime  print  linear  endpoint  config  create  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  linear  endpoint  config  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  linear  job  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  linear  endpoint  DEMO  linear  endpoint  time  strftime  time  gmtime  print  linear  endpoint  create  endpoint  response  sm  create  endpoint  EndpointName  linear  endpoint  EndpointConfigName  linear  endpoint  config  print  create  endpoint  response  EndpointArn  resp  sm  describe  endpoint  EndpointName  linear  endpoint  status  resp  EndpointStatus  print  Status  status  sm  get  waiter  endpoint  in  service  wait  EndpointName  linear  endpoint  resp  sm  describe  endpoint  EndpointName  linear  endpoint  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Status  status  if  status,amazon
  InService  raise  Exception  Endpoint  creation  did  not  succeed  def  np2csv  arr  csv  io  BytesIO  np  savetxt  csv  arr  delimiter  fmt  return  csv  getvalue  decode  rstrip  Function  to  generate  prediction  through  sample  data  def  do  predict  linear  data  endpoint  name  content  type  payload  np2csv  data  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  content  type  Body  payload  result  json  loads  response  Body  read  decode  preds  score  for  in  result  predictions  return  preds  Function  to  iterate  through  larger  data  set  and  generate  batch  predictions  def  batch  predict  linear  data  batch  size  endpoint  name  content  type  items  len  data  arrs  for  offset  in  range  items  batch  size  if  offset  batch  size  items  datav  data  iloc  offset  offset  batch  size  as  matrix  results  do  predict  linear  datav  endpoint  name  content  type  arrs  extend  results  else  datav  data  iloc  offset  items  as  matrix  arrs  e,amazon
xtend  do  predict  linear  datav  endpoint  name  content  type  sys  stdout  write  return  arrs  Predict  on  Training  Data  preds  train  lin  batch  predict  linear  data  train  iloc  100  linear  endpoint  text  csv  Predict  on  Validation  Data  preds  val  lin  batch  predict  linear  data  val  iloc  100  linear  endpoint  text  csv  Predict  on  Test  Data  preds  test  lin  batch  predict  linear  data  test  iloc  100  linear  endpoint  text  csv  print  Training  AUC  roc  auc  score  train  labels  preds  train  lin  9091  print  Validation  AUC  roc  auc  score  val  labels  preds  val  lin  8998  print  Test  AUC  roc  auc  score  test  labels  preds  test  lin  9033  ens  train  np  array  preds  train  xgb  np  array  preds  train  lin  ens  val  np  array  preds  val  xgb  np  array  preds  val  lin  ens  test  np  array  preds  test  xgb  np  array  preds  test  lin  Print  AUC  of  the  combined  model  print  Train  AUC  Xgboost  round  roc  auc  score  train  labels  preds  train  xg,amazon
b  print  Train  AUC  Linear  round  roc  auc  score  train  labels  preds  train  lin  print  Train  AUC  Ensemble  round  roc  auc  score  train  labels  ens  train  print  print  Validation  AUC  Xgboost  round  roc  auc  score  val  labels  preds  val  xgb  print  Validation  AUC  Linear  round  roc  auc  score  val  labels  preds  val  lin  print  Validation  AUC  Ensemble  round  roc  auc  score  val  labels  ens  val  print  print  Test  AUC  Xgboost  round  roc  auc  score  test  labels  preds  test  xgb  print  Test  AUC  Linear  round  roc  auc  score  test  labels  preds  test  lin  print  Test  AUC  Ensemble  round  roc  auc  score  test  labels  ens  test  final  pd  concat  data  test  iloc  pd  DataFrame  ens  test  axis  final  to  csv  Xgboost  linear  ensemble  prediction  csv  sep  header  False  index  False  sm  delete  endpoint  EndpointName  endpoint  name  sm  delete  endpoint  EndpointName  linear  endpoint  ,amazon
install  packages  RMySQL  library  RMySQL  Enter  the  values  for  you  database  connection  dsn  database  compose  for  example  BLUDB  dsn  hostname  sl  us  south  portal  22  dblayer  com  for  example  mydbinstance  cz6pjylrdjko  us  east  rds  amazonaws  com  dsn  port  39024  for  example  3306  without  quotation  marks  dsn  uid  admin  for  example  user1  dsn  pwd  moreal  for  example  7dBZ3jWt9xN6  o0JiX  conn  dbConnect  MySQL  user  dsn  uid  password  dsn  pwd  host  dsn  hostname  port  dsn  port  conncreate  command  paste  CREATE  DATABASE  IF  NOT  EXISTS  dsn  database  sep  use  command  paste  USE  dsn  database  sep  dbSendQuery  conn  create  command  dbSendQuery  conn  use  command  dbSendQuery  conn  DROP  TABLE  IF  EXISTS  Cars  dbSendQuery  conn  CREATE  TABLE  Cars  Id  INTEGER  PRIMARY  KEY  Name  VARCHAR  20  Price  INT  dbSendQuery  conn  INSERT  INTO  Cars  VALUES  Audi  52642  dbSendQuery  conn  INSERT  INTO  Cars  VALUES  Mercedes  57127  dbSendQuery  conn  INSERT  INT,ibm
O  Cars  VALUES  Skoda  9000  dbSendQuery  conn  INSERT  INTO  Cars  VALUES  Volvo  29000  dbSendQuery  conn  INSERT  INTO  Cars  VALUES  Bentley  350000  dbSendQuery  conn  INSERT  INTO  Cars  VALUES  Citroen  21000  dbSendQuery  conn  INSERT  INTO  Cars  VALUES  Hummer  41400  dbSendQuery  conn  INSERT  INTO  Cars  VALUES  Volkswagen  21600  query  SELECT  FROM  Cars  rs  dbSendQuery  conn  query  df  fetch  rs  dfdbDisconnect  conn  ,ibm
import  boto3  from  sagemaker  import  get  execution  role  from  time  import  gmtime  strftimerole  get  execution  role  smclient  boto3  client  sagemaker  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  xgboost  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  xgboost  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  xgboost  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  xgboost  latest  container  containers  boto3  Session  region  name  saved  model  smclient  describe  training  job  TrainingJobName  xgb  hpo  mar01  2139  a4e92  66ad85cb  34cf  4cb5  8a5d  7c9a71  model  url  saved  model  ModelArtifacts  S3ModelArtifacts  print  model  url  deploy  model  name  xgboost  hpo  deploymodel  strftime  gmtime  primary  container  Image  container  ModelDataUrl  model  url  create  model  response  smclient  create  model  ModelName  deploy  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  prin,amazon
t  create  model  response  ModelArn  create  endpoint  config  endpoint  config  name  xgboost  hpo  endpoint  config  strftime  gmtime  print  endpoint  config  name  create  endpoint  config  response  smclient  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  t2  medium  InitialInstanceCount  ModelName  deploy  model  name  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  create  endpoint  endpoint  name  xgboost  hpo  endpoint  strftime  gmtime  print  endpoint  name  create  endpoint  response  smclient  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  print  create  endpoint  response  EndpointArn  resp  smclient  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  try  smclient  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  finally  resp  smclient  describe  endpoin,amazon
t  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Create  endpoint  ended  with  status  status  if  status  InService  message  smclient  describe  endpoint  EndpointName  endpoint  name  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Endpoint  creation  did  not  succeed  ,amazon
matplotlib  inline  import  tensorflow  as  tf  import  matplotlib  pyplot  as  plt  import  numpy  as  np  from  tensorflow  examples  tutorials  mnist  import  input  data  mnist  input  data  read  data  sets  MNIST  data  one  hot  True  def  weight  variable  shape  name  return  tf  Variable  tf  truncated  normal  shape  shape  stddev  name  def  bias  variable  shape  name  return  tf  Variable  tf  constant  shape  shape  name  def  conv2d  return  tf  nn  conv2d  strides  padding  SAME  def  deconv2d  output  shape  return  tf  nn  conv2d  transpose  output  shape  strides  padding  SAME  def  build  graph  origin  tf  reshape  28  28  origin  noise  tf  reshape  noise  28  28  conv1  weight  variable  16  conv1  conv1  bias  variable  16  conv1  conv1  tf  nn  relu  tf  add  conv2d  origin  noise  conv1  conv1  conv2  weight  variable  16  32  conv2  conv2  bias  variable  32  conv2  conv2  tf  nn  relu  tf  add  conv2d  conv1  conv2  conv2  code  layer  conv2  print  code  layer  shape  conv2  get,microsoft
  shape  conv1  weight  variable  16  32  conv1  conv1  bias  variable  conv1  output  shape  conv1  tf  stack  tf  shape  14  14  16  conv1  tf  nn  relu  deconv2d  conv2  conv1  output  shape  conv1  conv2  weight  variable  16  conv2  conv2  bias  variable  16  conv2  output  shape  conv2  tf  stack  tf  shape  28  28  conv2  tf  nn  relu  deconv2d  conv1  conv2  output  shape  conv2  reconstruct  conv2  print  reconstruct  layer  shape  reconstruct  get  shape  return  origin  code  layer  reconstruct  tf  reset  default  graph  tf  placeholder  tf  float32  shape  None  784  noise  tf  placeholder  tf  float32  shape  None  784  origin  code  layer  reconstruct  build  graph  cost  tf  reduce  mean  tf  pow  reconstruct  origin  optimizer  tf  train  AdamOptimizer  01  minimize  cost  sess  tf  InteractiveSession  batch  size  50  init  op  tf  global  variables  initializer  sess  run  init  op  for  epoch  in  range  10000  batch  mnist  train  next  batch  batch  size  batch  raw  batch  batch  noise ,microsoft
 batch  np  random  randn  batch  size  784  if  epoch  1500  if  epoch  100  print  step  loss  epoch  cost  eval  feed  dict  batch  raw  noise  batch  noise  else  if  epoch  1000  print  step  loss  epoch  cost  eval  feed  dict  batch  raw  noise  batch  noise  optimizer  run  feed  dict  batch  raw  noise  batch  noise  print  final  loss  cost  eval  feed  dict  mnist  test  images  noise  mnist  test  images  def  plot  reconstruct  origin  img  reconstruct  img  10  plt  figure  figsize  10  for  in  range  ax  plt  subplot  plt  imshow  origin  img  reshape  28  28  plt  gray  ax  get  xaxis  set  visible  False  ax  get  yaxis  set  visible  False  ax  plt  subplot  plt  imshow  reconstruct  img  reshape  28  28  plt  gray  ax  get  xaxis  set  visible  False  ax  get  yaxis  set  visible  False  plt  show  test  size  10  test  origin  img  mnist  test  images  test  size  test  reconstruct  img  np  reshape  reconstruct  eval  feed  dict  test  origin  img  noise  test  origin  img  np  random  r,microsoft
andn  10  784  28  28  plot  reconstruct  test  origin  img  np  random  randn  10  784  test  reconstruct  img  test  size  10  test  origin  img  mnist  test  images  test  size  test  reconstruct  img  np  reshape  reconstruct  eval  feed  dict  test  origin  img  noise  test  origin  img  np  random  randn  10  784  28  28  plot  reconstruct  test  origin  img  np  random  randn  10  784  test  reconstruct  img  ,microsoft
python  get  model  py  ls  container  model  uncomment  for  linux  installation  if  required  wget  qO  https  get  docker  com  sh  image  my  model  v1  docker  build  container  tag  image  container  run  local  server  sh  image  curl  GET  http  localhost  8080  ping  curl  POST  image  pwd  images  dog  jpg  http  localhost  8080  predict  curl  POST  image  pwd  images  hotdog  jpg  http  localhost  8080  predict  docker  kill  mylocalservice  container  send  container  to  amazon  sh  image  S3  prefix  prefix  DEMO  scikit  byo  iris  Define  IAM  role  import  boto3  import  re  import  os  import  numpy  as  np  import  pandas  as  pd  from  sagemaker  import  get  execution  role  role  get  execution  role  import  sagemaker  as  sage  from  time  import  gmtime  strftime  sess  sage  Session  from  sagemaker  predictor  import  csv  serializer  predictor  tree  deploy  ml  m4  xlarge  serializer  csv  serializer  shape  pd  read  csv  data  iris  csv  header  None  import  itertools  50  fo,amazon
r  in  range  40  for  in  range  10  indices  for  in  itertools  product  test  data  shape  iloc  indices  test  test  data  iloc  test  test  data  iloc  print  predictor  predict  test  values  decode  utf  sess  delete  endpoint  predictor  endpoint  account  sess  boto  session  client  sts  get  caller  identity  Account  region  sess  boto  session  region  name  image  dkr  ecr  amazonaws  com  decision  trees  sample  latest  format  account  region  tree  sage  estimator  Estimator  image  role  ml  c4  2xlarge  output  path  s3  output  format  sess  default  bucket  sagemaker  session  sess  tree  fit  data  location  WORK  DIRECTORY  data  data  location  sess  upload  data  WORK  DIRECTORY  key  prefix  prefix  ,amazon
import  numpy  as  np  import  tensorflow  as  tf  import  matplotlib  pyplot  as  plt  from  tensorflow  examples  tutorials  mnist  import  input  data  matplotlib  inline  mnist  input  data  read  data  sets  MNIST  data  one  hot  True  rho  hat  np  linspace  1e  1e  100  rho  kl  div  rho  np  log  rho  rho  hat  rho  np  log  rho  rho  hat  plt  plot  rho  hat  kl  div  plt  xlabel  rho  hat  plt  ylabel  kl  div  def  weight  variable  shape  name  return  tf  Variable  tf  truncated  normal  shape  shape  stddev  name  def  bias  variable  shape  name  return  tf  Variable  tf  zeros  shape  shape  name  def  build  sae  weight  variable  784  300  bias  variable  300  tf  nn  sigmoid  tf  add  tf  matmul  weight  variable  300  30  bias  variable  30  tf  nn  sigmoid  tf  add  tf  matmul  weight  variable  30  300  bias  variable  300  tf  nn  sigmoid  tf  add  tf  matmul  weight  variable  300  784  bias  variable  784  tf  nn  sigmoid  tf  add  tf  matmul  return  def  kl  div  rho  rho  hat  inv,microsoft
rho  tf  subtract  tf  constant  rho  invrhohat  tf  subtract  tf  constant  rho  hat  logrho  tf  add  logfunc  rho  rho  hat  logfunc  invrho  invrhohat  return  logrho  def  logfunc  x2  return  tf  multiply  tf  log  tf  div  x2  def  plot  reconstruct  origin  img  reconstruct  img  10  plt  figure  figsize  10  for  in  range  display  original  ax  plt  subplot  plt  imshow  origin  img  reshape  28  28  plt  gray  ax  get  xaxis  set  visible  False  ax  get  yaxis  set  visible  False  display  reconstruction  ax  plt  subplot  plt  imshow  reconstruct  img  reshape  28  28  plt  gray  ax  get  xaxis  set  visible  False  ax  get  yaxis  set  visible  False  plt  show  tf  reset  default  graph  sess  tf  InteractiveSession  tf  placeholder  tf  float32  shape  None  784  reconstruct  build  sae  alpha  5e  beta  5e  kl  div  loss  reduce  lambda  map  lambda  tf  reduce  sum  kl  div  02  tf  reduce  mean  kl  div  loss  tf  reduce  sum  kl  div  02  tf  reduce  mean  l2  loss  reduce  lambda  map  ,microsoft
lambda  tf  nn  l2  loss  loss  tf  reduce  mean  tf  pow  reconstruct  alpha  l2  loss  beta  kl  div  loss  optimizer  tf  train  AdamOptimizer  01  minimize  loss  init  op  tf  global  variables  initializer  sess  run  init  op  for  in  range  20000  batch  mnist  train  next  batch  60  if  100  print  step  loss  loss  eval  feed  dict  mnist  test  images  optimizer  run  feed  dict  batch  print  final  loss  loss  eval  feed  dict  mnist  test  images  for  in  print  average  output  activation  value  tf  reduce  mean  eval  feed  dict  mnist  test  images  trainimg  mnist  train  images  trainlabel  mnist  train  labels  output  nd  reconstruct  eval  feed  dict  mnist  train  images  for  in  curr  img  np  reshape  trainimg  28  28  ae  img  np  reshape  output  nd  28  28  curr  label  np  argmax  trainlabel  plt  matshow  curr  img  cmap  plt  get  cmap  gray  plt  matshow  ae  img  cmap  plt  get  cmap  gray  ,microsoft
time  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  bucket  name  customize  to  your  bucket  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  image  classification  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  image  classification  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  image  classification  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  image  classification  latest  training  image  containers  boto3  Session  region  name  print  training  image  import  os  import  urllib  request  import  boto3  def  download  url  filename  url  split  if  not  os  path  exists  filename  urllib  request  urlretrieve  url  filename  def  upload  to  s3  channel  file  s3  boto3  resource  s3  data  open  file  rb  key  channel  file  s3  Bucket  bucket  put  object  Key  key  Body  data  caltech  256  download  http  data  mxnet  io  data  caltech  256  calte,amazon
ch  256  60  train  rec  download  http  data  mxnet  io  data  caltech  256  caltech  256  60  val  rec  upload  to  s3  validation  caltech  256  60  val  rec  upload  to  s3  train  caltech  256  60  train  rec  The  algorithm  supports  multiple  network  depth  number  of  layers  They  are  18  34  50  101  152  and  200  For  this  training  we  will  use  18  layers  num  layers  18  we  need  to  specify  the  input  image  shape  for  the  training  data  image  shape  224  224  we  also  need  to  specify  the  number  of  training  samples  in  the  training  set  for  caltech  it  is  15420  num  training  samples  15420  specify  the  number  of  output  classes  num  classes  257  batch  size  for  training  mini  batch  size  128  number  of  epochs  epochs  learning  rate  learning  rate  01  top  Since  we  are  using  transfer  learning  we  set  use  pretrained  model  to  so  that  weights  can  be  initialized  with  pre  trained  weights  use  pretrained  model  time  import  time  impo,amazon
rt  boto3  from  time  import  gmtime  strftime  s3  boto3  client  s3  create  unique  job  name  job  name  prefix  sagemaker  imageclassification  notebook  timestamp  time  strftime  time  gmtime  job  name  job  name  prefix  timestamp  training  params  specify  the  training  docker  image  AlgorithmSpecification  TrainingImage  training  image  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  s3  output  format  bucket  job  name  prefix  ResourceConfig  InstanceCount  InstanceType  ml  p2  8xlarge  VolumeSizeInGB  50  TrainingJobName  job  name  HyperParameters  image  shape  image  shape  num  layers  str  num  layers  num  training  samples  str  num  training  samples  num  classes  str  num  classes  mini  batch  size  str  mini  batch  size  epochs  str  epochs  learning  rate  str  learning  rate  use  pretrained  model  str  use  pretrained  model  StoppingCondition  MaxRuntimeInSeconds  360000  Training  data  should  be  inside  subdirectory  called  train  Validation ,amazon
 data  should  be  inside  subdirectory  called  validation  The  algorithm  currently  only  supports  fullyreplicated  model  where  data  is  copied  onto  each  machine  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  recordio  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  recordio  CompressionType  None  print  Training  job  name  format  job  name  print  nInput  Data  Location  format  training  params  InputDataConfig  DataSource  S3DataSource  create  the  Amazon  SageMaker  training  job  sagemaker  boto3  client  service  name  sagemaker  sagemaker  create  training  job  training  params  confirm  that  the  training  job  has  started  status  sagemaker  describe  training  job  TrainingJobName  job  name  ,amazon
TrainingJobStatus  print  Training  job  current  status  format  status  try  wait  for  the  job  to  finish  and  report  the  ending  status  sagemaker  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  except  print  Training  failed  to  start  if  exception  is  raised  that  means  it  has  failed  message  sagemaker  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  time  import  boto3  from  time  import  gmtime  strftime  sage  boto3  Session  client  service  name  sagemaker  model  name  test  image  classification  model  print  mo,amazon
del  name  info  sage  describe  training  job  TrainingJobName  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  data  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  image  classification  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  image  classification  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  image  classification  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  image  classification  latest  hosting  image  containers  boto3  Session  region  name  primary  container  Image  hosting  image  ModelDataUrl  model  data  create  model  response  sage  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  timestamp  time  strftime  time  gmtime  endpoint  config  name  job  name  prefix  epc  timestamp  endpoint  config  response  sage  create  endpoint  config ,amazon
 EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  configuration  name  format  endpoint  config  name  print  Endpoint  configuration  arn  format  endpoint  config  response  EndpointConfigArn  time  import  time  timestamp  time  strftime  time  gmtime  endpoint  name  job  name  prefix  ep  timestamp  print  Endpoint  name  format  endpoint  name  endpoint  params  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  endpoint  response  sagemaker  create  endpoint  endpoint  params  print  EndpointArn  format  endpoint  response  EndpointArn  get  the  status  of  the  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  response  EndpointStatus  print  EndpointStatus  format  status  wait  until  the  status  has  changed  sagemaker  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  print  the  st,amazon
atus  of  the  endpoint  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  endpoint  response  EndpointStatus  print  Endpoint  creation  ended  with  EndpointStatus  format  status  if  status  InService  raise  Exception  Endpoint  creation  failed  import  boto3  runtime  boto3  Session  client  service  name  runtime  sagemaker  wget  tmp  test  jpg  http  www  vision  caltech  edu  Image  Datasets  Caltech256  images  008  bathtub  008  0007  jpg  file  name  tmp  test  jpg  test  image  from  IPython  display  import  Image  Image  file  name  import  json  import  numpy  as  np  with  open  file  name  rb  as  payload  read  payload  bytearray  payload  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  image  Body  payload  result  response  Body  read  result  will  be  in  json  format  and  convert  it  to  ndarray  result  json  loads  result  the  result  will  output  the  probabilities  for  all  classes  find  the,amazon
  class  with  maximum  probability  and  print  the  class  index  index  np  argmax  result  object  categories  ak47  american  flag  backpack  baseball  bat  baseball  glove  basketball  hoop  bat  bathtub  bear  beer  mug  billiards  binoculars  birdbath  blimp  bonsai  101  boom  box  bowling  ball  bowling  pin  boxing  glove  brain  101  breadmaker  buddha  101  bulldozer  butterfly  cactus  cake  calculator  camel  cannon  canoe  car  tire  cartman  cd  centipede  cereal  box  chandelier  101  chess  board  chimp  chopsticks  cockroach  coffee  mug  coffin  coin  comet  computer  keyboard  computer  monitor  computer  mouse  conch  cormorant  covered  wagon  cowboy  hat  crab  101  desk  globe  diamond  ring  dice  dog  dolphin  101  doorknob  drinking  straw  duck  dumb  bell  eiffel  tower  electric  guitar  101  elephant  101  elk  ewer  101  eyeglasses  fern  fighter  jet  fire  extinguisher  fire  hydrant  fire  truck  fireworks  flashlight  floppy  disk  football  helmet  french  horn  fried  e,amazon
gg  frisbee  frog  frying  pan  galaxy  gas  pump  giraffe  goat  golden  gate  bridge  goldfish  golf  ball  goose  gorilla  grand  piano  101  grapes  grasshopper  guitar  pick  hamburger  hammock  harmonica  harp  harpsichord  hawksbill  101  head  phones  helicopter  101  hibiscus  homer  simpson  horse  horseshoe  crab  hot  air  balloon  hot  dog  hot  tub  hourglass  house  fly  human  skeleton  hummingbird  ibis  101  ice  cream  cone  iguana  ipod  iris  jesus  christ  joy  stick  kangaroo  101  kayak  ketch  101  killer  whale  knife  ladder  laptop  101  lathe  leopards  101  license  plate  lightbulb  light  house  lightning  llama  101  mailbox  mandolin  mars  mattress  megaphone  menorah  101  microscope  microwave  minaret  minotaur  motorbikes  101  mountain  bike  mushroom  mussels  necktie  octopus  ostrich  owl  palm  pilot  palm  tree  paperclip  paper  shredder  pci  card  penguin  people  pez  dispenser  photocopier  picnic  table  playing  card  porcupine  pram  praying  mantis  pyrami,amazon
d  raccoon  radio  telescope  rainbow  refrigerator  revolver  101  rifle  rotary  phone  roulette  wheel  saddle  saturn  school  bus  scorpion  101  screwdriver  segway  self  propelled  lawn  mower  sextant  sheet  music  skateboard  skunk  skyscraper  smokestack  snail  snake  sneaker  snowmobile  soccer  ball  socks  soda  can  spaghetti  speed  boat  spider  spoon  stained  glass  starfish  101  steering  wheel  stirrups  sunflower  101  superman  sushi  swan  swiss  army  knife  sword  syringe  tambourine  teapot  teddy  bear  teepee  telephone  box  tennis  ball  tennis  court  tennis  racket  theodolite  toaster  tomato  tombstone  top  hat  touring  bike  tower  pisa  traffic  light  treadmill  triceratops  tricycle  trilobite  101  tripod  shirt  tuning  fork  tweezer  umbrella  101  unicorn  vcr  video  projector  washing  machine  watch  101  waterfall  watermelon  welding  mask  wheelbarrow  windmill  wine  bottle  xylophone  yarmulke  yo  yo  zebra  airplanes  101  car  side  101  faces  easy  ,amazon
101  greyhound  tennis  shoes  toad  clutter  print  Result  label  object  categories  index  probability  str  result  index  sage  delete  endpoint  EndpointName  endpoint  name  ,amazon
from  IPython  core  debugger  import  set  trace  import  azure  mgmt  consumption  import  adal  from  msrestazure  azure  active  directory  import  AADTokenCredentials  from  dotenv  import  load  dotenv  find  dotenv  import  os  load  dotenv  find  dotenv  Parameters  need  for  API  subscription  os  getenv  AZURE  SUBSCRIPTION  ID  tenant  os  getenv  AZURE  TENANT  ID  client  id  os  getenv  AZURE  CLIENT  ID  client  secret  os  getenv  AZURE  CLIENT  SECRET  def  authenticate  client  key  tenant  client  id  client  secret  Authenticate  using  service  principal  key  authority  host  uri  https  login  microsoftonline  com  authority  uri  authority  host  uri  tenant  resource  uri  https  management  core  windows  net  context  adal  AuthenticationContext  authority  uri  api  version  None  mgmt  token  context  acquire  token  with  client  credentials  resource  uri  client  id  client  secret  credentials  AADTokenCredentials  mgmt  token  client  id  return  credentialscredentials  auth,microsoft
enticate  client  key  tenant  client  id  client  secret  client  azure  mgmt  consumption  ConsumptionManagementClient  credentials  subscription  client  operations  list  for  op  in  client  operations  list  print  op  dict  details  for  detail  in  client  usage  details  list  by  billing  period  billing  period  name  201808  expand  properties  additionalProperties  details  extend  detail  dict  import  pandas  as  pd  import  numpy  as  np  df  pd  DataFrame  details  df  billing  period  id  dfdf  pretax  cost  rounded  df  pretax  cost  astype  float  round  df  billing  period  id  unique  df  groupby  by  billing  period  id  pretax  cost  rounded  sum  df  groupby  by  billing  period  id  pretax  cost  sum  df  Only  EA  subscriptions  are  supported  for  this  request  sheet  client  price  sheet  get  df  to  excel  invoice  201808  xlsx  ,microsoft
import  os  import  sagemaker  from  sagemaker  import  get  execution  role  sagemaker  session  sagemaker  Session  role  get  execution  role  import  utils  utils  cifar10  download  inputs  sagemaker  session  upload  data  path  tmp  cifar10  data  key  prefix  data  DEMO  cifar10  from  sagemaker  tensorflow  import  TensorFlow  source  dir  os  path  join  os  getcwd  source  dir  estimator  TensorFlow  entry  point  resnet  cifar  10  py  source  dir  source  dir  role  role  framework  version  hyperparameters  throttle  secs  30  training  steps  1000  evaluation  steps  100  train  instance  count  train  instance  type  ml  c4  xlarge  base  job  name  tensorboard  example  estimator  fit  inputs  run  tensorboard  locally  True  predictor  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  import  numpy  as  np  random  image  data  np  random  rand  32  32  predictor  predict  random  image  data  sagemaker  Session  delete  endpoint  predictor  endpoint  ,amazon
import  boto3  import  matplotlib  pyplot  as  plt  from  matplotlib  patches  import  Polygon  from  PIL  import  Image  matplotlib  inline  plt  rcParams  figure  figsize  12  def  aws  text  image  Function  that  sends  cell  image  to  AWS  Rekognition  for  text  recognition  Receives  and  reutrns  the  results  as  polygons  client  boto3  client  rekognition  eu  west  im  Image  open  image  im  im  im  width  im  height  with  open  image  rb  as  im  response  client  detect  text  Image  Bytes  im  read  polygons  for  item  in  response  TextDetections  if  item  Type  WORD  text  item  DetectedText  polygon  item  Geometry  Polygon  coords  for  in  polygon  coords  append  int  im  coords  append  int  im  polygons  append  coords  text  return  polygonsimg  einsteinquote  jpg  result  aws  text  img  def  plot  overlay  img  polygons  remove  long  boxes  False  remove  short  boxes  False  color  fontsize  28  alpha  boxcolor  Function  that  overlays  text  labels  on  original  image  remo,microsoft
ve  long  boxes  bool  will  ignore  labels  with  bounding  boxes  that  are  wider  than  half  the  image  width  Depending  on  the  image  and  the  API  service  being  used  this  may  be  necessary  to  prevent  overlapping  redundant  boxes  from  being  displayed  remove  short  boxes  bool  will  ignore  labels  with  bounding  boxes  that  are  thinner  than  quarter  of  the  image  width  Depending  on  the  image  and  the  API  service  being  used  this  may  be  necessary  to  prevent  redundant  small  boxes  from  being  displayed  The  fontsize  variable  should  be  manually  adjusted  to  fit  the  image  text  size  image  Image  open  img  ax  plt  imshow  image  alpha  alpha  for  polygon  in  polygons  vertices  polygon  polygon  for  in  range  len  polygon  text  polygon  if  remove  long  boxes  if  vertices  vertices  image  size  continue  if  remove  short  boxes  if  vertices  vertices  image  size  continue  patch  Polygon  vertices  closed  True  fill  False  linewidth  col,microsoft
or  boxcolor  ax  axes  add  patch  patch  plt  text  vertices  vertices  vertices  vertices  text  fontsize  fontsize  color  color  va  center  ha  center  plt  axis  off  returnplt  imshow  Image  open  img  plt  axis  off  plot  overlay  img  result  Print  just  the  text  for  item  in  result  print  item  ,microsoft
time  import  os  import  boto3  import  re  import  json  from  sagemaker  import  get  execution  role  region  boto3  Session  region  name  role  get  execution  role  bucket  s3  bucket  put  your  s3  bucket  name  here  and  create  s3  bucket  prefix  sagemaker  DEMO  xgboost  byo  bucket  path  https  s3  amazonaws  com  format  region  bucket  customize  to  your  bucket  where  you  have  stored  the  data  conda  install  conda  forge  xgboost  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  gzip  open  mnist  pkl  gz  rb  train  set  valid  set  test  set  pickle  load  encoding  latin1  close  time  import  struct  import  io  import  boto3  def  get  dataset  import  pickle  import  gzip  with  gzip  open  mnist  pkl  gz  rb  as  pickle  Unpickler  encoding  latin1  return  load  train  set  valid  set  test  set  get  dataset  train  train  set  train  train  set  v,amazon
alid  valid  set  valid  valid  set  test  test  set  test  test  set  import  xgboost  as  xgb  import  sklearn  as  sk  bt  xgb  XGBClassifier  max  depth  learning  rate  estimators  10  objective  multi  softmax  Setup  xgboost  model  bt  fit  train  train  Train  it  to  our  data  eval  set  valid  valid  verbose  False  model  file  name  DEMO  local  xgboost  model  bt  Booster  save  model  model  file  name  tar  czvf  model  tar  gz  model  file  namefObj  open  model  tar  gz  rb  key  os  path  join  prefix  model  file  name  model  tar  gz  boto3  Session  resource  s3  Bucket  bucket  Object  key  upload  fileobj  fObj  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  xgboost  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  xgboost  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  xgboost  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  xgboost  latest  container  containers  boto3  Session  region  name  time  ,amazon
from  time  import  gmtime  strftime  model  name  model  file  name  strftime  gmtime  model  url  https  s3  amazonaws  com  format  region  bucket  key  sm  client  boto3  client  sagemaker  print  model  url  primary  container  Image  container  ModelDataUrl  model  url  create  model  response2  sm  client  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response2  ModelArn  from  time  import  gmtime  strftime  endpoint  config  name  DEMO  XGBoostEndpointConfig  strftime  gmtime  print  endpoint  config  name  create  endpoint  config  response  sm  client  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  InitialVariantWeight  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  import  time  endpoint  name  DEMO  XGBoostEndpoint  strftime  gmtime  pr,amazon
int  endpoint  name  create  endpoint  response  sm  client  create  endpoint  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  print  create  endpoint  response  EndpointArn  resp  sm  client  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  while  status  Creating  time  sleep  60  resp  sm  client  describe  endpoint  EndpointName  endpoint  name  status  resp  EndpointStatus  print  Status  status  print  Arn  resp  EndpointArn  print  Status  status  runtime  client  boto3  client  runtime  sagemaker  import  numpy  as  np  point  test  point  np  expand  dims  point  axis  point  test  np  savetxt  test  point  csv  point  delimiter  time  import  json  file  name  test  point  csv  customize  to  your  test  file  will  be  mnist  single  test  if  use  data  above  with  open  file  name  as  payload  read  strip  response  runtime  client  invoke  endpoint  EndpointName  endpoint  name  ContentType  text  csv  Body  payloa,amazon
d  result  response  Body  read  decode  ascii  print  Predicted  Class  Probabilities  format  result  floatArr  np  array  json  loads  result  predictedLabel  np  argmax  floatArr  print  Predicted  Class  Label  format  predictedLabel  print  Actual  Class  Label  format  point  sm  client  delete  endpoint  EndpointName  endpoint  name  ,amazon
bucket  your  s3  bucket  name  here  prefix  sagemaker  videogames  xgboost  import  sagemaker  role  sagemaker  get  execution  role  import  numpy  as  np  import  pandas  as  pd  import  matplotlib  pyplot  as  plt  from  IPython  display  import  Image  from  IPython  display  import  display  from  sklearn  datasets  import  dump  svmlight  file  from  time  import  gmtime  strftime  import  sys  import  math  import  json  import  boto3raw  data  filename  Video  Games  Sales  as  at  22  Dec  2016  csv  data  bucket  sagemaker  workshop  pdx  s3  boto3  resource  s3  s3  Bucket  data  bucket  download  file  raw  data  filename  raw  data  csv  data  pd  read  csv  raw  data  csv  pd  set  option  display  max  rows  20  datadata  data  Global  Sales  plt  bar  not  hit  hit  data  value  counts  plt  show  viz  data  filter  User  Score  Critic  Score  Global  Sales  axis  viz  User  Score  pd  Series  viz  User  Score  apply  pd  to  numeric  errors  coerce  viz  User  Score  viz  User  Score  mask ,amazon
 np  isnan  viz  User  Score  viz  Critic  Score  10  viz  plot  kind  scatter  logx  True  logy  True  Critic  Score  Global  Sales  viz  plot  kind  scatter  logx  True  logy  True  User  Score  Global  Sales  plt  show  data  data  drop  Name  Year  of  Release  NA  Sales  EU  Sales  JP  Sales  Other  Sales  Global  Sales  Critic  Count  User  Count  Developer  axis  data  isnull  sum  data  data  dropna  data  User  Score  data  User  Score  apply  pd  to  numeric  errors  coerce  data  User  Score  data  User  Score  mask  np  isnan  data  User  Score  data  Critic  Score  10  data  data  apply  lambda  yes  if  True  else  no  model  data  pd  get  dummies  data  train  data  validation  data  test  data  np  split  model  data  sample  frac  random  state  1729  int  len  model  data  int  len  model  data  dump  svmlight  file  train  data  drop  no  yes  axis  train  data  yes  train  libsvm  dump  svmlight  file  validation  data  drop  no  yes  axis  validation  data  yes  validation  libsvm  dump ,amazon
 svmlight  file  test  data  drop  no  yes  axis  test  data  yes  test  libsvm  boto3  Session  resource  s3  Bucket  bucket  Object  prefix  train  train  libsvm  upload  file  train  libsvm  boto3  Session  resource  s3  Bucket  bucket  Object  prefix  validation  validation  libsvm  upload  file  validation  libsvm  job  name  videogames  xgboost  strftime  gmtime  print  Training  job  job  name  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  xgboost  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  xgboost  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  xgboost  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  xgboost  latest  create  training  params  RoleArn  role  TrainingJobName  job  name  AlgorithmSpecification  TrainingImage  containers  boto3  Session  region  name  TrainingInputMode  File  ResourceConfig  InstanceCount  InstanceType  ml  c4  xlarge  VolumeSizeInGB  10  InputDataConfig  ChannelName  train  DataSo,amazon
urce  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  S3DataDistributionType  FullyReplicated  ContentType  libsvm  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  prefix  S3DataDistributionType  FullyReplicated  ContentType  libsvm  CompressionType  None  OutputDataConfig  S3OutputPath  s3  xgboost  video  games  output  format  bucket  prefix  HyperParameters  max  depth  eta  eval  metric  auc  scale  pos  weight  subsample  objective  binary  logistic  num  round  100  StoppingCondition  MaxRuntimeInSeconds  60  60  time  sm  boto3  client  sagemaker  sm  create  training  job  create  training  params  status  sm  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  status  try  sm  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  finally  status  sm  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print,amazon
  Training  job  ended  with  status  status  if  status  Failed  message  sm  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  create  model  response  sm  create  model  ModelName  job  name  ExecutionRoleArn  role  PrimaryContainer  Image  containers  boto3  Session  region  name  ModelDataUrl  sm  describe  training  job  TrainingJobName  job  name  ModelArtifacts  S3ModelArtifacts  print  create  model  response  ModelArn  xgboost  endpoint  config  videogames  xgboost  endpoint  config  strftime  gmtime  print  xgboost  endpoint  config  create  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  xgboost  endpoint  config  ProductionVariants  InstanceType  ml  t2  medium  InitialInstanceCount  ModelName  job  name  VariantName  AllTraffic  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  xgboost  endpoint  E,amazon
XAMPLE  videogames  xgb  endpoint  strftime  gmtime  print  xgboost  endpoint  create  endpoint  response  sm  create  endpoint  EndpointName  xgboost  endpoint  EndpointConfigName  xgboost  endpoint  config  print  create  endpoint  response  EndpointArn  resp  sm  describe  endpoint  EndpointName  xgboost  endpoint  status  resp  EndpointStatus  print  Status  status  try  sm  get  waiter  endpoint  in  service  wait  EndpointName  xgboost  endpoint  finally  resp  sm  describe  endpoint  EndpointName  xgboost  endpoint  status  resp  EndpointStatus  print  Arn  resp  EndpointArn  print  Status  status  if  status  InService  message  sm  describe  endpoint  EndpointName  xgboost  endpoint  FailureReason  print  Endpoint  creation  failed  with  the  following  error  format  message  raise  Exception  Endpoint  creation  did  not  succeed  runtime  boto3  client  runtime  sagemaker  def  do  predict  data  endpoint  name  content  type  payload  join  data  response  runtime  invoke  endpoint  EndpointName,amazon
  endpoint  name  ContentType  content  type  Body  payload  result  response  Body  read  result  result  decode  utf  result  result  split  preds  float  num  for  num  in  result  preds  round  num  for  num  in  preds  return  preds  def  batch  predict  data  batch  size  endpoint  name  content  type  items  len  data  arrs  for  offset  in  range  items  batch  size  if  offset  batch  size  items  results  do  predict  data  offset  offset  batch  size  endpoint  name  content  type  arrs  extend  results  else  arrs  extend  do  predict  data  offset  items  endpoint  name  content  type  sys  stdout  write  return  arrs  time  import  json  with  open  test  libsvm  as  payload  read  strip  labels  int  line  split  for  line  in  payload  split  test  data  line  for  line  in  payload  split  preds  batch  predict  test  data  100  xgboost  endpoint  text  libsvm  print  nerror  rate  sum  for  in  range  len  preds  if  preds  labels  float  len  preds  pd  crosstab  index  np  array  labels  c,amazon
olumns  np  array  preds  sm  delete  endpoint  EndpointName  xgboost  endpoint  ,amazon
IBM  Deep  Learning  IDE  Generated  Code  Compatible  Keras  Version  Tested  on  Python  Version  Choose  the  underlying  compiler  tensorflow  or  theano  import  json  import  os  with  open  os  path  expanduser  keras  keras  json  as  compiler  data  json  load  compiler  data  backend  tensorflow  compiler  data  image  data  format  channels  last  with  open  os  path  expanduser  keras  keras  json  as  outfile  json  dump  compiler  data  outfile  Global  variable  intilization  defined  metrics  defined  loss  import  all  the  required  packages  import  numpy  as  np  import  keras  from  keras  models  import  Model  import  keras  backend  as  import  keras  regularizers  as  import  keras  constraints  as  from  keras  layers  import  Activation  AveragePooling2D  BatchNormalization  Convolution2D  Dense  Flatten  GlobalAveragePooling2D  GlobalMaxPooling2D  Input  MaxPooling2D  from  keras  optimizers  import  Adam  Load  data  from  pickle  object  import  pickle  class  labels  count  wit,ibm
h  open  mnist  tf  train  pkl  rb  as  train  data  train  label  pickle  load  if  len  train  data  shape  if  tensorflow  tensorflow  train  data  train  data  reshape  train  data  shape  train  data  shape  train  data  shape  astype  float32  255  else  train  data  train  data  reshape  train  data  shape  train  data  shape  train  data  shape  astype  float32  255  if  len  train  label  shape  or  len  train  label  shape  and  train  label  shape  from  keras  utils  import  np  utils  class  labels  count  len  set  train  label  flatten  train  label  np  utils  to  categorical  train  label  class  labels  count  else  class  labels  count  train  label  shape  val  data  if  mnist  tf  valid  pkl  with  open  mnist  tf  valid  pkl  rb  as  val  data  val  label  pickle  load  if  len  val  data  shape  if  tensorflow  tensorflow  val  data  val  data  reshape  val  data  shape  val  data  shape  val  data  shape  astype  float32  255  else  val  data  val  data  reshape  val  data  shape  val ,ibm
 data  shape  val  data  shape  astype  float32  255  if  len  val  label  shape  or  len  val  label  shape  and  val  label  shape  from  keras  utils  import  np  utils  val  label  np  utils  to  categorical  val  label  class  labels  count  else  print  Validation  set  details  not  provided  test  data  if  mnist  tf  test  pkl  with  open  mnist  tf  test  pkl  rb  as  test  data  test  label  pickle  load  if  len  test  data  shape  if  tensorflow  tensorflow  test  data  test  data  reshape  test  data  shape  test  data  shape  test  data  shape  astype  float32  255  else  test  data  test  data  reshape  test  data  shape  test  data  shape  test  data  shape  astype  float32  255  if  len  test  label  shape  or  len  test  label  shape  and  test  label  shape  from  keras  utils  import  np  utils  test  label  np  utils  to  categorical  test  label  class  labels  count  else  print  Test  set  details  not  provided  print  train  data  shape  batch  input  shape  ImageData  aa612462  tra,ibm
in  data  shape  train  batch  size  64  if  True  Input  Layer  ImageData  aa612462  Input  shape  batch  input  shape  ImageData  aa612462  Convolution2D  Layer  Convolution2D  Convolution2D  32  kernel  initializer  glorot  normal  bias  initializer  glorot  normal  padding  valid  strides  data  format  channels  last  use  bias  False  name  Convolution2D  4991a3b7  ImageData  aa612462  Batch  Normalization  Layer  Convolution2D  BatchNormalization  axis  name  bn  Convolution2D  4991a3b7  Convolution2D  Rectification  Linear  Unit  ReLU  Activation  Layer  ReLU  Activation  relu  name  ReLU  ad8b7ea2  Convolution2D  Pooling2D  Layer  Pooling2D  MaxPooling2D  pool  size  padding  valid  data  format  channels  last  strides  name  Pooling2D  96e67797  ReLU  Convolution2D  Layer  Convolution2D  Convolution2D  64  kernel  initializer  glorot  normal  bias  initializer  glorot  normal  padding  valid  strides  data  format  channels  last  use  bias  False  name  Convolution2D  23a10a4b  Pooling2D  Batch  N,ibm
ormalization  Layer  Convolution2D  BatchNormalization  axis  name  bn  Convolution2D  23a10a4b  Convolution2D  Rectification  Linear  Unit  ReLU  Activation  Layer  ReLU  Activation  relu  name  ReLU  a3561d80  Convolution2D  Pooling2D  Layer  Pooling2D  MaxPooling2D  pool  size  padding  valid  data  format  channels  last  strides  name  Pooling2D  302b0c45  ReLU  Convolution2D  Layer  Convolution2D  Convolution2D  64  kernel  initializer  glorot  normal  bias  initializer  glorot  normal  padding  valid  strides  data  format  channels  last  use  bias  False  name  Convolution2D  9080ad6c  Pooling2D  Batch  Normalization  Layer  Convolution2D  BatchNormalization  axis  name  bn  Convolution2D  9080ad6c  Convolution2D  Rectification  Linear  Unit  ReLU  Activation  Layer  ReLU  Activation  relu  name  ReLU  aac1d5a9  Convolution2D  Pooling2D  Layer  Pooling2D  MaxPooling2D  pool  size  padding  valid  data  format  channels  last  strides  name  Pooling2D  53f9ddb1  ReLU  Flatten  Layer  Flatten  10  Flat,ibm
ten  name  Flatten  bd06de64  Pooling2D  Dense  or  Fully  Connected  FC  Layer  Dense  11  Dense  10  kernel  initializer  glorot  normal  bias  initializer  glorot  normal  use  bias  False  name  Dense  d2ec2247  Flatten  10  Softmax  Activation  Layer  Softmax  12  Activation  softmax  name  Softmax  3df1783a  Dense  11  Accuracy  Metric  defined  metrics  accuracy  SigmoidCrossEntropy  Loss  defined  loss  categorical  crossentropy  Define  keras  model  model  inputs  ImageData  aa612462  model  outputs  Softmax  12  model  Model  inputs  model  inputs  outputs  model  outputs  Set  the  required  hyperparameters  num  epochs  Defining  the  optimizer  function  adam  learning  rate  adam  decay  adam  beta  adam  beta  999  optimizer  fn  Adam  lr  adam  learning  rate  beta  adam  beta  beta  adam  beta  decay  adam  decay  performing  final  checks  if  not  defined  metrics  defined  metrics  None  if  not  defined  loss  defined  loss  categorical  crossentropy  if  ImageData  TextData  and  Lang  ,ibm
Model  adding  final  Dense  layer  which  has  vocab  length  units  layers  for  in  model  layers  for  in  range  len  layers  if  isinstance  layers  keras  layers  core  Dense  and  isinstance  layers  keras  layers  core  Activation  Dense  vocab  length  name  Dense  for  LM  str  layers  output  layers  inbound  nodes  assumption  there  are  no  merges  here  layers  model  Model  inputs  layers  input  outputs  layers  len  layers  output  Compile  and  train  the  model  model  compile  loss  defined  loss  optimizer  optimizer  fn  metrics  defined  metrics  if  len  model  outputs  train  label  train  label  len  model  outputs  if  len  val  data  val  label  val  label  len  model  outputs  if  len  test  data  test  label  test  label  len  model  outputs  validate  the  model  if  len  val  data  model  fit  train  data  train  label  batch  size  train  batch  size  epochs  num  epochs  verbose  validation  data  val  data  val  label  shuffle  True  else  model  fit  train  data  train  l,ibm
abel  batch  size  train  batch  size  epochs  num  epochs  verbose  shuffle  True  test  the  model  if  len  test  data  test  scores  model  evaluate  test  data  test  label  verbose  print  test  scores  saving  the  model  print  Saving  the  model  if  model  result  path  not  in  locals  and  model  result  path  not  in  globals  model  result  path  keras  model  hdf5  model  save  model  result  path  print  Model  saved  in  file  model  result  path  print  model  summary  import  cv2  import  matplotlib  pyplot  as  plt  model  load  weights  keras  model  3ep  hdf5  imgfile  number0  jpg  image  cv2  imread  imgfile  retval  image  cv2  threshold  image  128  255  cv2  THRESH  BINARY  plt  imshow  cv2  resize  image  255  255  28  28  image  np  array  cv2  resize  255  image  255  28  28  reshape  28  28  prediction  model  predict  image  print  prediction  is  prediction  predictednumber  maxprob  for  in  if  prediction  maxprob  predictednumber  str  maxprob  prediction  print  This  digi,ibm
t  is  predictednumber  with  str  maxprob  100  confidence  import  numpy  as  np  import  keras  from  keras  models  import  Model  import  keras  backend  as  import  keras  regularizers  as  import  keras  constraints  as  from  keras  layers  import  Activation  AveragePooling2D  BatchNormalization  Convolution2D  Dense  Flatten  GlobalAveragePooling2D  GlobalMaxPooling2D  Input  MaxPooling2D  import  cv2  import  matplotlib  pyplot  as  plt  ImageData  aa612462  Input  shape  28  28  Convolution2D  Layer  Convolution2D  Convolution2D  32  kernel  initializer  glorot  normal  bias  initializer  glorot  normal  padding  valid  strides  data  format  channels  last  use  bias  False  name  Convolution2D  4991a3b7  ImageData  aa612462  Batch  Normalization  Layer  Convolution2D  BatchNormalization  axis  name  bn  Convolution2D  4991a3b7  Convolution2D  Rectification  Linear  Unit  ReLU  Activation  Layer  ReLU  Activation  relu  name  ReLU  ad8b7ea2  Convolution2D  Pooling2D  Layer  Pooling2D  MaxPooling2,ibm
D  pool  size  padding  valid  data  format  channels  last  strides  name  Pooling2D  96e67797  ReLU  Convolution2D  Layer  Convolution2D  Convolution2D  64  kernel  initializer  glorot  normal  bias  initializer  glorot  normal  padding  valid  strides  data  format  channels  last  use  bias  False  name  Convolution2D  23a10a4b  Pooling2D  Batch  Normalization  Layer  Convolution2D  BatchNormalization  axis  name  bn  Convolution2D  23a10a4b  Convolution2D  Rectification  Linear  Unit  ReLU  Activation  Layer  ReLU  Activation  relu  name  ReLU  a3561d80  Convolution2D  Pooling2D  Layer  Pooling2D  MaxPooling2D  pool  size  padding  valid  data  format  channels  last  strides  name  Pooling2D  302b0c45  ReLU  Convolution2D  Layer  Convolution2D  Convolution2D  64  kernel  initializer  glorot  normal  bias  initializer  glorot  normal  padding  valid  strides  data  format  channels  last  use  bias  False  name  Convolution2D  9080ad6c  Pooling2D  Batch  Normalization  Layer  Convolution2D  BatchNormaliz,ibm
ation  axis  name  bn  Convolution2D  9080ad6c  Convolution2D  Rectification  Linear  Unit  ReLU  Activation  Layer  ReLU  Activation  relu  name  ReLU  aac1d5a9  Convolution2D  Pooling2D  Layer  Pooling2D  MaxPooling2D  pool  size  padding  valid  data  format  channels  last  strides  name  Pooling2D  53f9ddb1  ReLU  Flatten  Layer  Flatten  10  Flatten  name  Flatten  bd06de64  Pooling2D  Dense  or  Fully  Connected  FC  Layer  Dense  11  Dense  10  kernel  initializer  glorot  normal  bias  initializer  glorot  normal  use  bias  False  name  Dense  d2ec2247  Flatten  10  Softmax  Activation  Layer  Softmax  12  Activation  softmax  name  Softmax  3df1783a  Dense  11  Accuracy  Metric  defined  metrics  accuracy  SigmoidCrossEntropy  Loss  defined  loss  categorical  crossentropy  Define  keras  model  model  inputs  ImageData  aa612462  model  outputs  Softmax  12  model  Model  inputs  model  inputs  outputs  model  outputs  model  load  weights  keras  model  hdf5  imgfile  number0  jpg  cv2  imread  i,ibm
mgfile  this  means  that  the  image  will  be  loaded  as  grayscale  image  cv2  imread  imgfile  retval  image  cv2  threshold  image  128  255  cv2  THRESH  BINARY  plt  imshow  cv2  resize  image  255  255  28  28  image  np  array  cv2  resize  255  image  255  28  28  reshape  28  28  prediction  model  predict  image  print  prediction  is  prediction  predictednumber  maxprob  for  in  if  prediction  maxprob  predictednumber  str  maxprob  prediction  print  This  digit  is  predictednumber  with  str  maxprob  100  confidence  ,ibm
from  future  import  absolute  import  from  future  import  division  from  future  import  print  function  import  matplotlib  matplotlib  inline  import  matplotlib  pyplot  as  plt  import  math  import  numpy  as  np  import  tensorflow  as  tf  import  time  from  datasets  import  dataset  utils  Main  slim  library  from  tensorflow  contrib  import  slimdef  regression  model  inputs  is  training  True  scope  deep  regression  Creates  the  regression  model  Args  inputs  node  that  yields  Tensor  of  size  batch  size  dimensions  is  training  Whether  or  not  we  re  currently  training  the  model  scope  An  optional  variable  op  scope  for  the  model  Returns  predictions  Tensor  of  shape  batch  size  of  responses  end  points  dict  of  end  points  representing  the  hidden  layers  with  tf  variable  scope  scope  deep  regression  inputs  end  points  Set  the  default  weight  regularizer  and  acvitation  for  each  fully  connected  layer  with  slim  arg  scope  slim  fu,amazon
lly  connected  activation  fn  tf  nn  relu  weights  regularizer  slim  l2  regularizer  01  Creates  fully  connected  layer  from  the  inputs  with  32  hidden  units  net  slim  fully  connected  inputs  32  scope  fc1  end  points  fc1  net  Adds  dropout  layer  to  prevent  over  fitting  net  slim  dropout  net  is  training  is  training  Adds  another  fully  connected  layer  with  16  hidden  units  net  slim  fully  connected  net  16  scope  fc2  end  points  fc2  net  Creates  fully  connected  layer  with  single  hidden  unit  Note  that  the  layer  is  made  linear  by  setting  activation  fn  None  predictions  slim  fully  connected  net  activation  fn  None  scope  prediction  end  points  out  predictions  return  predictions  end  pointswith  tf  Graph  as  default  Dummy  placeholders  for  arbitrary  number  of  1d  inputs  and  outputs  inputs  tf  placeholder  tf  float32  shape  None  outputs  tf  placeholder  tf  float32  shape  None  Build  model  predictions  end  points  r,amazon
egression  model  inputs  Print  name  and  shape  of  each  tensor  print  Layers  for  in  end  points  items  print  name  shape  format  name  get  shape  Print  name  and  shape  of  parameter  nodes  values  not  yet  initialized  print  print  Parameters  for  in  slim  get  model  variables  print  name  shape  format  name  get  shape  def  produce  batch  batch  size  noise  xs  np  random  random  size  batch  size  10  ys  np  sin  xs  np  random  normal  size  batch  size  scale  noise  return  xs  astype  np  float32  ys  astype  np  float32  train  train  produce  batch  200  test  test  produce  batch  200  plt  scatter  train  train  def  convert  data  to  tensors  inputs  tf  constant  inputs  set  shape  None  outputs  tf  constant  outputs  set  shape  None  return  inputs  outputs  The  following  snippet  trains  the  regression  model  using  mean  squared  error  loss  ckpt  dir  tmp  regression  model  with  tf  Graph  as  default  tf  logging  set  verbosity  tf  logging  INFO  inpu,amazon
ts  targets  convert  data  to  tensors  train  train  Make  the  model  predictions  nodes  regression  model  inputs  is  training  True  Add  the  loss  function  to  the  graph  loss  tf  losses  mean  squared  error  labels  targets  predictions  predictions  The  total  loss  is  the  user  loss  plus  any  regularization  losses  total  loss  slim  losses  get  total  loss  Specify  the  optimizer  and  create  the  train  op  optimizer  tf  train  AdamOptimizer  learning  rate  005  train  op  slim  learning  create  train  op  total  loss  optimizer  Run  the  training  inside  session  final  loss  slim  learning  train  train  op  logdir  ckpt  dir  number  of  steps  5000  save  summaries  secs  log  every  steps  500  print  Finished  training  Last  batch  loss  final  loss  print  Checkpoint  saved  in  ckpt  dir  with  tf  Graph  as  default  inputs  targets  convert  data  to  tensors  train  train  predictions  end  points  regression  model  inputs  is  training  True  Add  multiple  loss  ,amazon
nodes  mean  squared  error  loss  tf  losses  mean  squared  error  labels  targets  predictions  predictions  absolute  difference  loss  slim  losses  absolute  difference  predictions  targets  The  following  two  ways  to  compute  the  total  loss  are  equivalent  regularization  loss  tf  add  slim  losses  get  regularization  losses  total  loss1  mean  squared  error  loss  absolute  difference  loss  regularization  loss  Regularization  Loss  is  included  in  the  total  loss  by  default  This  is  good  for  training  but  not  for  testing  total  loss2  slim  losses  get  total  loss  add  regularization  losses  True  init  op  tf  global  variables  initializer  with  tf  Session  as  sess  sess  run  init  op  Will  initialize  the  parameters  with  random  weights  total  loss1  total  loss2  sess  run  total  loss1  total  loss2  print  Total  Loss1  total  loss1  print  Total  Loss2  total  loss2  print  Regularization  Losses  for  loss  in  slim  losses  get  regularization  losses,amazon
  print  loss  print  Loss  Functions  for  loss  in  slim  losses  get  losses  print  loss  with  tf  Graph  as  default  inputs  targets  convert  data  to  tensors  test  test  Create  the  model  structure  Parameters  will  be  loaded  below  predictions  end  points  regression  model  inputs  is  training  False  Make  session  which  restores  the  old  parameters  from  checkpoint  sv  tf  train  Supervisor  logdir  ckpt  dir  with  sv  managed  session  as  sess  inputs  predictions  targets  sess  run  inputs  predictions  targets  plt  scatter  inputs  targets  plt  scatter  inputs  predictions  plt  title  red  true  blue  predicted  with  tf  Graph  as  default  inputs  targets  convert  data  to  tensors  test  test  predictions  end  points  regression  model  inputs  is  training  False  Specify  metrics  to  evaluate  names  to  value  nodes  names  to  update  nodes  slim  metrics  aggregate  metric  map  Mean  Squared  Error  slim  metrics  streaming  mean  squared  error  predictions  ta,amazon
rgets  Mean  Absolute  Error  slim  metrics  streaming  mean  absolute  error  predictions  targets  Make  session  which  restores  the  old  graph  parameters  and  then  run  eval  sv  tf  train  Supervisor  logdir  ckpt  dir  with  sv  managed  session  as  sess  metric  values  slim  evaluation  evaluation  sess  num  evals  Single  pass  over  data  eval  op  names  to  update  nodes  values  final  op  names  to  value  nodes  values  names  to  values  dict  zip  names  to  value  nodes  keys  metric  values  for  key  value  in  names  to  values  items  print  key  value  import  tensorflow  as  tf  from  datasets  import  dataset  utils  url  http  download  tensorflow  org  data  flowers  tar  gz  flowers  data  dir  tmp  flowers  if  not  tf  gfile  Exists  flowers  data  dir  tf  gfile  MakeDirs  flowers  data  dir  dataset  utils  download  and  uncompress  tarball  url  flowers  data  dir  from  datasets  import  flowers  import  tensorflow  as  tf  from  tensorflow  contrib  import  slim  wit,amazon
h  tf  Graph  as  default  dataset  flowers  get  split  train  flowers  data  dir  data  provider  slim  dataset  data  provider  DatasetDataProvider  dataset  common  queue  capacity  32  common  queue  min  image  label  data  provider  get  image  label  with  tf  Session  as  sess  with  slim  queues  QueueRunners  sess  for  in  range  np  image  np  label  sess  run  image  label  height  width  np  image  shape  class  name  name  dataset  labels  to  names  np  label  plt  figure  plt  imshow  np  image  plt  title  name  height  width  plt  axis  off  plt  show  def  my  cnn  images  num  classes  is  training  is  training  is  not  used  with  slim  arg  scope  slim  max  pool2d  kernel  size  stride  net  slim  conv2d  images  64  net  slim  max  pool2d  net  net  slim  conv2d  net  64  net  slim  max  pool2d  net  net  slim  flatten  net  net  slim  fully  connected  net  192  net  slim  fully  connected  net  num  classes  activation  fn  None  return  netimport  tensorflow  as  tf  with  tf  G,amazon
raph  as  default  The  model  can  handle  any  input  size  because  the  first  layer  is  convolutional  The  size  of  the  model  is  determined  when  image  node  is  first  passed  into  the  my  cnn  function  Once  the  variables  are  initialized  the  size  of  all  the  weight  matrices  is  fixed  Because  of  the  fully  connected  layers  this  means  that  all  subsequent  images  must  have  the  same  input  size  as  the  first  image  batch  size  height  width  channels  28  28  images  tf  random  uniform  batch  size  height  width  channels  maxval  Create  the  model  num  classes  10  logits  my  cnn  images  num  classes  is  training  True  probabilities  tf  nn  softmax  logits  Initialize  all  the  variables  including  parameters  randomly  init  op  tf  global  variables  initializer  with  tf  Session  as  sess  Run  the  init  op  evaluate  the  model  outputs  and  print  the  results  sess  run  init  op  probabilities  sess  run  probabilities  print  Probabilities  Sha,amazon
pe  print  probabilities  shape  batch  size  num  classes  print  nProbabilities  print  probabilities  print  nSumming  across  all  classes  Should  equal  print  np  sum  probabilities  Each  row  sums  to  1from  preprocessing  import  inception  preprocessing  import  tensorflow  as  tf  from  tensorflow  contrib  import  slim  def  load  batch  dataset  batch  size  32  height  299  width  299  is  training  False  Loads  single  batch  of  data  Args  dataset  The  dataset  to  load  batch  size  The  number  of  images  in  the  batch  height  The  size  of  each  image  after  preprocessing  width  The  size  of  each  image  after  preprocessing  is  training  Whether  or  not  we  re  currently  training  or  evaluating  Returns  images  Tensor  of  size  batch  size  height  width  image  samples  that  have  been  preprocessed  images  raw  Tensor  of  size  batch  size  height  width  image  samples  that  can  be  used  for  visualization  labels  Tensor  of  size  batch  size  whose  values  ,amazon
range  between  and  dataset  num  classes  data  provider  slim  dataset  data  provider  DatasetDataProvider  dataset  common  queue  capacity  32  common  queue  min  image  raw  label  data  provider  get  image  label  Preprocess  image  for  usage  by  Inception  image  inception  preprocessing  preprocess  image  image  raw  height  width  is  training  is  training  Preprocess  the  image  for  display  purposes  image  raw  tf  expand  dims  image  raw  image  raw  tf  image  resize  images  image  raw  height  width  image  raw  tf  squeeze  image  raw  Batch  it  up  images  images  raw  labels  tf  train  batch  image  image  raw  label  batch  size  batch  size  num  threads  capacity  batch  size  return  images  images  raw  labelsfrom  datasets  import  flowers  This  might  take  few  minutes  train  dir  tmp  tfslim  model  print  Will  save  model  to  train  dir  with  tf  Graph  as  default  tf  logging  set  verbosity  tf  logging  INFO  dataset  flowers  get  split  train  flowers  data,amazon
  dir  images  labels  load  batch  dataset  Create  the  model  logits  my  cnn  images  num  classes  dataset  num  classes  is  training  True  Specify  the  loss  function  one  hot  labels  slim  one  hot  encoding  labels  dataset  num  classes  slim  losses  softmax  cross  entropy  logits  one  hot  labels  total  loss  slim  losses  get  total  loss  Create  some  summaries  to  visualize  the  training  process  tf  summary  scalar  losses  Total  Loss  total  loss  Specify  the  optimizer  and  create  the  train  op  optimizer  tf  train  AdamOptimizer  learning  rate  01  train  op  slim  learning  create  train  op  total  loss  optimizer  Run  the  training  final  loss  slim  learning  train  train  op  logdir  train  dir  number  of  steps  For  speed  we  just  do  epoch  save  summaries  secs  print  Finished  training  Final  batch  loss  final  loss  from  datasets  import  flowers  This  might  take  few  minutes  with  tf  Graph  as  default  tf  logging  set  verbosity  tf  logging  DE,amazon
BUG  dataset  flowers  get  split  train  flowers  data  dir  images  labels  load  batch  dataset  logits  my  cnn  images  num  classes  dataset  num  classes  is  training  False  predictions  tf  argmax  logits  Define  the  metrics  names  to  values  names  to  updates  slim  metrics  aggregate  metric  map  eval  Accuracy  slim  metrics  streaming  accuracy  predictions  labels  eval  Recall  slim  metrics  streaming  recall  at  logits  labels  print  Running  evaluation  Loop  checkpoint  path  tf  train  latest  checkpoint  train  dir  metric  values  slim  evaluation  evaluate  once  master  checkpoint  path  checkpoint  path  logdir  train  dir  eval  op  names  to  updates  values  final  op  names  to  values  values  names  to  values  dict  zip  names  to  values  keys  metric  values  for  name  in  names  to  values  print  name  names  to  values  name  from  datasets  import  dataset  utils  url  http  download  tensorflow  org  models  inception  v1  2016  08  28  tar  gz  checkpoints  di,amazon
r  tmp  checkpoints  if  not  tf  gfile  Exists  checkpoints  dir  tf  gfile  MakeDirs  checkpoints  dir  dataset  utils  download  and  uncompress  tarball  url  checkpoints  dir  import  numpy  as  np  import  os  import  tensorflow  as  tf  try  import  urllib2  as  urllib  except  ImportError  import  urllib  request  as  urllib  from  datasets  import  imagenet  from  nets  import  inception  from  preprocessing  import  inception  preprocessing  from  tensorflow  contrib  import  slim  image  size  inception  inception  v1  default  image  size  with  tf  Graph  as  default  url  https  upload  wikimedia  org  wikipedia  commons  70  EnglishCockerSpaniel  simon  jpg  image  string  urllib  urlopen  url  read  image  tf  image  decode  jpeg  image  string  channels  processed  image  inception  preprocessing  preprocess  image  image  image  size  image  size  is  training  False  processed  images  tf  expand  dims  processed  image  Create  the  model  use  the  default  arg  scope  to  configure  the ,amazon
 batch  norm  parameters  with  slim  arg  scope  inception  inception  v1  arg  scope  logits  inception  inception  v1  processed  images  num  classes  1001  is  training  False  probabilities  tf  nn  softmax  logits  init  fn  slim  assign  from  checkpoint  fn  os  path  join  checkpoints  dir  inception  v1  ckpt  slim  get  model  variables  InceptionV1  with  tf  Session  as  sess  init  fn  sess  np  image  probabilities  sess  run  image  probabilities  probabilities  probabilities  sorted  inds  for  in  sorted  enumerate  probabilities  key  lambda  plt  figure  plt  imshow  np  image  astype  np  uint8  plt  axis  off  plt  show  names  imagenet  create  readable  names  for  imagenet  labels  for  in  range  index  sorted  inds  print  Probability  2f  probabilities  index  100  names  index  from  datasets  import  dataset  utils  import  tensorflow  as  tf  url  http  download  tensorflow  org  models  vgg  16  2016  08  28  tar  gz  checkpoints  dir  tmp  checkpoints  if  not  tf  gfile  Exi,amazon
sts  checkpoints  dir  tf  gfile  MakeDirs  checkpoints  dir  dataset  utils  download  and  uncompress  tarball  url  checkpoints  dir  import  numpy  as  np  import  os  import  tensorflow  as  tf  try  import  urllib2  except  ImportError  import  urllib  request  as  urllib  from  datasets  import  imagenet  from  nets  import  vgg  from  preprocessing  import  vgg  preprocessing  from  tensorflow  contrib  import  slim  image  size  vgg  vgg  16  default  image  size  with  tf  Graph  as  default  url  https  upload  wikimedia  org  wikipedia  commons  d9  First  Student  IC  school  bus  202076  jpg  image  string  urllib  urlopen  url  read  image  tf  image  decode  jpeg  image  string  channels  processed  image  vgg  preprocessing  preprocess  image  image  image  size  image  size  is  training  False  processed  images  tf  expand  dims  processed  image  Create  the  model  use  the  default  arg  scope  to  configure  the  batch  norm  parameters  with  slim  arg  scope  vgg  vgg  arg  scope  10,amazon
00  classes  instead  of  1001  logits  vgg  vgg  16  processed  images  num  classes  1000  is  training  False  probabilities  tf  nn  softmax  logits  init  fn  slim  assign  from  checkpoint  fn  os  path  join  checkpoints  dir  vgg  16  ckpt  slim  get  model  variables  vgg  16  with  tf  Session  as  sess  init  fn  sess  np  image  probabilities  sess  run  image  probabilities  probabilities  probabilities  sorted  inds  for  in  sorted  enumerate  probabilities  key  lambda  plt  figure  plt  imshow  np  image  astype  np  uint8  plt  axis  off  plt  show  names  imagenet  create  readable  names  for  imagenet  labels  for  in  range  index  sorted  inds  Shift  the  index  of  class  name  by  one  print  Probability  2f  probabilities  index  100  names  index  Note  that  this  may  take  several  minutes  import  os  from  datasets  import  flowers  from  nets  import  inception  from  preprocessing  import  inception  preprocessing  from  tensorflow  contrib  import  slim  image  size  incept,amazon
ion  inception  v1  default  image  size  def  get  init  fn  Returns  function  run  by  the  chief  worker  to  warm  start  the  training  checkpoint  exclude  scopes  InceptionV1  Logits  InceptionV1  AuxLogits  exclusions  scope  strip  for  scope  in  checkpoint  exclude  scopes  variables  to  restore  for  var  in  slim  get  model  variables  for  exclusion  in  exclusions  if  var  op  name  startswith  exclusion  break  else  variables  to  restore  append  var  return  slim  assign  from  checkpoint  fn  os  path  join  checkpoints  dir  inception  v1  ckpt  variables  to  restore  train  dir  tmp  inception  finetuned  with  tf  Graph  as  default  tf  logging  set  verbosity  tf  logging  INFO  dataset  flowers  get  split  train  flowers  data  dir  images  labels  load  batch  dataset  height  image  size  width  image  size  Create  the  model  use  the  default  arg  scope  to  configure  the  batch  norm  parameters  with  slim  arg  scope  inception  inception  v1  arg  scope  logits  ince,amazon
ption  inception  v1  images  num  classes  dataset  num  classes  is  training  True  Specify  the  loss  function  one  hot  labels  slim  one  hot  encoding  labels  dataset  num  classes  slim  losses  softmax  cross  entropy  logits  one  hot  labels  total  loss  slim  losses  get  total  loss  Create  some  summaries  to  visualize  the  training  process  tf  summary  scalar  losses  Total  Loss  total  loss  Specify  the  optimizer  and  create  the  train  op  optimizer  tf  train  AdamOptimizer  learning  rate  01  train  op  slim  learning  create  train  op  total  loss  optimizer  Run  the  training  final  loss  slim  learning  train  train  op  logdir  train  dir  init  fn  get  init  fn  number  of  steps  print  Finished  training  Last  batch  loss  final  loss  import  numpy  as  np  import  tensorflow  as  tf  from  datasets  import  flowers  from  nets  import  inception  from  tensorflow  contrib  import  slim  image  size  inception  inception  v1  default  image  size  batch  size  wi,amazon
th  tf  Graph  as  default  tf  logging  set  verbosity  tf  logging  INFO  dataset  flowers  get  split  train  flowers  data  dir  images  images  raw  labels  load  batch  dataset  height  image  size  width  image  size  Create  the  model  use  the  default  arg  scope  to  configure  the  batch  norm  parameters  with  slim  arg  scope  inception  inception  v1  arg  scope  logits  inception  inception  v1  images  num  classes  dataset  num  classes  is  training  True  probabilities  tf  nn  softmax  logits  checkpoint  path  tf  train  latest  checkpoint  train  dir  init  fn  slim  assign  from  checkpoint  fn  checkpoint  path  slim  get  variables  to  restore  with  tf  Session  as  sess  with  slim  queues  QueueRunners  sess  sess  run  tf  initialize  local  variables  init  fn  sess  np  probabilities  np  images  raw  np  labels  sess  run  probabilities  images  raw  labels  for  in  range  batch  size  image  np  images  raw  true  label  np  labels  predicted  label  np  argmax  np  proba,amazon
bilities  predicted  name  dataset  labels  to  names  predicted  label  true  name  dataset  labels  to  names  true  label  plt  figure  plt  imshow  image  astype  np  uint8  plt  title  Ground  Truth  Prediction  true  name  predicted  name  plt  axis  off  plt  show  ,amazon
install  packages  NetworkRiskMeasures  Example  from  Anand  Craig  and  Von  Peter  2015  628  Total  Liabilities  Total  Assets  Loads  the  package  library  NetworkRiskMeasures  Maximum  Entropy  Estimation  ME  matrix  estimation  rowsums  colsums  method  me  ME  round  ME  ME  Minimum  Density  Estimation  set  seed  192  seed  for  reproducibility  MD  matrix  estimation  method  md  MD  from  IPython  display  import  HTML  HTML  iframe  src  https  player  vimeo  com  video  26763844  title  byline  portrait  width  700  height  394  frameborder  webkitallowfullscreen  mozallowfullscreen  allowfullscreen  iframe  href  https  vimeo  com  26763844  BAXTER  DURY  CLAIRE  Dir  Cut  from  href  https  vimeo  com  dannysangra  Danny  Sangra  on  href  https  vimeo  com  Vimeo  See  the  code  to  generate  the  dataset  in  the  help  files  sim  data  data  sim  data  head  sim  data  seed  min  dens  estimation  is  stochastic  set  seed  15  minimum  density  estimation  verbose  to  prevent  printin,microsoft
g  md  mat  matrix  estimation  sim  data  assets  sim  data  liabilities  method  md  verbose  rownames  and  colnames  for  the  matrix  rownames  md  mat  colnames  md  mat  sim  data  bankinstall  packages  ggnetwork  library  ggplot2  library  ggnetwork  library  igraph  converting  our  network  to  an  igraph  object  gmd  graph  from  adjacency  matrix  md  mat  weighted  adding  other  node  attributes  to  the  network  gmd  buffer  sim  data  buffer  gmd  weights  sim  data  weights  sum  sim  data  weights  gmd  assets  sim  data  assets  gmd  liabilities  sim  data  liabilities  ploting  with  ggplot  and  ggnetwork  set  seed  20  netdf  ggnetwork  gmd  ggplot  netdf  aes  xend  xend  yend  yend  geom  edges  arrow  arrow  length  unit  pt  type  closed  color  grey50  curvature  alpha  geom  nodes  aes  size  weights  ggtitle  Estimated  interbank  network  theme  blank  network  density  edge  density  gmd  assortativity  assortativity  degree  gmd  sim  data  degree  igraph  degree  gmd  sim ,microsoft
 data  btw  igraph  betweenness  gmd  sim  data  close  igraph  closeness  gmd  sim  data  eigen  igraph  eigen  centrality  gmd  vector  sim  data  alpha  igraph  alpha  centrality  gmd  alpha  sim  data  imps  impact  susceptibility  exposures  gmd  buffer  sim  data  buffer  sim  data  impd  impact  diffusion  exposures  gmd  buffer  sim  data  buffer  weights  sim  data  weights  total  DebtRank  simulation  contdr  contagion  exposures  md  mat  buffer  sim  data  buffer  weights  sim  data  weights  shock  all  method  debtrank  verbose  summary  contdr  plot  contdr  contdr  summary  summary  contdr  sim  data  DebtRank  contdr  summary  summary  table  additional  stress  Traditional  default  cascades  simulation  contthr  contagion  exposures  md  mat  buffer  sim  data  buffer  weights  sim  data  weights  shock  all  method  threshold  verbose  summary  contthr  contthr  summary  summary  contthr  sim  data  cascade  contthr  summary  summary  table  additional  stresshead  sim  data  rankings  si,microsoft
m  data  rankings  cbind  rankings  lapply  sim  data  DebtRank  cascade  degree  eigen  impd  assets  liabilities  buffer  function  as  numeric  factor  rankings  rankings  order  rankings  DebtRank  head  rankings  10  cor  rankings  seq  01  25  by  01  shocks  lapply  function  rep  nrow  md  mat  names  shocks  paste  100  pct  shock  cont  contagion  exposures  gmd  buffer  sim  data  buffer  shock  shocks  weights  sim  data  weights  method  debtrank  verbose  summary  cont  plot  cont  size  ,microsoft
import  os  import  random  import  time  import  json  import  boto3  dir  name  data  dogscats  train  dogs  dir  name  data  dogscats  test1  endpoint  name  london  summit  demo  endpoint  file  name  dir  name  random  choice  os  listdir  dir  name  change  dir  name  to  whatever  print  file  name  file  name  data  dogscats  test1  9969  jpg  test  image  from  IPython  display  import  Image  Image  file  name  time  runtime  boto3  Session  client  runtime  sagemaker  print  Filename  is  file  name  with  open  file  name  rb  as  payload  read  payload  bytearray  payload  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  image  Body  payload  result  response  Body  read  print  json  loads  result  ,amazon
time  import  boto3  import  re  from  sagemaker  import  get  execution  role  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  role  get  execution  role  bucket  bucket  name  customize  to  your  bucket  training  image  get  image  uri  boto3  Session  region  name  image  classification  print  training  image  import  os  import  urllib  request  import  boto3  def  download  url  filename  url  split  if  not  os  path  exists  filename  urllib  request  urlretrieve  url  filename  def  upload  to  s3  channel  file  s3  boto3  resource  s3  data  open  file  rb  key  channel  file  s3  Bucket  bucket  put  object  Key  key  Body  data  caltech  256  download  http  data  mxnet  io  data  caltech  256  caltech  256  60  train  rec  download  http  data  mxnet  io  data  caltech  256  caltech  256  60  val  rec  upload  to  s3  validation  caltech  256  60  val  rec  upload  to  s3  train  caltech  256  60  train  rec  The  algorithm  supports  multiple  network  depth  number  of  ,amazon
layers  They  are  18  34  50  101  152  and  200  For  this  training  we  will  use  18  layers  num  layers  18  we  need  to  specify  the  input  image  shape  for  the  training  data  image  shape  224  224  we  also  need  to  specify  the  number  of  training  samples  in  the  training  set  for  caltech  it  is  15420  num  training  samples  15420  specify  the  number  of  output  classes  num  classes  257  batch  size  for  training  mini  batch  size  128  number  of  epochs  epochs  learning  rate  learning  rate  01  top  Since  we  are  using  transfer  learning  we  set  use  pretrained  model  to  so  that  weights  can  be  initialized  with  pre  trained  weights  use  pretrained  model  time  import  time  import  boto3  from  time  import  gmtime  strftime  s3  boto3  client  s3  create  unique  job  name  job  name  prefix  DEMO  imageclassification  timestamp  time  strftime  time  gmtime  job  name  job  name  prefix  timestamp  training  params  specify  the  training  docker  im,amazon
age  AlgorithmSpecification  TrainingImage  training  image  TrainingInputMode  File  RoleArn  role  OutputDataConfig  S3OutputPath  s3  output  format  bucket  job  name  prefix  ResourceConfig  InstanceCount  InstanceType  ml  p2  xlarge  VolumeSizeInGB  50  TrainingJobName  job  name  HyperParameters  image  shape  image  shape  num  layers  str  num  layers  num  training  samples  str  num  training  samples  num  classes  str  num  classes  mini  batch  size  str  mini  batch  size  epochs  str  epochs  learning  rate  str  learning  rate  use  pretrained  model  str  use  pretrained  model  StoppingCondition  MaxRuntimeInSeconds  360000  Training  data  should  be  inside  subdirectory  called  train  Validation  data  should  be  inside  subdirectory  called  validation  The  algorithm  currently  only  supports  fullyreplicated  model  where  data  is  copied  onto  each  machine  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  S3,amazon
DataDistributionType  FullyReplicated  ContentType  application  recordio  CompressionType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  S3DataDistributionType  FullyReplicated  ContentType  application  recordio  CompressionType  None  print  Training  job  name  format  job  name  print  nInput  Data  Location  format  training  params  InputDataConfig  DataSource  S3DataSource  create  the  Amazon  SageMaker  training  job  sagemaker  boto3  client  service  name  sagemaker  sagemaker  create  training  job  training  params  confirm  that  the  training  job  has  started  status  sagemaker  describe  training  job  TrainingJobName  job  name  TrainingJobStatus  print  Training  job  current  status  format  status  try  wait  for  the  job  to  finish  and  report  the  ending  status  sagemaker  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  job  name  training  info  sagemaker  describe  training  job  ,amazon
TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  except  print  Training  failed  to  start  if  exception  is  raised  that  means  it  has  failed  message  sagemaker  describe  training  job  TrainingJobName  job  name  FailureReason  print  Training  failed  with  the  following  error  format  message  training  info  sagemaker  describe  training  job  TrainingJobName  job  name  status  training  info  TrainingJobStatus  print  Training  job  ended  with  status  status  time  import  boto3  from  time  import  gmtime  strftime  sage  boto3  Session  client  service  name  sagemaker  model  name  DEMO  image  classification  model  print  model  name  info  sage  describe  training  job  TrainingJobName  job  name  model  data  info  ModelArtifacts  S3ModelArtifacts  print  model  data  hosting  image  get  image  uri  boto3  Session  region  name  image  classification  primary  container  Image  hosting  image  ModelDataUrl  mod,amazon
el  data  create  model  response  sage  create  model  ModelName  model  name  ExecutionRoleArn  role  PrimaryContainer  primary  container  print  create  model  response  ModelArn  from  time  import  gmtime  strftime  timestamp  time  strftime  time  gmtime  endpoint  config  name  job  name  prefix  epc  timestamp  endpoint  config  response  sage  create  endpoint  config  EndpointConfigName  endpoint  config  name  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  model  name  VariantName  AllTraffic  print  Endpoint  configuration  name  format  endpoint  config  name  print  Endpoint  configuration  arn  format  endpoint  config  response  EndpointConfigArn  time  import  time  timestamp  time  strftime  time  gmtime  endpoint  name  job  name  prefix  ep  timestamp  print  Endpoint  name  format  endpoint  name  endpoint  params  EndpointName  endpoint  name  EndpointConfigName  endpoint  config  name  endpoint  response  sagemaker  create  endpoint  endpoint  params,amazon
  print  EndpointArn  format  endpoint  response  EndpointArn  get  the  status  of  the  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  response  EndpointStatus  print  EndpointStatus  format  status  wait  until  the  status  has  changed  sagemaker  get  waiter  endpoint  in  service  wait  EndpointName  endpoint  name  print  the  status  of  the  endpoint  endpoint  response  sagemaker  describe  endpoint  EndpointName  endpoint  name  status  endpoint  response  EndpointStatus  print  Endpoint  creation  ended  with  EndpointStatus  format  status  if  status  InService  raise  Exception  Endpoint  creation  failed  import  boto3  runtime  boto3  Session  client  service  name  runtime  sagemaker  wget  tmp  test  jpg  http  www  vision  caltech  edu  Image  Datasets  Caltech256  images  008  bathtub  008  0007  jpg  file  name  tmp  test  jpg  test  image  from  IPython  display  import  Image  Image  file  name  import  json  import  numpy  as  np  with  open ,amazon
 file  name  rb  as  payload  read  payload  bytearray  payload  response  runtime  invoke  endpoint  EndpointName  endpoint  name  ContentType  application  image  Body  payload  result  response  Body  read  result  will  be  in  json  format  and  convert  it  to  ndarray  result  json  loads  result  the  result  will  output  the  probabilities  for  all  classes  find  the  class  with  maximum  probability  and  print  the  class  index  index  np  argmax  result  object  categories  ak47  american  flag  backpack  baseball  bat  baseball  glove  basketball  hoop  bat  bathtub  bear  beer  mug  billiards  binoculars  birdbath  blimp  bonsai  101  boom  box  bowling  ball  bowling  pin  boxing  glove  brain  101  breadmaker  buddha  101  bulldozer  butterfly  cactus  cake  calculator  camel  cannon  canoe  car  tire  cartman  cd  centipede  cereal  box  chandelier  101  chess  board  chimp  chopsticks  cockroach  coffee  mug  coffin  coin  comet  computer  keyboard  computer  monitor  computer  mouse  c,amazon
onch  cormorant  covered  wagon  cowboy  hat  crab  101  desk  globe  diamond  ring  dice  dog  dolphin  101  doorknob  drinking  straw  duck  dumb  bell  eiffel  tower  electric  guitar  101  elephant  101  elk  ewer  101  eyeglasses  fern  fighter  jet  fire  extinguisher  fire  hydrant  fire  truck  fireworks  flashlight  floppy  disk  football  helmet  french  horn  fried  egg  frisbee  frog  frying  pan  galaxy  gas  pump  giraffe  goat  golden  gate  bridge  goldfish  golf  ball  goose  gorilla  grand  piano  101  grapes  grasshopper  guitar  pick  hamburger  hammock  harmonica  harp  harpsichord  hawksbill  101  head  phones  helicopter  101  hibiscus  homer  simpson  horse  horseshoe  crab  hot  air  balloon  hot  dog  hot  tub  hourglass  house  fly  human  skeleton  hummingbird  ibis  101  ice  cream  cone  iguana  ipod  iris  jesus  christ  joy  stick  kangaroo  101  kayak  ketch  101  killer  whale  knife  ladder  laptop  101  lathe  leopards  101  license  plate  lightbulb  light  house  lightnin,amazon
g  llama  101  mailbox  mandolin  mars  mattress  megaphone  menorah  101  microscope  microwave  minaret  minotaur  motorbikes  101  mountain  bike  mushroom  mussels  necktie  octopus  ostrich  owl  palm  pilot  palm  tree  paperclip  paper  shredder  pci  card  penguin  people  pez  dispenser  photocopier  picnic  table  playing  card  porcupine  pram  praying  mantis  pyramid  raccoon  radio  telescope  rainbow  refrigerator  revolver  101  rifle  rotary  phone  roulette  wheel  saddle  saturn  school  bus  scorpion  101  screwdriver  segway  self  propelled  lawn  mower  sextant  sheet  music  skateboard  skunk  skyscraper  smokestack  snail  snake  sneaker  snowmobile  soccer  ball  socks  soda  can  spaghetti  speed  boat  spider  spoon  stained  glass  starfish  101  steering  wheel  stirrups  sunflower  101  superman  sushi  swan  swiss  army  knife  sword  syringe  tambourine  teapot  teddy  bear  teepee  telephone  box  tennis  ball  tennis  court  tennis  racket  theodolite  toaster  tomato  tombs,amazon
tone  top  hat  touring  bike  tower  pisa  traffic  light  treadmill  triceratops  tricycle  trilobite  101  tripod  shirt  tuning  fork  tweezer  umbrella  101  unicorn  vcr  video  projector  washing  machine  watch  101  waterfall  watermelon  welding  mask  wheelbarrow  windmill  wine  bottle  xylophone  yarmulke  yo  yo  zebra  airplanes  101  car  side  101  faces  easy  101  greyhound  tennis  shoes  toad  clutter  print  Result  label  object  categories  index  probability  str  result  index  sage  delete  endpoint  EndpointName  endpoint  name  ,amazon
pip  install  upgrade  watson  developer  cloud  pip  install  upgrade  nltkimport  json  import  sys  import  thread  import  time  import  watson  developer  cloud  from  watson  developer  cloud  import  NaturalLanguageUnderstandingV1  from  watson  developer  cloud  natural  language  understanding  v1  import  Features  EntitiesOptions  KeywordsOptions  SemanticRolesOptions  import  ibm  boto3  from  botocore  client  import  Config  import  operator  from  functools  import  reduce  from  io  import  StringIO  import  numpy  as  np  from  os  path  import  join  dirname  import  requests  import  re  import  pandas  as  pd  import  nltk  from  nltk  cluster  util  import  cosine  distance  from  nltk  import  word  tokenize  sent  tokenize  ne  chunk  from  nltk  corpus  import  stopwords  import  networkx  as  nx  import  matplotlib  pyplot  as  plt  import  numpynatural  language  understanding  NaturalLanguageUnderstandingV1  version  2017  02  27  username  xb5fbe544  8177  44d9  a049  b6c7a3150839x,ibm
  masking  password  ypE4YQSAVbvi  The  code  was  removed  by  DSX  for  sharing  Specify  file  for  sample  text  and  configuration  files  sampleTextFileName1  sample  text  txt  sampleTextFileName2  sample  text  txt  sampleConfigFileName  sample  config  txt  Maintain  tagged  text  and  plain  text  map  tagTextMap  Stop  words  nltk  download  stopwords  stopWords  stopwords  words  english  Additional  words  to  be  ignored  stopWords  extend  The  This  That  nltk  download  punkt  nltk  download  averaged  perceptron  tagger  nltk  download  maxent  ne  chunker  nltk  download  words  lemmatizer  nltk  WordNetLemmatizer  stemmer  nltk  stem  porter  PorterStemmer  Analyze  features  of  natural  language  content  def  analyze  using  NLU  analysistext  response  natural  language  understanding  analyze  text  analysistext  features  Features  entities  EntitiesOptions  keywords  KeywordsOptions  semantic  roles  SemanticRolesOptions  return  responsedef  split  sentences  text  Split  text  int,ibm
o  sentences  sentence  delimiters  re  compile  sentences  sentence  delimiters  split  text  return  sentences  def  split  into  tokens  text  Split  text  into  tokens  tokens  nltk  word  tokenize  text  return  tokens  def  POS  tagging  text  Generate  Part  of  speech  tagging  of  the  text  POSofText  nltk  tag  pos  tag  text  return  POSofText  def  keyword  tagging  tag  tagtext  text  Tag  the  text  matching  keywords  if  text  lower  find  tagtext  lower  return  text  text  lower  find  tagtext  lower  text  lower  find  tagtext  lower  len  tagtext  else  return  UNKNOWN  def  regex  tagging  tag  regex  text  Tag  the  text  matching  REGEX  re  compile  regex  re  IGNORECASE  matchtext  findall  text  regex  list  if  len  matchtext  for  regword  in  matchtext  regex  list  append  regword  return  regex  list  def  chunk  tagging  tag  chunk  text  Tag  the  text  using  chunking  parsed  cp  nltk  RegexpParser  chunk  pos  cp  parsed  cp  parse  text  chunk  list  for  root  in  pos  c,ibm
p  if  isinstance  root  nltk  tree  Tree  if  root  label  tag  chunk  word  for  child  root  in  root  chunk  word  chunk  word  child  root  chunk  list  append  chunk  word  return  chunk  list  def  augument  NLUResponse  responsejson  updateType  text  tag  Update  the  NLU  response  JSON  with  augumented  classifications  if  updateType  keyword  if  not  any  get  text  None  text  for  in  responsejson  keywords  responsejson  keywords  append  text  text  relevance  else  if  not  any  get  text  None  text  for  in  responsejson  entities  responsejson  entities  append  type  tag  text  text  relevance  count  def  chunk  sentence  text  Tag  the  sentence  using  chunking  grammar  NP  DT  JJ  PRP  NN  Chunk  sequences  of  DT  JJ  NN  VB  DT  JJ  RB  PRP  NN  Chink  sequences  of  VB  DT  JJ  NN  PP  IN  NP  Chunk  prepositions  followed  by  NP  Verb  VP  VB  NP  PP  CLAUSE  Chunk  verbs  and  their  arguments  CLAUSE  NP  VP  Chunk  NP  VP  parsed  cp  nltk  RegexpParser  grammar  loop  pos,ibm
  cp  parsed  cp  parse  text  return  pos  cp  def  find  attrs  subtree  phrase  attrs  if  phrase  NP  for  nodes  in  subtree  if  nodes  in  DT  PRP  POS  JJ  CD  ADJP  QP  NP  NNP  attrs  attrs  nodes  return  attrs  def  find  subject  for  in  subtrees  lambda  label  NP  return  find  attrs  NP  def  resolve  coreference  text  config  Resolve  coreferences  in  the  text  for  Nouns  that  are  Subjects  in  sentence  sentenceList  split  sentences  text  referenceSubject  sentenceText  configjson  json  loads  config  for  sentences  in  sentenceList  tokens  split  into  tokens  sentences  postags  POS  tagging  tokens  sentencetags  chunk  sentence  postags  subjects  find  subject  sentencetags  for  rules  in  configjson  configuration  coreference  rules  if  rules  type  chunking  for  tags  in  rules  chunk  chunktags  chunk  tagging  tags  tag  tags  pattern  postags  if  len  chunktags  for  words  in  chunktags  if  tags  tag  PRP  if  subjects  sentenceText  sentenceText  sentences  repl,ibm
ace  words  referenceSubject  elif  tags  tag  NAME  if  words  subjects  referenceSubject  words  sentenceText  sentenceText  sentences  return  sentenceText  def  disambiguate  entities  text  Resolve  disambiguity  in  the  text  using  entities  and  entity  resolution  performed  using  Watson  NLU  sentenceList  split  sentences  text  taggedtext  text  response  analyze  using  NLU  text  responsejson  response  for  sentences  in  sentenceList  tokens  split  into  tokens  sentences  postags  POS  tagging  tokens  name  tagged  text  chunk  tagging  NAME  NAME  NNP  postags  print  name  tagged  text  for  entities  in  responsejson  entities  regexstr  entities  text  regex  re  compile  regexstr  re  IGNORECASE  tagText  entities  type  entities  text  taggedtext  re  sub  regexstr  tagText  taggedtext  tagTextMap  tagText  entities  text  for  roles  in  responsejson  semantic  roles  if  entities  not  in  roles  subject  print  NO  ENTITY  else  for  entity  in  roles  subject  entities  if  disa,ibm
mbiguation  not  in  entity  print  NO  DISAMBIGUATION  else  regexstr  roles  subject  text  regex  re  compile  regexstr  re  IGNORECASE  tagText  entity  type  entity  text  taggedtext  re  sub  regexstr  tagText  taggedtext  tagTextMap  tagText  entity  text  return  taggedtext  def  extract  relations  text  config  relations  Extract  entity  relationships  in  sentence  sentenceList  split  sentences  text  configjson  json  loads  config  for  sentences  in  sentenceList  for  rules  in  configjson  configuration  relations  rules  if  rules  type  regex  for  regex  in  rules  regex  regextags  regex  tagging  regex  tag  regex  pattern  sentences  if  len  regextags  for  words  in  regextags  relations  append  tagTextMap  words  regex  tag  tagTextMap  words  return  relations  def  compute  text  similarity  text1  text2  text1tags  text2tags  Compute  text  similarity  using  cosine  sentences  text1  split  sentences  text1  sentences  text2  split  sentences  text2  tokens  text1  tokens  text,ibm
2  for  sentence  in  sentences  text1  tokenstemp  split  into  tokens  sentence  lower  tokens  text1  extend  tokenstemp  for  sentence  in  sentences  text2  tokenstemp  split  into  tokens  sentence  lower  tokens  text2  extend  tokenstemp  Add  text  tags  if  len  text1tags  tokens  text1  extend  text1tags  if  len  text2tags  tokens  text2  extend  text2tags  Stem  words  tokens1Filtered  stemmer  stem  for  in  tokens  text1  if  not  in  stopWords  tokens2Filtered  stemmer  stem  for  in  tokens  text2  if  not  in  stopWords  remove  duplicate  tokens  tokens1Filtered  set  tokens1Filtered  tokens2Filtered  set  tokens2Filtered  tokensList  text1vector  text2vector  if  len  tokens1Filtered  len  tokens2Filtered  tokensList  tokens1Filtered  else  tokensList  tokens2Filtered  for  token  in  tokensList  if  token  in  tokens1Filtered  text1vector  append  else  text1vector  append  if  token  in  tokens2Filtered  text2vector  append  else  text2vector  append  cosine  similarity  cosine  distance,ibm
  text1vector  text2vector  if  numpy  isnan  cosine  similarity  cosine  similarity  return  cosine  similaritydef  draw  simple  graph  graph  nodes  labels  edges  extract  nodes  from  graph  for  tuples  in  graph  nodes  append  tuples  nodes  append  tuples  extract  edges  from  graph  for  edgepairs  in  graph  edges  append  edgepairs  edgepairs  extract  edge  labels  from  graph  for  edgetuples  in  graph  labels  append  edgetuples  create  networkx  graph  nx  Graph  add  nodes  for  node  in  nodes  add  node  node  add  edges  for  edge  in  graph  add  edge  edge  edge  draw  graph  pos  nx  spring  layout  nx  draw  pos  with  labels  True  edge  labels  dict  zip  edges  labels  nx  draw  networkx  edge  labels  pos  edge  labels  edge  labels  show  graph  plt  show  cos  ibm  boto3  client  s3  ibm  api  key  id  credentials  IBM  API  KEY  ID  ibm  service  instance  id  credentials  IAM  SERVICE  ID  ibm  auth  endpoint  credentials  IBM  AUTH  ENDPOINT  config  Config  signature  vers,ibm
ion  oauth  endpoint  url  credentials  ENDPOINT  def  get  file  filename  Retrieve  file  from  Cloud  Object  Storage  fileobject  cos  get  object  Bucket  credentials  BUCKET  Key  filename  Body  return  fileobject  def  load  string  fileobject  Load  the  file  contents  into  Python  string  text  fileobject  read  return  texttext1  load  string  get  file  sampleTextFileName1  decode  utf8  text2  load  string  get  file  sampleTextFileName2  decode  utf8  config  load  string  get  file  sampleConfigFileName  decode  utf8  relationships  text1  decode  utf8  resolved  text1  resolve  coreference  text1  config  resolved  text2  resolve  coreference  text2  config  disambiguated  text1  disambiguate  entities  resolved  text1  disambiguated  text2  disambiguate  entities  resolved  text2  extract  relations  disambiguated  text1  config  relationships  extract  relations  disambiguated  text2  config  relationships  print  Document  similarity  score  print  compute  text  similarity  text1  text2 ,ibm
 print  print  Entities  and  relations  print  relationships  draw  simple  graph  relationships  ,ibm
Setup  from  sagemaker  import  get  execution  role  import  sagemaker  sagemaker  session  sagemaker  Session  This  role  retrieves  the  SageMaker  compatible  role  used  by  this  notebook  instance  role  get  execution  role  import  chainer  from  chainer  datasets  import  get  cifar10  train  test  get  cifar10  import  os  import  shutil  import  numpy  as  np  train  data  element  for  element  in  train  train  labels  element  for  element  in  train  test  data  element  for  element  in  test  test  labels  element  for  element  in  test  try  os  makedirs  tmp  data  train  cifar  os  makedirs  tmp  data  test  cifar  np  savez  tmp  data  train  cifar  train  npz  data  train  data  labels  train  labels  np  savez  tmp  data  test  cifar  test  npz  data  test  data  labels  test  labels  train  input  sagemaker  session  upload  data  path  os  path  join  tmp  data  train  cifar  key  prefix  notebook  chainer  cifar  train  test  input  sagemaker  session  upload  data  path  os  path,amazon
  join  tmp  data  test  cifar  key  prefix  notebook  chainer  cifar  test  finally  shutil  rmtree  tmp  data  print  training  data  at  train  input  print  test  data  at  test  input  pygmentize  src  chainer  cifar  vgg  single  machine  py  from  sagemaker  chainer  estimator  import  Chainer  chainer  estimator  Chainer  entry  point  chainer  cifar  vgg  single  machine  py  source  dir  src  role  role  sagemaker  session  sagemaker  session  train  instance  count  train  instance  type  ml  p2  xlarge  hyperparameters  epochs  batch  size  64  from  sagemaker  tuner  import  ContinuousParameter  hyperparameter  ranges  learning  rate  ContinuousParameter  05  06  objective  metric  name  Validation  accuracy  metric  definitions  Name  Validation  accuracy  Regex  J1  max  jobs  max  parallel  jobs  2from  sagemaker  tuner  import  HyperparameterTuner  chainer  tuner  HyperparameterTuner  estimator  chainer  estimator  objective  metric  name  objective  metric  name  hyperparameter  ranges  hype,amazon
rparameter  ranges  metric  definitions  metric  definitions  max  jobs  max  jobs  max  parallel  jobs  max  parallel  jobs  chainer  tuner  fit  train  train  input  test  test  input  chainer  tuner  wait  predictor  chainer  tuner  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  skimage  import  io  import  numpy  as  np  def  read  image  filename  img  io  imread  filename  img  np  array  img  transpose  img  np  expand  dims  img  axis  img  img  astype  np  float32  img  255  img  img  reshape  32  32  return  img  def  read  images  filenames  return  np  array  read  image  for  in  filenames  filenames  images  airplane1  png  images  automobile1  png  images  bird1  png  images  cat1  png  images  deer1  png  images  dog1  png  images  frog1  png  images  horse1  png  images  ship1  png  images  truck1  png  image  data  read  images  filenames  response  predictor  predict  image  data  for  prediction  in  enumerate  response  print  image  prediction  format  prediction,amazon
  argmax  axis  chainer  tuner  delete  endpoint  ,amazon
import  numpy  as  np  import  pandas  as  pd  Importar  librer  de  plots  sicos  import  matplotlib  pyplot  as  pltX1  np  random  normal  100  X2  np  random  normal  100  plt  plot  X1  plt  plot  X2  plt  xlabel  Tiempo  plt  ylabel  Retorno  plt  legend  X1  X2  X1  ,ibm
Put  these  at  the  top  of  every  notebook  to  get  automatic  reloading  and  inline  plotting  reload  ext  autoreload  autoreload  matplotlib  inlineimport  dill  as  dill  import  boto3  This  file  contains  all  the  main  external  libs  we  ll  use  from  fastai  imports  import  from  fastai  transforms  import  from  fastai  conv  learner  import  from  fastai  model  import  from  fastai  dataset  import  from  fastai  sgdr  import  from  fastai  plots  import  PATH  data  dogscats  sz  299  arch  resnext50  bs  28tfms  tfms  from  model  arch  sz  aug  tfms  transforms  side  on  max  zoom  data  ImageClassifierData  from  paths  PATH  tfms  tfms  bs  bs  num  workers  learn  ConvLearner  pretrained  arch  data  precompute  True  ps  learn  fit  1e  learn  precompute  Falselearn  fit  1e  cycle  len  learn  unfreeze  lr  np  array  1e  1e  1e  learn  fit  lr  cycle  len  learn  save  224  all  50  learn  load  224  all  50  log  preds  learn  TTA  probs  np  mean  np  exp  log  preds  accuracy,amazon
  np  probs  preds  np  argmax  probs  axis  probs  probs  from  sklearn  metrics  import  confusion  matrix  cm  confusion  matrix  preds  plot  confusion  matrix  cm  data  classes  def  rand  by  mask  mask  return  np  random  choice  np  where  mask  replace  False  def  rand  by  correct  is  correct  return  rand  by  mask  preds  data  val  is  correct  def  plot  val  with  title  idxs  title  imgs  np  stack  data  val  ds  for  in  idxs  title  probs  probs  for  in  idxs  print  title  return  plots  data  val  ds  denorm  imgs  rows  titles  title  probs  def  plots  ims  figsize  12  rows  titles  None  plt  figure  figsize  figsize  for  in  range  len  ims  sp  add  subplot  rows  len  ims  rows  sp  axis  Off  if  titles  is  not  None  sp  set  title  titles  fontsize  16  plt  imshow  ims  def  load  img  id  ds  idx  return  np  array  PIL  Image  open  PATH  ds  fnames  idx  def  plot  val  with  title  idxs  title  imgs  load  img  id  data  val  ds  for  in  idxs  title  probs  probs  f,amazon
or  in  idxs  print  title  return  plots  imgs  rows  titles  title  probs  figsize  16  def  most  by  mask  mask  mult  idxs  np  where  mask  return  idxs  np  argsort  mult  probs  idxs  def  most  by  correct  is  correct  mult  if  is  correct  else  return  most  by  mask  preds  data  val  is  correct  data  val  mult  plot  val  with  title  most  by  correct  False  Most  incorrect  cats  plot  val  with  title  most  by  correct  False  Most  incorrect  dogs  torch  save  learn  model  PATH  models  dogscats  resnext50  pt  pickle  module  dill  with  open  PATH  models  classes  json  as  outfile  json  dump  json  dumps  data  classes  outfile  tar  czvf  data  dogscats  model  tar  gz  data  dogscats  models  dogscats  resnext50  pt  classes  jsonbucket  sagemaker  mcclean  eu  west  customize  to  the  name  of  your  S3  bucket  key  models  dogscats  fastai  model  tar  gz  prefix  of  the  S3  bucket  of  the  model  fileboto3  client  s3  upload  file  PATH  model  tar  gz  bucket  key  pr,amazon
int  Uploaded  model  artefacts  to  s3  bucket  key  ,amazon
import  os  from  pyspark  import  SparkContext  SparkConf  from  pyspark  sql  import  SparkSession  import  sagemaker  from  sagemaker  import  get  execution  role  import  sagemaker  pyspark  role  get  execution  role  Configure  Spark  to  use  the  SageMaker  Spark  dependency  jars  jars  sagemaker  pyspark  classpath  jars  classpath  join  sagemaker  pyspark  classpath  jars  See  the  SageMaker  Spark  Github  repo  under  sagemaker  pyspark  sdk  to  learn  how  to  connect  to  remote  EMR  cluster  running  Spark  from  Notebook  Instance  spark  SparkSession  builder  config  spark  driver  extraClassPath  classpath  master  local  getOrCreate  import  boto3  region  boto3  Session  region  name  trainingData  spark  read  format  libsvm  option  numFeatures  784  load  s3a  sagemaker  sample  data  spark  mnist  train  format  region  testData  spark  read  format  libsvm  option  numFeatures  784  load  s3a  sagemaker  sample  data  spark  mnist  test  format  region  trainingData  show  impo,amazon
rt  random  from  sagemaker  pyspark  import  IAMRole  S3DataPath  from  sagemaker  pyspark  algorithms  import  KMeansSageMakerEstimator  kmeans  estimator  KMeansSageMakerEstimator  sagemakerRole  IAMRole  role  trainingInstanceType  ml  m4  xlarge  trainingInstanceCount  endpointInstanceType  ml  m4  xlarge  endpointInitialInstanceCount  kmeans  estimator  setK  10  kmeans  estimator  setFeatureDim  784  train  model  kmeans  estimator  fit  trainingData  transformedData  model  transform  testData  transformedData  show  from  pyspark  sql  types  import  DoubleType  import  matplotlib  pyplot  as  plt  import  numpy  as  np  helper  function  to  display  digit  def  show  digit  img  caption  xlabel  subplot  None  if  subplot  None  subplot  plt  subplots  imgr  img  reshape  28  28  subplot  axes  get  xaxis  set  ticks  subplot  axes  get  yaxis  set  ticks  plt  title  caption  plt  xlabel  xlabel  subplot  imshow  imgr  cmap  gray  images  np  array  transformedData  select  features  cache  take  ,amazon
250  clusters  transformedData  select  closest  cluster  cache  take  250  for  cluster  in  range  10  print  nCluster  format  int  cluster  digits  img  for  img  in  zip  clusters  images  if  int  closest  cluster  cluster  height  len  digits  width  plt  rcParams  figure  figsize  width  height  subplots  plt  subplots  height  width  subplots  np  ndarray  flatten  subplots  for  subplot  image  in  zip  subplots  digits  show  digit  image  subplot  subplot  for  subplot  in  subplots  len  digits  subplot  axis  off  plt  show  Delete  the  endpoint  from  sagemaker  pyspark  import  SageMakerResourceCleanup  resource  cleanup  SageMakerResourceCleanup  model  sagemakerClient  resource  cleanup  deleteResources  model  getCreatedResources  ,amazon
pip  install  mglearn  Lab  Minimizing  Cost  import  tensorflow  as  tf  import  numpy  as  np  import  matplotlib  as  plt  import  pandas  as  pd  import  mglearn  from  sklearn  model  selection  import  train  test  split  from  sklearn  datasets  import  load  irisiris  dataset  load  iris  train  test  train  test  train  test  split  iris  dataset  data  iris  dataset  target  random  state  test  size  25  default  print  test  shape  test  shape  print  train  shape  train  shape  test  tf  convert  to  tensor  test  dtype  tf  float32  test  tf  convert  to  tensor  test  dtype  tf  float32  train  tf  convert  to  tensor  train  dtype  tf  float32  train  tf  convert  to  tensor  train  dtype  tf  float32  test  get  shape  print  type  train  train  get  shape  tf  set  random  seed  777  for  reproducibility  tf  Variable  tf  random  normal  name  weight  hypothesis  tf  matmul  train  cost  tf  reduce  mean  tf  square  hypothesis  train  optimizer  tf  train  GradientDescentOptimizer  learnin,ibm
g  rate  1e  train  optimizer  minimize  cost  sess  tf  Session  sess  run  tf  global  variables  initializer  for  step  in  range  100  print  step  sess  run  print  step  sess  run  cost  sess  run  sess  run  train  print  sess  run  prediction  tf  matmul  test  print  sess  run  prediction  sess  run  test  print  test  ,ibm
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  xgboost  dm  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  numpy  as  np  For  matrix  operations  and  numerical  processing  import  pandas  as  pd  For  munging  tabular  data  import  matplotlib  pyplot  as  plt  For  charts  and  visualizations  from  IPython  display  import  Image  For  displaying  images  in  the  notebook  from  IPython  display  import  display  For  displaying  outputs  in  the  notebook  from  time  import  gmtime  strftime  For  labeling  SageMaker  models  endpoints  etc  import  sys  For  writing  outputs  to  notebook  import  math  For  ceiling  function  import  json  For  parsing  hosting  outputs  import  os  For  manipulating  filepath  names  import  sagemaker  Amazon  SageMaker  Python  SDK  provides  many  helper  functions  from  sagemaker  predictor  import  csv  serializer  Converts  strings  for  HTTP  POST  requests  on ,amazon
 inference  wget  https  archive  ics  uci  edu  ml  machine  learning  databases  00222  bank  additional  zip  unzip  bank  additional  zipdata  pd  read  csv  bank  additional  bank  additional  full  csv  sep  pd  set  option  display  max  columns  500  Make  sure  we  can  see  all  of  the  columns  pd  set  option  display  max  rows  20  Keep  the  output  on  one  page  data  Frequency  tables  for  each  categorical  feature  for  column  in  data  select  dtypes  include  object  columns  display  pd  crosstab  index  data  column  columns  observations  normalize  columns  Histograms  for  each  numeric  features  display  data  describe  matplotlib  inline  hist  data  hist  bins  30  sharey  True  figsize  10  10  for  column  in  data  select  dtypes  include  object  columns  if  column  display  pd  crosstab  index  data  column  columns  data  normalize  columns  for  column  in  data  select  dtypes  exclude  object  columns  print  column  hist  data  column  hist  by  bins  30  plt  show,amazon
  display  data  corr  pd  plotting  scatter  matrix  data  figsize  12  12  plt  show  data  no  previous  contact  np  where  data  pdays  999  Indicator  variable  to  capture  when  pdays  takes  value  of  999  data  not  working  np  where  np  in1d  data  job  student  retired  unemployed  Indicator  for  individuals  not  actively  employed  model  data  pd  get  dummies  data  Convert  categorical  variables  to  sets  of  indicatorsmodel  data  model  data  drop  duration  emp  var  rate  cons  price  idx  cons  conf  idx  euribor3m  nr  employed  axis  train  data  validation  data  test  data  np  split  model  data  sample  frac  random  state  1729  int  len  model  data  int  len  model  data  Randomly  sort  the  data  then  split  out  first  70  second  20  and  last  10  pd  concat  train  data  yes  train  data  drop  no  yes  axis  axis  to  csv  train  csv  index  False  header  False  pd  concat  validation  data  yes  validation  data  drop  no  yes  axis  axis  to  csv  validation  cs,amazon
v  index  False  header  False  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  train  csv  upload  file  train  csv  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  validation  csv  upload  file  validation  csv  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  boto3  Session  region  name  xgboost  s3  input  train  sagemaker  s3  input  s3  data  s3  train  format  bucket  prefix  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  s3  validation  format  bucket  prefix  content  type  csv  sess  sagemaker  Session  xgb  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  m4  xlarge  output  path  s3  output  format  bucket  prefix  sagemaker  session  sess  xgb  set  hyperparameters  max  depth  eta  gamma  min  child  weight  subsample  silent  objective  binary  logistic  num  round  100  xgb  fit  train  s3,amazon
  input  train  validation  s3  input  validation  xgb  predictor  xgb  deploy  initial  instance  count  instance  type  ml  m4  xlarge  xgb  predictor  content  type  text  csv  xgb  predictor  serializer  csv  serializerdef  predict  data  rows  500  split  array  np  array  split  data  int  data  shape  float  rows  predictions  for  array  in  split  array  predictions  join  predictions  xgb  predictor  predict  array  decode  utf  return  np  fromstring  predictions  sep  predictions  predict  test  data  drop  no  yes  axis  as  matrix  pd  crosstab  index  test  data  yes  columns  np  round  predictions  rownames  actuals  colnames  predictions  sagemaker  Session  delete  endpoint  xgb  predictor  endpoint  ,amazon
Setup  from  sagemaker  import  get  execution  role  import  sagemaker  sagemaker  session  sagemaker  Session  This  role  retrieves  the  SageMaker  compatible  role  used  by  this  Notebook  Instance  role  get  execution  role  import  chainer  from  chainer  datasets  import  get  cifar10  train  test  get  cifar10  import  os  import  shutil  import  numpy  as  np  train  data  element  for  element  in  train  train  labels  element  for  element  in  train  test  data  element  for  element  in  test  test  labels  element  for  element  in  test  try  os  makedirs  tmp  data  train  cifar  os  makedirs  tmp  data  test  cifar  np  savez  tmp  data  train  cifar  train  npz  data  train  data  labels  train  labels  np  savez  tmp  data  test  cifar  test  npz  data  test  data  labels  test  labels  train  input  sagemaker  session  upload  data  path  os  path  join  tmp  data  train  cifar  key  prefix  notebook  chainer  cifar  train  test  input  sagemaker  session  upload  data  path  os  path,amazon
  join  tmp  data  test  cifar  key  prefix  notebook  chainer  cifar  test  finally  shutil  rmtree  tmp  data  print  training  data  at  train  input  print  test  data  at  test  input  pygmentize  src  chainer  cifar  vgg  single  machine  py  from  sagemaker  chainer  estimator  import  Chainer  chainer  estimator  Chainer  entry  point  chainer  cifar  vgg  single  machine  py  source  dir  src  role  role  sagemaker  session  sagemaker  session  train  instance  count  train  instance  type  ml  p2  xlarge  hyperparameters  epochs  50  batch  size  64  chainer  estimator  fit  train  train  input  test  test  input  from  s3  util  import  retrieve  output  from  s3  chainer  training  job  chainer  estimator  latest  training  job  name  desc  sagemaker  session  sagemaker  client  describe  training  job  TrainingJobName  chainer  training  job  output  data  desc  ModelArtifacts  S3ModelArtifacts  replace  model  tar  gz  output  tar  gz  retrieve  output  from  s3  output  data  output  single  ma,amazon
chine  cifar  from  IPython  display  import  Image  from  IPython  display  import  display  accuracy  graph  Image  filename  output  single  machine  cifar  accuracy  png  width  800  height  800  loss  graph  Image  filename  output  single  machine  cifar  loss  png  width  800  height  800  display  accuracy  graph  loss  graph  predictor  chainer  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  skimage  import  io  import  numpy  as  np  def  read  image  filename  img  io  imread  filename  img  np  array  img  transpose  img  np  expand  dims  img  axis  img  img  astype  np  float32  img  255  img  img  reshape  32  32  return  img  def  read  images  filenames  return  np  array  read  image  for  in  filenames  filenames  images  airplane1  png  images  automobile1  png  images  bird1  png  images  cat1  png  images  deer1  png  images  dog1  png  images  frog1  png  images  horse1  png  images  ship1  png  images  truck1  png  image  data  read  images  filename,amazon
s  response  predictor  predict  image  data  for  prediction  in  enumerate  response  print  image  prediction  format  prediction  argmax  axis  chainer  estimator  delete  endpoint  ,amazon
Lab  Learning  rate  and  Evaluation  import  tensorflow  as  tf  import  random  import  matplotlib  pyplot  as  plt  from  tensorflow  examples  tutorials  mnist  import  input  data  tf  set  random  seed  777  reproducibility  parameters  learning  rate  001  training  epochs  15  batch  size  100  input  place  holders  tf  placeholder  tf  float32  None  784  tf  placeholder  tf  float32  None  10  weights  bias  for  nn  layers  tf  Variable  tf  random  normal  784  10  tf  Variable  tf  random  normal  10  hypothesis  tf  matmul  define  cost  loss  optimizer  Adam  cost  tf  reduce  mean  tf  nn  softmax  cross  entropy  with  logits  logits  hypothesis  labels  optimizer  tf  train  AdamOptimizer  learning  rate  learning  rate  minimize  cost  initialize  sess  tf  Session  sess  run  tf  global  variables  initializer  mnist  input  data  read  data  sets  MNIST  data  one  hot  True  Check  out  https  www  tensorflow  org  get  started  mnist  beginners  for  more  information  about  the  mnis,ibm
t  dataset  train  my  model  for  epoch  in  range  training  epochs  avg  cost  total  batch  int  mnist  train  num  examples  batch  size  for  in  range  total  batch  batch  xs  batch  ys  mnist  train  next  batch  batch  size  feed  dict  batch  xs  batch  ys  sess  run  cost  optimizer  feed  dict  feed  dict  avg  cost  total  batch  print  Epoch  04d  epoch  cost  9f  format  avg  cost  print  Learning  Finished  Test  model  and  check  accuracy  correct  prediction  tf  equal  tf  argmax  hypothesis  tf  argmax  accuracy  tf  reduce  mean  tf  cast  correct  prediction  tf  float32  print  Accuracy  sess  run  accuracy  feed  dict  mnist  test  images  mnist  test  labels  Get  one  and  predict  random  randint  mnist  test  num  examples  print  Label  sess  run  tf  argmax  mnist  test  labels  print  Prediction  sess  run  tf  argmax  hypothesis  feed  dict  mnist  test  images  plt  imshow  mnist  test  images  reshape  28  28  cmap  Greys  interpolation  nearest  plt  show  Epoch  0001  cos,ibm
t  888845987  Epoch  0002  cost  860620173  Epoch  0003  cost  159035648  Epoch  0004  cost  892340870  Epoch  0005  cost  751155428  Epoch  0006  cost  662484806  Epoch  0007  cost  601544010  Epoch  0008  cost  556526115  Epoch  0009  cost  521186961  Epoch  0010  cost  493068354  Epoch  0011  cost  469686249  Epoch  0012  cost  449967254  Epoch  0013  cost  433519321  Epoch  0014  cost  419000337  Epoch  0015  cost  406490815  Learning  Finished  Accuracy  9035  ,ibm
time  import  sagemaker  from  sagemaker  import  get  execution  role  role  get  execution  role  print  role  sess  sagemaker  Session  bucket  your  s3  bucket  name  here  custom  bucket  name  bucket  sess  default  bucket  prefix  DEMO  ObjectDetection  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  training  image  get  image  uri  sess  boto  region  name  object  detection  repo  version  latest  print  training  image  import  os  import  urllib  request  def  download  url  filename  url  split  if  not  os  path  exists  filename  urllib  request  urlretrieve  url  filename  MSCOCO  validation  image  files  download  http  images  cocodataset  org  zips  val2017  zip  download  http  images  cocodataset  org  annotations  annotations  trainval2017  zip  bash  unzip  qo  val2017  zip  unzip  qo  annotations  trainval2017  zip  rm  val2017  zip  annotations  trainval2017  zip  bash  Create  folders  to  store  the  data  and  annotation  files  mkdir  generated  train  train ,amazon
 annotation  validation  validation  annotationimport  json  import  logging  def  get  coco  mapper  original  list  10  11  13  14  15  16  17  18  19  20  21  22  23  24  25  27  28  31  32  33  34  35  36  37  38  39  40  41  42  43  44  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  67  70  72  73  74  75  76  77  78  79  80  81  82  84  85  86  87  88  89  90  iter  counter  COCO  for  orig  in  original  list  COCO  orig  iter  counter  iter  counter  return  COCOdef  get  mapper  fn  map  def  mapper  in  category  return  map  in  category  return  mapper  fix  index  mapping  get  mapper  fn  get  coco  mapper  file  name  annotations  instances  val2017  json  with  open  file  name  as  js  json  load  images  js  images  categories  js  categories  annotations  js  annotations  for  in  images  jsonFile  file  name  jsonFile  jsonFile  split  json  line  line  file  file  name  line  image  size  width  int  width  height  int  height  depth  line  annotations  li,amazon
ne  categories  for  in  annotations  if  image  id  id  and  len  bbox  line  annotations  append  class  id  int  fix  index  mapping  category  id  top  int  bbox  left  int  bbox  width  int  bbox  height  int  bbox  class  name  for  in  categories  if  int  category  id  id  class  name  str  name  assert  class  name  is  not  line  categories  append  class  id  int  category  id  name  class  name  if  line  annotations  with  open  os  path  join  generated  jsonFile  as  json  dump  line  import  os  import  json  jsons  os  listdir  generated  print  There  are  images  have  annotation  files  format  len  jsons  import  shutil  train  jsons  jsons  4452  val  jsons  jsons  4452  Moving  training  files  to  the  training  folders  for  in  train  jsons  image  file  val2017  split  jpg  shutil  move  image  file  train  shutil  move  generated  train  annotation  Moving  validation  files  to  the  validation  folders  for  in  val  jsons  image  file  val2017  split  jpg  shutil  move  image  f,amazon
ile  validation  shutil  move  generated  validation  annotation  time  train  channel  prefix  train  validation  channel  prefix  validation  train  annotation  channel  prefix  train  annotation  validation  annotation  channel  prefix  validation  annotation  sess  upload  data  path  train  bucket  bucket  key  prefix  train  channel  sess  upload  data  path  validation  bucket  bucket  key  prefix  validation  channel  sess  upload  data  path  train  annotation  bucket  bucket  key  prefix  train  annotation  channel  sess  upload  data  path  validation  annotation  bucket  bucket  key  prefix  validation  annotation  channel  s3  train  data  s3  format  bucket  train  channel  s3  validation  data  s3  format  bucket  validation  channel  s3  train  annotation  s3  format  bucket  train  annotation  channel  s3  validation  annotation  s3  format  bucket  validation  annotation  channel  s3  output  location  s3  output  format  bucket  prefix  od  model  sagemaker  estimator  Estimator  training  ,amazon
image  role  train  instance  count  train  instance  type  ml  p3  2xlarge  train  volume  size  50  train  max  run  360000  input  mode  File  output  path  s3  output  location  sagemaker  session  sess  od  model  set  hyperparameters  base  network  resnet  50  use  pretrained  model  num  classes  80  mini  batch  size  16  epochs  30  learning  rate  001  lr  scheduler  step  10  lr  scheduler  factor  optimizer  sgd  momentum  weight  decay  0005  overlap  threshold  nms  threshold  45  image  shape  512  label  width  600  num  training  samples  4452  train  data  sagemaker  session  s3  input  s3  train  data  distribution  FullyReplicated  content  type  image  jpeg  s3  data  type  S3Prefix  validation  data  sagemaker  session  s3  input  s3  validation  data  distribution  FullyReplicated  content  type  image  jpeg  s3  data  type  S3Prefix  train  annotation  sagemaker  session  s3  input  s3  train  annotation  distribution  FullyReplicated  content  type  image  jpeg  s3  data  type  S3Pre,amazon
fix  validation  annotation  sagemaker  session  s3  input  s3  validation  annotation  distribution  FullyReplicated  content  type  image  jpeg  s3  data  type  S3Prefix  data  channels  train  train  data  validation  validation  data  train  annotation  train  annotation  validation  annotation  validation  annotation  od  model  fit  inputs  data  channels  logs  True  object  detector  od  model  deploy  initial  instance  count  instance  type  ml  m4  xlarge  wget  test  jpg  https  images  pexels  com  photos  980382  pexels  photo  980382  jpeg  file  name  test  jpg  with  open  file  name  rb  as  image  image  read  bytearray  ne  open  txt  wb  ne  write  import  json  object  detector  content  type  image  jpeg  results  object  detector  predict  detections  json  loads  results  print  detections  def  visualize  detection  img  file  dets  classes  thresh  visualize  detections  in  one  image  Parameters  img  numpy  array  image  in  bgr  format  dets  numpy  array  ssd  detections  numpy,amazon
  array  id  score  x1  y1  x2  y2  each  row  is  one  object  classes  tuple  or  list  of  str  class  names  thresh  float  score  threshold  import  random  import  matplotlib  pyplot  as  plt  import  matplotlib  image  as  mpimg  img  mpimg  imread  img  file  plt  imshow  img  height  img  shape  width  img  shape  colors  dict  for  det  in  dets  klass  score  x0  y0  x1  y1  det  if  score  thresh  continue  cls  id  int  klass  if  cls  id  not  in  colors  colors  cls  id  random  random  random  random  random  random  xmin  int  x0  width  ymin  int  y0  height  xmax  int  x1  width  ymax  int  y1  height  rect  plt  Rectangle  xmin  ymin  xmax  xmin  ymax  ymin  fill  False  edgecolor  colors  cls  id  linewidth  plt  gca  add  patch  rect  class  name  str  cls  id  if  classes  and  len  classes  cls  id  class  name  classes  cls  id  plt  gca  text  xmin  ymin  3f  format  class  name  score  bbox  dict  facecolor  colors  cls  id  alpha  fontsize  12  color  white  plt  show  object  cate,amazon
gories  person  bicycle  car  motorbike  aeroplane  bus  train  truck  boat  traffic  light  fire  hydrant  stop  sign  parking  meter  bench  bird  cat  dog  horse  sheep  cow  elephant  bear  zebra  giraffe  backpack  umbrella  handbag  tie  suitcase  frisbee  skis  snowboard  sports  ball  kite  baseball  bat  baseball  glove  skateboard  surfboard  tennis  racket  bottle  wine  glass  cup  fork  knife  spoon  bowl  banana  apple  sandwich  orange  broccoli  carrot  hot  dog  pizza  donut  cake  chair  sofa  pottedplant  bed  diningtable  toilet  tvmonitor  laptop  mouse  remote  keyboard  cell  phone  microwave  oven  toaster  sink  refrigerator  book  clock  vase  scissors  teddy  bear  hair  drier  toothbrush  Setting  threshold  20  will  only  plot  detection  results  that  have  confidence  score  greater  than  20  threshold  20  Visualize  the  detections  visualize  detection  file  name  detections  prediction  object  categories  threshold  sagemaker  Session  delete  endpoint  object  detector,amazon
  endpoint  ,amazon
import  os  import  io  import  re  import  boto3  import  pandas  as  pd  import  numpy  as  np  import  time  from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  S3  bucket  prefix  sagemaker  DEMO  parquet  conda  install  conda  forge  fastparquet  scikit  learn  time  import  pickle  gzip  numpy  urllib  request  json  Load  the  dataset  urllib  request  urlretrieve  http  deeplearning  net  data  mnist  mnist  pkl  gz  mnist  pkl  gz  with  gzip  open  mnist  pkl  gz  rb  as  train  set  valid  set  test  set  pickle  load  encoding  latin1  from  fastparquet  import  write  from  fastparquet  import  ParquetFile  def  save  as  parquet  file  dataset  filename  label  col  dataset  dataset  data  pd  DataFrame  data  label  col  data  columns  data  columns  astype  str  Parquet  expexts  the  column  names  to  be  strings  write  filename  data  def  read  parquet  file  filename  pf  ParquetFile  filename  return  pf  to  pandas  def  features  and  target  df  label ,amazon
 col  df  loc  df  columns  label  col  values  df  label  col  values  return  trainFile  train  parquet  validFile  valid  parquet  testFile  test  parquet  label  col  target  save  as  parquet  file  train  set  trainFile  label  col  save  as  parquet  file  valid  set  validFile  label  col  save  as  parquet  file  test  set  testFile  label  col  dfTrain  read  parquet  file  trainFile  dfValid  read  parquet  file  validFile  dfTest  read  parquet  file  testFile  train  train  features  and  target  dfTrain  label  col  valid  valid  features  and  target  dfValid  label  col  test  test  features  and  target  dfTest  label  col  import  io  import  numpy  as  np  import  sagemaker  amazon  common  as  smac  trainVectors  np  array  tolist  for  in  train  astype  float32  trainLabels  np  where  np  array  tolist  for  in  train  astype  float32  bufTrain  io  BytesIO  smac  write  numpy  to  dense  tensor  bufTrain  trainVectors  trainLabels  bufTrain  seek  validVectors  np  array  tolist  for  ,amazon
in  valid  astype  float32  validLabels  np  where  np  array  tolist  for  in  valid  astype  float32  bufValid  io  BytesIO  smac  write  numpy  to  dense  tensor  bufValid  validVectors  validLabels  bufValid  seek  import  boto3  import  os  key  recordio  pb  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  key  upload  fileobj  bufTrain  s3  train  data  s3  train  format  bucket  prefix  key  print  uploaded  training  data  location  format  s3  train  data  boto3  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  key  upload  fileobj  bufValid  s3  validation  data  s3  validation  format  bucket  prefix  key  print  uploaded  validation  data  location  format  s3  validation  data  containers  us  west  174872318107  dkr  ecr  us  west  amazonaws  com  linear  learner  latest  us  east  382416733822  dkr  ecr  us  east  amazonaws  com  linear  learner  latest  us  east  404615174143  dkr  ecr  us  east  amazonaws  com  linear  learner  latest,amazon
  eu  west  438346466558  dkr  ecr  eu  west  amazonaws  com  linear  learner  latest  linear  job  DEMO  linear  time  strftime  time  gmtime  print  Job  name  is  linear  job  linear  training  params  RoleArn  role  TrainingJobName  linear  job  AlgorithmSpecification  TrainingImage  containers  boto3  Session  region  name  TrainingInputMode  File  ResourceConfig  InstanceCount  InstanceType  ml  c4  2xlarge  VolumeSizeInGB  10  InputDataConfig  ChannelName  train  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  train  format  bucket  prefix  S3DataDistributionType  FullyReplicated  CompressionType  None  RecordWrapperType  None  ChannelName  validation  DataSource  S3DataSource  S3DataType  S3Prefix  S3Uri  s3  validation  format  bucket  prefix  S3DataDistributionType  FullyReplicated  CompressionType  None  RecordWrapperType  None  OutputDataConfig  S3OutputPath  s3  format  bucket  prefix  HyperParameters  feature  dim  784  mini  batch  size  200  predictor  type  binary  classifier  epoc,amazon
hs  10  num  models  32  loss  absolute  loss  StoppingCondition  MaxRuntimeInSeconds  60  60  time  sm  boto3  Session  client  sagemaker  sm  create  training  job  linear  training  params  status  sm  describe  training  job  TrainingJobName  linear  job  TrainingJobStatus  print  status  sm  get  waiter  training  job  completed  or  stopped  wait  TrainingJobName  linear  job  if  status  Failed  message  sm  describe  training  job  TrainingJobName  linear  job  FailureReason  print  Training  failed  with  the  following  error  format  message  raise  Exception  Training  job  failed  sm  describe  training  job  TrainingJobName  linear  job  TrainingJobStatus  ,amazon
matplotlib  inline  import  json  import  re  import  datetime  import  math  import  numpy  as  np  import  matplotlib  pyplot  as  plt  from  textblob  import  TextBlob  from  iexfinance  import  get  historical  data  DAY  datetime  timedelta  days  Read  our  mined  Tweets  into  list  7M  Tweets  from  2016  to  31  2017  for  Apple  or  2M  from  2016  to  2018  for  Tesla  tweets  for  in  range  file  name  apple  tweets  2016  str  json  with  open  file  name  as  read  file  tweets  json  load  read  file  common  name  of  the  company  we  are  analyzing  NAME  Apple  TIMEFRAME  over  which  the  Tweets  were  mined  TIMEFRAME  datetime  datetime  2016  datetime  datetime  2017  31  specify  the  number  of  input  features  used  in  our  machine  learning  model  NUM  FEATURES  the  increment  of  days  before  the  current  day  available  to  the  predictive  model  INC  datetime  timedelta  days  record  processTweets  TIMEFRAME  NUM  FEATURES  ticker  APPL  exportRecord  record  apple  reco,amazon
rd  ex  Process  Tweets  into  Numpy  array  with  Numpy  array  entries  representing  data  points  each  trading  day  is  associated  with  data  point  consisting  of  these  input  features  and  label  today  tweets  today  tweets  yesterday  tweets  yesterday  tweets  yesterday  price  ROC  today  price  ROC  Sets  thresholds  of  sentiment  polarity  above  which  Tweets  are  considered  positive  and  below  which  Tweets  are  considered  negative  other  Tweets  are  ignored  as  neutral  factual  POSITIVE  THRESHOLD  NEGATIVE  THRESHOLD  def  processTweets  timeframe  num  features  ticker  stock  data  get  historical  data  ticker  timeframe  timeframe  output  format  json  record  generateRecord  stock  data  num  features  ticker  analyze  sentiment  of  Tweets  for  in  range  len  tweets  tweet  TextBlob  tweets  text  date  toDatetime  tweets  timestamp  10  if  date  not  in  record  continue  we  only  analyze  Tweets  posted  on  trading  days  current  sentiment  sentiment  tweet  ti,amazon
cker  if  current  sentiment  POSITIVE  THRESHOLD  record  date  elif  current  sentiment  NEGATIVE  THRESHOLD  record  date  fill  in  yesterday  data  for  today  in  record  tomorrow  today  DAY  if  tomorrow  in  record  record  tomorrow  record  today  record  tomorrow  record  today  record  addStockTrends  stock  data  record  ticker  record  list  np  array  list  record  values  return  record  list  Returns  the  record  dictionary  which  associates  datetime  objects  with  data  points  def  generateRecord  stock  data  num  features  ticker  datetime  list  for  date  string  in  stock  data  ticker  keys  datetime  list  append  toDatetime  date  string  datetime  list  sort  datetime  list  date  for  date  in  datetime  list  if  date  INC  in  datetime  list  record  for  date  in  datetime  list  record  date  np  zeros  num  features  astype  float32  return  record  converts  string  of  the  form  YYYY  MM  DD  into  the  corresponding  datetime  object  def  toDatetime  date  string  re,amazon
turn  datetime  datetime  int  date  string  int  date  string  int  date  string  10  accepts  TextBlob  formats  as  parsable  tweet  and  returns  its  polarity  cashtag  re  re  compile  def  sentiment  tweet  ticker  if  tweet  find  http  return  tweet  tweet  replace  ticker  NAME  replace  ticker  NAME  replace  NAME  NAME  if  cashtag  re  search  str  tweet  None  return  return  tweet  polarity  adds  price  rate  of  change  ROC  to  the  data  points  in  the  record  def  addStockTrends  stock  data  record  ticker  for  today  in  list  record  keys  if  record  today  and  record  today  del  record  today  else  last  day  today  INC  record  today  ROC  stock  data  last  day  ticker  record  today  ROC  stock  data  today  ticker  empty  dates  date  for  date  in  record  if  record  date  and  record  date  for  date  in  empty  dates  record  pop  date  None  return  record  converts  datetime  object  to  string  of  form  YYYY  MM  DD  def  datetimeToString  date  str  date  year  str ,amazon
 date  month  str  date  day  return  if  int  10  else  if  int  10  else  returns  the  price  rate  of  change  of  stock  on  the  given  datetime  with  day  def  ROC  stock  data  date  ticker  todays  close  stock  data  ticker  datetimeToString  date  close  yesterdays  close  stock  data  ticker  datetimeToString  date  DAY  close  return  todays  close  yesterdays  close  yesterdays  close  export  our  list  with  Numpy  def  exportRecord  record  name  np  save  twitter  project  name  npy  record  MODEL  ACCURACY  REPORTS  accuracyReport  Apple  7M  Tweets  RR  aapl  predictions  True  accuracyReport  Apple  7M  Tweets  aapl  model  predictions  True  accuracyReport  Apple  7M  Tweets  CONTROL  aapl  model  ctrl  predictions  True  accuracyReport  Tesla  270k  Tweets  tsla  model  predictions  True  accuracyReport  Tesla  560k  Tweets  tesla  True  accuracyReport  Tesla  2M  Tweets  tsla  model  predictions  True  accuracyReport  Tesla  2M  Tweets  CONTROL  tsla  model  ctrl  predictions  True  d,amazon
ef  accuracyReport  name  file  save  model  predictions  np  load  twitter  project  file  npy  mae  MAE  model  predictions  mape  MAPE  model  predictions  print  Mean  Absolute  Error  of  name  mae  plotModelAccuracy  name  model  predictions  print  Mean  Absolute  Percentage  Error  mape  if  save  plt  savefig  twitter  project  name  png  def  plotModelAccuracy  name  model  predictions  estimates  model  predictions  actuals  model  predictions  days  range  len  estimates  plt  plot  days  estimates  bo  label  ROC  Prediction  plt  plot  days  actuals  ro  label  Actual  ROC  plt  title  name  Model  Accuracy  plt  xlabel  Day  plt  ylabel  Price  rate  of  change  plt  legend  plt  grid  True  def  MAE  model  predictions  residuals  for  prediction  in  model  predictions  residuals  abs  prediction  prediction  return  residuals  len  model  predictions  def  MAPE  model  predictions  residuals  for  prediction  in  model  predictions  if  prediction  residuals  abs  prediction  prediction  pre,amazon
diction  return  residuals  len  model  predictions  ,amazon
import  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  Import  all  deployment  statistics  since  2013  train  pd  read  csv  deployment  stats  csv  train  head  rbr  train  prod  rollback  reason  rbr  replace  12007  Bad  Instructions  inplace  True  rbr  replace  12802  Requirement  Changes  inplace  True  rbr  replace  13100  Config  Issues  inplace  True  rbr  replace  12101  Dependency  inplace  True  rbr  replace  12008  Mistakes  inplace  True  rbr  replace  12006  Exceptions  Logging  inplace  True  rbr  replace  12727  Exchange  Broker  inplace  True  rbr  replace  12803  Hardware  Infra  inplace  True  rbr  replace  12003  New  Bug  inplace  True  rbr  replace  12005  Performancec  inplace  True  rbr  replace  12500  Planned  inplace  True  rbr  replace  12100  Precautionary  inplace  True  rbr  replace  12004  Regression  inplace  True  rbr  value  counts  plot  kind  bar  plt  show  now  take  look  whether  rollback  are  concentrated  at  certain  dates  train  ha,google
s  rollback  train  pilot  rollback  reason  isnull  train  prod  rollback  reason  isnull  train  resolution  pd  to  datetime  train  resolution  date  errors  coerce  dt  to  period  trb  train  pivot  table  index  resolution  values  has  rollback  aggfunc  np  sum  plt  figure  figsize  trb  plot  kind  line  plt  xlabel  Quarter  fontsize  12  plt  ylabel  Rollback  Count  fontsize  12  plt  show  add  intelligence  on  the  number  of  years  in  working  experience  and  plot  whether  rollback  is  correlated  to  this  in  any  way  fill  in  developer  as  reporter  if  developer  is  NULL  train  developer  replace  NULL  missing  inplace  True  train  developer  unique  year  exp  ahui  13  jng  20  szeng  elam  13  tleung  12  hkyeung  10  hng  wchan  ajing  ttong  18  otsang  mchauhan  kpang  jsuen  vpatil  rbruce  11  tlai  btegarden  13  llam  10  avale  skecskes  azhao  kzhu  12  bflynn  11  wlu  szhang  13  mleung  vneeraj  sjain  myao  dxu  bdravid  12  mswami  17  ljackson  16  jcheng  l,google
leung  13  hmamdani  jpeng  achan  12  mchiu  12  mgoldschmidt  15  afacchi  abeaulne  pmembrey  eyang  odeblic  dchandola  llaszko  11  ahampshire  13  jchandran  10  kaddanki  syi  ggriffin  20  train  years  exp  train  developer  map  year  exp  trb  yr  train  pivot  table  index  years  exp  values  has  rollback  aggfunc  np  sum  plt  figure  figsize  trb  yr  plot  kind  bar  plt  xlabel  Yr  of  Experience  fontsize  12  plt  ylabel  Count  fontsize  12  plt  show  Breaking  down  by  category  applications  rbc  train  category  id  rbc  replace  10100  Auto  Trader  inplace  True  rbc  replace  10000  Framework  inplace  True  rbc  replace  10700  GW  Data  inplace  True  rbc  replace  10701  GW  Execution  inplace  True  rbc  replace  10703  Others  inplace  True  rbc  replace  10300  DevOps  inplace  True  rbc  replace  10002  UI  inplace  True  rbc  replace  10400  Quant  inplace  True  rbc  replace  10800  TSD  inplace  True  rbc  replace  10200  Performance  inplace  True  rbc  replace  10704,google
  Support  inplace  True  rbc  replace  10101  Others  inplace  True  rbc  replace  10003  Others  inplace  True  rbc  replace  10600  Others  inplace  True  rbc  value  counts  plot  kind  bar  plt  show  generate  the  project  key  sort  by  it  train  project  train  ticket  str  split  str  train  train  sort  values  by  project  resolution  date  find  out  the  last  deployed  date  in  the  same  project  find  number  of  days  train  last  deployed  train  groupby  project  resolution  date  shift  train  time  since  last  deployed  pd  to  datetime  train  resolution  date  pd  to  datetime  train  last  deployed  train  head  get  number  of  deployments  of  the  same  project  in  the  last  days  This  cell  is  disabled  because  the  current  implmentation  takes  too  much  CPU  to  compute  train  resolution  date  pd  to  datetime  train  resolution  date  for  index  row  in  train  iterrows  mask  train  project  row  project  train  resolution  date  row  resolution  date  pd  Timedel,google
ta  days  14  train  resolution  date  row  resolution  date  train  has  rollback  True  train  loc  index  values  sum  mask  insert  to  the  new  column  import  tensorflow  as  tf  ,google
import  sagemaker  import  tensorflow  as  tf  import  os  from  sagemaker  tensorflow  import  TensorFlow  from  sagemaker  import  get  execution  role  role  get  execution  role  QUESTION  INSTANCE  TYPE  ml  c4  xlarge  sagemaker  session  sagemaker  Session  inputs  sagemaker  session  upload  data  path  data  key  prefix  data  print  sagemaker  session  sagemaker  runtime  client  client  config  user  agent  runs  fine  with  framework  version  set  to  crashes  with  classifier  TensorFlow  entry  point  entry  point  py  role  role  training  steps  1e1  evaluation  steps  train  instance  count  train  instance  type  INSTANCE  TYPE  py  version  py2  framework  version  hyperparameters  words  classifier  fit  inputs  ,amazon
import  numpy  as  np  import  os  import  six  moves  urllib  as  urllib  import  sys  import  tarfile  import  tensorflow  as  tf  import  zipfile  from  collections  import  defaultdict  from  io  import  StringIO  from  matplotlib  import  pyplot  as  plt  from  PIL  import  Image  This  is  needed  since  the  notebook  is  stored  in  the  object  detection  folder  sys  path  append  from  object  detection  utils  import  ops  as  utils  ops  if  tf  version  raise  ImportError  Please  upgrade  your  tensorflow  installation  to  v1  or  later  This  is  needed  to  display  the  images  matplotlib  inlinefrom  utils  import  label  map  util  from  utils  import  visualization  utils  as  vis  util  What  model  to  download  MODEL  NAME  ssd  mobilenet  v1  coco  2017  11  17  MODEL  FILE  MODEL  NAME  tar  gz  DOWNLOAD  BASE  http  download  tensorflow  org  models  object  detection  Path  to  frozen  detection  graph  This  is  the  actual  model  that  is  used  for  the  object  detection  PAT,amazon
H  TO  CKPT  MODEL  NAME  frozen  inference  graph  pb  List  of  the  strings  that  is  used  to  add  correct  label  for  each  box  PATH  TO  LABELS  os  path  join  data  mscoco  label  map  pbtxt  NUM  CLASSES  90opener  urllib  request  URLopener  opener  retrieve  DOWNLOAD  BASE  MODEL  FILE  MODEL  FILE  tar  file  tarfile  open  MODEL  FILE  for  file  in  tar  file  getmembers  file  name  os  path  basename  file  name  if  frozen  inference  graph  pb  in  file  name  tar  file  extract  file  os  getcwd  detection  graph  tf  Graph  with  detection  graph  as  default  od  graph  def  tf  GraphDef  with  tf  gfile  GFile  PATH  TO  CKPT  rb  as  fid  serialized  graph  fid  read  od  graph  def  ParseFromString  serialized  graph  tf  import  graph  def  od  graph  def  name  label  map  label  map  util  load  labelmap  PATH  TO  LABELS  categories  label  map  util  convert  label  map  to  categories  label  map  max  num  classes  NUM  CLASSES  use  display  name  True  category  index  lab,amazon
el  map  util  create  category  index  categories  def  load  image  into  numpy  array  image  im  width  im  height  image  size  return  np  array  image  getdata  reshape  im  height  im  width  astype  np  uint8  For  the  sake  of  simplicity  we  will  use  only  images  image1  jpg  image2  jpg  If  you  want  to  test  the  code  with  your  images  just  add  path  to  the  images  to  the  TEST  IMAGE  PATHS  PATH  TO  TEST  IMAGES  DIR  test  images  TEST  IMAGE  PATHS  os  path  join  PATH  TO  TEST  IMAGES  DIR  image  jpg  format  for  in  range  Size  in  inches  of  the  output  images  IMAGE  SIZE  12  def  run  inference  for  single  image  image  graph  with  graph  as  default  with  tf  Session  as  sess  Get  handles  to  input  and  output  tensors  ops  tf  get  default  graph  get  operations  all  tensor  names  output  name  for  op  in  ops  for  output  in  op  outputs  tensor  dict  for  key  in  num  detections  detection  boxes  detection  scores  detection  classes  detecti,amazon
on  masks  tensor  name  key  if  tensor  name  in  all  tensor  names  tensor  dict  key  tf  get  default  graph  get  tensor  by  name  tensor  name  if  detection  masks  in  tensor  dict  The  following  processing  is  only  for  single  image  detection  boxes  tf  squeeze  tensor  dict  detection  boxes  detection  masks  tf  squeeze  tensor  dict  detection  masks  Reframe  is  required  to  translate  mask  from  box  coordinates  to  image  coordinates  and  fit  the  image  size  real  num  detection  tf  cast  tensor  dict  num  detections  tf  int32  detection  boxes  tf  slice  detection  boxes  real  num  detection  detection  masks  tf  slice  detection  masks  real  num  detection  detection  masks  reframed  utils  ops  reframe  box  masks  to  image  masks  detection  masks  detection  boxes  image  shape  image  shape  detection  masks  reframed  tf  cast  tf  greater  detection  masks  reframed  tf  uint8  Follow  the  convention  by  adding  back  the  batch  dimension  tensor  dict  de,amazon
tection  masks  tf  expand  dims  detection  masks  reframed  image  tensor  tf  get  default  graph  get  tensor  by  name  image  tensor  Run  inference  output  dict  sess  run  tensor  dict  feed  dict  image  tensor  np  expand  dims  image  all  outputs  are  float32  numpy  arrays  so  convert  types  as  appropriate  output  dict  num  detections  int  output  dict  num  detections  output  dict  detection  classes  output  dict  detection  classes  astype  np  uint8  output  dict  detection  boxes  output  dict  detection  boxes  output  dict  detection  scores  output  dict  detection  scores  if  detection  masks  in  output  dict  output  dict  detection  masks  output  dict  detection  masks  return  output  dictfor  image  path  in  TEST  IMAGE  PATHS  image  Image  open  image  path  the  array  based  representation  of  the  image  will  be  used  later  in  order  to  prepare  the  result  image  with  boxes  and  labels  on  it  image  np  load  image  into  numpy  array  image  Expand  dim,amazon
ensions  since  the  model  expects  images  to  have  shape  None  None  image  np  expanded  np  expand  dims  image  np  axis  Actual  detection  output  dict  run  inference  for  single  image  image  np  detection  graph  Visualization  of  the  results  of  detection  vis  util  visualize  boxes  and  labels  on  image  array  image  np  output  dict  detection  boxes  output  dict  detection  classes  output  dict  detection  scores  category  index  instance  masks  output  dict  get  detection  masks  use  normalized  coordinates  True  line  thickness  plt  figure  figsize  IMAGE  SIZE  plt  imshow  image  np  ,amazon
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  xgboost  churn  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  import  io  import  os  import  sys  import  time  import  json  from  IPython  display  import  display  from  time  import  strftime  gmtime  import  sagemaker  from  sagemaker  predictor  import  csv  serializer  wget  http  dataminingconsultant  com  DKD2e  data  sets  zip  unzip  DKD2e  data  sets  zipchurn  pd  read  csv  Data  sets  churn  txt  pd  set  option  display  max  columns  500  churn  Frequency  tables  for  each  categorical  feature  for  column  in  churn  select  dtypes  include  object  columns  display  pd  crosstab  index  churn  column  columns  observations  normalize  columns  Histograms  for  each  numeric  features  display  churn  describe  matplotlib  inline  hist  churn  hist  bins  30  sharey  Tru,amazon
e  figsize  10  10  churn  churn  drop  Phone  axis  churn  Area  Code  churn  Area  Code  astype  object  for  column  in  churn  select  dtypes  include  object  columns  if  column  Churn  display  pd  crosstab  index  churn  column  columns  churn  Churn  normalize  columns  for  column  in  churn  select  dtypes  exclude  object  columns  print  column  hist  churn  column  Churn  hist  by  Churn  bins  30  plt  show  display  churn  corr  pd  plotting  scatter  matrix  churn  figsize  12  12  plt  show  churn  churn  drop  Day  Charge  Eve  Charge  Night  Charge  Intl  Charge  axis  model  data  pd  get  dummies  churn  model  data  pd  concat  model  data  Churn  True  model  data  drop  Churn  False  Churn  True  axis  axis  train  data  validation  data  test  data  np  split  model  data  sample  frac  random  state  1729  int  len  model  data  int  len  model  data  train  data  to  csv  train  csv  header  False  index  False  validation  data  to  csv  validation  csv  header  False  index  Fals,amazon
e  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  train  csv  upload  file  train  csv  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  validation  csv  upload  file  validation  csv  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  xgboost  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  xgboost  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  xgboost  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  xgboost  latest  s3  input  train  sagemaker  s3  input  s3  data  s3  train  format  bucket  prefix  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  s3  validation  format  bucket  prefix  content  type  csv  sess  sagemaker  Session  xgb  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  role  train  instance  count  train  instance  type  ml  m4  xlarge  output  path  s3  output  format  bucket  pre,amazon
fix  sagemaker  session  sess  xgb  set  hyperparameters  max  depth  eta  gamma  min  child  weight  subsample  silent  objective  binary  logistic  num  round  100  xgb  fit  train  s3  input  train  validation  s3  input  validation  xgb  predictor  xgb  deploy  initial  instance  count  instance  type  ml  m4  xlarge  xgb  predictor  content  type  text  csv  xgb  predictor  serializer  csv  serializer  xgb  predictor  deserializer  Nonedef  predict  data  rows  500  split  array  np  array  split  data  int  data  shape  float  rows  predictions  for  array  in  split  array  predictions  join  predictions  xgb  predictor  predict  array  decode  utf  return  np  fromstring  predictions  sep  predictions  predict  test  data  as  matrix  pd  crosstab  index  test  data  iloc  columns  np  round  predictions  rownames  actual  colnames  predictions  plt  hist  predictions  plt  show  pd  crosstab  index  test  data  iloc  columns  np  where  predictions  cutoffs  np  arange  01  01  costs  for  in  cutoff,amazon
s  costs  append  np  sum  np  sum  np  array  100  500  100  pd  crosstab  index  test  data  iloc  columns  np  where  predictions  costs  np  array  costs  plt  plot  cutoffs  costs  plt  show  print  Cost  is  minimized  near  cutoff  of  cutoffs  np  argmin  costs  for  cost  of  np  min  costs  sagemaker  Session  delete  endpoint  xgb  predictor  endpoint  ,amazon
conda  install  scipy  matplotlib  inline  import  os  re  import  boto3  import  matplotlib  pyplot  as  plt  import  numpy  as  np  np  set  printoptions  precision  suppress  True  some  helpful  utility  functions  are  defined  in  the  Python  module  generate  example  data  located  in  the  same  directory  as  this  notebook  from  generate  example  data  import  generate  griffiths  data  plot  lda  match  estimated  topics  accessing  the  SageMaker  Python  SDK  import  sagemaker  from  sagemaker  amazon  common  import  numpy  to  record  serializer  from  sagemaker  predictor  import  csv  serializer  json  deserializerfrom  sagemaker  import  get  execution  role  role  get  execution  role  bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  lda  introduction  print  Training  input  output  will  be  stored  in  format  bucket  prefix  print  nIAM  Role  format  role  print  Generating  example  data  num  documents  6000  num  topics  known  alpha  known  beta  documents  topic ,amazon
 mixtures  generate  griffiths  data  num  documents  num  documents  num  topics  num  topics  vocabulary  size  len  documents  separate  the  generated  data  into  training  and  tests  subsets  num  documents  training  int  num  documents  num  documents  test  num  documents  num  documents  training  documents  training  documents  num  documents  training  documents  test  documents  num  documents  training  topic  mixtures  training  topic  mixtures  num  documents  training  topic  mixtures  test  topic  mixtures  num  documents  training  print  documents  training  shape  format  documents  training  shape  print  documents  test  shape  format  documents  test  shape  print  First  training  document  format  documents  print  nVocabulary  size  format  vocabulary  size  print  Known  topic  mixture  of  first  document  format  topic  mixtures  training  print  nNumber  of  topics  format  num  topics  print  Sum  of  elements  format  topic  mixtures  training  sum  matplotlib  inline  fig  p,amazon
lot  lda  documents  training  nrows  ncols  cmap  gray  with  colorbar  True  fig  suptitle  Example  Document  Word  Counts  fig  set  dpi  160  convert  documents  training  to  Protobuf  RecordIO  format  recordio  protobuf  serializer  numpy  to  record  serializer  fbuffer  recordio  protobuf  serializer  documents  training  upload  to  S3  in  bucket  prefix  train  fname  lda  data  s3  object  os  path  join  prefix  train  fname  boto3  Session  resource  s3  Bucket  bucket  Object  s3  object  upload  fileobj  fbuffer  s3  train  data  s3  format  bucket  s3  object  print  Uploaded  data  to  S3  format  s3  train  data  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  select  the  algorithm  container  based  on  this  notebook  current  location  region  name  boto3  Session  region  name  container  get  image  uri  region  name  lda  print  Using  SageMaker  LDA  container  format  container  region  name  session  sagemaker  Session  specify  general  training  job  infor,amazon
mation  lda  sagemaker  estimator  Estimator  container  role  output  path  s3  output  format  bucket  prefix  train  instance  count  train  instance  type  ml  c4  2xlarge  sagemaker  session  session  set  algorithm  specific  hyperparameters  lda  set  hyperparameters  num  topics  num  topics  feature  dim  vocabulary  size  mini  batch  size  num  documents  training  alpha0  run  the  training  job  on  input  data  stored  in  S3  lda  fit  train  s3  train  data  print  Training  job  name  format  lda  latest  training  job  job  name  lda  inference  lda  deploy  initial  instance  count  instance  type  ml  m4  xlarge  LDA  inference  may  work  better  at  scale  on  ml  c4  instances  print  Endpoint  name  format  lda  inference  endpoint  lda  inference  content  type  text  csv  lda  inference  serializer  csv  serializer  lda  inference  deserializer  json  deserializerresults  lda  inference  predict  documents  test  12  print  results  computed  topic  mixtures  np  array  prediction  t,amazon
opic  mixture  for  prediction  in  results  predictions  print  computed  topic  mixtures  print  topic  mixtures  test  known  test  topic  mixture  print  computed  topic  mixtures  computed  topic  mixture  topics  permuted  sagemaker  Session  delete  endpoint  lda  inference  endpoint  ,amazon
import  os  from  pyspark  import  SparkContext  SparkConf  from  pyspark  sql  import  SparkSession  import  sagemaker  from  sagemaker  import  get  execution  role  import  sagemaker  pyspark  role  get  execution  role  Configure  Spark  to  use  the  SageMaker  Spark  dependency  jars  jars  sagemaker  pyspark  classpath  jars  classpath  join  sagemaker  pyspark  classpath  jars  See  the  SageMaker  Spark  Github  repo  under  sagemaker  pyspark  sdk  to  learn  how  to  connect  to  remote  EMR  cluster  running  Spark  from  Notebook  Instance  spark  SparkSession  builder  config  spark  driver  extraClassPath  classpath  master  local  getOrCreate  import  boto3  region  boto3  Session  region  name  trainingData  spark  read  format  libsvm  option  numFeatures  784  option  vectorType  dense  load  s3a  sagemaker  sample  data  spark  mnist  train  format  region  testData  spark  read  format  libsvm  option  numFeatures  784  option  vectorType  dense  load  s3a  sagemaker  sample  data  spark ,amazon
 mnist  test  format  region  trainingData  show  import  random  from  sagemaker  pyspark  import  IAMRole  S3DataPath  from  sagemaker  pyspark  algorithms  import  XGBoostSageMakerEstimator  xgboost  estimator  XGBoostSageMakerEstimator  sagemakerRole  IAMRole  role  trainingInstanceType  ml  m4  xlarge  trainingInstanceCount  endpointInstanceType  ml  m4  xlarge  endpointInitialInstanceCount  xgboost  estimator  setEta  xgboost  estimator  setGamma  xgboost  estimator  setMinChildWeight  xgboost  estimator  setSilent  xgboost  estimator  setObjective  multi  softmax  xgboost  estimator  setNumClasses  10  xgboost  estimator  setNumRound  10  train  model  xgboost  estimator  fit  trainingData  transformedData  model  transform  trainingData  transformedData  show  from  pyspark  sql  types  import  DoubleType  import  matplotlib  pyplot  as  plt  import  numpy  as  np  helper  function  to  display  digit  def  show  digit  img  caption  xlabel  subplot  None  if  subplot  None  subplot  plt  subplots  im,amazon
gr  img  reshape  28  28  subplot  axes  get  xaxis  set  ticks  subplot  axes  get  yaxis  set  ticks  plt  title  caption  plt  xlabel  xlabel  subplot  imshow  imgr  cmap  gray  images  np  array  transformedData  select  features  cache  take  250  clusters  transformedData  select  prediction  cache  take  250  for  cluster  in  range  10  print  nCluster  format  int  cluster  digits  img  for  img  in  zip  clusters  images  if  int  prediction  cluster  height  len  digits  width  plt  rcParams  figure  figsize  width  height  subplots  plt  subplots  height  width  subplots  np  ndarray  flatten  subplots  for  subplot  image  in  zip  subplots  digits  show  digit  image  subplot  subplot  for  subplot  in  subplots  len  digits  subplot  axis  off  plt  show  Delete  the  endpoint  from  sagemaker  pyspark  import  SageMakerResourceCleanup  resource  cleanup  SageMakerResourceCleanup  model  sagemakerClient  resource  cleanup  deleteResources  model  getCreatedResources  ,amazon
data  bucket  amazon  sagemaker  in  practice  workshop  user  number  CHANGE  TO  YOUR  NUMBER  user  name  user  format  user  number  output  bucket  amazon  sagemaker  in  practice  bucket  format  user  name  path  criteo  display  ad  challenge  key  sample  csv  data  location  s3  format  data  bucket  path  key  import  boto3  from  sagemaker  import  get  execution  role  role  get  execution  role  print  role  import  numpy  as  np  For  matrix  operations  and  numerical  processing  import  pandas  as  pd  For  munging  tabular  data  import  matplotlib  pyplot  as  plt  For  charts  and  visualizations  from  IPython  display  import  Image  For  displaying  images  in  the  notebook  from  IPython  display  import  display  For  displaying  outputs  in  the  notebook  from  time  import  gmtime  strftime  For  labeling  SageMaker  models  endpoints  etc  import  sys  For  writing  outputs  to  notebook  import  math  For  ceiling  function  import  json  For  parsing  hosting  outputs  import ,amazon
 os  For  manipulating  filepath  names  import  sagemaker  Amazon  SageMaker  Python  SDK  provides  helper  functions  from  sagemaker  predictor  import  csv  serializer  Converts  strings  for  HTTP  POST  requests  on  inference  from  sagemaker  tuner  import  IntegerParameter  Importing  HPO  elements  from  sagemaker  tuner  import  CategoricalParameter  from  sagemaker  tuner  import  ContinuousParameter  from  sagemaker  tuner  import  HyperparameterTunerdata  pd  read  csv  data  location  header  None  sep  pd  set  option  display  max  columns  500  Make  sure  we  can  see  all  of  the  columns  pd  set  option  display  max  rows  20  Keep  the  output  on  one  page  Histograms  for  each  numeric  features  display  data  describe  matplotlib  inline  hist  data  hist  bins  30  sharey  True  figsize  10  10  display  data  corr  pd  plotting  scatter  matrix  data  figsize  12  12  plt  show  Frequency  tables  for  each  categorical  feature  for  column  in  data  select  dtypes  include,amazon
  object  columns  display  pd  crosstab  index  data  column  columns  observations  normalize  columns  categorical  feature  data  14  unique  values  data  14  unique  print  Number  of  unique  values  in  14th  feature  format  len  unique  values  print  data  14  for  column  in  data  select  dtypes  include  object  columns  size  data  groupby  column  size  print  Column  number  of  categories  format  column  len  size  for  column  in  data  select  dtypes  include  number  columns  size  data  groupby  column  size  print  Column  number  of  categories  format  column  len  size  for  column  in  data  select  dtypes  include  object  columns  print  Converting  column  to  indexed  values  format  column  indexed  column  index  format  column  data  indexed  column  pd  Categorical  data  column  data  indexed  column  data  indexed  column  cat  codescategorical  feature  data  14  index  unique  values  data  14  index  unique  print  Number  of  unique  values  in  14th  feature  format ,amazon
 len  unique  values  print  data  14  index  for  column  in  data  select  dtypes  include  object  columns  data  drop  column  axis  inplace  True  display  data  Replace  all  to  NaN  for  column  in  data  columns  data  column  data  column  replace  np  nan  testing  data  testing  unique  values  data  unique  print  Number  of  unique  values  in  2nd  feature  format  len  testing  unique  values  print  testing  Randomly  sort  the  data  then  split  out  first  70  second  20  and  last  10  data  len  len  data  sampled  data  data  sample  frac  train  data  validation  data  test  data  np  split  sampled  data  int  data  len  int  data  len  train  data  to  csv  train  sample  csv  index  False  header  False  validation  data  to  csv  validation  sample  csv  index  False  header  False  s3client  boto3  Session  resource  s3  train  csv  file  os  path  join  path  train  train  csv  validation  csv  file  os  path  join  path  validation  validation  csv  s3client  Bucket  output  buc,amazon
ket  Object  train  csv  file  upload  file  train  sample  csv  s3client  Bucket  output  bucket  Object  validation  csv  file  upload  file  validation  sample  csv  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  boto3  Session  region  name  xgboost  train  csv  key  s3  train  train  csv  format  output  bucket  path  validation  csv  key  s3  validation  validation  csv  format  output  bucket  path  s3  input  train  sagemaker  s3  input  s3  data  train  csv  key  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  validation  csv  key  content  type  csv  sess  sagemaker  Session  xgb  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  m4  xlarge  base  job  name  user  name  output  path  s3  output  format  output  bucket  path  sagemaker  session  sess  xgb  set  hyperparameters  eval  metric  logloss  objective  binary  logistic  eta  max  depth  10  colsample  bytree  colsam,amazon
ple  bylevel  min  child  weight  rate  drop  num  round  75  gamma  xgb  fit  train  s3  input  train  validation  s3  input  validation  hpo  sess  sagemaker  Session  hpo  xgb  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  m4  xlarge  output  path  s3  output  hpo  format  output  bucket  path  sagemaker  session  hpo  sess  hpo  xgb  set  hyperparameters  eval  metric  logloss  objective  binary  logistic  colsample  bytree  colsample  bylevel  num  round  75  rate  drop  gamma  hyperparameter  ranges  eta  ContinuousParameter  min  child  weight  ContinuousParameter  10  alpha  ContinuousParameter  max  depth  IntegerParameter  10  objective  metric  name  validation  logloss  objective  type  Minimize  tuner  HyperparameterTuner  hpo  xgb  objective  metric  name  hyperparameter  ranges  base  tuning  job  name  user  name  max  jobs  20  max  parallel  jobs  objective  type  objective  type  tuner  fit  train  s3  input  train  validation  s3  inpu,amazon
t  validation  smclient  boto3  client  sagemaker  job  name  tuner  latest  tuning  job  job  name  hpo  job  smclient  describe  hyper  parameter  tuning  job  HyperParameterTuningJobName  job  name  hpo  job  HyperParameterTuningJobStatus  xgb  predictor  xgb  deploy  initial  instance  count  instance  type  ml  m4  xlarge  xgb  predictor  hpo  tuner  deploy  initial  instance  count  instance  type  ml  m4  xlarge  xgb  predictor  content  type  text  csv  xgb  predictor  serializer  csv  serializerxgb  predictor  hpo  content  type  text  csv  xgb  predictor  hpo  serializer  csv  serializerdef  predict  predictor  data  rows  500  split  array  np  array  split  data  int  data  shape  float  rows  predictions  for  array  in  split  array  predictions  join  predictions  predictor  predict  array  decode  utf  return  np  fromstring  predictions  sep  predictions  predict  xgb  predictor  test  data  drop  axis  values  hpo  predictions  predict  xgb  predictor  hpo  test  data  drop  axis  values  ro,amazon
ws  actuals  cols  predictions  clicks  np  round  predictions  result  pd  crosstab  index  test  data  columns  clicks  rownames  rows  colnames  cols  display  Single  job  results  display  result  display  result  apply  lambda  sum  axis  hpo  clicks  np  round  hpo  predictions  result  hpo  pd  crosstab  index  test  data  columns  hpo  clicks  rownames  rows  colnames  cols  display  HPO  job  results  display  result  hpo  display  result  hpo  apply  lambda  sum  axis  sagemaker  Session  delete  endpoint  xgb  predictor  endpoint  sagemaker  Session  delete  endpoint  xgb  predictor  hpo  endpoint  ,amazon
import  sagemaker  sagemaker  session  sagemaker  Session  bucket  sagemaker  session  default  bucket  prefix  sagemaker  DEMO  pytorch  rnn  lstm  role  sagemaker  get  execution  role  bash  aws  s3  cp  s3  research  metamind  io  wikitext  wikitext  raw  v1  zip  wikitext  raw  v1  zip  unzip  wikitext  raw  v1  zip  cd  wikitext  raw  mv  wiki  test  raw  test  mv  wiki  train  raw  train  mv  wiki  valid  raw  valid  head  wikitext  raw  traininputs  sagemaker  session  upload  data  path  wikitext  raw  bucket  bucket  key  prefix  prefix  print  input  spec  in  this  case  just  an  S3  path  format  inputs  pygmentize  source  train  py  ls  sourcefrom  sagemaker  pytorch  import  PyTorch  estimator  PyTorch  entry  point  train  py  role  role  framework  version  train  instance  count  train  instance  type  ml  p2  xlarge  source  dir  source  available  hyperparameters  emsize  nhid  nlayers  lr  clip  epochs  batch  size  bptt  dropout  tied  seed  log  interval  hyperparameters  epochs  tied,amazon
  True  estimator  fit  training  inputs  pygmentize  source  generate  py  from  sagemaker  predictor  import  RealTimePredictor  json  serializer  json  deserializer  class  JSONPredictor  RealTimePredictor  def  init  self  endpoint  name  sagemaker  session  super  JSONPredictor  self  init  endpoint  name  sagemaker  session  json  serializer  json  deserializer  from  sagemaker  pytorch  import  PyTorchModel  training  job  name  estimator  latest  training  job  name  desc  sagemaker  session  sagemaker  client  describe  training  job  TrainingJobName  training  job  name  trained  model  location  desc  ModelArtifacts  S3ModelArtifacts  model  PyTorchModel  model  data  trained  model  location  role  role  framework  version  entry  point  generate  py  source  dir  source  predictor  cls  JSONPredictor  predictor  model  deploy  initial  instance  count  instance  type  ml  m4  xlarge  input  seed  111  temperature  words  100  response  predictor  predict  input  print  response  sagemaker  sessio,amazon
n  delete  endpoint  predictor  endpoint  ,amazon
mkdir  data  wget  data  aclImdb  v1  tar  gz  http  ai  stanford  edu  amaas  data  sentiment  aclImdb  v1  tar  gz  tar  zxf  data  aclImdb  v1  tar  gz  dataimport  os  import  glob  def  read  imdb  data  data  dir  data  aclImdb  data  labels  for  data  type  in  train  test  data  data  type  labels  data  type  for  sentiment  in  pos  neg  data  data  type  sentiment  labels  data  type  sentiment  path  os  path  join  data  dir  data  type  sentiment  txt  files  glob  glob  path  for  in  files  with  open  as  review  data  data  type  sentiment  append  review  read  Here  we  represent  positive  review  by  and  negative  review  by  labels  data  type  sentiment  append  if  sentiment  pos  else  assert  len  data  data  type  sentiment  len  labels  data  type  sentiment  data  size  does  not  match  labels  size  format  data  type  sentiment  return  data  labelsdata  labels  read  imdb  data  print  IMDB  reviews  train  pos  neg  test  pos  neg  format  len  data  train  pos  len  data ,amazon
 train  neg  len  data  test  pos  len  data  test  neg  from  sklearn  utils  import  shuffle  def  prepare  imdb  data  data  labels  Prepare  training  and  test  sets  from  IMDb  movie  reviews  Combine  positive  and  negative  reviews  and  labels  data  train  data  train  pos  data  train  neg  data  test  data  test  pos  data  test  neg  labels  train  labels  train  pos  labels  train  neg  labels  test  labels  test  pos  labels  test  neg  Shuffle  reviews  and  corresponding  labels  within  training  and  test  sets  data  train  labels  train  shuffle  data  train  labels  train  data  test  labels  test  shuffle  data  test  labels  test  Return  unified  training  data  test  data  training  labels  test  labets  return  data  train  data  test  labels  train  labels  testtrain  test  train  test  prepare  imdb  data  data  labels  print  IMDb  reviews  combined  train  test  format  len  train  len  test  train  100  import  nltk  nltk  download  stopwords  from  nltk  corpus  import  stop,amazon
words  from  nltk  stem  porter  import  stemmer  PorterStemmer  import  re  from  bs4  import  BeautifulSoup  def  review  to  words  review  text  BeautifulSoup  review  html  parser  get  text  Remove  HTML  tags  text  re  sub  zA  Z0  text  lower  Convert  to  lower  case  words  text  split  Split  string  into  words  words  for  in  words  if  not  in  stopwords  words  english  Remove  stopwords  words  PorterStemmer  stem  for  in  words  stem  return  wordsimport  pickle  cache  dir  os  path  join  cache  sentiment  analysis  where  to  store  cache  files  os  makedirs  cache  dir  exist  ok  True  ensure  cache  directory  exists  def  preprocess  data  data  train  data  test  labels  train  labels  test  cache  dir  cache  dir  cache  file  preprocessed  data  pkl  Convert  each  review  to  words  read  from  cache  if  available  If  cache  file  is  not  None  try  to  read  from  it  first  cache  data  None  if  cache  file  is  not  None  try  with  open  os  path  join  cache  dir  cach,amazon
e  file  rb  as  cache  data  pickle  load  print  Read  preprocessed  data  from  cache  file  cache  file  except  pass  unable  to  read  from  cache  but  that  okay  If  cache  is  missing  then  do  the  heavy  lifting  if  cache  data  is  None  Preprocess  training  and  test  data  to  obtain  words  for  each  review  words  train  list  map  review  to  words  data  train  words  test  list  map  review  to  words  data  test  words  train  review  to  words  review  for  review  in  data  train  words  test  review  to  words  review  for  review  in  data  test  Write  to  cache  file  for  future  runs  if  cache  file  is  not  None  cache  data  dict  words  train  words  train  words  test  words  test  labels  train  labels  train  labels  test  labels  test  with  open  os  path  join  cache  dir  cache  file  wb  as  pickle  dump  cache  data  print  Wrote  preprocessed  data  to  cache  file  cache  file  else  Unpack  data  loaded  from  cache  file  words  train  words  test  labels  tr,amazon
ain  labels  test  cache  data  words  train  cache  data  words  test  cache  data  labels  train  cache  data  labels  test  return  words  train  words  test  labels  train  labels  test  Preprocess  data  train  test  train  test  preprocess  data  train  test  train  test  import  numpy  as  np  from  sklearn  feature  extraction  text  import  CountVectorizer  from  sklearn  externals  import  joblib  joblib  is  an  enhanced  version  of  pickle  that  is  more  efficient  for  storing  NumPy  arrays  def  extract  BoW  features  words  train  words  test  vocabulary  size  5000  cache  dir  cache  dir  cache  file  bow  features  pkl  Extract  Bag  of  Words  for  given  set  of  documents  already  preprocessed  into  words  If  cache  file  is  not  None  try  to  read  from  it  first  cache  data  None  if  cache  file  is  not  None  try  with  open  os  path  join  cache  dir  cache  file  rb  as  cache  data  joblib  load  print  Read  features  from  cache  file  cache  file  except  pass  una,amazon
ble  to  read  from  cache  but  that  okay  If  cache  is  missing  then  do  the  heavy  lifting  if  cache  data  is  None  Fit  vectorizer  to  training  documents  and  use  it  to  transform  them  NOTE  Training  documents  have  already  been  preprocessed  and  tokenized  into  words  pass  in  dummy  functions  to  skip  those  steps  preprocessor  lambda  vectorizer  CountVectorizer  max  features  vocabulary  size  preprocessor  lambda  tokenizer  lambda  already  preprocessed  features  train  vectorizer  fit  transform  words  train  toarray  Apply  the  same  vectorizer  to  transform  the  test  documents  ignore  unknown  words  features  test  vectorizer  transform  words  test  toarray  NOTE  Remember  to  convert  the  features  using  toarray  for  compact  representation  Write  to  cache  file  for  future  runs  store  vocabulary  as  well  if  cache  file  is  not  None  vocabulary  vectorizer  vocabulary  cache  data  dict  features  train  features  train  features  test  features  ,amazon
test  vocabulary  vocabulary  with  open  os  path  join  cache  dir  cache  file  wb  as  joblib  dump  cache  data  print  Wrote  features  to  cache  file  cache  file  else  Unpack  data  loaded  from  cache  file  features  train  features  test  vocabulary  cache  data  features  train  cache  data  features  test  cache  data  vocabulary  Return  both  the  extracted  features  as  well  as  the  vocabulary  return  features  train  features  test  vocabulary  Extract  Bag  of  Words  features  for  both  training  and  test  datasets  train  test  vocabulary  extract  BoW  features  train  test  import  pandas  as  pd  val  pd  DataFrame  train  10000  train  pd  DataFrame  train  10000  val  pd  DataFrame  train  10000  train  pd  DataFrame  train  10000  test  pd  DataFrame  test  test  pd  DataFrame  test  First  we  make  sure  that  the  local  directory  in  which  we  like  to  store  the  training  and  validation  csv  files  exists  data  dir  data  xgboost  if  not  os  path  exists  data  ,amazon
dir  os  makedirs  data  dir  First  save  the  test  data  to  test  csv  in  the  data  dir  directory  Note  that  we  do  not  save  the  associated  ground  truth  labels  instead  we  will  use  them  later  to  compare  with  our  model  output  Solution  The  test  data  shouldn  contain  the  ground  truth  labels  as  they  are  what  the  model  is  trying  to  predict  We  will  end  up  using  them  afterward  to  compare  the  predictions  to  pd  concat  test  test  axis  to  csv  os  path  join  data  dir  test  csv  header  False  index  False  pd  DataFrame  test  to  csv  os  path  join  data  dir  test  csv  header  False  index  False  pd  concat  val  val  axis  to  csv  os  path  join  data  dir  validation  csv  header  False  index  False  pd  concat  train  train  axis  to  csv  os  path  join  data  dir  train  csv  header  False  index  False  To  save  bit  of  memory  we  can  set  text  train  val  train  and  val  to  None  train  val  train  val  Noneimport  sagemaker  session,amazon
  sagemaker  Session  Store  the  current  SageMaker  session  S3  prefix  which  folder  will  we  use  prefix  sentiment  xgboost  test  location  session  upload  data  os  path  join  data  dir  test  csv  key  prefix  prefix  val  location  session  upload  data  os  path  join  data  dir  validation  csv  key  prefix  prefix  train  location  session  upload  data  os  path  join  data  dir  train  csv  key  prefix  prefix  from  sagemaker  import  get  execution  role  Our  current  execution  role  is  require  when  creating  the  model  as  the  training  and  inference  code  will  need  to  access  the  model  artifacts  role  get  execution  role  We  need  to  retrieve  the  location  of  the  container  which  is  provided  by  Amazon  for  using  XGBoost  As  matter  of  convenience  the  training  and  inference  code  both  use  the  same  container  from  sagemaker  amazon  amazon  estimator  import  get  image  uri  container  get  image  uri  session  boto  region  name  xgboost  TODO  Cr,amazon
eate  SageMaker  estimator  using  the  container  location  determined  in  the  previous  cell  It  is  recommended  that  you  use  single  training  instance  of  type  ml  m4  xlarge  It  is  also  recommended  that  you  use  s3  output  format  session  default  bucket  prefix  as  the  output  path  xgb  None  Solution  xgb  sagemaker  estimator  Estimator  container  The  location  of  the  container  we  wish  to  use  role  What  is  our  current  IAM  Role  train  instance  count  How  many  compute  instances  train  instance  type  ml  m4  xlarge  What  kind  of  compute  instances  output  path  s3  output  format  session  default  bucket  prefix  sagemaker  session  session  TODO  Set  the  XGBoost  hyperparameters  in  the  xgb  object  Don  forget  that  in  this  case  we  have  binary  label  so  we  should  be  using  the  binary  logistic  objective  Solution  xgb  set  hyperparameters  max  depth  eta  gamma  min  child  weight  subsample  silent  objective  binary  logistic  early  st,amazon
opping  rounds  10  num  round  500  First  make  sure  to  import  the  relevant  objects  used  to  construct  the  tuner  from  sagemaker  tuner  import  IntegerParameter  ContinuousParameter  HyperparameterTuner  TODO  Create  the  hyperparameter  tuner  object  xgb  hyperparameter  tuner  None  Solution  xgb  hyperparameter  tuner  HyperparameterTuner  estimator  xgb  The  estimator  object  to  use  as  the  basis  for  the  training  jobs  objective  metric  name  validation  rmse  The  metric  used  to  compare  trained  models  objective  type  Minimize  Whether  we  wish  to  minimize  or  maximize  the  metric  max  jobs  20  The  total  number  of  models  to  train  max  parallel  jobs  The  number  of  models  to  train  in  parallel  hyperparameter  ranges  max  depth  IntegerParameter  12  eta  ContinuousParameter  05  min  child  weight  IntegerParameter  subsample  ContinuousParameter  gamma  ContinuousParameter  10  s3  input  train  sagemaker  s3  input  s3  data  train  location  content ,amazon
 type  csv  s3  input  validation  sagemaker  s3  input  s3  data  val  location  content  type  csv  xgb  hyperparameter  tuner  fit  train  s3  input  train  validation  s3  input  validation  xgb  hyperparameter  tuner  wait  TODO  Create  new  estimator  object  attached  to  the  best  training  job  found  during  hyperparameter  tuning  xgb  attached  None  Solution  xgb  attached  sagemaker  estimator  Estimator  attach  xgb  hyperparameter  tuner  best  training  job  TODO  Create  transformer  object  from  the  attached  estimator  Using  an  instance  count  of  and  an  instance  type  of  ml  m4  xlarge  should  be  more  than  enough  xgb  transformer  None  Solution  xgb  transformer  xgb  attached  transformer  instance  count  instance  type  ml  m4  xlarge  TODO  Start  the  transform  job  Make  sure  to  specify  the  content  type  and  the  split  type  of  the  test  data  xgb  transformer  transform  test  location  content  type  text  csv  split  type  Line  xgb  transformer  wait  ,amazon
aws  s3  cp  recursive  xgb  transformer  output  path  data  dirpredictions  pd  read  csv  os  path  join  data  dir  test  csv  out  header  None  predictions  round  num  for  num  in  predictions  squeeze  values  from  sklearn  metrics  import  accuracy  score  accuracy  score  test  predictions  First  we  will  remove  all  of  the  files  contained  in  the  data  dir  directory  rm  data  dir  And  then  we  delete  the  directory  itself  rmdir  data  dir  Similarly  we  will  remove  the  files  in  the  cache  dir  directory  and  the  directory  itself  rm  cache  dir  rmdir  cache  dir  ,amazon
pip  install  upgrade  watson  developer  cloud  coding  utf  import  json  import  re  import  watson  developer  cloud  assistant  watson  developer  cloud  AssistantV1  username  password  version  2018  02  16  workspace  assistant  create  workspace  language  ja  name  description  2018  24  entities  entity  sys  date  entity  sys  time  entity  sys  currency  entity  sys  percentage  entity  sys  number  workspaceid  workspace  workspace  id  mynode  assistant  create  dialog  node  workspace  id  workspaceid  title  conditions  welcome  output  text  dialog  node  start  mynode  assistant  create  dialog  node  workspace  id  workspaceid  title  conditions  anything  else  output  text  dialog  node  end  previous  sibling  start  response  assistant  list  intents  workspace  id  workspaceid  myintent  myintents  for  index  intent  in  enumerate  response  intents  index  intent  intent  find  if  intent  intent  index  myintent  myintent  intent  intent  index  myintents  append  myintent  for  fn,ibm
ame  in  myintents  assistant  create  dialog  node  workspace  id  workspaceid  title  fname  conditions  true  dialog  node  fname  node  type  folder  digress  in  returns  previous  sibling  start  print  Creating  folder  fname  for  fname  in  myintents  for  index  intent  in  enumerate  response  intents  nodename  intent  intent  if  nodename  find  fname  myparent  fname  assistant  create  dialog  node  workspace  id  workspaceid  dialog  node  nodename  conditions  nodename  output  text  title  intent  description  parent  myparent  print  Creating  node  nodename  response  assistant  update  dialog  node  workspace  id  workspaceid  dialog  node  General  Greetings  new  output  text  Watson  Studio  response  assistant  update  dialog  node  workspace  id  workspaceid  dialog  node  General  Jokes  new  output  text  response  assistant  update  dialog  node  workspace  id  workspaceid  dialog  node  eCommerce  Create  Product  Order  new  output  text  response  assistant  update  dialog  nod,ibm
e  workspace  id  workspaceid  dialog  node  eCommerce  Cancel  Product  Order  new  output  text  response  assistant  create  dialog  node  workspace  id  workspaceid  dialog  node  eCommerce  Check  Create  Product  Order  parent  eCommerce  Cancel  Product  Order  conditions  sys  date  output  digress  out  allow  all  title  response  assistant  create  dialog  node  workspace  id  workspaceid  dialog  node  eCommerce  Check  Create  Product  Order  parent  eCommerce  Check  Create  Product  Order  node  type  response  condition  conditions  sys  date  after  2018  04  07  output  text  response  assistant  create  dialog  node  workspace  id  workspaceid  dialog  node  eCommerce  Check  Create  Product  Order  parent  eCommerce  Check  Create  Product  Order  node  type  response  condition  output  text  previous  sibling  eCommerce  Check  Create  Product  Order  response  assistant  list  dialog  nodes  workspace  id  workspaceid  for  node  in  response  dialog  nodes  if  node  type  folder  or  ,ibm
node  dialog  node  end  or  node  dialog  node  start  nodename  node  dialog  node  response  assistant  delete  dialog  node  workspace  id  workspaceid  dialog  node  nodename  response  assistant  delete  workspace  workspace  id  workspaceid  ,ibm
import  sagemaker  import  mxnet  as  mx  from  sagemaker  mxnet  import  MXNet  as  MXNetEstimator  import  os  sagemaker  session  sagemaker  Session  role  sagemaker  get  execution  role  mx  test  utils  get  cifar10  Downloads  Cifar  10  dataset  to  data  cifar  sagemaker  session  sagemaker  Session  inputs  sagemaker  session  upload  data  path  data  cifar  key  prefix  data  cifar10  source  dir  os  path  join  os  getcwd  training  code  train  instance  type  ml  p3  8xlarge  estimator  MXNetEstimator  entry  point  multitrain  py  source  dir  source  dir  role  role  train  instance  count  train  instance  type  train  instance  type  hyperparameters  batch  size  256  epochs  140  momentum  88  lr  multiplier  estimator  fit  inputs  predictor  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  load  the  CIFAR  10  samples  and  convert  them  into  format  we  can  use  with  the  prediction  endpoint  from  readimage  utils  import  read  images  filenames  ima,amazon
ges  airplane1  png  images  automobile1  png  images  bird1  png  images  cat1  png  images  deer1  png  images  dog1  png  images  frog1  png  images  horse1  png  images  ship1  png  images  truck1  png  image  data  read  images  filenames  for  img  in  enumerate  image  data  response  predictor  predict  img  print  image  class  format  int  response  estimator  delete  endpoint  ,amazon
import  numpy  as  np  import  tensorflow  as  tf  import  matplotlib  pyplot  as  plt  matplotlib  inline  from  tensorflow  examples  tutorials  mnist  import  input  data  mnist  input  data  read  data  sets  MNIST  data  one  hot  True  mnist  print  type  mnist  print  mnist  train  num  examples  print  mnist  validation  num  examples  print  mnist  test  num  examples  print  MNIST  train  img  mnist  train  images  train  label  mnist  train  labels  test  img  mnist  test  images  test  label  mnist  test  labels  print  print  train  img  type  type  train  img  print  train  img  dimension  train  img  shape  print  train  label  type  type  train  label  print  train  label  dimension  train  label  shape  print  test  img  type  type  test  img  print  test  img  dimension  test  img  shape  print  test  label  type  type  test  label  print  test  label  dimension  test  label  shape  trainimg  mnist  train  images  trainlabel  mnist  train  labels  nsample  randidx  np  random  randint  train,microsoft
img  shape  size  nsample  for  in  curr  img  np  reshape  trainimg  28  28  28  by  28  matrix  curr  label  np  argmax  trainlabel  Label  plt  matshow  curr  img  cmap  plt  get  cmap  gray  plt  title  str  th  Training  Data  Label  is  str  curr  label  import  tensorflow  as  tf  learning  rate  tf  placeholder  tf  float32  None  784  tf  Variable  tf  zeros  784  10  tf  Variable  tf  zeros  10  tf  nn  softmax  tf  matmul  tf  placeholder  tf  float32  None  10  cross  entropy  tf  reduce  mean  tf  nn  softmax  cross  entropy  with  logits  logits  labels  cross  entropy  tf  reduce  mean  tf  reduce  sum  tf  log  reduction  indices  train  step  tf  train  GradientDescentOptimizer  learning  rate  minimize  cross  entropy  init  tf  global  variables  initializer  sess  tf  Session  sess  run  init  for  in  range  1000  batch  xs  batch  ys  mnist  train  next  batch  100  sess  run  train  step  feed  dict  batch  xs  batch  ys  correct  prediction  tf  equal  tf  argmax  tf  argmax  accuracy ,microsoft
 tf  reduce  mean  tf  cast  correct  prediction  tf  float32  print  sess  run  accuracy  feed  dict  mnist  test  images  mnist  test  labels  print  sess  run  feed  dict  mnist  test  images  mnist  test  labels  ,microsoft
from  sagemaker  import  get  execution  role  import  sagemaker  as  sage  import  boto3  import  timerole  get  execution  role  sess  sage  Session  sm  boto3  client  sagemaker  account  sess  boto  session  client  sts  get  caller  identity  Account  region  sess  boto  session  region  namemodel  cntkdemo  ecr  demo  model  just  an  arbitrary  name  for  the  model  instantiation  setup  model  artifact  uri  see  the  HowToPrepare  notebook  model  data  s3  sagemaker  output  cntkdemo  ecr  2018  01  21  13  26  49  604  output  model  tar  gz  format  region  account  setup  image  reference  name  image  dkr  ecr  amazonaws  com  cntkdemo  ecr  format  account  region  image  name  on  ECR  time  create  model  response  sm  create  model  ModelName  model  ExecutionRoleArn  role  PrimaryContainer  Image  image  ModelDataUrl  model  data  print  create  model  response  ModelArn  configuring  endpoint  cntkdemo  endpoint  config  cntkdemo  poc  endpoint  config  time  strftime  time  gmtime  cntkd,amazon
emo  endpoint  cntkdemo  poc  endpoint  time  strftime  time  gmtime  time  create  endpoint  config  response  sm  create  endpoint  config  EndpointConfigName  cntkdemo  endpoint  config  ProductionVariants  InstanceType  ml  m4  xlarge  InitialInstanceCount  ModelName  model  VariantName  cntkdemo  print  Endpoint  Config  Arn  create  endpoint  config  response  EndpointConfigArn  time  be  patience  this  block  can  require  or  more  minutes  for  the  execution  create  endpoint  response  sm  create  endpoint  EndpointName  cntkdemo  endpoint  EndpointConfigName  cntkdemo  endpoint  config  resp  sm  describe  endpoint  EndpointName  cntkdemo  endpoint  status  resp  EndpointStatus  print  Status  status  sm  get  waiter  endpoint  in  service  wait  EndpointName  cntkdemo  endpoint  resp  sm  describe  endpoint  EndpointName  cntkdemo  endpoint  status  resp  EndpointStatus  print  Status  status  print  Arn  resp  EndpointArn  if  status  InService  print  Endpoint  creation  did  not  succeed  pri,amazon
nt  cntkdemo  endpoint  ,amazon
import  os  os  system  aws  s3  cp  s3  sagemaker  workshop  pdx  mnist  utils  py  utils  py  os  system  aws  s3  cp  s3  sagemaker  workshop  pdx  mnist  mnist  py  mnist  py  import  sagemaker  import  utils  import  numpy  as  np  import  matplotlib  pyplot  as  plt  from  tensorflow  contrib  learn  python  learn  datasets  import  mnist  import  tensorflow  as  tf  import  boto3  role  sagemaker  get  execution  role  sagemaker  session  sagemaker  Session  os  system  aws  s3  cp  recursive  s3  sagemaker  workshop  pdx  mnist  data  data  data  sets  mnist  read  data  sets  mnist  data  dtype  tf  uint8  reshape  False  validation  size  5000  utils  convert  to  data  sets  train  train  mnist  data  utils  convert  to  data  sets  validation  validation  mnist  data  utils  convert  to  data  sets  test  test  mnist  data  cat  utils  pybatch  xs  batch  ys  data  sets  train  next  batch  Change  train  to  test  or  select  different  batch  utils  gen  image  batch  xs  show  utils  gen  image,amazon
  batch  xs  show  utils  gen  image  batch  xs  show  inputs  sagemaker  session  upload  data  path  mnist  data  key  prefix  data  mnist  cat  mnist  py  from  sagemaker  tensorflow  import  TensorFlow  mnist  estimator  TensorFlow  entry  point  mnist  py  role  role  training  steps  1000  evaluation  steps  100  train  instance  count  train  instance  type  ml  c4  8xlarge  mnist  estimator  fit  inputs  mnist  predictor  mnist  estimator  deploy  initial  instance  count  instance  type  ml  m4  xlarge  from  tensorflow  examples  tutorials  mnist  import  input  data  mnist  input  data  read  data  sets  tmp  data  one  hot  True  for  in  range  10  data  mnist  test  images  tolist  tensor  proto  tf  make  tensor  proto  values  np  asarray  data  shape  len  data  dtype  tf  float32  predict  response  mnist  predictor  predict  tensor  proto  image  mnist  test  images  image  np  array  image  dtype  float  plt  imshow  image  reshape  28  28  plt  show  label  np  argmax  mnist  test  labels,amazon
  print  Label  is  format  label  prediction  predict  response  outputs  classes  int64Val  print  Prediction  is  format  prediction  print  sagemaker  Session  delete  endpoint  mnist  predictor  endpoint  ,amazon
import  ee  from  PIL  import  ImageTk  from  IPython  display  import  Imageee  Initialize  nex  ee  ImageCollection  NASA  NEX  DCP30  ENSEMBLE  STATS  pull  data  from  the  nasa  nex  dcp30  climate  collection  filtered  nex  filterDate  2010  01  01  2018  07  05  define  filtered  date  range  nex  image  filtered  median  define  the  image  mosaic  from  the  filtered  date  range  point  ee  Geometry  Point  118  37  california  point  buffer  50000000  bounds  getInfo  coordinates  def  createTimeBand  image  Add  time  band  to  the  specified  image  scale  milliseconds  by  large  constant  return  image  addBands  image  metadata  system  time  start  divide  1e18  def  createConstantBand  image  Add  constant  time  band  to  the  specified  image  return  ee  Image  addBands  image  collection  ee  ImageCollection  NASA  NEX  DCP30  ENSEMBLE  STATS  filterDate  ee  Date  2006  01  01  ee  Date  2099  01  01  filter  ee  Filter  eq  scenario  rcp85  map  createTimeBand  map  createConstantBand,google
  select  constant  system  time  start  pr  mean  tasmax  mean  compute  least  squares  regression  coefficients  linearRegression  collection  reduce  ee  Reducer  linearRegression  numX  numY  Compute  robust  linear  regression  coefficients  robustLinearRegression  collection  reduce  ee  Reducer  robustLinearRegression  numX  numY  The  results  are  array  images  that  must  be  flattened  for  display  These  lists  label  the  information  along  each  axis  of  the  arrays  bandNames  constant  time  axis  variation  precip  temp  axis  variation  Flatten  the  array  images  to  get  multi  band  images  according  to  the  labels  lrImage  linearRegression  select  coefficients  arrayFlatten  bandNames  rlrImage  robustLinearRegression  select  coefficients  arrayFlatten  bandNames  Image  url  lrImage  getThumbUrl  region  california  print  rlrImage  getInfo  lrImage  getInfo  print  OLS  estimates  lrImage  reduceRegion  reducer  ee  Reducer  first  geometry  ee  Geometry  Point  96  41  scal,google
e  1000  print  Robust  estimates  rlrImage  reduceRegion  reducer  ee  Reducer  first  geometry  ee  Geometry  Point  96  41  scale  1000  Image  https  developers  google  com  earth  engine  images  Reducers  linearFit  png  precp  collection  ee  ImageCollection  NASA  NEX  DCP30  ENSEMBLE  STATS  filter  ee  Filter  eq  scenario  rcp85  filterDate  ee  Date  2006  01  01  ee  Date  2050  01  01  map  createTimeBand  linearFit  precp  collection  select  system  time  start  pr  mean  reduce  ee  Reducer  linearFit  ,google
import  tensorflow  as  tf  import  os  os  makedirs  tmp  model  os  makedirs  tmp  model  subset  v1  tf  Variable  name  v1  v2  tf  Variable  name  v2  variables  ops  init  op  tf  global  variables  initializer  saver  saver  tf  train  Saver  with  tf  Session  as  sess  sess  run  init  op  v2  ops  tf  assign  v2  sess  run  ops  print  sess  run  tf  global  variables  MNIST  save  path  saver  save  sess  tmp  model  model  ckpt  tmp  model  ckpt  inspect  checkpoint  py  tool  ckpt  python  usr  local  lib  python2  dist  packages  tensorflow  python  tools  inspect  checkpoint  py  file  name  tmp  model  model  ckpt  save  specific  variables  tf  reset  default  graph  v1  tf  Variable  name  v1  v2  tf  Variable  name  v2  init  ops  tf  global  variables  initializer  with  tf  Session  as  sess  sess  run  init  ops  saver  tf  train  Saver  my  v2  v2  save  path  saver  save  sess  tmp  model  subset  model  ckpt  Model  restoration  model  tf  reset  default  graph  v1  tf  Variable  tf  ,microsoft
constant  shape  name  v1  v2  tf  Variable  tf  constant  shape  name  v2  saver  tf  train  Saver  with  tf  Session  as  sess  saver  restore  sess  tmp  model  model  ckpt  print  Model  restored  print  all  values  sess  run  tf  global  variables  print  v2  value  sess  run  v2  Model  restoration  model  tf  reset  default  graph  with  tf  Session  as  sess  saver  tf  train  import  meta  graph  tmp  model  subset  model  ckpt  meta  saver  restore  sess  tf  train  latest  checkpoint  tmp  model  subset  sess  run  tf  global  variables  initializer  all  vars  tf  trainable  variables  for  in  all  vars  print  with  value  name  sess  run  ,microsoft
import  os  import  tensorflow  as  tf  Input  images  as  two  dimensional  tensor  containing  an  arbitrary  number  of  images  represented  strings  import  azureml  contrib  brainwave  models  utils  as  utils  in  images  tf  placeholder  tf  string  image  tensors  utils  preprocess  array  in  images  print  image  tensors  shape  from  azureml  contrib  brainwave  models  import  QuantizedResnet50  Resnet50  model  path  os  path  expanduser  models  model  QuantizedResnet50  model  path  is  frozen  True  feature  tensor  model  import  graph  def  image  tensors  print  model  version  print  feature  tensor  name  print  feature  tensor  shape  classifier  output  model  get  default  classifier  feature  tensor  from  azureml  contrib  brainwave  pipeline  import  ModelDefinition  TensorflowStage  BrainWaveStage  save  path  os  path  expanduser  models  save  model  def  path  os  path  join  save  path  model  def  zip  model  def  ModelDefinition  with  tf  Session  as  sess  model  def  pipe,microsoft
line  append  TensorflowStage  sess  in  images  image  tensors  model  def  pipeline  append  BrainWaveStage  sess  model  model  def  pipeline  append  TensorflowStage  sess  feature  tensor  classifier  output  model  def  save  model  def  path  print  model  def  path  from  azureml  core  import  Workspace  ws  Workspace  from  config  print  ws  name  ws  resource  group  ws  location  ws  subscription  id  sep  from  azureml  core  model  import  Model  model  name  resnet  50  rtai  registered  model  Model  register  ws  model  def  path  model  name  from  azureml  core  webservice  import  Webservice  from  azureml  exceptions  import  WebserviceException  from  azureml  contrib  brainwave  import  BrainwaveWebservice  BrainwaveImage  service  name  imagenet  infer  service  None  try  service  Webservice  ws  service  name  except  WebserviceException  image  config  BrainwaveImage  image  configuration  deployment  config  BrainwaveWebservice  deploy  configuration  service  Webservice  deploy  ,microsoft
from  model  ws  service  name  registered  model  image  config  deployment  config  service  wait  for  deployment  true  import  requests  classes  entries  requests  get  https  raw  githubusercontent  com  Lasagne  Recipes  master  examples  resnet50  imagenet  classes  txt  text  splitlines  results  service  run  snowleopardgaze  jpg  map  results  class  id  confidence  results  enumerate  results  sort  results  by  confidence  sorted  results  sorted  results  key  lambda  reverse  True  print  top  results  for  top  in  sorted  results  print  classes  entries  top  confidence  top  service  delete  registered  model  delete  ,microsoft
time  import  os  import  pandas  as  pd  import  urllib  request  data  dir  data  data  filename  nyc  taxi  csv  data  source  https  raw  githubusercontent  com  numenta  NAB  master  data  realKnownCause  nyc  taxi  csv  if  not  os  path  exists  data  dir  os  makedirs  data  dir  urllib  request  urlretrieve  data  source  data  dir  data  filename  matplotlib  inline  import  matplotlib  import  matplotlib  pyplot  as  plt  taxi  data  pd  read  csv  data  dir  data  filename  delimiter  matplotlib  rcParams  figure  dpi  100  taxi  data  plot  import  os  import  sagemaker  import  boto3  Upload  files  to  S3  sess  sagemaker  Session  bucket  sess  default  bucket  prefix  notebook  rcf  taxi  train  input  sess  upload  data  path  data  dir  data  filename  key  prefix  prefix  Show  S3  path  print  Training  data  is  uploaded  to  train  input  from  sagemaker  import  RandomCutForest  from  sagemaker  import  get  execution  role  specify  general  training  job  information  rcf  RandomCutF,amazon
orest  role  get  execution  role  train  instance  count  train  instance  type  ml  m4  xlarge  data  location  s3  data  dir  format  bucket  prefix  output  path  s3  output  format  bucket  prefix  num  samples  per  tree  512  num  trees  50  automatically  upload  the  training  data  to  S3  and  run  the  training  job  rcf  fit  rcf  record  set  taxi  data  value  as  matrix  reshape  print  Training  job  name  format  rcf  latest  training  job  job  name  rcf  inference  rcf  deploy  initial  instance  count  instance  type  ml  m4  xlarge  print  Endpoint  name  format  rcf  inference  endpoint  from  sagemaker  predictor  import  csv  serializer  json  deserializer  rcf  inference  content  type  text  csv  rcf  inference  serializer  csv  serializer  rcf  inference  accept  application  json  rcf  inference  deserializer  json  deserializertaxi  data  numpy  taxi  data  value  as  matrix  reshape  results  rcf  inference  predict  taxi  data  numpy  scores  datum  score  for  datum  in  resul,amazon
ts  scores  add  scores  to  taxi  data  frame  and  print  first  few  values  taxi  data  score  pd  Series  scores  index  taxi  data  index  taxi  data  head  fig  ax1  plt  subplots  ax2  ax1  twinx  Try  this  out  change  start  and  end  to  zoom  in  on  the  anomaly  found  earlier  in  this  notebook  start  end  len  taxi  data  start  end  5500  6500  taxi  data  subset  taxi  data  start  end  ax1  plot  taxi  data  subset  value  color  C0  alpha  ax2  plot  taxi  data  subset  score  color  C1  ax1  grid  which  major  axis  both  ax1  set  ylabel  Taxi  Ridership  color  C0  ax2  set  ylabel  Anomaly  Score  color  C1  ax1  tick  params  colors  C0  ax2  tick  params  colors  C1  ax1  set  ylim  40000  ax2  set  ylim  min  scores  max  scores  fig  set  figwidth  10  taxi  data  subset  sort  values  score  ascending  False  head  10  sagemaker  Session  delete  endpoint  rcf  inference  endpoint  import  numpy  as  np  def  shingle  data  shingle  size  num  data  len  data  shingled  data  ,amazon
np  zeros  num  data  shingle  size  shingle  size  for  in  range  num  data  shingle  size  shingled  data  data  shingle  size  return  shingled  data  single  data  with  shingle  size  48  one  day  shingle  size  48  prefix  shingled  sagemaker  randomcutforest  shingled  taxi  data  shingled  shingle  taxi  data  values  shingle  size  print  taxi  data  shingled  session  sagemaker  Session  specify  general  training  job  information  rcf  RandomCutForest  role  execution  role  train  instance  count  train  instance  type  ml  m4  xlarge  data  location  s3  format  bucket  prefix  shingled  output  path  s3  output  format  bucket  prefix  shingled  num  samples  per  tree  512  num  trees  50  automatically  upload  the  training  data  to  S3  and  run  the  training  job  rcf  fit  rcf  record  set  taxi  data  shingled  from  sagemaker  predictor  import  csv  serializer  json  deserializer  rcf  inference  rcf  deploy  initial  instance  count  instance  type  ml  m4  xlarge  rcf  inference ,amazon
 content  type  text  csv  rcf  inference  serializer  csv  serializer  rcf  inference  accept  appliation  json  rcf  inference  deserializer  json  deserializerfig  ax1  plt  subplots  ax2  ax1  twinx  Try  this  out  change  start  and  end  to  zoom  in  on  the  anomaly  found  earlier  in  this  notebook  start  end  len  taxi  data  taxi  data  subset  taxi  data  start  end  ax1  plot  taxi  data  value  color  C0  alpha  ax2  plot  scores  color  C1  ax1  grid  which  major  axis  both  ax1  set  ylabel  Taxi  Ridership  color  C0  ax2  set  ylabel  Anomaly  Score  color  C1  ax1  tick  params  colors  C0  ax2  tick  params  colors  C1  ax1  set  ylim  40000  ax2  set  ylim  min  scores  max  scores  fig  set  figwidth  10  taxi  data  subset  sort  values  score  ascending  False  head  10  sagemaker  Session  delete  endpoint  rcf  inference  endpoint  ,amazon
cat  container  Dockerfile  sh  The  name  of  our  algorithm  algorithm  name  tensorflow  cifar10  example  cd  container  chmod  cifar10  train  chmod  cifar10  serve  account  aws  sts  get  caller  identity  query  Account  output  text  Get  the  region  defined  in  the  current  configuration  default  to  us  west  if  none  defined  region  aws  configure  get  region  region  region  us  west  fullname  account  dkr  ecr  region  amazonaws  com  algorithm  name  latest  If  the  repository  doesn  exist  in  ECR  create  it  aws  ecr  describe  repositories  repository  names  algorithm  name  dev  null  if  ne  then  aws  ecr  create  repository  repository  name  algorithm  name  dev  null  fi  Get  the  login  command  from  ECR  and  execute  it  directly  aws  ecr  get  login  region  region  no  include  email  Build  the  docker  image  locally  with  the  image  name  and  then  push  it  to  ECR  with  the  full  name  docker  build  algorithm  name  docker  tag  algorithm  name  fullname ,amazon
 docker  push  fullname  python  utils  generate  cifar10  tfrecords  py  data  dir  tmp  cifar  10  data  There  should  be  three  tfrecords  eval  train  validation  ls  tmp  cifar  10  datafrom  sagemaker  import  get  execution  role  role  get  execution  role  Lets  set  up  our  SageMaker  notebook  instance  for  local  mode  bin  bash  utils  setup  shfrom  sagemaker  estimator  import  Estimator  hyperparameters  train  steps  100  instance  type  local  estimator  Estimator  role  role  train  instance  count  train  instance  type  instance  type  image  name  tensorflow  cifar10  example  latest  hyperparameters  hyperparameters  estimator  fit  file  tmp  cifar  10  data  predictor  estimator  deploy  instance  type  pip  install  opencv  pythonimport  cv2  import  numpy  from  sagemaker  predictor  import  json  serializer  json  deserializer  image  cv2  imread  data  cat  png  resize  as  our  model  is  expecting  images  in  32x32  image  cv2  resize  image  32  32  data  instances  numpy ,amazon
 asarray  image  astype  float  tolist  The  request  and  response  format  is  JSON  for  TensorFlow  Serving  For  more  information  https  www  tensorflow  org  serving  api  rest  predict  api  predictor  accept  application  json  predictor  content  type  application  json  predictor  serializer  json  serializer  predictor  deserializer  json  deserializer  For  more  information  on  the  predictor  class  https  github  com  aws  sagemaker  python  sdk  blob  master  src  sagemaker  predictor  py  predictor  predict  data  predictor  delete  endpoint  S3  prefix  prefix  DEMO  tensorflow  cifar10  import  sagemaker  as  sage  sess  sage  Session  WORK  DIRECTORY  tmp  cifar  10  data  data  location  sess  upload  data  WORK  DIRECTORY  key  prefix  prefix  import  boto3  client  boto3  client  sts  account  client  get  caller  identity  Account  my  session  boto3  session  Session  region  my  session  region  name  algorithm  name  tensorflow  cifar10  example  ecr  image  dkr  ecr  amazonaws  ,amazon
com  latest  format  account  region  algorithm  name  print  ecr  image  from  sagemaker  estimator  import  Estimator  hyperparameters  train  steps  100  instance  type  ml  m4  xlarge  estimator  Estimator  role  role  train  instance  count  train  instance  type  instance  type  image  name  ecr  image  hyperparameters  hyperparameters  estimator  fit  data  location  predictor  estimator  deploy  instance  type  image  cv2  imread  data  cat  png  resize  as  our  model  is  expecting  images  in  32x32  image  cv2  resize  image  32  32  data  instances  numpy  asarray  image  astype  float  tolist  predictor  accept  application  json  predictor  content  type  application  json  predictor  serializer  json  serializer  predictor  deserializer  json  deserializer  predictor  predict  data  predictor  delete  endpoint  ,amazon
Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  bucket  sagemaker  periscopedata  demo  nyc  data  key  enhancedtotodataset  csv  data  location  s3  format  bucket  data  key  set  prefix  for  this  instance  please  input  your  name  in  the  following  set  of  square  brackets  making  sure  to  use  appropriate  directory  characters  prefix  sagemaker  your  name  here  xgboost  dm  import  pandas  as  pd  import  numpy  as  np  import  matplotlib  pyplot  as  plt  from  io  import  BytesIO  import  os  import  time  import  json  import  sagemaker  amazon  common  as  smac  import  sagemaker  from  sagemaker  predictor  import  csv  serializer  json  deserializer  read  the  csv  from  S3  df  pd  read  csv  data  location  display  the  first  records  to  verify  the  import  df  head  some  of  the  categorical  variables  are  currently  encoded  as  numeric  The  number  of  categories  is  low  and  can  easily  be  one  h,amazon
ot  encoded  using  get  dummy  categorical  columns  max  dog  size  min  dog  size  requester  gender  provider  gender  experience  continuous  walk  count  dog  count  requester  fee  previous  client  count  price  per  walk  provider  fee  percent  morning  walks  percent  afternoon  walks  percent  evening  walks  df  pd  get  dummies  df  columns  max  dog  size  min  dog  size  requester  gender  provider  gender  experience  verify  that  the  one  hot  encoding  creation  of  boolean  for  each  categorical  variable  succeeded  df  head  train  data  validation  data  test  data  np  split  df  sample  frac  random  state  1729  int  len  df  int  len  df  pd  concat  train  data  lifetime  revenue  train  data  drop  lifetime  revenue  axis  axis  to  csv  train  csv  index  False  header  False  pd  concat  validation  data  lifetime  revenue  validation  data  drop  lifetime  revenue  axis  axis  to  csv  validation  csv  index  False  header  False  boto3  Session  resource  s3  Bucket  bucket,amazon
  Object  os  path  join  prefix  train  train  csv  upload  file  train  csv  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  validation  csv  upload  file  validation  csv  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  xgboost  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  xgboost  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  xgboost  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  xgboost  latest  ap  northeast  501404015308  dkr  ecr  ap  northeast  amazonaws  com  xgboost  latest  ap  northeast  306986355934  dkr  ecr  ap  northeast  amazonaws  com  xgboost  latest  s3  input  train  sagemaker  s3  input  s3  data  s3  train  format  bucket  prefix  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  s3  validation  format  bucket  prefix  content  type  csv  sess  sagemaker  Session  xgb  sagemaker  estimator  Estimator  containers  boto3  Session  r,amazon
egion  name  role  train  instance  count  train  instance  type  ml  m4  xlarge  output  path  s3  output  format  bucket  prefix  sagemaker  session  sess  xgb  set  hyperparameters  max  depth  eta  gamma  min  child  weight  subsample  silent  objective  reg  linear  use  linear  regression  to  create  continuous  output  num  round  100  xgb  fit  train  s3  input  train  validation  s3  input  validation  xgb  predictor  xgb  deploy  initial  instance  count  instance  type  ml  m4  xlarge  xgb  predictor  content  type  text  csv  xgb  predictor  serializer  csv  serializerdef  predict  data  rows  500  split  array  np  array  split  data  int  data  shape  float  rows  predictions  for  array  in  split  array  predictions  join  predictions  xgb  predictor  predict  array  decode  utf  return  np  fromstring  predictions  sep  predictions  predict  test  data  drop  lifetime  revenue  axis  values  check  the  top  predictions  to  make  sure  they  are  reasonable  print  predictions  def  rmse  p,amazon
redictions  actuals  rmse  predictions  actuals  mean  return  rmse  rmse  predictions  np  round  predictions  actuals  test  data  lifetime  revenue  import  seaborn  as  sns  from  scipy  import  stats  plt  figure  figsize  plt  gca  set  aspect  equal  adjustable  box  plt  scatter  np  round  predictions  test  data  lifetime  revenue  color  thistle  max  lim  max  int  np  max  np  round  predictions  int  np  max  test  data  lifetime  revenue  np  linspace  max  lim  10  plt  plot  linewidth  linestyle  alpha  regression  part  sns  regplot  np  round  predictions  test  data  lifetime  revenue  color  thistle  plt  show  import  seaborn  as  sns  sns  regplot  np  round  predictions  test  data  lifetime  revenue  color  thistle  sagemaker  Session  delete  endpoint  xgb  predictor  endpoint  ,amazon
rm  rf  PIP  BUILD  watson  machine  learning  client  echo  PIP  BUILD  ls  al  pmml  sample  model  Install  the  WML  client  if  required  pip  install  watson  machine  learning  client  upgrade  Enter  your  credentials  here  wml  credentials  url  https  ibm  watson  ml  mybluemix  net  access  key  j86cnGBYCcsTMsUFBCcgKBkjX5PqkAa  t9ma71APWHOEWHn65lGUX  iucyKk  LVaHxGxQ3pIogjgEOjN0TGDTcL0h32gVzPkwMbmHXNpi  FQYUqQmv73SQJrb1WXWeZv  username  5c4c9072  3613  4a1b  9eb9  e77d3c55c04a  password  899810ee  80d6  48bc  9567  2e5a401d4066  instance  id  0c748ad9  d4fc  441e  9b48  a6f5bf9e18d7  from  watson  machine  learning  client  import  WatsonMachineLearningAPIClientwml  client  WatsonMachineLearningAPIClient  wml  credentials  List  information  about  your  WML  service  instance  print  wml  client  instance  details  List  information  about  the  stored  models  wml  client  repository  list  models  List  the  deployed  models  wml  client  deployments  list  pip  install  wget  Download  sample ,ibm
 PMML  model  iris  chaid  xml  from  Git  import  wget  import  ossample  dir  pmml  sample  model  if  not  os  path  isdir  sample  dir  os  mkdir  sample  dir  filename  os  path  join  sample  dir  iris  chaid  xml  if  not  os  path  isfile  filename  filename  wget  download  https  github  com  pmservice  wml  sample  models  raw  master  pmml  iris  species  model  iris  chaid  xml  out  sample  dir  filename  Save  the  downloaded  file  to  the  WML  repository  props  pmml  wml  client  repository  ModelMetaNames  NAME  CHAID  PMML  model  for  Iris  data  wml  client  repository  ModelMetaNames  AUTHOR  NAME  IBM  wml  client  repository  ModelMetaNames  AUTHOR  EMAIL  ibm  ibm  com  wml  client  repository  ModelMetaNames  FRAMEWORK  NAME  pmml  wml  client  repository  ModelMetaNames  FRAMEWORK  VERSION  model  details  wml  client  repository  store  model  filename  props  pmml  wml  client  repository  list  models  Create  the  online  deployment  model  uid  wml  client  repository  get  m,ibm
odel  uid  model  details  model  deployment  wml  client  deployments  create  model  uid  name  Iris  species  prediction  wml  client  deployments  list  Verify  the  online  scoring  endpoint  model  scoring  url  wml  client  deployments  get  scoring  url  model  deployment  print  model  scoring  url  Score  data  and  predict  species  of  flower  scoring  data  fields  Sepal  Length  Sepal  Width  Petal  Length  Petal  Width  values  predictions  wml  client  deployments  score  model  scoring  url  scoring  data  print  model  scoring  url  print  predictions  ,ibm
pip  install  qiskitfrom  qiskit  import  QuantumProgram  from  qiskit  tools  visualization  import  plot  histogram  set  up  registers  and  program  qp  QuantumProgram  qr  qp  create  quantum  register  qr  16  cr  qp  create  classical  register  cr  16  qc  qp  create  circuit  smiley  writer  qr  cr  print  qc  name  rightmost  eight  qu  bits  have  00101001  qc  qr  qc  qr  qc  qr  second  eight  qu  bits  have  superposition  of  00111000  00111011  these  differ  only  on  the  rightmost  two  bits  qc  qr  create  superposition  on  qc  cx  qr  qr  spread  it  to  with  cnot  qc  qr  11  qc  qr  12  qc  qr  13  measure  for  in  range  16  qc  measure  qr  cr  print  qc  name  writefile  Qconfig  py  APItoken  xxc0c666b4933dcdaea3d00aaa265feee436d3e7595eed67931df07eb0dfa301e094d4b5aafbe6eb710d832007a6f40f49e80237a4ba533e94f0fe8e946ff07a  config  url  https  quantumexperience  ng  bluemix  net  api  If  you  have  access  to  IBM  features  you  also  need  to  fill  the  hub  group  and  project ,ibm
 details  Replace  None  on  the  lines  below  with  your  details  from  Quantum  Experience  quoting  the  strings  for  example  hub  my  hub  You  will  also  need  to  update  the  url  above  pointing  it  to  your  custom  URL  for  IBM  hub  None  group  None  project  None  if  APItoken  not  in  locals  raise  Exception  Please  set  up  your  access  token  See  Qconfig  py  import  Qconfig  and  set  APIToken  and  API  url  try  import  sys  sys  path  append  go  to  parent  dir  import  Qconfig  qx  config  APItoken  Qconfig  APItoken  url  Qconfig  config  url  except  Exception  as  print  qx  config  APItoken  YOUR  TOKEN  HERE  url  https  quantumexperience  ng  bluemix  net  api  print  111set  api  from  IBMQuantumExperience  import  IBMQuantumExperience  api  IBMQuantumExperience  token  qx  config  APItoken  config  url  qx  config  url  token  xxc0c666b4933dcdaea3d00aaa265feee436d3e7595eed67931df07eb0dfa301e094d4b5aafbe6eb710d832007a6f40f49e80237a4ba533e94f0fe8e946ff07a  con  https  q,ibm
uantumexperience  ng  bluemix  net  api  api  IBMQuantumExperience  token  config  url  con  print  222prepare  remote  backends  from  qiskit  backends  import  discover  local  backends  discover  remote  backends  get  backend  instance  remote  backends  discover  remote  backends  api  we  have  to  call  this  to  connect  to  remote  backends  local  backends  discover  local  backends  print  Remote  Backends  print  remote  backends  print  Local  Backends  print  local  backends  backend  ibmqx  hpc  qasm  simulator  shots  sim  128  results  sim  qp  execute  smiley  writer  backend  backend  shots  shots  sim  wait  timeout  300  stats  sim  results  sim  get  counts  smiley  writer  plot  histogram  stats  sim  import  matplotlib  pyplot  as  plt  matplotlib  inline  plt  rc  font  family  monospace  def  plot  smiley  stats  shots  for  bitString  in  stats  char  chr  int  bitString  get  string  of  the  leftmost  bits  and  convert  to  an  ASCII  character  char  chr  int  bitString  16  do ,ibm
 the  same  for  string  of  rightmost  bits  and  add  it  to  the  previous  character  prob  stats  bitString  shots  fraction  of  shots  for  which  this  result  occurred  create  plot  with  all  characters  on  top  of  each  other  with  alpha  given  by  how  often  it  turned  up  in  the  output  plt  annotate  char  va  center  ha  center  color  prob  size  300  if  prob  05  list  prob  and  char  for  the  dominant  results  occurred  for  more  than  of  shots  print  str  prob  char  plt  axis  off  plt  show  plot  smiley  stats  sim  shots  sim  backend  ibmqx5  if  get  backend  instance  backend  status  available  is  True  shots  device  1000  results  device  qp  execute  smiley  writer  backend  backend  shots  shots  device  wait  timeout  300  stats  device  results  device  get  counts  smiley  writer  plot  smiley  stats  device  shots  device  print  Installed  packages  are  as  the  following  python  version  print  conda  list  qiskit  IBMQuantumExperience  numpy  scipy  mat,ibm
plotlib  ,ibm
Put  file  path  as  string  here  CIFAR  DIR  def  unpickle  file  import  pickle  with  open  file  rb  as  fo  cifar  dict  pickle  load  fo  encoding  bytes  return  cifar  dictdirs  batches  meta  data  batch  data  batch  data  batch  data  batch  data  batch  test  batch  all  data  for  direc  in  zip  all  data  dirs  all  data  unpickle  CIFAR  DIR  direc  batch  meta  all  data  data  batch1  all  data  data  batch2  all  data  data  batch3  all  data  data  batch4  all  data  data  batch5  all  data  test  batch  all  data  batch  metadata  batch1  keys  import  matplotlib  pyplot  as  plt  matplotlib  inline  import  numpy  as  np  Put  the  code  here  that  transforms  the  array  plt  imshow  plt  imshow  plt  imshow  def  one  hot  encode  vec  vals  10  For  use  to  one  hot  encode  the  10  possible  labels  len  vec  out  np  zeros  vals  out  range  vec  return  outclass  CifarHelper  def  init  self  self  Grabs  list  of  all  the  data  batches  for  training  self  all  train  batch,amazon
es  data  batch1  data  batch2  data  batch3  data  batch4  data  batch5  Grabs  list  of  all  the  test  batches  really  just  one  batch  self  test  batch  test  batch  Intialize  some  empty  variables  for  later  on  self  training  images  None  self  training  labels  None  self  test  images  None  self  test  labels  None  def  set  up  images  self  print  Setting  Up  Training  Images  and  Labels  Vertically  stacks  the  training  images  self  training  images  np  vstack  data  for  in  self  all  train  batches  train  len  len  self  training  images  Reshapes  and  normalizes  training  images  self  training  images  self  training  images  reshape  train  len  32  32  transpose  255  One  hot  Encodes  the  training  labels  self  training  labels  one  hot  encode  np  hstack  labels  for  in  self  all  train  batches  10  print  Setting  Up  Test  Images  and  Labels  Vertically  stacks  the  test  images  self  test  images  np  vstack  data  for  in  self  test  batch  test  len  l,amazon
en  self  test  images  Reshapes  and  normalizes  test  images  self  test  images  self  test  images  reshape  test  len  32  32  transpose  255  One  hot  Encodes  the  test  labels  self  test  labels  one  hot  encode  np  hstack  labels  for  in  self  test  batch  10  def  next  batch  self  batch  size  Note  that  the  100  dimension  in  the  reshape  call  is  set  by  an  assumed  batch  size  of  100  self  training  images  self  self  batch  size  reshape  100  32  32  self  training  labels  self  self  batch  size  self  self  batch  size  len  self  training  images  return  Before  Your  tf  Session  run  these  two  lines  ch  CifarHelper  ch  set  up  images  During  your  session  to  grab  the  next  batch  use  this  line  Just  like  we  did  for  mnist  train  next  batch  batch  ch  next  batch  100  64  ,amazon
bash  echo  apt  get  install  python  mpltoolkits  basemapimport  csv  import  urllib2  import  cStringIO  import  numpy  as  np  import  matplotlib  as  mpl  import  matplotlib  pyplot  as  plt  from  mpl  toolkits  basemap  import  Basemapclass  EarthQuake  def  init  self  row  Parse  earthquake  data  from  USGS  self  timestamp  row  self  lat  float  row  self  lon  float  row  try  self  magnitude  float  row  except  ValueError  self  magnitude  def  get  earthquake  data  url  Read  CSV  earthquake  data  from  USGS  response  urllib2  urlopen  url  csvio  cStringIO  StringIO  response  read  reader  csv  reader  csvio  header  next  reader  quakes  EarthQuake  row  for  row  in  reader  quakes  for  in  quakes  if  magnitude  return  quakes  quakes  get  earthquake  data  http  earthquake  usgs  gov  earthquakes  feed  v1  summary  all  week  csv  print  quakes  dict  Set  up  Basemap  mpl  rcParams  figure  figsize  16  12  Basemap  projection  kav7  lon  90  resolution  area  thresh  1000  drawco,google
astlines  drawcountries  drawmapboundary  fill  color  drawparallels  np  arange  90  99  30  junk  drawmeridians  np  arange  180  180  60  control  marker  color  and  size  based  on  magnitude  def  get  marker  magnitude  markersize  magnitude  if  magnitude  return  bo  markersize  if  magnitude  return  go  markersize  elif  magnitude  return  yo  markersize  else  return  ro  markersize  sort  earthquakes  by  magnitude  so  that  weaker  earthquakes  are  plotted  after  on  top  of  stronger  ones  the  stronger  quakes  have  bigger  circles  so  we  ll  see  both  quakes  sort  key  lambda  magnitude  reverse  True  add  earthquake  info  to  the  plot  for  in  quakes  lon  lat  mcolor  msize  get  marker  magnitude  plot  mcolor  markersize  msize  add  title  plt  title  Earthquakes  to  format  quakes  timestamp  10  quakes  timestamp  10  ,google
bucket  your  s3  bucket  name  here  prefix  sagemaker  DEMO  xgboost  dm  Define  IAM  role  import  boto3  import  re  from  sagemaker  import  get  execution  role  role  get  execution  role  import  numpy  as  np  For  matrix  operations  and  numerical  processing  import  pandas  as  pd  For  munging  tabular  data  import  matplotlib  pyplot  as  plt  For  charts  and  visualizations  from  IPython  display  import  Image  For  displaying  images  in  the  notebook  from  IPython  display  import  display  For  displaying  outputs  in  the  notebook  from  time  import  gmtime  strftime  For  labeling  SageMaker  models  endpoints  etc  import  sys  For  writing  outputs  to  notebook  import  math  For  ceiling  function  import  json  For  parsing  hosting  outputs  import  os  For  manipulating  filepath  names  import  sagemaker  Amazon  SageMaker  Python  SDK  provides  many  helper  functions  from  sagemaker  predictor  import  csv  serializer  Converts  strings  for  HTTP  POST  requests  on ,amazon
 inference  wget  https  archive  ics  uci  edu  ml  machine  learning  databases  00222  bank  additional  zip  unzip  bank  additional  zipdata  pd  read  csv  bank  additional  bank  additional  full  csv  sep  pd  set  option  display  max  columns  500  Make  sure  we  can  see  all  of  the  columns  pd  set  option  display  max  rows  20  Keep  the  output  on  one  page  data  Frequency  tables  for  each  categorical  feature  for  column  in  data  select  dtypes  include  object  columns  display  pd  crosstab  index  data  column  columns  observations  normalize  columns  Histograms  for  each  numeric  features  display  data  describe  matplotlib  inline  hist  data  hist  bins  30  sharey  True  figsize  10  10  for  column  in  data  select  dtypes  include  object  columns  if  column  display  pd  crosstab  index  data  column  columns  data  normalize  columns  for  column  in  data  select  dtypes  exclude  object  columns  print  column  hist  data  column  hist  by  bins  30  plt  show,amazon
  display  data  corr  pd  plotting  scatter  matrix  data  figsize  12  12  plt  show  data  no  previous  contact  np  where  data  pdays  999  Indicator  variable  to  capture  when  pdays  takes  value  of  999  data  not  working  np  where  np  in1d  data  job  student  retired  unemployed  Indicator  for  individuals  not  actively  employed  model  data  pd  get  dummies  data  Convert  categorical  variables  to  sets  of  indicatorsmodel  data  model  data  drop  duration  emp  var  rate  cons  price  idx  cons  conf  idx  euribor3m  nr  employed  axis  train  data  validation  data  test  data  np  split  model  data  sample  frac  random  state  1729  int  len  model  data  int  len  model  data  Randomly  sort  the  data  then  split  out  first  70  second  20  and  last  10  pd  concat  train  data  yes  train  data  drop  no  yes  axis  axis  to  csv  train  csv  index  False  header  False  pd  concat  validation  data  yes  validation  data  drop  no  yes  axis  axis  to  csv  validation  cs,amazon
v  index  False  header  False  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  train  train  csv  upload  file  train  csv  boto3  Session  resource  s3  Bucket  bucket  Object  os  path  join  prefix  validation  validation  csv  upload  file  validation  csv  containers  us  west  433757028032  dkr  ecr  us  west  amazonaws  com  xgboost  latest  us  east  811284229777  dkr  ecr  us  east  amazonaws  com  xgboost  latest  us  east  825641698319  dkr  ecr  us  east  amazonaws  com  xgboost  latest  eu  west  685385470294  dkr  ecr  eu  west  amazonaws  com  xgboost  latest  s3  input  train  sagemaker  s3  input  s3  data  s3  train  format  bucket  prefix  content  type  csv  s3  input  validation  sagemaker  s3  input  s3  data  s3  validation  format  bucket  prefix  content  type  csv  sess  sagemaker  Session  xgb  sagemaker  estimator  Estimator  containers  boto3  Session  region  name  role  train  instance  count  train  instance  type  ml  m4  xlarge  output  path  s3,amazon
  output  format  bucket  prefix  sagemaker  session  sess  xgb  set  hyperparameters  max  depth  eta  gamma  min  child  weight  subsample  silent  objective  binary  logistic  num  round  100  xgb  fit  train  s3  input  train  validation  s3  input  validation  xgb  predictor  xgb  deploy  initial  instance  count  instance  type  ml  m4  xlarge  xgb  predictor  content  type  text  csv  xgb  predictor  serializer  csv  serializerdef  predict  data  rows  500  split  array  np  array  split  data  int  data  shape  float  rows  predictions  for  array  in  split  array  predictions  join  predictions  xgb  predictor  predict  array  decode  utf  return  np  fromstring  predictions  sep  predictions  predict  test  data  drop  no  yes  axis  as  matrix  pd  crosstab  index  test  data  yes  columns  np  round  predictions  rownames  actuals  colnames  predictions  sagemaker  Session  delete  endpoint  xgb  predictor  endpoint  ,amazon
Basic  set  up  to  define  IAM  Role  import  boto3  import  re  import  os  import  numpy  as  np  import  pandas  as  pd  from  sagemaker  import  get  execution  role  role  get  execution  role  Create  SageMaker  Session  that  will  be  used  to  perform  all  SageMaker  operations  import  sagemaker  as  sage  from  time  import  gmtime  strftime  sess  sage  Session  account  sess  boto  session  client  sts  get  caller  identity  Account  region  sess  boto  session  region  name  Note  make  sure  to  include  the  Docker  image  tag  eg  latest  since  there  seem  to  be  some  issues  with  deploying  model  if  you  don  include  the  tag  image  dkr  ecr  amazonaws  com  npng  sagemaker  repo  latest  format  account  region  sagemaker  ml  sage  estimator  Estimator  image  role  ml  c4  2xlarge  output  path  s3  output  format  sess  default  bucket  sagemaker  session  sess  AutoML  only  requires  training  data  during  the  training  process  Additional  data  can  be  ingested  later ,amazon
 for  prediction  Specify  location  of  the  data  with  dictionary  where  the  value  is  the  path  to  the  s3  bucket  containing  the  training  data  data  location  training  s3  h2o  sagemaker  npng  higgs  train  10k  csv  sagemaker  ml  fit  data  location  Deploying  an  actual  predictor  so  that  we  can  make  predictions  on  test  data  here  from  sagemaker  predictor  import  csv  serializer  predictor  sagemaker  ml  deploy  ml  m4  xlarge  serializer  csv  serializer  import  io  s3  boto3  client  s3  obj  s3  get  object  Bucket  h2o  sagemaker  npng  Key  higgs  test  5k  csv  df  pd  read  csv  io  BytesIO  obj  Body  read  np  array  df  columns  reshape  29  test  vals  df  values  valid  np  append  test  vals  axis  preds  predictor  predict  valid  decode  utf  preds  list  preds  split  full  preds  one  row  for  item  in  preds  list  if  in  item  rmloc  item  find  item  item  rmloc  one  row  append  item  full  preds  append  one  row  one  row  else  one  row  append  i,amazon
tem  full  preds  only  run  for  cleanup  deletes  the  endpoint  for  the  predictor  sess  delete  endpoint  predictor  endpoint  ,amazon
import  sagemaker  from  sagemaker  import  get  execution  role  import  boto3  import  json  sess  sagemaker  Session  role  get  execution  role  print  role  This  is  the  role  that  SageMaker  would  use  to  leverage  AWS  resources  S3  CloudWatch  on  your  behalf  bucket  sess  default  bucket  Replace  with  your  own  bucket  name  if  needed  print  bucket  prefix  SagemakerBlazingText  Replace  with  the  prefix  under  which  you  want  to  store  the  data  if  neededs3  train  data  s3  format  sagemaker  test  ninja  BlazingTextInput  s3  output  location  s3  BlazingText  Model  Output  word2vec  pitchfork  2018  09  19  format  sagemaker  test  ninja  region  name  boto3  Session  region  namecontainer  sagemaker  amazon  amazon  estimator  get  image  uri  region  name  blazingtext  latest  print  Using  SageMaker  BlazingText  container  format  container  region  name  bt  model  sagemaker  estimator  Estimator  container  role  train  instance  count  train  instance  type  ml  c4  2x,amazon
large  train  volume  size  train  max  run  360000  input  mode  File  output  path  s3  output  location  sagemaker  session  sess  bt  model  set  hyperparameters  mode  batch  skipgram  epochs  min  count  sampling  threshold  0001  learning  rate  05  window  size  vector  dim  100  negative  samples  batch  size  11  window  size  Preferred  Used  only  if  mode  is  batch  skipgram  evaluation  True  Perform  similarity  evaluation  on  WS  353  dataset  at  the  end  of  training  subwords  False  Subword  embedding  learning  is  not  supported  by  batch  skipgramtrain  data  sagemaker  session  s3  input  s3  train  data  distribution  FullyReplicated  content  type  text  plain  s3  data  type  S3Prefix  data  channels  train  train  data  bt  model  fit  inputs  data  channels  logs  True  bt  endpoint  bt  model  deploy  initial  instance  count  instance  type  ml  m4  xlarge  words  addictive  recorded  payload  instances  words  response  bt  endpoint  predict  json  dumps  payload  vecs  jso,amazon
n  loads  response  print  vecs  s3  boto3  resource  s3  key  bt  model  model  data  bt  model  model  data  find  s3  Bucket  sagemaker  test  ninja  download  file  key  model  tar  gz  tar  xvzf  model  tar  gz  cat  eval  jsonimport  numpy  as  np  from  sklearn  preprocessing  import  normalize  Read  the  400  most  frequent  word  vectors  The  vectors  in  the  file  are  in  descending  order  of  frequency  num  points  400  first  line  True  index  to  word  with  open  vectors  txt  as  for  line  num  line  in  enumerate  if  first  line  dim  int  line  strip  split  word  vecs  np  zeros  num  points  dim  dtype  float  first  line  False  continue  line  line  strip  word  line  split  vec  word  vecs  line  num  for  index  vec  val  in  enumerate  line  split  vec  index  float  vec  val  index  to  word  append  word  if  line  num  num  points  break  word  vecs  normalize  word  vecs  copy  False  return  norm  False  from  sklearn  manifold  import  TSNE  tsne  TSNE  perplexity  40  c,amazon
omponents  init  pca  iter  10000  two  embeddings  tsne  fit  transform  word  vecs  num  points  labels  index  to  word  num  points  from  matplotlib  import  pylab  matplotlib  inline  def  plot  embeddings  labels  pylab  figure  figsize  20  20  for  label  in  enumerate  labels  embeddings  pylab  scatter  pylab  annotate  label  xy  xytext  textcoords  offset  points  ha  right  va  bottom  pylab  show  plot  two  embeddings  labels  sess  delete  endpoint  bt  endpoint  endpoint  ,amazon
from  sagemaker  tensorflow  import  TensorFlow  from  sagemaker  import  get  execution  roleparams  train  filename  minimal  train  data  csv  test  filename  minimal  train  data  csv  shuffle  buffer  size  3000  batch  size  300  label  is  VideoAds  Show  hidden  units  10  10  10  learning  rate  001  role  get  execution  role  data  dir  tf  estimator  TensorFlow  entry  point  issue  tf  script  py  role  role  training  steps  200  hyperparameters  params  train  instance  count  train  instance  type  ml  c5  xlarge  tf  estimator  fit  data  dir  wait  True  ,amazon
